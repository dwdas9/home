---
layout: default
title: Hive Concepts
parent: spark-hive-hadoop
grand_parent: Docker
nav_order: 10
---

<details open markdown="block">
  <summary>
    Table of contents
  </summary>
  {: .text-delta }
1. TOC
{:toc}
</details>

# Where to Store Hive Warehouse Data in a Distributed System

When we are working with Hive in a distributed system, one important thing is **where the data** should be stored. By "data," we mean the actual table data that you create using Hive. This is different from the metadata, which is stored in the Hive Metastore.

Let’s discuss the different options for storing Hive's warehouse data in a distributed system and how it works when you have multiple services like **Spark**, **Hive**, and **Hadoop** running in separate containers or machines.

## Understanding the Problem

In a multi-container or multi-node system (like when you are running Spark, Hive, Hadoop, etc., on separate machines or containers), you need to make sure that **all nodes can access the same data**. If each node stores its data locally (e.g., `/opt/hive/data/warehouse` on each machine), the data won’t be shared across all the nodes, which creates problems. Each machine will have its own local data, and other machines won't be able to see or access it.

To avoid this, we need to store the data in a **shared location** that all nodes can access.

## 1. Use HDFS

The most common approach is to use **HDFS**. HDFS is designed to store data in a distributed way, so all nodes in the cluster can read and write data to the same HDFS location.

For example, you would set up Hive’s warehouse directory like this in the `hive-site.xml` file:

```xml
<property>
  <name>hive.metastore.warehouse.dir</name>
  <value>hdfs://namenode:8020/user/hive/warehouse</value>
  <description>This is the location where Hive stores table data in HDFS.</description>
</property>
```

Here, `hdfs://namenode:8020` is the HDFS path where the actual data is stored. This way, **all nodes** (Spark, Hive, Hadoop, etc.) can access the same data because they are all connected to HDFS.

### How it works in a distributed system:
- All your containers or nodes (e.g., Spark, Hive, Hadoop) are configured to use HDFS.
- When you create a table in Hive, the data is stored in the `hdfs://namenode:8020/user/hive/warehouse` folder.
- Whether you're using Spark to query the data or Hive to manage it, **everyone accesses the same location** in HDFS, which avoids data inconsistency.

### 2. Use Cloud Storage (Amazon S3, ADLS etc)

If you’re running Hive in a cloud environment, you can also use cloud storage services like **Amazon S3** or **ADLS**. These services act like distributed storage systems where all nodes can access the same data, just like HDFS.

For example, if you are using **Amazon S3**, you can set the warehouse directory in `hive-site.xml` like this:

```xml
<property>
  <name>hive.metastore.warehouse.dir</name>
  <value>s3a://my-bucket/hive/warehouse</value>
  <description>This is the location where Hive stores table data in Amazon S3.</description>
</property>
```

#### How it works in a distributed system:
- All your containers (Spark, Hive, Hadoop) are configured to access S3.
- When you create a table in Hive, the actual data is stored in the `s3a://my-bucket/hive/warehouse` directory.
- Whether you are using Spark, Hive, or any other service, everyone accesses the same location in S3.

This works well if you are running your system in the cloud, where distributed storage solutions like S3 or Azure Blob Storage are available.

### 3. Use NFS (Network File System) or Shared Filesystems

If you don’t want to use HDFS or cloud storage, you can set up a **Network File System (NFS)** or another type of shared filesystem that all nodes can access.

For example, you might set up an NFS mount like `/mnt/nfs/hive/warehouse`, which is shared across all your servers or containers. In this case, the `hive-site.xml` configuration would look like this:

```xml
<property>
  <name>hive.metastore.warehouse.dir</name>
  <value>/mnt/nfs/hive/warehouse</value>
  <description>This is the location where Hive stores table data in the shared NFS mount.</description>
</property>
```

### How it works in a distributed system:
- All your containers or servers have the same NFS mount point (e.g., `/mnt/nfs/hive/warehouse`).
- When you create a table in Hive, the actual data is stored in the shared directory.
- All nodes (Spark, Hive, Hadoop, etc.) can read and write data to the same location.

This works for on-premises setups where a shared network drive is used to store data.

### 4. Avoid Using Local File System

In a distributed system, **don’t use the local file system** (like `/opt/hive/data/warehouse`). If you store data on the local file system, each node will have its own copy of the data, and this can lead to problems because:

- Data will be spread across different machines, and there will be no consistency.
- One machine won’t know about the data on another machine, making it impossible to run distributed queries properly.

This option is only fine for **single-node setups** or local testing, where everything is running on one machine.