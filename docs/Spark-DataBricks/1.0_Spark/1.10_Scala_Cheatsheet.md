
## Scala Cheatsheet for Spark

| **Category**               | **Operation**                                | **Code Snippet**                                                                                       |
|----------------------------|----------------------------------------------|--------------------------------------------------------------------------------------------------------|
| **Basic Operations**       | **Variable Declaration**                     | `val x: Int = 10  // Immutable`<br>`var y: Int = 20  // Mutable`                                        |
|                            | **Collections**                              | `val list = List(1, 2, 3, 4, 5)`<br>`val array = Array(1, 2, 3, 4, 5)`<br>`val map = Map("a" -> 1, "b" -> 2, "c" -> 3)` |
| **Spark Setup**            | **Initialize Spark Session**                 | `import org.apache.spark.sql.SparkSession`<br>`val spark = SparkSession.builder.appName("Spark App").config("spark.master", "local").getOrCreate()` |
| **RDD Operations**         | **Create RDD**                               | `val rdd = spark.sparkContext.parallelize(Seq(1, 2, 3, 4, 5))`                                         |
|                            | **Transformations**                          | `val mappedRDD = rdd.map(_ * 2)`<br>`val filteredRDD = rdd.filter(_ > 2)`                              |
|                            | **Actions**                                  | `val collected = rdd.collect()`<br>`val count = rdd.count()`<br>`val firstElement = rdd.first()`       |
| **DataFrame Operations**   | **Create DataFrame**                         | `import spark.implicits._`<br>`val df = Seq((1, "a"), (2, "b"), (3, "c")).toDF("id", "value")`         |
|                            | **Show DataFrame**                           | `df.show()`                                                                                            |
|                            | **DataFrame Transformations**                | `val filteredDF = df.filter($"id" > 1)`<br>`val selectedDF = df.select("value")`<br>`val withColumnDF = df.withColumn("new_column", $"id" * 2)` |
|                            | **SQL Queries**                              | `df.createOrReplaceTempView("table")`<br>`val sqlDF = spark.sql("SELECT * FROM table WHERE id > 1")`   |
| **Dataset Operations**     | **Create Dataset**                           | `case class Record(id: Int, value: String)`<br>`val ds = Seq(Record(1, "a"), Record(2, "b"), Record(3, "c")).toDS()` |
|                            | **Dataset Transformations**                  | `val filteredDS = ds.filter(_.id > 1)`<br>`val mappedDS = ds.map(record => record.copy(value = record.value.toUpperCase))` |
| **Conversions**            | **RDD to DataFrame**                         | `val rddToDF = rdd.toDF("numbers")`                                                                    |
|                            | **DataFrame to RDD**                         | `val dfToRDD = df.rdd`                                                                                 |
|                            | **DataFrame to Dataset**                     | `val dfToDS = df.as[Record]`                                                                           |
|                            | **Dataset to DataFrame**                     | `val dsToDF = ds.toDF()`                                                                               |
| **Reading and Writing Data**| **Read CSV**                                | `val csvDF = spark.read.option("header", "true").csv("path/to/file.csv")`                              |
|                            | **Write CSV**                                | `df.write.option("header", "true").csv("path/to/save")`                                                |
|                            | **Read Parquet**                             | `val parquetDF = spark.read.parquet("path/to/file.parquet")`                                           |
|                            | **Write Parquet**                            | `df.write.parquet("path/to/save")`                                                                     |
| **Common Data Engineering Functions** | **GroupBy and Aggregations**           | `val groupedDF = df.groupBy("id").count()`<br>`val aggregatedDF = df.groupBy("id").agg(sum("value"))`  |
|                            | **Join Operations**                          | `val df1 = Seq((1, "a"), (2, "b")).toDF("id", "value1")`<br>`val df2 = Seq((1, "x"), (2, "y")).toDF("id", "value2")`<br>`val joinedDF = df1.join(df2, "id")` |
|                            | **Window Functions**                         | `import org.apache.spark.sql.expressions.Window`<br>`import org.apache.spark.sql.functions._`<br>`val windowSpec = Window.partitionBy("id").orderBy("value")`<br>`val windowedDF = df.withColumn("rank", rank().over(windowSpec))` |
|                            | **UDFs (User-Defined Functions)**            | `import org.apache.spark.sql.functions.udf`<br>`val addOne = udf((x: Int) => x + 1)`<br>`val dfWithUDF = df.withColumn("new_value", addOne($"id"))` |
