{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"Airflow/1.0.0_AirFlow_Concepts/","text":"What is Airflow? Airflow, short for Air(Bnb)(Work)flow, is an open-source Linux-based platform to build, schedule, and monitor workflows. It\u2019s mainly used for data-related (ETL) workflows. The main strength of Airflow is that using it you can create workflows as code . Remember: - Airflow is for Linux. It is meant for Linux ONLY. You might find some installation method to prove otherwise, but behind the scenes, there will always be a Linux kernel, Docker container, WSL, or something similar. And since most servers run on Linux, why bother trying to run workflows on Windows? - Built on Python: In Airflow, Python is everywhere. Airflow Core Components Web Server: Provides the primary web interface for interacting with Airflow. Scheduler: Responsible for scheduling tasks. Meta Database: Stores information about tasks, their status, and other metadata. Trigger: Initiates tasks based on predefined conditions. Executor: Determines how and where tasks will be executed but doesn\u2019t run the tasks itself. Queue: A list of tasks waiting to be executed. Worker: The process that actually performs the tasks. To get a quick understanding of Airflow's concepts, you might want to check out this video . Airflow Core Concepts DAG (Directed Acyclic Graph): This is the core of your workflow. A DAG is essentially a data pipeline and must be acyclic, meaning it cannot contain any loops. Operator: An operator represents a task. Airflow offers a wide variety of operators, including those for Python to execute Python code. Finding Operators: To explore available operators, visit the Astronomer Registry. Tasks/Task Instance: A task is a specific instance of an operator, representing the actual unit of work that gets executed. Workflow: The entire process defined by the DAG. Essentially, the \"DAG is the workflow.\" What Airflow is Not Not a Data Processing Framework: Airflow can't process large volumes of data by itself. Not a Real-Time Streaming Framework: For real-time streaming, tools like Kafka are more appropriate. Not a Data Storage System: Although Airflow uses databases for its operations, it is not meant for data storage. When Airflow Might Not Be the Best Solution: High-Frequency Scheduling: If you need to schedule tasks every second, Airflow may not be suitable. Large Data Processing: Airflow is not designed to process large datasets directly. If you need to handle terabytes of data, it\u2019s better to trigger a Spark job from Airflow and let Spark do the heavy lifting. Real-Time Data Processing: Airflow is not ideal for real-time data processing; Kafka would be a better option. Simple Workflows: For straightforward workflows, Airflow might be overkill. Alternatives like ADF, cron jobs, or Power Automate may be more appropriate. Different type of Airflow Setup Single-Node Architecture In a single-node setup, all components of Airflow run on one machine. This architecture is ideal for smaller workflows and getting started with Airflow. Multi-Node Airflow Architecture As your workflows grow, you might consider a multi-node architecture for better scalability and performance.","title":"Airflow Concepts"},{"location":"Airflow/1.0.0_AirFlow_Concepts/#what-is-airflow","text":"Airflow, short for Air(Bnb)(Work)flow, is an open-source Linux-based platform to build, schedule, and monitor workflows. It\u2019s mainly used for data-related (ETL) workflows. The main strength of Airflow is that using it you can create workflows as code . Remember: - Airflow is for Linux. It is meant for Linux ONLY. You might find some installation method to prove otherwise, but behind the scenes, there will always be a Linux kernel, Docker container, WSL, or something similar. And since most servers run on Linux, why bother trying to run workflows on Windows? - Built on Python: In Airflow, Python is everywhere.","title":"What is Airflow?"},{"location":"Airflow/1.0.0_AirFlow_Concepts/#airflow-core-components","text":"Web Server: Provides the primary web interface for interacting with Airflow. Scheduler: Responsible for scheduling tasks. Meta Database: Stores information about tasks, their status, and other metadata. Trigger: Initiates tasks based on predefined conditions. Executor: Determines how and where tasks will be executed but doesn\u2019t run the tasks itself. Queue: A list of tasks waiting to be executed. Worker: The process that actually performs the tasks. To get a quick understanding of Airflow's concepts, you might want to check out this video .","title":"Airflow Core Components"},{"location":"Airflow/1.0.0_AirFlow_Concepts/#airflow-core-concepts","text":"DAG (Directed Acyclic Graph): This is the core of your workflow. A DAG is essentially a data pipeline and must be acyclic, meaning it cannot contain any loops. Operator: An operator represents a task. Airflow offers a wide variety of operators, including those for Python to execute Python code. Finding Operators: To explore available operators, visit the Astronomer Registry. Tasks/Task Instance: A task is a specific instance of an operator, representing the actual unit of work that gets executed. Workflow: The entire process defined by the DAG. Essentially, the \"DAG is the workflow.\"","title":"Airflow Core Concepts"},{"location":"Airflow/1.0.0_AirFlow_Concepts/#what-airflow-is-not","text":"Not a Data Processing Framework: Airflow can't process large volumes of data by itself. Not a Real-Time Streaming Framework: For real-time streaming, tools like Kafka are more appropriate. Not a Data Storage System: Although Airflow uses databases for its operations, it is not meant for data storage.","title":"What Airflow is Not"},{"location":"Airflow/1.0.0_AirFlow_Concepts/#when-airflow-might-not-be-the-best-solution","text":"High-Frequency Scheduling: If you need to schedule tasks every second, Airflow may not be suitable. Large Data Processing: Airflow is not designed to process large datasets directly. If you need to handle terabytes of data, it\u2019s better to trigger a Spark job from Airflow and let Spark do the heavy lifting. Real-Time Data Processing: Airflow is not ideal for real-time data processing; Kafka would be a better option. Simple Workflows: For straightforward workflows, Airflow might be overkill. Alternatives like ADF, cron jobs, or Power Automate may be more appropriate.","title":"When Airflow Might Not Be the Best Solution:"},{"location":"Airflow/1.0.0_AirFlow_Concepts/#different-type-of-airflow-setup","text":"","title":"Different type of Airflow Setup"},{"location":"Airflow/1.0.0_AirFlow_Concepts/#single-node-architecture","text":"In a single-node setup, all components of Airflow run on one machine. This architecture is ideal for smaller workflows and getting started with Airflow.","title":"Single-Node Architecture"},{"location":"Airflow/1.0.0_AirFlow_Concepts/#multi-node-airflow-architecture","text":"As your workflows grow, you might consider a multi-node architecture for better scalability and performance.","title":"Multi-Node Airflow Architecture"},{"location":"Airflow/1.0.1_A_Dags_Anatomy/","text":"A DAG's Anatomy A DAG in Airflow is basically a Python file (.py file) that lives inside the dags folder. It has to be on the web server, and usually, this folder is mapped to the /opt/airflow/dags folder across all servers. So, how do you create a DAG? First you create a blabla.py python file inside the /dags folder. Then, you need to import the DAG object. This is how Airflow knows that your Python file is a DAG: Then, you define the DAG itself using a unique name: Unique DAG ID: This name has to be super unique across your entire Airflow setup. Start Date: You need to tell Airflow when to start running this DAG. Schedule Interval: This is where you define how often the DAG should run, usually with a cron expression. catchup=False This prevents Airflow from trying to catch up with all the past runs, which can save you from a lot of unnecessary DAG runs and give you more manual control. Finally, you would add your tasks under this DAG. To keep it simple we can just add None . So, total code would look like: from airflow import DAG from datetime import datetime with DAG('donald_kim', start_date=datetime(2022,1,1), schedule_interval='@daily', catchup=False) as dag: None We've created our base let's create a task Now, we will create a task to create a table in Postgress. Firs step would be to import hte postgres operator: from airflow.providers.postgres.operators.postgres import PostgresOperator Then we create a table. Inside it we create a task_id. It has to be unique. create_table = PostgresOperator(task_id='create_table', postgres_conn_id='postgres', sql=''' CREATE TABLE IF NOT EXISTS users(firstname text NOT NULL)''')","title":"A dags anatomy"},{"location":"Airflow/1.0.1_A_Dags_Anatomy/#a-dags-anatomy","text":"A DAG in Airflow is basically a Python file (.py file) that lives inside the dags folder. It has to be on the web server, and usually, this folder is mapped to the /opt/airflow/dags folder across all servers.","title":"A DAG's Anatomy"},{"location":"Airflow/1.0.1_A_Dags_Anatomy/#so-how-do-you-create-a-dag","text":"First you create a blabla.py python file inside the /dags folder. Then, you need to import the DAG object. This is how Airflow knows that your Python file is a DAG: Then, you define the DAG itself using a unique name: Unique DAG ID: This name has to be super unique across your entire Airflow setup. Start Date: You need to tell Airflow when to start running this DAG. Schedule Interval: This is where you define how often the DAG should run, usually with a cron expression. catchup=False This prevents Airflow from trying to catch up with all the past runs, which can save you from a lot of unnecessary DAG runs and give you more manual control. Finally, you would add your tasks under this DAG. To keep it simple we can just add None . So, total code would look like: from airflow import DAG from datetime import datetime with DAG('donald_kim', start_date=datetime(2022,1,1), schedule_interval='@daily', catchup=False) as dag: None","title":"So, how do you create a DAG?"},{"location":"Airflow/1.0.1_A_Dags_Anatomy/#weve-created-our-base-lets-create-a-task","text":"Now, we will create a task to create a table in Postgress. Firs step would be to import hte postgres operator: from airflow.providers.postgres.operators.postgres import PostgresOperator Then we create a table. Inside it we create a task_id. It has to be unique. create_table = PostgresOperator(task_id='create_table', postgres_conn_id='postgres', sql=''' CREATE TABLE IF NOT EXISTS users(firstname text NOT NULL)''')","title":"We've created our base let's create a task"},{"location":"Airflow/1.0.2_Hello_Airflow/","text":"Setup Airflow on Docker and create a simple dag Steps 1. Create the Airflow Container Download Airflow Docker Image Create a Docker Volume Initialize Airflow Database Start the Airflow Webserver 2. Connect to Your Container from VS Code 3. Create our first dag - a hello_world.py Script 4. Start the Airflow Scheduler Setup Airflow on Docker and create a simple dag This article serves as your first step into the Airflow fold. Here, I'll walk you through creating a standalone Airflow container with all necessary components, and then guide you in setting up your first DAG. Our envionrment will contain these components: A Windows laptop Docker Desktop installed Visual Studio Code VS Code Remote Development pack - This lets you connect to the container and develop code locally. Steps 1. Create the Airflow Container Download Airflow Docker Image Run the following command in your command prompt or power shell to pull the latest Airflow Docker image: docker pull apache/airflow:latest Create a Docker Volume Execute this command to create a Docker volume named airflow-volume for data persistence: docker volume create airflow-volume Initialize Airflow Database Initialize the Airflow database using the following two commands: docker run --rm --network dasnet -v airflow-volume:/opt/airflow apache/airflow:latest db init docker run --rm --network dasnet -v airflow-volume:/opt/airflow apache/airflow:latest users create --username airflow --firstname FIRST_NAME --lastname LAST_NAME --role Admin --email admin@example.com --password airflow Note: I use a network dasnet. Hence --network part. You can totally remove the --network. Start the Airflow Webserver To start the Airflow webserver, use this command: docker run -d --name airflow --network dasnet -p 8080:8080 -e AIRFLOW_UID=50000 -v airflow-volume:/opt/airflow apache/airflow:latest webserver Note: I use a network dasnet , which is why I added the --network part. You can skip the --network if you don\u2019t need it. Also, 8080 is a very common port. If it clashes with any other apps on your laptop, you can change it to something like 8084:8084 or any other random number that might not cause a conflict. 2. Connect to Your Container from VS Code After creating the container, connect to it using Visual Studio Code: Open VS Code and click on the \"Remote Explorer\" icon on the left sidebar. Click on the Remote Development icon (usually in the bottom left corner). Select the Airflow container from the list to connect. 3. Create our first dag - a hello_world.py Script Now, let\u2019s create your first DAG inside the container: Note: For this example we are using a single-node Airflow container. As there is only one machine(container). The dags folder will be inside the /opt/airflow/dags inside the conatienr. Also, to make it simple we haven't mounted it anywhere on the local system. In VS Code, go to File > New File . Select Python as the file type. Go to the /opt/airflow/dags directory. Name the file hello_world.py . Paste the following code into the file and save it: ```python # Contact: das.d@hotmail.com # Import necessary modules from Airflow and Python from airflow import DAG # All DAGs(.py files) MUST have this import. from airflow.operators.python import PythonOperator # Used to define tasks that run Python functions from datetime import datetime # This is always imported as you need to schedule the dag and it needs date and time # Define a simple Python function that prints 'Hello World' # This function will be the task executed by the DAG def print_hello(): print('Hello World') #This block, the DAG instance dag, is what makes this code an actual DAG. with DAG( 'hello_world', # Unique identifier for the DAG; should be descriptive start_date=datetime(2023, 1, 1), # The date when the DAG will start running; set to a past date for immediate start schedule_interval='@daily' # How often the DAG should run; '@daily' means it runs once a day ) as dag: # Define a task using PythonOperator # Any task defined inside the 'with' becomes part of the dag t1 = PythonOperator( task_id='print_hello', # 'task_id' is a unique identifier for the task within the DAG python_callable=print_hello # 'python_callable' is the function that the task will execute. The function to be executed when this task runs ) # The DAG and task are now defined # Airflow will take care of scheduling and running the task based on the DAG's schedule ## <span style=\"color: #00574F; font-family: Segoe UI, sans-serif;\">4. **Start the Airflow Scheduler**</span> To run your DAG, you need to start the Airflow scheduler: - Open the terminal in VS Code (which is already connected to the container). - Run the following command: ```bash airflow scheduler The scheduler will start, and it will pick up your hello_world DAG to run it according to the schedule you've set (daily in this case).","title":"Hello-Airflow"},{"location":"Airflow/1.0.2_Hello_Airflow/#setup-airflow-on-docker-and-create-a-simple-dag","text":"This article serves as your first step into the Airflow fold. Here, I'll walk you through creating a standalone Airflow container with all necessary components, and then guide you in setting up your first DAG. Our envionrment will contain these components: A Windows laptop Docker Desktop installed Visual Studio Code VS Code Remote Development pack - This lets you connect to the container and develop code locally.","title":"Setup Airflow on Docker and create a simple dag"},{"location":"Airflow/1.0.2_Hello_Airflow/#steps","text":"","title":"Steps"},{"location":"Airflow/1.0.2_Hello_Airflow/#1-create-the-airflow-container","text":"","title":"1. Create the Airflow Container"},{"location":"Airflow/1.0.2_Hello_Airflow/#download-airflow-docker-image","text":"Run the following command in your command prompt or power shell to pull the latest Airflow Docker image: docker pull apache/airflow:latest","title":"Download Airflow Docker Image"},{"location":"Airflow/1.0.2_Hello_Airflow/#create-a-docker-volume","text":"Execute this command to create a Docker volume named airflow-volume for data persistence: docker volume create airflow-volume","title":"Create a Docker Volume"},{"location":"Airflow/1.0.2_Hello_Airflow/#initialize-airflow-database","text":"Initialize the Airflow database using the following two commands: docker run --rm --network dasnet -v airflow-volume:/opt/airflow apache/airflow:latest db init docker run --rm --network dasnet -v airflow-volume:/opt/airflow apache/airflow:latest users create --username airflow --firstname FIRST_NAME --lastname LAST_NAME --role Admin --email admin@example.com --password airflow Note: I use a network dasnet. Hence --network part. You can totally remove the --network.","title":"Initialize Airflow Database"},{"location":"Airflow/1.0.2_Hello_Airflow/#start-the-airflow-webserver","text":"To start the Airflow webserver, use this command: docker run -d --name airflow --network dasnet -p 8080:8080 -e AIRFLOW_UID=50000 -v airflow-volume:/opt/airflow apache/airflow:latest webserver Note: I use a network dasnet , which is why I added the --network part. You can skip the --network if you don\u2019t need it. Also, 8080 is a very common port. If it clashes with any other apps on your laptop, you can change it to something like 8084:8084 or any other random number that might not cause a conflict.","title":"Start the Airflow Webserver"},{"location":"Airflow/1.0.2_Hello_Airflow/#2-connect-to-your-container-from-vs-code","text":"After creating the container, connect to it using Visual Studio Code: Open VS Code and click on the \"Remote Explorer\" icon on the left sidebar. Click on the Remote Development icon (usually in the bottom left corner). Select the Airflow container from the list to connect.","title":"2. Connect to Your Container from VS Code"},{"location":"Airflow/1.0.2_Hello_Airflow/#3-create-our-first-dag-a-hello_worldpy-script","text":"Now, let\u2019s create your first DAG inside the container: Note: For this example we are using a single-node Airflow container. As there is only one machine(container). The dags folder will be inside the /opt/airflow/dags inside the conatienr. Also, to make it simple we haven't mounted it anywhere on the local system. In VS Code, go to File > New File . Select Python as the file type. Go to the /opt/airflow/dags directory. Name the file hello_world.py . Paste the following code into the file and save it: ```python # Contact: das.d@hotmail.com # Import necessary modules from Airflow and Python from airflow import DAG # All DAGs(.py files) MUST have this import. from airflow.operators.python import PythonOperator # Used to define tasks that run Python functions from datetime import datetime # This is always imported as you need to schedule the dag and it needs date and time # Define a simple Python function that prints 'Hello World' # This function will be the task executed by the DAG def print_hello(): print('Hello World') #This block, the DAG instance dag, is what makes this code an actual DAG. with DAG( 'hello_world', # Unique identifier for the DAG; should be descriptive start_date=datetime(2023, 1, 1), # The date when the DAG will start running; set to a past date for immediate start schedule_interval='@daily' # How often the DAG should run; '@daily' means it runs once a day ) as dag: # Define a task using PythonOperator # Any task defined inside the 'with' becomes part of the dag t1 = PythonOperator( task_id='print_hello', # 'task_id' is a unique identifier for the task within the DAG python_callable=print_hello # 'python_callable' is the function that the task will execute. The function to be executed when this task runs ) # The DAG and task are now defined # Airflow will take care of scheduling and running the task based on the DAG's schedule ## <span style=\"color: #00574F; font-family: Segoe UI, sans-serif;\">4. **Start the Airflow Scheduler**</span> To run your DAG, you need to start the Airflow scheduler: - Open the terminal in VS Code (which is already connected to the container). - Run the following command: ```bash airflow scheduler The scheduler will start, and it will pick up your hello_world DAG to run it according to the schedule you've set (daily in this case).","title":"3. Create our first dag - a hello_world.py Script"},{"location":"Airflow/1.0.3_Airflow_Dbt_Docker/","text":"Project Link","title":"Project Airflow dbt"},{"location":"DE-Projects/Csv-To-MSSQL/","text":"Build an MSSQL table from CSV schema, split large CSVs, and populate rows using Pandas. Part 1: Creating an MSSQL Table from a CSV Schema Part 2: Splitting the Large CSV into Smaller Chunks Part 3: Importing Data from Split Files with Error Handling Conclusion Build an MSSQL table from CSV schema, split large CSVs, and populate rows using Pandas. How to use python to create a MSSQL table from the schema of a csv file. Split the large csv file into manageable smaller chunks and upload the data from these segments into the SQL server table. The code is divided into three main parts: Part 1: Creating an MSSQL Table from a CSV Schema Establish a connection to an MSSQL Server using specified connection details. Read a large CSV file into a pandas DataFrame. Define a mapping of pandas data types to SQL Server data types. Create a list of column definitions with data types based on DataFrame's columns. Generate an SQL statement to create a new table in the MSSQL database using the column definitions. Execute the SQL statement to create the table. # Import necessary libraries import pyodbc import pandas as pd # Establish a connection to the SQL Server (replace placeholders with your server details) conn = pyodbc.connect('DRIVER={Your_ODBC_Driver};SERVER=Your_Server;DATABASE=Your_Database;UID=Your_Username;PWD=Your_Password') #Example: conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=localhost;DATABASE=Nyctaxi;UID=sa;PWD=Passw_r123') # Specify the path to your large CSV file large_csv_file = 'path_to_your_large_csv_file.csv' # Read the CSV file into a pandas DataFrame df = pd.read_csv(large_csv_file) # Define a mapping of pandas data types to SQL Server data types data_type_mapping = { 'int64': 'BIGINT', 'float64': 'FLOAT', 'object': 'VARCHAR(MAX)', # Use VARCHAR(MAX) for string data # Add more mappings as needed for other data types } # Create a list of column definitions with data types column_definitions = [f'{col} {data_type_mapping[str(df[col].dtype)]}' for col in df.columns] # Create a SQL statement to create the table create_table_sql = f''' CREATE TABLE YourTableName ( {', '.join(column_definitions)} ) ''' # Execute the SQL statement to create the table cursor = conn.cursor() cursor.execute(create_table_sql) cursor.commit() # Close the database connection conn.close() Part 2: Splitting the Large CSV into Smaller Chunks Define the desired number of rows per chunk to manage data processing. Read the large CSV file in chunks of the specified size. Specify a directory to save the CSV chunks. Iterate through the chunks and save them as separate CSV files in the directory. # Define the desired number of rows per chunk rows_per_chunk = 10000 # Adjust this number as needed # Read the large CSV file in chunks chunk_size = rows_per_chunk chunks = pd.read_csv(large_csv_file, chunksize=chunk_size) # Directory to save the CSV chunks csv_chunks_directory = 'Path to the folder where split files will be placed' #Remember, for windows use C:/Users/rocky [forward slash] or C:\\\\Users\\\\rocky [double back slash] # Iterate through the chunks and save them as separate CSV files chunk_number = 0 for chunk in chunks: chunk_number += 1 chunk.to_csv(f'{csv_chunks_directory}/chunk_{chunk_number}.csv', index=False) print(f'Split into {chunk_number} chunks.') Part 3: Importing Data from Split Files with Error Handling Initialize a new database connection (separate from Part 1) as we have closed the previous connection. Define the directory where CSV chunks from Part 2 are located. Define a file to log rows with errors. Iterate through the CSV chunks in the directory. Read CSV data into a pandas DataFrame for each chunk. Prepare an SQL INSERT statement for the database table created in Part 1. Create a cursor for database operations. Iterate through the rows in the DataFrame and attempt to insert each row into the SQL Server table. Incorporate error handling to log errors during insertion and continue with the next row. Commit the transaction after processing each chunk of data. Close the error log file and the database connection. import os import pandas as pd import pyodbc # Initialize your database connection. Refer to part one for details. conn = pyodbc.connect(\"your_connection_string_here\") #Example, #Example: conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=localhost;DATABASE=Nyctaxi;UID=sa;PWD=Passw_r123') # Define the directory where CSV chunks are located csv_chunks_directory = \"your_csv_directory_here\" # Define a file to log rows with errors error_log_file = \"error_log.txt\" # Open the error log file in append mode with open(error_log_file, 'a') as error_log: # Iterate through the CSV chunks and load them into the SQL Server table for filename in os.listdir(csv_chunks_directory): if filename.endswith(\".csv\"): csv_file_path = os.path.join(csv_chunks_directory, filename) # Read the CSV chunk into a pandas DataFrame df = pd.read_csv(csv_file_path) # Prepare an SQL INSERT statement insert_sql = f\"INSERT INTO dbo.yellowtaxitrips ({', '.join(df.columns)}) VALUES ({', '.join(['?']*len(df.columns))})\" # Create a cursor cursor = conn.cursor() # Iterate through the rows and insert them into the SQL Server table for _, row in df.iterrows(): try: cursor.execute(insert_sql, tuple(row)) except pyodbc.Error as e: # Log the error and the problematic row to the error log file error_log.write(f\"Error: {e}\\n\") error_log.write(f\"Problematic Row: {row}\\n\") error_log.write(\"\\n\") # Add a separator for readability continue # Skip this row and continue with the next one # Commit the transaction conn.commit() # Close the error log file error_log.close() # Close the database connection conn.close() Conclusion The script is effective for analyzing large datasets and predicting column types. However, for large files, it's not recommended to use it for splitting and importing. Instead, SSIS would be a better choice. SQL Server Bulk Import is the fastest method for well-defined files. Although SSIS is notably fast, the SQL Server Import and Export Data Wizard (an SSIS tool) crashes with large csv files(1 GB). It's advisable to use the SSIS studio in Visual Studio with the mainstream version.","title":"Python - CSV To MSSQL"},{"location":"DE-Projects/Csv-To-MSSQL/#build-an-mssql-table-from-csv-schema-split-large-csvs-and-populate-rows-using-pandas","text":"How to use python to create a MSSQL table from the schema of a csv file. Split the large csv file into manageable smaller chunks and upload the data from these segments into the SQL server table. The code is divided into three main parts:","title":"Build an MSSQL table from CSV schema, split large CSVs, and populate rows using Pandas."},{"location":"DE-Projects/Csv-To-MSSQL/#part-1-creating-an-mssql-table-from-a-csv-schema","text":"Establish a connection to an MSSQL Server using specified connection details. Read a large CSV file into a pandas DataFrame. Define a mapping of pandas data types to SQL Server data types. Create a list of column definitions with data types based on DataFrame's columns. Generate an SQL statement to create a new table in the MSSQL database using the column definitions. Execute the SQL statement to create the table. # Import necessary libraries import pyodbc import pandas as pd # Establish a connection to the SQL Server (replace placeholders with your server details) conn = pyodbc.connect('DRIVER={Your_ODBC_Driver};SERVER=Your_Server;DATABASE=Your_Database;UID=Your_Username;PWD=Your_Password') #Example: conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=localhost;DATABASE=Nyctaxi;UID=sa;PWD=Passw_r123') # Specify the path to your large CSV file large_csv_file = 'path_to_your_large_csv_file.csv' # Read the CSV file into a pandas DataFrame df = pd.read_csv(large_csv_file) # Define a mapping of pandas data types to SQL Server data types data_type_mapping = { 'int64': 'BIGINT', 'float64': 'FLOAT', 'object': 'VARCHAR(MAX)', # Use VARCHAR(MAX) for string data # Add more mappings as needed for other data types } # Create a list of column definitions with data types column_definitions = [f'{col} {data_type_mapping[str(df[col].dtype)]}' for col in df.columns] # Create a SQL statement to create the table create_table_sql = f''' CREATE TABLE YourTableName ( {', '.join(column_definitions)} ) ''' # Execute the SQL statement to create the table cursor = conn.cursor() cursor.execute(create_table_sql) cursor.commit() # Close the database connection conn.close()","title":"Part 1: Creating an MSSQL Table from a CSV Schema"},{"location":"DE-Projects/Csv-To-MSSQL/#part-2-splitting-the-large-csv-into-smaller-chunks","text":"Define the desired number of rows per chunk to manage data processing. Read the large CSV file in chunks of the specified size. Specify a directory to save the CSV chunks. Iterate through the chunks and save them as separate CSV files in the directory. # Define the desired number of rows per chunk rows_per_chunk = 10000 # Adjust this number as needed # Read the large CSV file in chunks chunk_size = rows_per_chunk chunks = pd.read_csv(large_csv_file, chunksize=chunk_size) # Directory to save the CSV chunks csv_chunks_directory = 'Path to the folder where split files will be placed' #Remember, for windows use C:/Users/rocky [forward slash] or C:\\\\Users\\\\rocky [double back slash] # Iterate through the chunks and save them as separate CSV files chunk_number = 0 for chunk in chunks: chunk_number += 1 chunk.to_csv(f'{csv_chunks_directory}/chunk_{chunk_number}.csv', index=False) print(f'Split into {chunk_number} chunks.')","title":"Part 2: Splitting the Large CSV into Smaller Chunks"},{"location":"DE-Projects/Csv-To-MSSQL/#part-3-importing-data-from-split-files-with-error-handling","text":"Initialize a new database connection (separate from Part 1) as we have closed the previous connection. Define the directory where CSV chunks from Part 2 are located. Define a file to log rows with errors. Iterate through the CSV chunks in the directory. Read CSV data into a pandas DataFrame for each chunk. Prepare an SQL INSERT statement for the database table created in Part 1. Create a cursor for database operations. Iterate through the rows in the DataFrame and attempt to insert each row into the SQL Server table. Incorporate error handling to log errors during insertion and continue with the next row. Commit the transaction after processing each chunk of data. Close the error log file and the database connection. import os import pandas as pd import pyodbc # Initialize your database connection. Refer to part one for details. conn = pyodbc.connect(\"your_connection_string_here\") #Example, #Example: conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=localhost;DATABASE=Nyctaxi;UID=sa;PWD=Passw_r123') # Define the directory where CSV chunks are located csv_chunks_directory = \"your_csv_directory_here\" # Define a file to log rows with errors error_log_file = \"error_log.txt\" # Open the error log file in append mode with open(error_log_file, 'a') as error_log: # Iterate through the CSV chunks and load them into the SQL Server table for filename in os.listdir(csv_chunks_directory): if filename.endswith(\".csv\"): csv_file_path = os.path.join(csv_chunks_directory, filename) # Read the CSV chunk into a pandas DataFrame df = pd.read_csv(csv_file_path) # Prepare an SQL INSERT statement insert_sql = f\"INSERT INTO dbo.yellowtaxitrips ({', '.join(df.columns)}) VALUES ({', '.join(['?']*len(df.columns))})\" # Create a cursor cursor = conn.cursor() # Iterate through the rows and insert them into the SQL Server table for _, row in df.iterrows(): try: cursor.execute(insert_sql, tuple(row)) except pyodbc.Error as e: # Log the error and the problematic row to the error log file error_log.write(f\"Error: {e}\\n\") error_log.write(f\"Problematic Row: {row}\\n\") error_log.write(\"\\n\") # Add a separator for readability continue # Skip this row and continue with the next one # Commit the transaction conn.commit() # Close the error log file error_log.close() # Close the database connection conn.close()","title":"Part 3: Importing Data from Split Files with Error Handling"},{"location":"DE-Projects/Csv-To-MSSQL/#conclusion","text":"The script is effective for analyzing large datasets and predicting column types. However, for large files, it's not recommended to use it for splitting and importing. Instead, SSIS would be a better choice. SQL Server Bulk Import is the fastest method for well-defined files. Although SSIS is notably fast, the SQL Server Import and Export Data Wizard (an SSIS tool) crashes with large csv files(1 GB). It's advisable to use the SSIS studio in Visual Studio with the mainstream version.","title":"Conclusion"},{"location":"DE-Projects/CurrencyPredictor/","text":"My Approach 1. Data Collection Source Forex Data : I will use one of the free Forex APIs mentioned earlier (like Alpha Vantage, CurrencyLayer, or Free Forex API) to collect real-time and historical currency exchange rate data. Additional Data : I\u2019ll consider integrating other data sources like economic indicators (e.g., interest rates, inflation data), news feeds (using APIs like NewsAPI), and social sentiment (from Twitter API). 2. Data Processing Stream Data to Kafka : I\u2019ll set up a Kafka pipeline to stream live Forex data into my system, ensuring that the data is continuously updated. Data Storage : I\u2019ll use a database like PostgreSQL or a time-series database like InfluxDB to store historical data for analysis. 3. AI Model Development Feature Engineering : I\u2019ll extract useful features from the data, such as currency pair volatility, moving averages, or sentiment scores. Model Selection : I plan to use machine learning models like Random Forests , Gradient Boosting , or LSTM (Long Short-Term Memory) networks for predicting currency price trends. Training the Model : I\u2019ll use historical data to train my AI models to predict future currency movements, possibly using platforms like Azure Machine Learning or TensorFlow . Model Evaluation : I\u2019ll regularly evaluate the model\u2019s performance using metrics like accuracy, precision, and recall. 4. AI-Powered Suggestions Decision Logic : Based on the model\u2019s predictions, I\u2019ll develop logic that suggests which currency to buy. This could be as simple as recommending currencies predicted to appreciate or a more complex strategy considering multiple factors. User Input : I\u2019ll allow users to input preferences or constraints (e.g., risk tolerance, preferred currency pairs). 5. Dashboard Development Visualization : I\u2019ll use tools like Tableau , Power BI , or Grafana to create interactive visualizations of Forex trends, AI predictions, and suggested trades. Integration : I\u2019ll integrate the AI model\u2019s output into the dashboard to provide real-time trading recommendations. User Interface : I\u2019ll ensure the dashboard is user-friendly, displaying key metrics like predicted price changes, confidence levels, and suggested trades clearly. 6. Deployment Web Hosting : I\u2019ll host the dashboard on a cloud platform like Azure, AWS, or Google Cloud. Monitoring : I\u2019ll implement monitoring for both the data pipeline and AI models to ensure everything runs smoothly and the predictions remain accurate. 7. Continuous Improvement Feedback Loop : I\u2019ll collect user feedback and actual market outcomes to continuously improve the AI model. Model Retraining : I\u2019ll regularly retrain the model with new data to keep it up-to-date with the latest market conditions. In short : I\u2019ll be streaming real-time Forex data, processing it with AI to predict currency movements, and presenting these insights in a user-friendly dashboard that suggests which currencies to buy based on the model\u2019s predictions.","title":"CurrencyPredictor"},{"location":"DE-Projects/CurrencyPredictor/#my-approach","text":"","title":"My Approach"},{"location":"DE-Projects/CurrencyPredictor/#1-data-collection","text":"Source Forex Data : I will use one of the free Forex APIs mentioned earlier (like Alpha Vantage, CurrencyLayer, or Free Forex API) to collect real-time and historical currency exchange rate data. Additional Data : I\u2019ll consider integrating other data sources like economic indicators (e.g., interest rates, inflation data), news feeds (using APIs like NewsAPI), and social sentiment (from Twitter API).","title":"1. Data Collection"},{"location":"DE-Projects/CurrencyPredictor/#2-data-processing","text":"Stream Data to Kafka : I\u2019ll set up a Kafka pipeline to stream live Forex data into my system, ensuring that the data is continuously updated. Data Storage : I\u2019ll use a database like PostgreSQL or a time-series database like InfluxDB to store historical data for analysis.","title":"2. Data Processing"},{"location":"DE-Projects/CurrencyPredictor/#3-ai-model-development","text":"Feature Engineering : I\u2019ll extract useful features from the data, such as currency pair volatility, moving averages, or sentiment scores. Model Selection : I plan to use machine learning models like Random Forests , Gradient Boosting , or LSTM (Long Short-Term Memory) networks for predicting currency price trends. Training the Model : I\u2019ll use historical data to train my AI models to predict future currency movements, possibly using platforms like Azure Machine Learning or TensorFlow . Model Evaluation : I\u2019ll regularly evaluate the model\u2019s performance using metrics like accuracy, precision, and recall.","title":"3. AI Model Development"},{"location":"DE-Projects/CurrencyPredictor/#4-ai-powered-suggestions","text":"Decision Logic : Based on the model\u2019s predictions, I\u2019ll develop logic that suggests which currency to buy. This could be as simple as recommending currencies predicted to appreciate or a more complex strategy considering multiple factors. User Input : I\u2019ll allow users to input preferences or constraints (e.g., risk tolerance, preferred currency pairs).","title":"4. AI-Powered Suggestions"},{"location":"DE-Projects/CurrencyPredictor/#5-dashboard-development","text":"Visualization : I\u2019ll use tools like Tableau , Power BI , or Grafana to create interactive visualizations of Forex trends, AI predictions, and suggested trades. Integration : I\u2019ll integrate the AI model\u2019s output into the dashboard to provide real-time trading recommendations. User Interface : I\u2019ll ensure the dashboard is user-friendly, displaying key metrics like predicted price changes, confidence levels, and suggested trades clearly.","title":"5. Dashboard Development"},{"location":"DE-Projects/CurrencyPredictor/#6-deployment","text":"Web Hosting : I\u2019ll host the dashboard on a cloud platform like Azure, AWS, or Google Cloud. Monitoring : I\u2019ll implement monitoring for both the data pipeline and AI models to ensure everything runs smoothly and the predictions remain accurate.","title":"6. Deployment"},{"location":"DE-Projects/CurrencyPredictor/#7-continuous-improvement","text":"Feedback Loop : I\u2019ll collect user feedback and actual market outcomes to continuously improve the AI model. Model Retraining : I\u2019ll regularly retrain the model with new data to keep it up-to-date with the latest market conditions. In short : I\u2019ll be streaming real-time Forex data, processing it with AI to predict currency movements, and presenting these insights in a user-friendly dashboard that suggests which currencies to buy based on the model\u2019s predictions.","title":"7. Continuous Improvement"},{"location":"DE-Projects/Dbrk-E2E-AttritionProject/","text":"Problem Statement Age analysis - people leaving at what age Department analysis Marital Status Attrition by edution Attrition by envionrment Other kpi","title":"Problem Statement"},{"location":"DE-Projects/Dbrk-E2E-AttritionProject/#problem-statement","text":"Age analysis - people leaving at what age Department analysis Marital Status Attrition by edution Attrition by envionrment Other kpi","title":"Problem Statement"},{"location":"DE-Projects/Download-Haddop-Jars/","text":"Table of Contents How to Download JAR Files from Apache Maven Repository Steps to Download JAR Files Go to Apache Maven Repository Search for the JAR File Pick the Version You Want Download the JAR File How to Use the JARs How to Download JAR Files from Apache Maven Repository Need to download JAR files like hadoop-azure-x.x.x.jar for your project? Don't worry, I'll show you how to get them from Apache Maven Repository. It's easy and doesn't take much time. Steps to Download JAR Files Go to Apache Maven Repository Open your browser and visit mvnrepository.com . This website has a lot of JAR files. Search for the JAR File In the website, there will be a search box. Type the name of the JAR file you are looking for (like hadoop-azure ) and press enter. You will see a list of JAR files. Pick the Version You Want After you click on the JAR file name, you'll see different versions. Click on the version you need. Download the JAR File In the page for your chosen version, look for the \"Files\" section. There will be a link for a .jar file. Click on this link to start downloading the file. How to Use the JARs Once downloaded, you can store the JAR files in a location of your choice, for example, C:\\spark_jars\\ . You can reference these JARs in your code when needed. For instance, if you are trying to access Azure Data Lake Storage (ADLS) from a local Spark installation on a Windows machine, you can set up your Spark session like this: from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"ADLS Access\") \\ .config(\"spark.jars\", \"C:\\\\spark_jars\\\\hadoop-azure-3.3.3.jar;C:\\\\spark_jars\\\\hadoop-azure-datalake-3.3.3.jar;C:\\\\spark_jars\\\\hadoop-common-3.3.3.jar\") \\ .getOrCreate() \u00a9 D Das \ud83d\udce7 das.d@hotmail.com |","title":"Download JARs-Apache Maven Repo"},{"location":"DE-Projects/Download-Haddop-Jars/#table-of-contents","text":"How to Download JAR Files from Apache Maven Repository Steps to Download JAR Files Go to Apache Maven Repository Search for the JAR File Pick the Version You Want Download the JAR File How to Use the JARs","title":"Table of Contents"},{"location":"DE-Projects/Download-Haddop-Jars/#how-to-download-jar-files-from-apache-maven-repository","text":"Need to download JAR files like hadoop-azure-x.x.x.jar for your project? Don't worry, I'll show you how to get them from Apache Maven Repository. It's easy and doesn't take much time.","title":"How to Download JAR Files from Apache Maven Repository"},{"location":"DE-Projects/Download-Haddop-Jars/#steps-to-download-jar-files","text":"","title":"Steps to Download JAR Files"},{"location":"DE-Projects/Download-Haddop-Jars/#go-to-apache-maven-repository","text":"Open your browser and visit mvnrepository.com . This website has a lot of JAR files.","title":"Go to Apache Maven Repository"},{"location":"DE-Projects/Download-Haddop-Jars/#search-for-the-jar-file","text":"In the website, there will be a search box. Type the name of the JAR file you are looking for (like hadoop-azure ) and press enter. You will see a list of JAR files.","title":"Search for the JAR File"},{"location":"DE-Projects/Download-Haddop-Jars/#pick-the-version-you-want","text":"After you click on the JAR file name, you'll see different versions. Click on the version you need.","title":"Pick the Version You Want"},{"location":"DE-Projects/Download-Haddop-Jars/#download-the-jar-file","text":"In the page for your chosen version, look for the \"Files\" section. There will be a link for a .jar file. Click on this link to start downloading the file.","title":"Download the JAR File"},{"location":"DE-Projects/Download-Haddop-Jars/#how-to-use-the-jars","text":"Once downloaded, you can store the JAR files in a location of your choice, for example, C:\\spark_jars\\ . You can reference these JARs in your code when needed. For instance, if you are trying to access Azure Data Lake Storage (ADLS) from a local Spark installation on a Windows machine, you can set up your Spark session like this: from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"ADLS Access\") \\ .config(\"spark.jars\", \"C:\\\\spark_jars\\\\hadoop-azure-3.3.3.jar;C:\\\\spark_jars\\\\hadoop-azure-datalake-3.3.3.jar;C:\\\\spark_jars\\\\hadoop-common-3.3.3.jar\") \\ .getOrCreate() \u00a9 D Das \ud83d\udce7 das.d@hotmail.com |","title":"How to Use the JARs"},{"location":"DE-Projects/FetchJsonWriteParquet/","text":"Table of Contents Json Transformation Using Spark And Azure Prerequisites Script Breakdown Initialize Spark Session Fetch and Load JSON Data Define Paths and Partition Data Stop Spark Session Appendix Reason for Partitioning Register App and Assign Blob Contributor Role The complete script Conclusion Json Transformation Using Spark And Azure In this article, I'll guide you on how to retrieve JSON data from an online API using Spark and Hadoop tools. We'll learn to split the data and save it in a 'silver' level. Then, we'll take the data from 'silver', partition it again, and store it in a *'gold'* level as Parquet files. This process is in line with the lakehouse architecture standards too. Prerequisites You'll need the following: Azure Subscription : For using Azure services. Azure Data Lake Storage Gen2 : Two contaienrs, silver and gold inside a storage account. Python with Pyspark : To develop and test this code a simple pyspark environment will do. Hadoop ADLS Jars : You need to download the jars using tools like wget and store it and then refrence it in spark configuration. To know more refer to my article . Script Breakdown Initialize Spark Session The script starts by creating a spark session by passing the downloaded hadoop jars for ADLS. The authentication method used is OAuth with service princiapl. Note, the registered app should have storage blob contributor role at the storage account level(preferred) or individually for silver and bronze. See appendix below. # Author: Das, Purpose: Fetch and Partition Jsons from pyspark.sql import SparkSession import requests # Note: Chek your actual client ID, tenant ID, and client secret value. Your's will be diffferent storage_account_name = \"TheStorageAccountNameHoldingSilverAndGoldContainers\" regapp_client_id = \"Registered App's Client_ID\" regapp_directory_id = \"Registered App's Directory ID\" regapp_client_secret = \"Registered App's Client Secret Value\" # Initialize Spark session with Azure Data Lake Storage Gen2 configurations for OAuth Authentication using service princiapl spark = SparkSession.builder \\ .appName(\"ADLS Access\") \\ .config(\"spark.jars\", \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,\"\\ \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,\"\\ \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar\") \\ .config(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\") \\ .config(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") \\ .config(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", regapp_client_id) \\ .config(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", regapp_client_secret) \\ .config(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{regapp_directory_id}/oauth2/token\") \\ .getOrCreate() Fetch and Load JSON Data The next snippet fetches JSON data from a web API using the requests library and loads it into a DataFrame: import requests # URL of the JSON data source json_url = 'https://data.ct.gov/resource/5mzw-sjtu.json' response = requests.get(json_url) json_data = response.json() # Convert JSON to a DataFrame df = spark.createDataFrame(json_data) Define Paths and Partition Data It's crucial to define the storage paths for 'silver' and 'gold' layers. Here's how the script partitions data: # Define base paths for silver and gold storage layers silver_path = f\"abfss://{storage_account_name}.dfs.core.windows.net/silver\" gold_path = f\"abfss://{storage_account_name}.dfs.core.windows.net/gold\" # Example column to partition the data by partition_column = 'date_column' # Write the data to the silver layer, partitioned by 'partition_column' df.write.partitionBy(partition_column).mode(\"overwrite\").json(silver_path) # Read and rename the partition column for clarity df_silver = spark.read.json(silver_path).withColumnRenamed(partition_column, \"date_partition\") # Write the transformed data to the gold layer as Parquet df_silver.write.partitionBy(\"date_partition\").mode(\"overwrite\").parquet(gold_path) Stop Spark Session Once all operations are complete, terminate the Spark session: spark.stop() Appendix Reason for Partitioning Partitioning is crucial when dealing with large datasets. It helps in breaking the data into smaller, more manageable pieces, which can be processed faster and more efficiently. In this script, I have partitoned the data by date_column. However, you can partiton it further by other criteria. Register App and Assign Blob Contributor Role Follow these merged steps to register your app in Azure AD and give it access to the silver and gold containers: Go to Azure Portal \u2192 Azure Active Directory \u2192 App registrations , and create a new registration. Note your Application (client) ID and Directory (tenant) ID. Within the app, navigate to Certificates & secrets to generate a new client secret. Remember to save the client secret value securely. In your Azure Storage account, under Access Control (IAM) , add a role assignment. Select \"Storage Blob Data Contributor\" and assign it to your registered app using the Application (client) ID. The complete script You can use this script directly in a jupyter notebook or create a function with it. For higher loads the script can be run in spark cluster. from pyspark.sql import SparkSession import requests # Note: Chek your actual client ID, tenant ID, and client secret value. Your's will be diffferent storage_account_name = \"<input yours here>\" regapp_client_id = \"<input yours here>\" regapp_directory_id = \"<input yours here>\" regapp_client_secret = \"<input yours here>\" # Initialize Spark session with Azure Data Lake Storage Gen2 configurations for OAuth Authentication using service princiapl spark = SparkSession.builder \\ .appName(\"ADLS Access\") \\ .config(\"spark.jars\", \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,\"\\ \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,\"\\ \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar\") \\ .config(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\") \\ .config(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") \\ .config(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", regapp_client_id) \\ .config(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", regapp_client_secret) \\ .config(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{regapp_directory_id}/oauth2/token\") \\ .getOrCreate() # Azure Storage configuration silver_container_name = \"silver\" gold_container_name = \"gold\" # Download JSON data from the web json_url = 'https://data.ct.gov/resource/5mzw-sjtu.json' response = requests.get(json_url) json_data = response.json() # Assuming json_data is a list of dictionaries (records) # Create a DataFrame from the JSON data df = spark.createDataFrame(json_data) # Define the base path for silver and gold containers base_silver_path = f\"abfss://{silver_container_name}@{storage_account_name}.dfs.core.windows.net/\" base_gold_path = f\"abfss://{gold_container_name}@{storage_account_name}.dfs.core.windows.net/\" # Add the relative path and partition the data by the 'date_column' # Replace 'date_column' with the actual column name you want to partition on partition_column = 'daterecorded' # Example partition column. You can choose another column in your case. silver_path = f\"{base_silver_path}data/\" gold_path = f\"{base_gold_path}data/\" # Write JSON data to the silver container, partitioned by 'date_column' df.write.partitionBy(partition_column).mode(\"overwrite\").json(silver_path) # Read the JSON file back into a DataFrame, it will recognize partitions automatically df_silver = spark.read.json(f\"{silver_path}\").withColumnRenamed(\"daterecorded\", \"date_partition\") # Write the DataFrame to the gold container as a Parquet file, also partitioned df_silver.write.partitionBy(\"date_partition\").mode(\"overwrite\").parquet(gold_path) # Stop the Spark session spark.stop() Conclusion This is a versatile script that can be executed directly in a Jupyter notebook or encapsulated within a function for convenience. With minor configuration tweaks it can be run in a spark cluster to handle larger workloads. Note: with this script you might see warnings like *Warning: Ignoring non-Spark config property: fs.azure.account.oauth.provider.type.strgacweatherapp.dfs.core.windows.net*","title":"Json To Parquet Using Spark And Azure"},{"location":"DE-Projects/FetchJsonWriteParquet/#table-of-contents","text":"Json Transformation Using Spark And Azure Prerequisites Script Breakdown Initialize Spark Session Fetch and Load JSON Data Define Paths and Partition Data Stop Spark Session Appendix Reason for Partitioning Register App and Assign Blob Contributor Role The complete script Conclusion","title":"Table of Contents"},{"location":"DE-Projects/FetchJsonWriteParquet/#json-transformation-using-spark-and-azure","text":"In this article, I'll guide you on how to retrieve JSON data from an online API using Spark and Hadoop tools. We'll learn to split the data and save it in a 'silver' level. Then, we'll take the data from 'silver', partition it again, and store it in a *'gold'* level as Parquet files. This process is in line with the lakehouse architecture standards too.","title":"Json Transformation Using Spark And Azure"},{"location":"DE-Projects/FetchJsonWriteParquet/#prerequisites","text":"You'll need the following: Azure Subscription : For using Azure services. Azure Data Lake Storage Gen2 : Two contaienrs, silver and gold inside a storage account. Python with Pyspark : To develop and test this code a simple pyspark environment will do. Hadoop ADLS Jars : You need to download the jars using tools like wget and store it and then refrence it in spark configuration. To know more refer to my article .","title":"Prerequisites"},{"location":"DE-Projects/FetchJsonWriteParquet/#script-breakdown","text":"","title":"Script Breakdown"},{"location":"DE-Projects/FetchJsonWriteParquet/#initialize-spark-session","text":"The script starts by creating a spark session by passing the downloaded hadoop jars for ADLS. The authentication method used is OAuth with service princiapl. Note, the registered app should have storage blob contributor role at the storage account level(preferred) or individually for silver and bronze. See appendix below. # Author: Das, Purpose: Fetch and Partition Jsons from pyspark.sql import SparkSession import requests # Note: Chek your actual client ID, tenant ID, and client secret value. Your's will be diffferent storage_account_name = \"TheStorageAccountNameHoldingSilverAndGoldContainers\" regapp_client_id = \"Registered App's Client_ID\" regapp_directory_id = \"Registered App's Directory ID\" regapp_client_secret = \"Registered App's Client Secret Value\" # Initialize Spark session with Azure Data Lake Storage Gen2 configurations for OAuth Authentication using service princiapl spark = SparkSession.builder \\ .appName(\"ADLS Access\") \\ .config(\"spark.jars\", \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,\"\\ \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,\"\\ \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar\") \\ .config(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\") \\ .config(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") \\ .config(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", regapp_client_id) \\ .config(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", regapp_client_secret) \\ .config(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{regapp_directory_id}/oauth2/token\") \\ .getOrCreate()","title":"Initialize Spark Session"},{"location":"DE-Projects/FetchJsonWriteParquet/#fetch-and-load-json-data","text":"The next snippet fetches JSON data from a web API using the requests library and loads it into a DataFrame: import requests # URL of the JSON data source json_url = 'https://data.ct.gov/resource/5mzw-sjtu.json' response = requests.get(json_url) json_data = response.json() # Convert JSON to a DataFrame df = spark.createDataFrame(json_data)","title":"Fetch and Load JSON Data"},{"location":"DE-Projects/FetchJsonWriteParquet/#define-paths-and-partition-data","text":"It's crucial to define the storage paths for 'silver' and 'gold' layers. Here's how the script partitions data: # Define base paths for silver and gold storage layers silver_path = f\"abfss://{storage_account_name}.dfs.core.windows.net/silver\" gold_path = f\"abfss://{storage_account_name}.dfs.core.windows.net/gold\" # Example column to partition the data by partition_column = 'date_column' # Write the data to the silver layer, partitioned by 'partition_column' df.write.partitionBy(partition_column).mode(\"overwrite\").json(silver_path) # Read and rename the partition column for clarity df_silver = spark.read.json(silver_path).withColumnRenamed(partition_column, \"date_partition\") # Write the transformed data to the gold layer as Parquet df_silver.write.partitionBy(\"date_partition\").mode(\"overwrite\").parquet(gold_path)","title":"Define Paths and Partition Data"},{"location":"DE-Projects/FetchJsonWriteParquet/#stop-spark-session","text":"Once all operations are complete, terminate the Spark session: spark.stop()","title":"Stop Spark Session"},{"location":"DE-Projects/FetchJsonWriteParquet/#appendix","text":"","title":"Appendix"},{"location":"DE-Projects/FetchJsonWriteParquet/#reason-for-partitioning","text":"Partitioning is crucial when dealing with large datasets. It helps in breaking the data into smaller, more manageable pieces, which can be processed faster and more efficiently. In this script, I have partitoned the data by date_column. However, you can partiton it further by other criteria.","title":"Reason for Partitioning"},{"location":"DE-Projects/FetchJsonWriteParquet/#register-app-and-assign-blob-contributor-role","text":"Follow these merged steps to register your app in Azure AD and give it access to the silver and gold containers: Go to Azure Portal \u2192 Azure Active Directory \u2192 App registrations , and create a new registration. Note your Application (client) ID and Directory (tenant) ID. Within the app, navigate to Certificates & secrets to generate a new client secret. Remember to save the client secret value securely. In your Azure Storage account, under Access Control (IAM) , add a role assignment. Select \"Storage Blob Data Contributor\" and assign it to your registered app using the Application (client) ID.","title":"Register App and Assign Blob Contributor Role"},{"location":"DE-Projects/FetchJsonWriteParquet/#the-complete-script","text":"You can use this script directly in a jupyter notebook or create a function with it. For higher loads the script can be run in spark cluster. from pyspark.sql import SparkSession import requests # Note: Chek your actual client ID, tenant ID, and client secret value. Your's will be diffferent storage_account_name = \"<input yours here>\" regapp_client_id = \"<input yours here>\" regapp_directory_id = \"<input yours here>\" regapp_client_secret = \"<input yours here>\" # Initialize Spark session with Azure Data Lake Storage Gen2 configurations for OAuth Authentication using service princiapl spark = SparkSession.builder \\ .appName(\"ADLS Access\") \\ .config(\"spark.jars\", \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,\"\\ \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,\"\\ \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar\") \\ .config(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\") \\ .config(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") \\ .config(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", regapp_client_id) \\ .config(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", regapp_client_secret) \\ .config(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{regapp_directory_id}/oauth2/token\") \\ .getOrCreate() # Azure Storage configuration silver_container_name = \"silver\" gold_container_name = \"gold\" # Download JSON data from the web json_url = 'https://data.ct.gov/resource/5mzw-sjtu.json' response = requests.get(json_url) json_data = response.json() # Assuming json_data is a list of dictionaries (records) # Create a DataFrame from the JSON data df = spark.createDataFrame(json_data) # Define the base path for silver and gold containers base_silver_path = f\"abfss://{silver_container_name}@{storage_account_name}.dfs.core.windows.net/\" base_gold_path = f\"abfss://{gold_container_name}@{storage_account_name}.dfs.core.windows.net/\" # Add the relative path and partition the data by the 'date_column' # Replace 'date_column' with the actual column name you want to partition on partition_column = 'daterecorded' # Example partition column. You can choose another column in your case. silver_path = f\"{base_silver_path}data/\" gold_path = f\"{base_gold_path}data/\" # Write JSON data to the silver container, partitioned by 'date_column' df.write.partitionBy(partition_column).mode(\"overwrite\").json(silver_path) # Read the JSON file back into a DataFrame, it will recognize partitions automatically df_silver = spark.read.json(f\"{silver_path}\").withColumnRenamed(\"daterecorded\", \"date_partition\") # Write the DataFrame to the gold container as a Parquet file, also partitioned df_silver.write.partitionBy(\"date_partition\").mode(\"overwrite\").parquet(gold_path) # Stop the Spark session spark.stop()","title":"The complete script"},{"location":"DE-Projects/FetchJsonWriteParquet/#conclusion","text":"This is a versatile script that can be executed directly in a Jupyter notebook or encapsulated within a function for convenience. With minor configuration tweaks it can be run in a spark cluster to handle larger workloads. Note: with this script you might see warnings like *Warning: Ignoring non-Spark config property: fs.azure.account.oauth.provider.type.strgacweatherapp.dfs.core.windows.net*","title":"Conclusion"},{"location":"DE-Projects/InstallScala/","text":"Installing Scala on Windows Download and Install Scala Set Up Environment Variables Installing Scala on Windows Scala can be installed on Windows using Coursier tool. It's basically a command line tool which opens up when you click on the scala installer exe. Follow these steps to install scala on Windows. Download and Install Scala Download the .zip file and open it . Run the cs-x86_64-pc-win32.exe file. A Command Prompt window will open. When prompted, press Y to proceed. Set Up Environment Variables The installation might not automatically set `SCALA_HOME` and add the `\\bin` folder to your PATH. This setup is necessary for Scala to work properly. After the installation, you need to manually add SCALA_HOME and SCALA_HOME\\data\\bin to your system's PATH. Note: Scala's bin should be in the path. Else, you won't be able to run it from command prompt.","title":"Scala Install"},{"location":"DE-Projects/InstallScala/#installing-scala-on-windows","text":"Scala can be installed on Windows using Coursier tool. It's basically a command line tool which opens up when you click on the scala installer exe. Follow these steps to install scala on Windows.","title":"Installing Scala on Windows"},{"location":"DE-Projects/InstallScala/#download-and-install-scala","text":"Download the .zip file and open it . Run the cs-x86_64-pc-win32.exe file. A Command Prompt window will open. When prompted, press Y to proceed.","title":"Download and Install Scala"},{"location":"DE-Projects/InstallScala/#set-up-environment-variables","text":"The installation might not automatically set `SCALA_HOME` and add the `\\bin` folder to your PATH. This setup is necessary for Scala to work properly. After the installation, you need to manually add SCALA_HOME and SCALA_HOME\\data\\bin to your system's PATH. Note: Scala's bin should be in the path. Else, you won't be able to run it from command prompt.","title":"Set Up Environment Variables"},{"location":"DE-Projects/JsonFlatAzureSDK/","text":"Table of Contents Flatten JSON Files in Azure Blob Storage using Azure SDK for Python Background Prerequisites Create the script Explanation of key elements in the script Appendix The complete script Azure Python SDK(libs) ecosystem Convert the script into an Azure Function Additional samples Uploading Data to Azure Blob Storage Downloading Data from Azure Blob Storage Listing Blobs in a Container Querying Data (Example with CSV Data) Flatten JSON Files in Azure Blob Storage using Azure SDK for Python Background A ADLS container has many JSON files with nested structure. This article shows how to flatten those Json files for better handling later. Prerequisites A python-development ready VS Code environment Install the required Azure SDK(libs) to work with Azure storage. pip install azure-storage-blob A source container with the JSON files. A destination container where the flattened json will be kept. The script can either replace the original files with the processed ones or store them in a separate folder. Storage account keys. This code uses Account key method to authenticate. You can get the keys using these steps: Open the Storage Accounts containing the container. Go to Settings , Access keys . Copy and keep the Connection string from here. Create the script To create the Python code in Visual Studio Code follow these steps: Create a new file with the .py extension. Import the necessary libraries: import json from azure.storage.blob import BlobServiceClient import logging Define the flatten_json() function: def flatten_json(y, parent_key='', sep='_'): items = [] for k, v in y.items(): new_key = f\"{parent_key}{sep}{k}\" if parent_key else k if isinstance(v, dict): items.extend(flatten_json(v, new_key, sep=sep).items()) else: items.append((new_key, v)) return dict(items) Define the main() function: def main(): # Initialize Blob Service Client blob_service_client = BlobServiceClient.from_connection_string( \"DefaultEndpointsProtocol=https;AccountName=<Your_Storage_Account_Name>;AccountKey=<The_Storage_Act_Key>;EndpointSuffix=core.windows.net\") # Iterate over blobs in the \"source_container\" container container_client = blob_service_client.get_container_client(\"source_container\") for blob in container_client.list_blobs(): try: # Download the blob blob_client = blob_service_client.get_blob_client(container=\"dest_container\", blob=blob.name) data_str = blob_client.download_blob().readall().decode('utf-8') # Decode the blob data to a string data = json.loads(data_str) # Flatten the JSON data flattened_data = flatten_json(data) # Move blob to the \"silver\" container target_blob_client = blob_service_client.get_blob_client(container=\"dest_container\", blob=blob.name) target_blob_client.upload_blob(json.dumps(flattened_data), overwrite=True) # Delete the original blob (optional) # blob_client.delete_blob() except Exception as e: logging.error(f\"Error processing blob {blob.name}: {e}\") if __name__ == \"__main__\": main() Save the file. Press F5 to run the code. Explanation of key elements in the script Here is what the scirpt does. This will help you understand how Azure SDK for Blob Stroage works: Initializes a BlobServiceClient object using the from_connection_string() method. This object is used to interact with the Azure Blob Storage service. Gets a container client for the silver container using the get_container_client() method. This object is used to interact with the specified container. Iterates over all blobs in the container using the list_blobs() method. For each blob, the function does the following: Downloads the blob using the get_blob_client() and download_blob() methods. Decodes the blob data to a string using the decode() method. Parses the JSON data in the string using the json.loads() function. Flattens the JSON data using the flatten_json() function that we provided. Moves the blob to the silver container using the upload_blob() method. Deletes the original blob (optional). Appendix The complete script Here is the complete script in one piece: \"\"\" Author: Das LTS: Very easy to run this code. Just pip install azure-storage-blob. And run the code anywhere with python. Fully working code anywhere. Windows/Ubuntu. --- This script uses Azure SDK BlobServiceClient class to interact with Azure Blob Storage. It downloads blobs from a container, flattens their JSON content, and uploads them back to another container. The BlobServiceClient class is part of the Azure SDK for Python and is used to interact with Azure Blob Storage. It provides methods for getting clients for specific containers and blobs, downloading blobs, and uploading blobs. The from_connection_string method is used to create an instance of BlobServiceClient using a connection string. The download_blob method returns a stream of data that can be read by calling readall. The upload_blob method uploads data to a blob, overwriting it if it already exists. \"\"\" import json from azure.storage.blob import BlobServiceClient # BlobServiceClient is part of the Azure SDK for Python. It's used to interact with Azure Blob Storage. import logging # Function to flatten JSON objects def flatten_json(y, parent_key='', sep='_'): items = [] for k, v in y.items(): new_key = f\"{parent_key}{sep}{k}\" if parent_key else k if isinstance(v, dict): items.extend(flatten_json(v, new_key, sep=sep).items()) else: items.append((new_key, v)) return dict(items) # Main function def main(): # Initialize Blob Service Client using connection string. This is the main entry point for interacting with blobs in Azure Storage. blob_service_client = BlobServiceClient.from_connection_string(\"DefaultEndpointsProtocol=https;AccountName=<your-storage-account-Name>;AccountKey=<Your_Storage_Act_Con_String>;EndpointSuffix=core.windows.net\") # Get a client for the \"silver\" container in the Blob Service. This client provides operations to interact with a specific container. container_client = blob_service_client.get_container_client(\"silver\") # Iterate over blobs in the \"weather-http\" container for blob in container_client.list_blobs(): try: # Get a client for the current blob. This client provides operations to interact with a specific blob. blob_client = blob_service_client.get_blob_client(container=\"silver\", blob=blob.name) # Download the blob data and decode it from bytes to string. The download_blob method returns a stream of data. data_str = blob_client.download_blob().readall().decode('utf-8') try: data = json.loads(data_str) except json.JSONDecodeError: data = json.loads(blob_client.download_blob().readall().decode('utf-8')) # Flatten the JSON data flattened_data = flatten_json(data) # Get a client for the target blob in the \"silver\" container. This client will be used to upload data to this blob. Move blob to the \"silver\" container target_blob_client = blob_service_client.get_blob_client(container=\"silver\", blob=blob.name) # Upload the flattened JSON data to the target blob, overwriting it if it already exists. The upload_blob method uploads data to a blob. target_blob_client.upload_blob(json.dumps(flattened_data), overwrite=True) # Uncomment the following line to delete the original blob after moving it to the \"silver\" container #blob_client.delete_blob() # Delete the original blob after moving except Exception as e: logging.error(f\"Error processing blob {blob.name}: {e}\") if __name__ == \"__main__\": main() Azure Python SDK(libs) ecosystem Library Explanation \ud83d\udc0d Azure SDK For Python Superset of all python packages (libs) for Azure. Can't be installed with a single pip. \ud83d\udce6 Azure Storage SDKs Subset of Azure SDK. Multiple libraries. Hence, no single pip command. \ud83d\udca6 Azure Blob Storage SDK Subset of Azure Storage SDK. Single Library - pip install azure-storage-blob \ud83d\udee0\ufe0f BlobServiceClient Class Storage Account Level \ud83d\udcc1 Container Client Class Container Level \ud83d\udcc4 Blob Client Class Blob Level Convert the script into an Azure Function The logic from my script can be easily incoporated into an azure function. You can easily put the entire logic into the functions function_app.py . Refer to my other articles on how to work with Azure Functions. \ud83c\udf1f Conclusion : The Azure SDK for Python is a superset of libraries to work with Azure services. The Azure Blob Storage SDK for Python is a subset of the Azure SDK for working with Azure Blob Storage. The script in this article uses the Azure Blob Storage SDK for Python to flatten JSON files in an Azure Blob Storage container. The script first downloads the blob from the container, then flattens the JSON data, and finally uploads the flattened JSON data back to the container. Additional samples Uploading Data to Azure Blob Storage from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient # Connection string to your Azure Storage account connection_string = \"your_connection_string\" container_name = \"your_container_name\" blob_name = \"your_blob_name\" data = \"Sample data about your exes\" # Create a BlobServiceClient blob_service_client = BlobServiceClient.from_connection_string(connection_string) # Create a container if it doesn't exist container_client = blob_service_client.get_container_client(container_name) container_client.create_container() # Create a BlobClient blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name) # Upload data to the blob blob_client.upload_blob(data, overwrite=True) print(\"Data uploaded successfully\") Downloading Data from Azure Blob Storage from azure.storage.blob import BlobServiceClient # Connection string to your Azure Storage account connection_string = \"your_connection_string\" container_name = \"your_container_name\" blob_name = \"your_blob_name\" # Create a BlobServiceClient blob_service_client = BlobServiceClient.from_connection_string(connection_string) # Create a BlobClient blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name) # Download data from the blob blob_data = blob_client.download_blob().readall() print(\"Downloaded data:\", blob_data.decode()) Listing Blobs in a Container from azure.storage.blob import BlobServiceClient # Connection string to your Azure Storage account connection_string = \"your_connection_string\" container_name = \"your_container_name\" # Create a BlobServiceClient blob_service_client = BlobServiceClient.from_connection_string(connection_string) # Get a container client container_client = blob_service_client.get_container_client(container_name) # List blobs in the container blobs = container_client.list_blobs() for blob in blobs: print(\"Blob name:\", blob.name) Querying Data (Example with CSV Data) If your data is in a CSV format and stored in blobs, you can query it using Azure Synapse or Data Lake Analytics for more advanced queries. Here's a simple example using CSV data in blobs: import pandas as pd from azure.storage.blob import BlobServiceClient # Connection string to your Azure Storage account connection_string = \"your_connection_string\" container_name = \"your_container_name\" blob_name = \"your_blob_name.csv\" # Create a BlobServiceClient blob_service_client = BlobServiceClient.from_connection_string(connection_string) # Create a BlobClient blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name) # Download blob data to a stream stream = blob_client.download_blob().readall() # Read CSV data into a DataFrame df = pd.read_csv(pd.compat.BytesIO(stream)) print(\"Data from CSV blob:\\n\", df) # Example query: Find whereabouts of a specific ex whereabouts = df[df['Name'] == 'ExName']['Whereabouts'].iloc[0] print(\"Whereabouts of ExName:\", whereabouts) \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Flatten Json Using Azure SDK"},{"location":"DE-Projects/JsonFlatAzureSDK/#table-of-contents","text":"Flatten JSON Files in Azure Blob Storage using Azure SDK for Python Background Prerequisites Create the script Explanation of key elements in the script Appendix The complete script Azure Python SDK(libs) ecosystem Convert the script into an Azure Function Additional samples Uploading Data to Azure Blob Storage Downloading Data from Azure Blob Storage Listing Blobs in a Container Querying Data (Example with CSV Data)","title":"Table of Contents"},{"location":"DE-Projects/JsonFlatAzureSDK/#flatten-json-files-in-azure-blob-storage-using-azure-sdk-for-python","text":"","title":"Flatten JSON Files in Azure Blob Storage using Azure SDK for Python"},{"location":"DE-Projects/JsonFlatAzureSDK/#background","text":"A ADLS container has many JSON files with nested structure. This article shows how to flatten those Json files for better handling later.","title":"Background"},{"location":"DE-Projects/JsonFlatAzureSDK/#prerequisites","text":"A python-development ready VS Code environment Install the required Azure SDK(libs) to work with Azure storage. pip install azure-storage-blob A source container with the JSON files. A destination container where the flattened json will be kept. The script can either replace the original files with the processed ones or store them in a separate folder. Storage account keys. This code uses Account key method to authenticate. You can get the keys using these steps: Open the Storage Accounts containing the container. Go to Settings , Access keys . Copy and keep the Connection string from here.","title":"Prerequisites"},{"location":"DE-Projects/JsonFlatAzureSDK/#create-the-script","text":"To create the Python code in Visual Studio Code follow these steps: Create a new file with the .py extension. Import the necessary libraries: import json from azure.storage.blob import BlobServiceClient import logging Define the flatten_json() function: def flatten_json(y, parent_key='', sep='_'): items = [] for k, v in y.items(): new_key = f\"{parent_key}{sep}{k}\" if parent_key else k if isinstance(v, dict): items.extend(flatten_json(v, new_key, sep=sep).items()) else: items.append((new_key, v)) return dict(items) Define the main() function: def main(): # Initialize Blob Service Client blob_service_client = BlobServiceClient.from_connection_string( \"DefaultEndpointsProtocol=https;AccountName=<Your_Storage_Account_Name>;AccountKey=<The_Storage_Act_Key>;EndpointSuffix=core.windows.net\") # Iterate over blobs in the \"source_container\" container container_client = blob_service_client.get_container_client(\"source_container\") for blob in container_client.list_blobs(): try: # Download the blob blob_client = blob_service_client.get_blob_client(container=\"dest_container\", blob=blob.name) data_str = blob_client.download_blob().readall().decode('utf-8') # Decode the blob data to a string data = json.loads(data_str) # Flatten the JSON data flattened_data = flatten_json(data) # Move blob to the \"silver\" container target_blob_client = blob_service_client.get_blob_client(container=\"dest_container\", blob=blob.name) target_blob_client.upload_blob(json.dumps(flattened_data), overwrite=True) # Delete the original blob (optional) # blob_client.delete_blob() except Exception as e: logging.error(f\"Error processing blob {blob.name}: {e}\") if __name__ == \"__main__\": main() Save the file. Press F5 to run the code.","title":"Create the script"},{"location":"DE-Projects/JsonFlatAzureSDK/#explanation-of-key-elements-in-the-script","text":"Here is what the scirpt does. This will help you understand how Azure SDK for Blob Stroage works: Initializes a BlobServiceClient object using the from_connection_string() method. This object is used to interact with the Azure Blob Storage service. Gets a container client for the silver container using the get_container_client() method. This object is used to interact with the specified container. Iterates over all blobs in the container using the list_blobs() method. For each blob, the function does the following: Downloads the blob using the get_blob_client() and download_blob() methods. Decodes the blob data to a string using the decode() method. Parses the JSON data in the string using the json.loads() function. Flattens the JSON data using the flatten_json() function that we provided. Moves the blob to the silver container using the upload_blob() method. Deletes the original blob (optional).","title":"Explanation of key elements in the script"},{"location":"DE-Projects/JsonFlatAzureSDK/#appendix","text":"","title":"Appendix"},{"location":"DE-Projects/JsonFlatAzureSDK/#the-complete-script","text":"Here is the complete script in one piece: \"\"\" Author: Das LTS: Very easy to run this code. Just pip install azure-storage-blob. And run the code anywhere with python. Fully working code anywhere. Windows/Ubuntu. --- This script uses Azure SDK BlobServiceClient class to interact with Azure Blob Storage. It downloads blobs from a container, flattens their JSON content, and uploads them back to another container. The BlobServiceClient class is part of the Azure SDK for Python and is used to interact with Azure Blob Storage. It provides methods for getting clients for specific containers and blobs, downloading blobs, and uploading blobs. The from_connection_string method is used to create an instance of BlobServiceClient using a connection string. The download_blob method returns a stream of data that can be read by calling readall. The upload_blob method uploads data to a blob, overwriting it if it already exists. \"\"\" import json from azure.storage.blob import BlobServiceClient # BlobServiceClient is part of the Azure SDK for Python. It's used to interact with Azure Blob Storage. import logging # Function to flatten JSON objects def flatten_json(y, parent_key='', sep='_'): items = [] for k, v in y.items(): new_key = f\"{parent_key}{sep}{k}\" if parent_key else k if isinstance(v, dict): items.extend(flatten_json(v, new_key, sep=sep).items()) else: items.append((new_key, v)) return dict(items) # Main function def main(): # Initialize Blob Service Client using connection string. This is the main entry point for interacting with blobs in Azure Storage. blob_service_client = BlobServiceClient.from_connection_string(\"DefaultEndpointsProtocol=https;AccountName=<your-storage-account-Name>;AccountKey=<Your_Storage_Act_Con_String>;EndpointSuffix=core.windows.net\") # Get a client for the \"silver\" container in the Blob Service. This client provides operations to interact with a specific container. container_client = blob_service_client.get_container_client(\"silver\") # Iterate over blobs in the \"weather-http\" container for blob in container_client.list_blobs(): try: # Get a client for the current blob. This client provides operations to interact with a specific blob. blob_client = blob_service_client.get_blob_client(container=\"silver\", blob=blob.name) # Download the blob data and decode it from bytes to string. The download_blob method returns a stream of data. data_str = blob_client.download_blob().readall().decode('utf-8') try: data = json.loads(data_str) except json.JSONDecodeError: data = json.loads(blob_client.download_blob().readall().decode('utf-8')) # Flatten the JSON data flattened_data = flatten_json(data) # Get a client for the target blob in the \"silver\" container. This client will be used to upload data to this blob. Move blob to the \"silver\" container target_blob_client = blob_service_client.get_blob_client(container=\"silver\", blob=blob.name) # Upload the flattened JSON data to the target blob, overwriting it if it already exists. The upload_blob method uploads data to a blob. target_blob_client.upload_blob(json.dumps(flattened_data), overwrite=True) # Uncomment the following line to delete the original blob after moving it to the \"silver\" container #blob_client.delete_blob() # Delete the original blob after moving except Exception as e: logging.error(f\"Error processing blob {blob.name}: {e}\") if __name__ == \"__main__\": main()","title":"The complete script"},{"location":"DE-Projects/JsonFlatAzureSDK/#azure-python-sdklibs-ecosystem","text":"Library Explanation \ud83d\udc0d Azure SDK For Python Superset of all python packages (libs) for Azure. Can't be installed with a single pip. \ud83d\udce6 Azure Storage SDKs Subset of Azure SDK. Multiple libraries. Hence, no single pip command. \ud83d\udca6 Azure Blob Storage SDK Subset of Azure Storage SDK. Single Library - pip install azure-storage-blob \ud83d\udee0\ufe0f BlobServiceClient Class Storage Account Level \ud83d\udcc1 Container Client Class Container Level \ud83d\udcc4 Blob Client Class Blob Level","title":"Azure Python SDK(libs) ecosystem"},{"location":"DE-Projects/JsonFlatAzureSDK/#convert-the-script-into-an-azure-function","text":"The logic from my script can be easily incoporated into an azure function. You can easily put the entire logic into the functions function_app.py . Refer to my other articles on how to work with Azure Functions. \ud83c\udf1f Conclusion : The Azure SDK for Python is a superset of libraries to work with Azure services. The Azure Blob Storage SDK for Python is a subset of the Azure SDK for working with Azure Blob Storage. The script in this article uses the Azure Blob Storage SDK for Python to flatten JSON files in an Azure Blob Storage container. The script first downloads the blob from the container, then flattens the JSON data, and finally uploads the flattened JSON data back to the container.","title":"Convert the script into an Azure Function"},{"location":"DE-Projects/JsonFlatAzureSDK/#additional-samples","text":"","title":"Additional samples"},{"location":"DE-Projects/JsonFlatAzureSDK/#uploading-data-to-azure-blob-storage","text":"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient # Connection string to your Azure Storage account connection_string = \"your_connection_string\" container_name = \"your_container_name\" blob_name = \"your_blob_name\" data = \"Sample data about your exes\" # Create a BlobServiceClient blob_service_client = BlobServiceClient.from_connection_string(connection_string) # Create a container if it doesn't exist container_client = blob_service_client.get_container_client(container_name) container_client.create_container() # Create a BlobClient blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name) # Upload data to the blob blob_client.upload_blob(data, overwrite=True) print(\"Data uploaded successfully\")","title":"Uploading Data to Azure Blob Storage"},{"location":"DE-Projects/JsonFlatAzureSDK/#downloading-data-from-azure-blob-storage","text":"from azure.storage.blob import BlobServiceClient # Connection string to your Azure Storage account connection_string = \"your_connection_string\" container_name = \"your_container_name\" blob_name = \"your_blob_name\" # Create a BlobServiceClient blob_service_client = BlobServiceClient.from_connection_string(connection_string) # Create a BlobClient blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name) # Download data from the blob blob_data = blob_client.download_blob().readall() print(\"Downloaded data:\", blob_data.decode())","title":"Downloading Data from Azure Blob Storage"},{"location":"DE-Projects/JsonFlatAzureSDK/#listing-blobs-in-a-container","text":"from azure.storage.blob import BlobServiceClient # Connection string to your Azure Storage account connection_string = \"your_connection_string\" container_name = \"your_container_name\" # Create a BlobServiceClient blob_service_client = BlobServiceClient.from_connection_string(connection_string) # Get a container client container_client = blob_service_client.get_container_client(container_name) # List blobs in the container blobs = container_client.list_blobs() for blob in blobs: print(\"Blob name:\", blob.name)","title":"Listing Blobs in a Container"},{"location":"DE-Projects/JsonFlatAzureSDK/#querying-data-example-with-csv-data","text":"If your data is in a CSV format and stored in blobs, you can query it using Azure Synapse or Data Lake Analytics for more advanced queries. Here's a simple example using CSV data in blobs: import pandas as pd from azure.storage.blob import BlobServiceClient # Connection string to your Azure Storage account connection_string = \"your_connection_string\" container_name = \"your_container_name\" blob_name = \"your_blob_name.csv\" # Create a BlobServiceClient blob_service_client = BlobServiceClient.from_connection_string(connection_string) # Create a BlobClient blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name) # Download blob data to a stream stream = blob_client.download_blob().readall() # Read CSV data into a DataFrame df = pd.read_csv(pd.compat.BytesIO(stream)) print(\"Data from CSV blob:\\n\", df) # Example query: Find whereabouts of a specific ex whereabouts = df[df['Name'] == 'ExName']['Whereabouts'].iloc[0] print(\"Whereabouts of ExName:\", whereabouts) \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Querying Data (Example with CSV Data)"},{"location":"DE-Projects/LocalPython_AzureBlob/","text":"Table of contents Rearranging Files in Azure Blob Storage Using Python Background Prerequisites Overview Kickstart: Step-by-step guide to writing the code Conclusion Appendix Why use a structure like year=2023/month=10/day=23/hour=00.json ? Complete tested code Rearranging Files in Azure Blob Storage Using Python Background During some of my projects, I often had to rearrange files inside Azure blob containers. In this write-up, I'm sharing a simple Python method I frequently use with the Azure SDKs. But before we dive in, it's essential to understand there are many other ways to achieve the same thing, some of which might be better for regular company setups: You can run Spark externally and use Hadoop Jars with Pyspark. This is quite useful if you're looking for an in-house solution and don't want to spend too much on Databricks or ADFs. I've explained this method in detail in other articles on my site. There are also options like Databricks, Azure Logic Apps, ADF, and the like. One thing to note: While this Python method is easy, it's a bit slower compared to using Hadoop Jars with Spark. Also, if you're using Python externally, monitoring and maintenance can be a bit challenging. I'm sharing this technique because it shows the capabilities of Azure SDK when paired with Python for Blob containers. It's good to know what can be done, even if there might be faster or more efficient methods out there. Prerequisites Azure Blob Storage Account and a container with files. azure-identity and azure-storage-blob Python packages. Install these with: bash pip install azure-identity azure-storage-blob Overview Our primary goal is to rearrange files stored in my container named like 2023-10-23-00.json into a hierarchical structured format like year=2023/month=10/day=23/hour=00.json . Kickstart: Step-by-step guide to writing the code Set up Azure Blob Storage SDK : Start by authenticating against Azure with the credentials from your registered Azure app: ```python from azure.identity import ClientSecretCredential from azure.storage.blob import BlobServiceClient credential = ClientSecretCredential( tenant_id=\"YOUR_TENANT_ID\", client_id=\"YOUR_CLIENT_ID\", client_secret=\"YOUR_CLIENT_SECRET\" ) account_url = \"https://YOUR_STORAGE_ACCOUNT_NAME.blob.core.windows.net/\" blob_service_client = BlobServiceClient(account_url=account_url, credential=credential) ``` Listing Blobs : Retrieve all blobs in your container and then filter the desired files: python container_client = blob_service_client.get_container_client(\"YOUR_CONTAINER_NAME\") blobs = container_client.list_blobs() old_files = [blob.name for blob in blobs if \"-*.json\" in blob.name] File Rearrangement : Deconstruct each blob's name to get the year, month, day, and hour. Then, design the new path: python for old_file_path in old_files: filename = old_file_path.split('/')[-1] year, month, day, hour = filename.split('-')[:4] new_directory = f\"year={year}/month={month}/day={day}/\" new_blob_path = new_directory + f\"hour={hour}.json\" Copying & Deleting Blobs : Azure Blob Storage requires copying the blob to the new position and deleting the original: python source_blob = container_client.get_blob_client(old_file_path) destination_blob = container_client.get_blob_client(new_blob_path) destination_blob.start_copy_from_url(source_blob.url) source_blob.delete_blob() Simulating Directory-like Structure : Azure Blob Storage doesn't have real directories. Instead, using / in blob names can emulate directory structures. By copying a blob to a new path with / , we can recreate a folder-like structure. Error Handling : It's essential to manage errors. I ran into an InvalidUri error when attempting to create a directory-like structure using 0-byte blobs. I tackled this by directly copying the blob to the desired path, which handled the pathing effectively. Conclusion Shifting files in Azure Blob Storage can initially appear challenging, mainly due to its flat structure. Nevertheless, with the Azure SDK for Python and the correct strategy, restructuring blobs becomes feasible. Always remember to rigorously test your approach in a non-production environment before applying it to vital data. Appendix Why use a structure like year=2023/month=10/day=23/hour=00.json ? This hierarchical directory structure aligns very well with common best practices for storing data in columnar storage formats like Parquet and ORC. Here's how: Partitioning : The structure year=2023/month=10/day=23/hour=00.json inherently sets up partitioning for the data. When converted to Parquet or other storage formats, this structure will make it incredibly efficient to read specific partitions (like all data from a particular month or day) without scanning through the entire dataset. This reduces the amount of data that needs to be read and thus speeds up queries. Column Pruning : If your dataset inside the JSON files includes multiple columns, Parquet (and other columnar formats) allows for column pruning. This means that if a particular analysis requires only a subset of columns, only those specific columns are read from the storage, saving both time and resources. Compression : Both Parquet and ORC are known for efficient compression. When you have data organized hierarchically, and you're storing it in a columnar format, you can achieve significant storage savings. The structure also ensures that similar data types (like timestamps) are stored together, which can result in better compression ratios. Compatibility with Big Data Tools : Tools like Spark and Hive work exceptionally well with columnar storage formats and can directly utilize the hierarchical structure for optimized data reads. If you ever decide to analyze the data using these tools, having them in this structure and format would be advantageous. Schema Evolution : One of the advantages of Parquet (and to some extent, ORC) is the support for schema evolution. If your data structure changes over time (new columns are added, for example), these formats can handle those changes gracefully. Having an organized directory structure makes managing these schema changes over different time periods more straightforward. Long story short: Given these advantages, if you're considering a transition to Parquet or another efficient storage format in the future, this hierarchical structure will certainly come in handy and help in making the data storage and retrieval processes more efficient. Complete tested code If you want to test the complete code together here is it. Remember, place your original ids, secret in the placeholder. Also, its not a good practice to use creds like this they shoudl be stored in vaults. But for focussing on the core functionality and to reduce the number of code I used the ids and passwords right in the code. # Import necessary libraries from azure.identity import ClientSecretCredential from azure.storage.blob import BlobServiceClient import re # Replace your actual Azure 'App registrations' credentials here tenant_dir_id_regd_app = \"YOUR_TENANT_ID\" client_id_regd_app = \"YOUR_CLIENT_ID\" client_secret_regd_app = \"YOUR_CLIENT_SECRET\" # Specify the Azure Blob container name and storage account name container_name = \"YOUR_CONTAINER_NAME\" storage_act_name = \"YOUR_STORAGE_ACCOUNT_NAME\" # Set up authentication using Azure ClientSecretCredential # This is useful when you're using Azure AD for authentication credential = ClientSecretCredential( tenant_id=tenant_dir_id_regd_app, client_id=client_id_regd_app, client_secret=client_secret_regd_app ) # Initialize BlobServiceClient with the given account and credentials account_url = f\"https://{storage_act_name}.blob.core.windows.net/\" blob_service_client = BlobServiceClient(account_url=account_url, credential=credential) container_client = blob_service_client.get_container_client(container_name) # List all blobs in the specified ADLS container # Then, filter out the files using a regular expression to match the desired format blobs = container_client.list_blobs() old_files = [blob.name for blob in blobs if re.match(r'^\\d{4}-\\d{2}-\\d{2}-\\d{2}.json$', blob.name)] # Display the number of files that match the format and will be processed print(f\"Number of files to be processed: {len(old_files)}\") for old_file_path in old_files: # Break down the old file path to extract year, month, day, and hour filename = old_file_path.split('/')[-1] year, month, day, hour = filename.split('-')[:4] # Create the new hierarchical directory structure based on extracted date and time details new_blob_path = f\"year={year}/month={month}/day={day}/hour={hour}\" # Display the file movement details print(f\"Moving {old_file_path} to {new_blob_path}\") # Copy content from the old blob to the new blob location source_blob = container_client.get_blob_client(old_file_path) destination_blob = container_client.get_blob_client(new_blob_path) destination_blob.start_copy_from_url(source_blob.url) # Once the content is successfully copied, remove the old blob source_blob.delete_blob() print(\"Files rearranged successfully!\") \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Local Python Code to Rearrange Files in a Azure Blob Container"},{"location":"DE-Projects/LocalPython_AzureBlob/#table-of-contents","text":"Rearranging Files in Azure Blob Storage Using Python Background Prerequisites Overview Kickstart: Step-by-step guide to writing the code Conclusion Appendix Why use a structure like year=2023/month=10/day=23/hour=00.json ? Complete tested code","title":"Table of contents"},{"location":"DE-Projects/LocalPython_AzureBlob/#rearranging-files-in-azure-blob-storage-using-python","text":"","title":"Rearranging Files in Azure Blob Storage Using Python"},{"location":"DE-Projects/LocalPython_AzureBlob/#background","text":"During some of my projects, I often had to rearrange files inside Azure blob containers. In this write-up, I'm sharing a simple Python method I frequently use with the Azure SDKs. But before we dive in, it's essential to understand there are many other ways to achieve the same thing, some of which might be better for regular company setups: You can run Spark externally and use Hadoop Jars with Pyspark. This is quite useful if you're looking for an in-house solution and don't want to spend too much on Databricks or ADFs. I've explained this method in detail in other articles on my site. There are also options like Databricks, Azure Logic Apps, ADF, and the like. One thing to note: While this Python method is easy, it's a bit slower compared to using Hadoop Jars with Spark. Also, if you're using Python externally, monitoring and maintenance can be a bit challenging. I'm sharing this technique because it shows the capabilities of Azure SDK when paired with Python for Blob containers. It's good to know what can be done, even if there might be faster or more efficient methods out there.","title":"Background"},{"location":"DE-Projects/LocalPython_AzureBlob/#prerequisites","text":"Azure Blob Storage Account and a container with files. azure-identity and azure-storage-blob Python packages. Install these with: bash pip install azure-identity azure-storage-blob","title":"Prerequisites"},{"location":"DE-Projects/LocalPython_AzureBlob/#overview","text":"Our primary goal is to rearrange files stored in my container named like 2023-10-23-00.json into a hierarchical structured format like year=2023/month=10/day=23/hour=00.json .","title":"Overview"},{"location":"DE-Projects/LocalPython_AzureBlob/#kickstart-step-by-step-guide-to-writing-the-code","text":"Set up Azure Blob Storage SDK : Start by authenticating against Azure with the credentials from your registered Azure app: ```python from azure.identity import ClientSecretCredential from azure.storage.blob import BlobServiceClient credential = ClientSecretCredential( tenant_id=\"YOUR_TENANT_ID\", client_id=\"YOUR_CLIENT_ID\", client_secret=\"YOUR_CLIENT_SECRET\" ) account_url = \"https://YOUR_STORAGE_ACCOUNT_NAME.blob.core.windows.net/\" blob_service_client = BlobServiceClient(account_url=account_url, credential=credential) ``` Listing Blobs : Retrieve all blobs in your container and then filter the desired files: python container_client = blob_service_client.get_container_client(\"YOUR_CONTAINER_NAME\") blobs = container_client.list_blobs() old_files = [blob.name for blob in blobs if \"-*.json\" in blob.name] File Rearrangement : Deconstruct each blob's name to get the year, month, day, and hour. Then, design the new path: python for old_file_path in old_files: filename = old_file_path.split('/')[-1] year, month, day, hour = filename.split('-')[:4] new_directory = f\"year={year}/month={month}/day={day}/\" new_blob_path = new_directory + f\"hour={hour}.json\" Copying & Deleting Blobs : Azure Blob Storage requires copying the blob to the new position and deleting the original: python source_blob = container_client.get_blob_client(old_file_path) destination_blob = container_client.get_blob_client(new_blob_path) destination_blob.start_copy_from_url(source_blob.url) source_blob.delete_blob() Simulating Directory-like Structure : Azure Blob Storage doesn't have real directories. Instead, using / in blob names can emulate directory structures. By copying a blob to a new path with / , we can recreate a folder-like structure. Error Handling : It's essential to manage errors. I ran into an InvalidUri error when attempting to create a directory-like structure using 0-byte blobs. I tackled this by directly copying the blob to the desired path, which handled the pathing effectively.","title":"Kickstart: Step-by-step guide to writing the code"},{"location":"DE-Projects/LocalPython_AzureBlob/#conclusion","text":"Shifting files in Azure Blob Storage can initially appear challenging, mainly due to its flat structure. Nevertheless, with the Azure SDK for Python and the correct strategy, restructuring blobs becomes feasible. Always remember to rigorously test your approach in a non-production environment before applying it to vital data.","title":"Conclusion"},{"location":"DE-Projects/LocalPython_AzureBlob/#appendix","text":"","title":"Appendix"},{"location":"DE-Projects/LocalPython_AzureBlob/#why-use-a-structure-like-year2023month10day23hour00json","text":"This hierarchical directory structure aligns very well with common best practices for storing data in columnar storage formats like Parquet and ORC. Here's how: Partitioning : The structure year=2023/month=10/day=23/hour=00.json inherently sets up partitioning for the data. When converted to Parquet or other storage formats, this structure will make it incredibly efficient to read specific partitions (like all data from a particular month or day) without scanning through the entire dataset. This reduces the amount of data that needs to be read and thus speeds up queries. Column Pruning : If your dataset inside the JSON files includes multiple columns, Parquet (and other columnar formats) allows for column pruning. This means that if a particular analysis requires only a subset of columns, only those specific columns are read from the storage, saving both time and resources. Compression : Both Parquet and ORC are known for efficient compression. When you have data organized hierarchically, and you're storing it in a columnar format, you can achieve significant storage savings. The structure also ensures that similar data types (like timestamps) are stored together, which can result in better compression ratios. Compatibility with Big Data Tools : Tools like Spark and Hive work exceptionally well with columnar storage formats and can directly utilize the hierarchical structure for optimized data reads. If you ever decide to analyze the data using these tools, having them in this structure and format would be advantageous. Schema Evolution : One of the advantages of Parquet (and to some extent, ORC) is the support for schema evolution. If your data structure changes over time (new columns are added, for example), these formats can handle those changes gracefully. Having an organized directory structure makes managing these schema changes over different time periods more straightforward. Long story short: Given these advantages, if you're considering a transition to Parquet or another efficient storage format in the future, this hierarchical structure will certainly come in handy and help in making the data storage and retrieval processes more efficient.","title":"Why use a structure like year=2023/month=10/day=23/hour=00.json?"},{"location":"DE-Projects/LocalPython_AzureBlob/#complete-tested-code","text":"If you want to test the complete code together here is it. Remember, place your original ids, secret in the placeholder. Also, its not a good practice to use creds like this they shoudl be stored in vaults. But for focussing on the core functionality and to reduce the number of code I used the ids and passwords right in the code. # Import necessary libraries from azure.identity import ClientSecretCredential from azure.storage.blob import BlobServiceClient import re # Replace your actual Azure 'App registrations' credentials here tenant_dir_id_regd_app = \"YOUR_TENANT_ID\" client_id_regd_app = \"YOUR_CLIENT_ID\" client_secret_regd_app = \"YOUR_CLIENT_SECRET\" # Specify the Azure Blob container name and storage account name container_name = \"YOUR_CONTAINER_NAME\" storage_act_name = \"YOUR_STORAGE_ACCOUNT_NAME\" # Set up authentication using Azure ClientSecretCredential # This is useful when you're using Azure AD for authentication credential = ClientSecretCredential( tenant_id=tenant_dir_id_regd_app, client_id=client_id_regd_app, client_secret=client_secret_regd_app ) # Initialize BlobServiceClient with the given account and credentials account_url = f\"https://{storage_act_name}.blob.core.windows.net/\" blob_service_client = BlobServiceClient(account_url=account_url, credential=credential) container_client = blob_service_client.get_container_client(container_name) # List all blobs in the specified ADLS container # Then, filter out the files using a regular expression to match the desired format blobs = container_client.list_blobs() old_files = [blob.name for blob in blobs if re.match(r'^\\d{4}-\\d{2}-\\d{2}-\\d{2}.json$', blob.name)] # Display the number of files that match the format and will be processed print(f\"Number of files to be processed: {len(old_files)}\") for old_file_path in old_files: # Break down the old file path to extract year, month, day, and hour filename = old_file_path.split('/')[-1] year, month, day, hour = filename.split('-')[:4] # Create the new hierarchical directory structure based on extracted date and time details new_blob_path = f\"year={year}/month={month}/day={day}/hour={hour}\" # Display the file movement details print(f\"Moving {old_file_path} to {new_blob_path}\") # Copy content from the old blob to the new blob location source_blob = container_client.get_blob_client(old_file_path) destination_blob = container_client.get_blob_client(new_blob_path) destination_blob.start_copy_from_url(source_blob.url) # Once the content is successfully copied, remove the old blob source_blob.delete_blob() print(\"Files rearranged successfully!\") \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Complete tested code"},{"location":"DE-Projects/Microsoft_OpenJDK/","text":"Installing Microsoft OpenJDK There are many versions of JDK. Refer to the table at the end to see them. Here we will focus on OpenJDK. Many popular applications and platforms use OpenJDK as their Java runtime. For example, Minecraft (Java Edition) , Apache Cassandra , Apache Kafka , Jenkins , and ElasticSearch all rely on OpenJDK. Development tools like IntelliJ IDEA and Eclipse also run well on it. Spring Boot applications , Atlassian products like Jira and Confluence , big data tools like Apache Hadoop and Apache Spark , as well as Docker images and web servers like Apache Tomcat and Jetty , commonly use OpenJDK. Even Android Studio can run with OpenJDK. So, OpenJDK is used a lot in real-world applications. In this article I will give you a brief intro to Microsoft OpenJDK and how to install it. When to Use Microsoft OpenJDK? If you're using JDK in the Microsoft ecosystem, it's best to go with Microsoft's JDK. If any issues come up, you can easily get support from Microsoft. But if you're in a general setup or prefer tools not tied to one company, OpenJDK might be a better choice. Microsoft also says their OpenJDK can replace any OpenJDK, even in non-Microsoft environments like AWS + Linux. Plus, their version has extra fixes and enhancements, making it work better in the Microsoft ecosystem. Installation Steps For Windows: Download and Run the installer: Go to Microsoft Build of OpenJDK website . Choose the Windows .msi installer and download and run it. During installation, choose the option to set JAVA_HOME and update the Path . Verify Installation and check JAVA_HOME variable Open Command Prompt and type java -version . You should see the installed version of Microsoft OpenJDK Enter set in command prompt. It will show you all the environment variable. Look for JAVA_HOME to see if it is set properly. For macOS: Download Archive: Visit the Microsoft Build of OpenJDK website . Download the .tar.gz archive for macOS. Extract and Install: Open Terminal and navigate to the downloaded archive. Extract it using: bash sudo tar zxvf microsoft-jdk-17-macos-x64.tar.gz -C /Library/Java/JavaVirtualMachines/ Set JAVA_HOME: Add the following to your .bash_profile , .zshrc , or .bashrc : bash export JAVA_HOME=/Library/Java/JavaVirtualMachines/microsoft-17.jdk/Contents/Home export PATH=$JAVA_HOME/bin:$PATH Apply changes with: bash source ~/.bash_profile Verify Installation: Run java -version You should see the Microsoft OpenJDK version installed. For Linux (Ubuntu/Debian): Download Archive: Head to the Microsoft Build of OpenJDK website and download the .tar.gz for Linux. Extract and Install: Open Terminal and navigate to the download location. Extract using: bash sudo tar zxvf microsoft-jdk-17-linux-x64.tar.gz -C /usr/lib/jvm Set JAVA_HOME and Update Alternatives: Add to your .bashrc or equivalent: bash export JAVA_HOME=/usr/lib/jvm/microsoft-17 export PATH=$JAVA_HOME/bin:$PATH Set as default: bash sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/microsoft-17/bin/java 1 sudo update-alternatives --config java Verify Installation: Run java -version You should see the installed Microsoft OpenJDK version. Conclusion Microsoft Build of OpenJDK is just like the regular OpenJDK when it comes to working with apps. The main difference is that it's made by Microsoft, so it might work better with Microsoft products. Plus, if you're using it on Azure and something goes wrong with Java, you can get help from Microsoft support. In short, it's basically OpenJDK with Microsoft's name on it. JDK Brands JDK Version Free to Download Advantages Oracle JDK Yes (for personal use, development, and testing)** Official JDK from Oracle, commercial support, and long-term updates available. OpenJDK Yes Open-source, reference implementation of Java SE, widely supported. Amazon Corretto Yes Production-ready, free support, optimized for AWS, multiplatform. Eclipse Temurin (AdoptOpenJDK) Yes Free, well-supported, and widely used in various environments. Microsoft Build of OpenJDK Yes Free, optimized for Azure and Microsoft products, integrated with MS ecosystem. GraalVM Yes (Community Edition) High-performance, supports multiple languages, ahead-of-time compilation. Azul Zulu Yes Certified OpenJDK build, various support options, optimized for enterprise use.","title":"Microsoft OpenJDK"},{"location":"DE-Projects/Microsoft_OpenJDK/#installing-microsoft-openjdk","text":"There are many versions of JDK. Refer to the table at the end to see them. Here we will focus on OpenJDK. Many popular applications and platforms use OpenJDK as their Java runtime. For example, Minecraft (Java Edition) , Apache Cassandra , Apache Kafka , Jenkins , and ElasticSearch all rely on OpenJDK. Development tools like IntelliJ IDEA and Eclipse also run well on it. Spring Boot applications , Atlassian products like Jira and Confluence , big data tools like Apache Hadoop and Apache Spark , as well as Docker images and web servers like Apache Tomcat and Jetty , commonly use OpenJDK. Even Android Studio can run with OpenJDK. So, OpenJDK is used a lot in real-world applications. In this article I will give you a brief intro to Microsoft OpenJDK and how to install it.","title":"Installing Microsoft OpenJDK"},{"location":"DE-Projects/Microsoft_OpenJDK/#when-to-use-microsoft-openjdk","text":"If you're using JDK in the Microsoft ecosystem, it's best to go with Microsoft's JDK. If any issues come up, you can easily get support from Microsoft. But if you're in a general setup or prefer tools not tied to one company, OpenJDK might be a better choice. Microsoft also says their OpenJDK can replace any OpenJDK, even in non-Microsoft environments like AWS + Linux. Plus, their version has extra fixes and enhancements, making it work better in the Microsoft ecosystem.","title":"When to Use Microsoft OpenJDK?"},{"location":"DE-Projects/Microsoft_OpenJDK/#installation-steps","text":"","title":"Installation Steps"},{"location":"DE-Projects/Microsoft_OpenJDK/#for-windows","text":"Download and Run the installer: Go to Microsoft Build of OpenJDK website . Choose the Windows .msi installer and download and run it. During installation, choose the option to set JAVA_HOME and update the Path . Verify Installation and check JAVA_HOME variable Open Command Prompt and type java -version . You should see the installed version of Microsoft OpenJDK Enter set in command prompt. It will show you all the environment variable. Look for JAVA_HOME to see if it is set properly.","title":"For Windows:"},{"location":"DE-Projects/Microsoft_OpenJDK/#for-macos","text":"Download Archive: Visit the Microsoft Build of OpenJDK website . Download the .tar.gz archive for macOS. Extract and Install: Open Terminal and navigate to the downloaded archive. Extract it using: bash sudo tar zxvf microsoft-jdk-17-macos-x64.tar.gz -C /Library/Java/JavaVirtualMachines/ Set JAVA_HOME: Add the following to your .bash_profile , .zshrc , or .bashrc : bash export JAVA_HOME=/Library/Java/JavaVirtualMachines/microsoft-17.jdk/Contents/Home export PATH=$JAVA_HOME/bin:$PATH Apply changes with: bash source ~/.bash_profile Verify Installation: Run java -version You should see the Microsoft OpenJDK version installed.","title":"For macOS:"},{"location":"DE-Projects/Microsoft_OpenJDK/#for-linux-ubuntudebian","text":"Download Archive: Head to the Microsoft Build of OpenJDK website and download the .tar.gz for Linux. Extract and Install: Open Terminal and navigate to the download location. Extract using: bash sudo tar zxvf microsoft-jdk-17-linux-x64.tar.gz -C /usr/lib/jvm Set JAVA_HOME and Update Alternatives: Add to your .bashrc or equivalent: bash export JAVA_HOME=/usr/lib/jvm/microsoft-17 export PATH=$JAVA_HOME/bin:$PATH Set as default: bash sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/microsoft-17/bin/java 1 sudo update-alternatives --config java Verify Installation: Run java -version You should see the installed Microsoft OpenJDK version.","title":"For Linux (Ubuntu/Debian):"},{"location":"DE-Projects/Microsoft_OpenJDK/#conclusion","text":"Microsoft Build of OpenJDK is just like the regular OpenJDK when it comes to working with apps. The main difference is that it's made by Microsoft, so it might work better with Microsoft products. Plus, if you're using it on Azure and something goes wrong with Java, you can get help from Microsoft support. In short, it's basically OpenJDK with Microsoft's name on it.","title":"Conclusion"},{"location":"DE-Projects/Microsoft_OpenJDK/#jdk-brands","text":"JDK Version Free to Download Advantages Oracle JDK Yes (for personal use, development, and testing)** Official JDK from Oracle, commercial support, and long-term updates available. OpenJDK Yes Open-source, reference implementation of Java SE, widely supported. Amazon Corretto Yes Production-ready, free support, optimized for AWS, multiplatform. Eclipse Temurin (AdoptOpenJDK) Yes Free, well-supported, and widely used in various environments. Microsoft Build of OpenJDK Yes Free, optimized for Azure and Microsoft products, integrated with MS ecosystem. GraalVM Yes (Community Edition) High-performance, supports multiple languages, ahead-of-time compilation. Azul Zulu Yes Certified OpenJDK build, various support options, optimized for enterprise use.","title":"JDK Brands"},{"location":"DE-Projects/Project_MigrationToAzureBlob/","text":"Project Overview: Migrating and Enabling Search for Archived PDF Data in Azure Blob Storage The client, had several hundred GBs of archived data, mostly in PDF format, stored in another CMS. The CMS incurred heavy licensing fees and was on-prem without any DR or HA. The goal was to move this data to Azure Blob Storage and enable efficient search capabilities to improve accessibility and enable DR and reduce maintance costs from the current CMS system. Solution Implementation We began by assessing the existing data structure, which was organized by deparment. We developed a migration plan to transfer the data in batches, ensuring minimal disruption to the client\u2019s operations. Tools Used : - Azure Blob Storage - PowerShell & AzCopy for scripting and automating the data migration process - Azure Cognitive Search for indexing the content of the PDF files - Azure Storage Explorer for manual verification and management of the files - Azure SDKs for custom development and integration tasks The migration process involved extracting data from the legacy CMS and staging it in a network folder. Using PowerShell scripts and AzCopy, we transferred these files to Azure Blob Storage, preserving the original directory structure by using paths in the blob names. To enable searching within the uploaded PDFs, we set up Azure Cognitive Search. This involved creating a data source connected to the Azure Blob Storage container, defining an index to capture necessary metadata and content fields, and configuring an indexer to handle text extraction from PDFs. The indexer was scheduled to run periodically to ensure that new files were indexed promptly. This setup allowed users to perform efficient searches on the archived PDF data using the Azure Cognitive Search REST API or SDKs, providing quick and reliable access to the information stored in the cloud. Outcome The project successfully moved the client\u2019s archived PDF data to Azure Blob Storage and enabled search capabilities through Azure Cognitive Search. This solution provided the client with improved data accessibility and better search functionality. The use of tools and technologies from 2017 ensured a maintainable solution that met the client\u2019s needs effectively.","title":"CMS Migration to Azure Blob"},{"location":"DE-Projects/Project_MigrationToAzureBlob/#project-overview-migrating-and-enabling-search-for-archived-pdf-data-in-azure-blob-storage","text":"The client, had several hundred GBs of archived data, mostly in PDF format, stored in another CMS. The CMS incurred heavy licensing fees and was on-prem without any DR or HA. The goal was to move this data to Azure Blob Storage and enable efficient search capabilities to improve accessibility and enable DR and reduce maintance costs from the current CMS system.","title":"Project Overview: Migrating and Enabling Search for Archived PDF Data in Azure Blob Storage"},{"location":"DE-Projects/Project_MigrationToAzureBlob/#solution-implementation","text":"We began by assessing the existing data structure, which was organized by deparment. We developed a migration plan to transfer the data in batches, ensuring minimal disruption to the client\u2019s operations. Tools Used : - Azure Blob Storage - PowerShell & AzCopy for scripting and automating the data migration process - Azure Cognitive Search for indexing the content of the PDF files - Azure Storage Explorer for manual verification and management of the files - Azure SDKs for custom development and integration tasks The migration process involved extracting data from the legacy CMS and staging it in a network folder. Using PowerShell scripts and AzCopy, we transferred these files to Azure Blob Storage, preserving the original directory structure by using paths in the blob names. To enable searching within the uploaded PDFs, we set up Azure Cognitive Search. This involved creating a data source connected to the Azure Blob Storage container, defining an index to capture necessary metadata and content fields, and configuring an indexer to handle text extraction from PDFs. The indexer was scheduled to run periodically to ensure that new files were indexed promptly. This setup allowed users to perform efficient searches on the archived PDF data using the Azure Cognitive Search REST API or SDKs, providing quick and reliable access to the information stored in the cloud.","title":"Solution Implementation"},{"location":"DE-Projects/Project_MigrationToAzureBlob/#outcome","text":"The project successfully moved the client\u2019s archived PDF data to Azure Blob Storage and enabled search capabilities through Azure Cognitive Search. This solution provided the client with improved data accessibility and better search functionality. The use of tools and technologies from 2017 ensured a maintainable solution that met the client\u2019s needs effectively.","title":"Outcome"},{"location":"DE-Projects/Project_MongoCMS/","text":"Building a Simple CMS with MongoDB, Python, and Flask Project Background Create a MongoDB Container Creating Sample PDFs and Metadata Files Script to Create Metadata and PDF Files Upload the Content and Metadata to MongoDB Create and Run the Flask Application to Search CMS index.html search.html app.py Use the search application Appendix How the pdf is uploaded and linked with the metadata How MongoDB Stores Files and Metadata Default Metadata (GridFS Metadata) Custom Metadata COnclusion: Building a Simple CMS with MongoDB, Python, and Flask Usually, we use Content Management Software like IBM Filenet or OpenText Documentum to create content warehouses. These applications store content (PDFs, JPEGs) as objects and save the associated metadata in databases like MSSQL or Oracle. They offer front-end applications like Webtop and Documentum Administrator to access the files. With MongoDB, you can create a similar application almost in no time. This article will show you how easy it is to build such an app. We will use MongoDB as a container running in Docker. Our application for generating PDFs and metadata, as well as the uploading part, will run on a local Windows machine. We'll use Python 3.11 to develop the application. The front end will be created using Flask. Project Background The project can be divided into the following parts: Creating the MongoDB container Creating a script to generate PDFs and associated metadata in JSON Uploading the PDFs to MongoDB Creating a front-end UI using Flask to search the content in MongoDB Create a MongoDB Container First, create a Docker network and volume: docker network create spark-network docker volume create mongo_data Execute the following command to run the MongoDB container: docker run -d --name mongoDB --network spark-network -p 27017:27017 -v mongo-data:/data/db mongo Explanation of the command options: - -d : Run the container in detached mode. - --name mongoDB : Assign the name \"mongoDB\" to the container. - --network spark-network : Connect the container to the network \"spark-network\". - -p 27017:27017 : Map port 27017 on the host to port 27017 in the container. - -v mongo-data:/data/db : Use a Docker volume named \"mongo-data\" to persist MongoDB data. You can connect to MongoDB using a MongoDB client or management tool like MongoDB Compass, Robo 3T, or Studio 3T. The connection string is: mongodb://localhost:27017 Creating Sample PDFs and Metadata Files Create the folder structure mongoDB_CMS\\content_and_metadata and place the script files inside it. Note: Before running the scripts, install the required Python packages using command prompt/terminal: **pip install fpdf pymongo** Script to Create Metadata and PDF Files Save the following script as 1_create_files.py : from fpdf import FPDF import json import os # Base directory to save content and metadata files inside content_and_metadata folder #base_dir = os.path.join(os.getcwd(), 'content_and_metadata') #os.makedirs(base_dir, exist_ok=True) # Base directory to save content and metadata files inside a particular folder # base_dir = r'C:\\Users\\dwaip\\Desktop\\mongoDB_CMS' # Base directory, to save the content and metadata in the current folder base_dir = os.getcwd() # Get the current working directory # Generate 10 PDF files and corresponding metadata files metadata_list = [ {\"title\": f\"Document {i}\", \"description\": f\"Description for document {i}\", \"file_name\": f\"document{i}.pdf\"} for i in range(1, 11) ] for i, metadata in enumerate(metadata_list, 1): # Create PDF file pdf = FPDF() pdf.add_page() pdf.set_font(\"Arial\", size=12) pdf.cell(200, 10, txt=f\"Document {i}\", ln=True, align='C') pdf.cell(200, 10, txt=\"This is a sample PDF file.\", ln=True, align='C') pdf_file_path = os.path.join(base_dir, f\"document{i}.pdf\") pdf.output(pdf_file_path) # Create metadata file metadata_file_path = os.path.join(base_dir, f'metadata{i}.json') with open(metadata_file_path, 'w') as f: json.dump(metadata, f, indent=4) print(f\"PDF and metadata files created successfully inside: {base_dir}\") Navigate to the folder and run the script: python 1_create_files.py Upload the Content and Metadata to MongoDB Save the following script as 2_upload_to_mongodb.py : from pymongo import MongoClient import json import gridfs import os # Connect to MongoDB client = MongoClient('mongodb://localhost:27017/') db = client['cms_db'] fs = gridfs.GridFS(db) # Directory path base_dir = os.getcwd() # Get the current working directory # base_dir = r'C:\\Users\\dwaip\\Desktop\\mongoDB_CMS' #For any custom directory # Read and upload metadata and content files for file in os.listdir(base_dir): if file.endswith('.json'): # Read metadata file metadata_path = os.path.join(base_dir, file) with open(metadata_path, 'r') as f: metadata = json.load(f) file_name = metadata['file_name'] file_path = os.path.join(base_dir, file_name) # Check if the content file exists if os.path.exists(file_path): # Upload file to GridFS with open(file_path, 'rb') as content_file: file_id = fs.put(content_file, filename=file_name) # Add file_id to metadata metadata['file_id'] = str(file_id) # Convert ObjectId to string # Insert metadata into MongoDB db.metadata.insert_one(metadata) else: print(f\"Content file {file_name} not found. Skipping...\") print(f\"Files and metadata uploaded successfully in: {base_dir}\") Using cmd, go to the folder and run the script: python 2_upload_to_mongodb.py Create and Run the Flask Application to Search CMS Create the folder structure mongoDB_CMS\\FlaskApp\\templates and create two HTML files inside the templates folder. index.html <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <title>PDF CMS</title> </head> <body> <h1>Welcome to PDF CMS</h1> <form action=\"/search\" method=\"post\"> <input type=\"text\" name=\"query\" placeholder=\"Search for PDFs...\"> <button type=\"submit\">Search</button> </form> </body> </html> search.html <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <title>Search Results</title> </head> <body> <h1>Search Results for \"{{ query }}\"</h1> <ul> {% for result in results %} <li> <strong>{{ result['title'] }}</strong><br> {{ result['description'] }}<br> <a href=\"/download/{{ result['file_id'] }}\">Download PDF</a> </li> {% else %} <li>No results found.</li> {% endfor %} </ul> <a href=\"/\">Go back</a> </body> </html> app.py Save the following script as app.py inside mongoDB_CMS\\FlaskApp\\ : from flask import Flask, render_template, request, send_file, jsonify from pymongo import MongoClient from gridfs import GridFS from bson.objectid import ObjectId import io app = Flask(__name__) # Connect to MongoDB client = MongoClient('mongodb://localhost:27017/') db = client['cms_db'] fs = GridFS(db) # Create text index for searching db.metadata.create_index([(\"$**\", \"text\")]) @app.route('/') def index(): return render_template('index.html') @app.route('/search', methods=['GET', 'POST']) def search(): query = request.form.get('query', '') print(f\"Search query: {query}\") if query: results = db.metadata.find({\"$text\": {\"$search\": query}}) results_list = list(results) print(f\"Search results: {results_list}\") else: results_list = [] return render_template('search.html', results=results_list, query=query) @app.route('/download/<file_id>') def download(file_id): file_id = ObjectId(file_id) file_data = fs.get(file_id) return send_file(io.BytesIO(file_data.read()), download_name=file_data.filename, as_attachment=True) @app.route('/api/search', methods=['POST']) def api_search(): query = request.json.get('query', '') results = db.metadata.find({\"$text\": {\"$search\": query}}) return jsonify([result for result in results]) if __name__ == '__main__': app.run(debug=True) Using CMD go the to the mongoDB_CMS\\FlaskApp\\ folder and run the Flask application: python app.py Use the search application Navigate to http://127.0.0.1:5000/ to access the application. You will see a search interface where you can search for PDF documents. The search results will display the documents with a link to download the PDFs. Appendix How the pdf is uploaded and linked with the metadata The script iterates over .json files in the directory, reading each metadata file. python for file in os.listdir(base_dir): if file.endswith('.json'): with open(os.path.join(base_dir, file), 'r') as f: metadata = json.load(f) - It extracts the file_name from the metadata to locate the PDF file. python file_name = metadata['file_name'] file_path = os.path.join(base_dir, file_name) - If the PDF file exists, it is uploaded to GridFS, generating a unique file_id . python if os.path.exists(file_path): with open(file_path, 'rb') as content_file: file_id = fs.put(content_file, filename=file_name) - The file_id is added to the metadata and stored in the metadata collection in MongoDB. python metadata['file_id'] = str(file_id) db.metadata.insert_one(metadata) How MongoDB Stores Files and Metadata Files : Files are stored in MongoDB using GridFS, which breaks each file into 255 KB chunks. These chunks are stored as separate documents in the fs.chunks collection. The fs.files collection stores information about each file, like the filename, upload date, file length, and references to the chunks that make up the file. Metadata : Metadata for each PDF is stored in a separate collection called cms_db.metadata . This includes details like the title, description, and filename of the PDF. Each metadata document has a file_id field, which stores the ObjectId of the corresponding file in GridFS. This file_id links the metadata to the actual file, allowing the application to find and serve the file based on user requests. Default Metadata (GridFS Metadata) GridFS automatically handles the basic information about each file. It stores files in chunks of 255 KB in the fs.chunks collection. The fs.files collection holds metadata like the filename, upload date, file length, chunk size, MD5 hash for checking file integrity, and references to the file chunks. This ensures that files are stored and retrieved correctly. Custom Metadata Apart from the default metadata, custom metadata is stored in a separate collection (e.g., cms_db.metadata ). This includes details specific to the application, such as the PDF title, description, and the user-friendly filename. Each custom metadata document also has a file_id field that links to the corresponding file in GridFS. This makes it easy for the application to find and serve the file when needed. COnclusion: Using MongoDB for Enterprise Content Management can be a good choice if you have specific needs, but traditional ECM systems are often better for several reasons. Traditional ECM systems like SharePoint, Alfresco, and Documentum come with built-in features such as document libraries, version control, metadata management, workflow automation, compliance tools, and user access controls. MongoDB, on the other hand, does not offer these features out-of-the-box. Additionally, ECM systems have user-friendly interfaces designed for document management, requiring less customization compared to MongoDB. ECM systems also help meet regulatory requirements for document retention, data privacy, and security by providing built-in tools for audit trails, access controls, and document lifecycle management. They offer strong governance features to manage documents according to company policies. Moreover, ECM solutions integrate smoothly with other enterprise applications like Microsoft 365 and ERP systems, whereas MongoDB requires more development effort for similar integration. ECM systems come with ready-to-use APIs and connectors, reducing the need for custom development. Another advantage of ECM systems is their quick deployment with minimal setup effort. Using MongoDB would require significant custom development to achieve the same functionalities. Finally, ECM solutions come with support and maintenance services to keep the system up-to-date and secure.","title":"CMS using MongoDB"},{"location":"DE-Projects/Project_MongoCMS/#building-a-simple-cms-with-mongodb-python-and-flask","text":"Usually, we use Content Management Software like IBM Filenet or OpenText Documentum to create content warehouses. These applications store content (PDFs, JPEGs) as objects and save the associated metadata in databases like MSSQL or Oracle. They offer front-end applications like Webtop and Documentum Administrator to access the files. With MongoDB, you can create a similar application almost in no time. This article will show you how easy it is to build such an app. We will use MongoDB as a container running in Docker. Our application for generating PDFs and metadata, as well as the uploading part, will run on a local Windows machine. We'll use Python 3.11 to develop the application. The front end will be created using Flask.","title":"Building a Simple CMS with MongoDB, Python, and Flask"},{"location":"DE-Projects/Project_MongoCMS/#project-background","text":"The project can be divided into the following parts: Creating the MongoDB container Creating a script to generate PDFs and associated metadata in JSON Uploading the PDFs to MongoDB Creating a front-end UI using Flask to search the content in MongoDB","title":"Project Background"},{"location":"DE-Projects/Project_MongoCMS/#create-a-mongodb-container","text":"First, create a Docker network and volume: docker network create spark-network docker volume create mongo_data Execute the following command to run the MongoDB container: docker run -d --name mongoDB --network spark-network -p 27017:27017 -v mongo-data:/data/db mongo Explanation of the command options: - -d : Run the container in detached mode. - --name mongoDB : Assign the name \"mongoDB\" to the container. - --network spark-network : Connect the container to the network \"spark-network\". - -p 27017:27017 : Map port 27017 on the host to port 27017 in the container. - -v mongo-data:/data/db : Use a Docker volume named \"mongo-data\" to persist MongoDB data. You can connect to MongoDB using a MongoDB client or management tool like MongoDB Compass, Robo 3T, or Studio 3T. The connection string is: mongodb://localhost:27017","title":"Create a MongoDB Container"},{"location":"DE-Projects/Project_MongoCMS/#creating-sample-pdfs-and-metadata-files","text":"Create the folder structure mongoDB_CMS\\content_and_metadata and place the script files inside it. Note: Before running the scripts, install the required Python packages using command prompt/terminal: **pip install fpdf pymongo**","title":"Creating Sample PDFs and Metadata Files"},{"location":"DE-Projects/Project_MongoCMS/#script-to-create-metadata-and-pdf-files","text":"Save the following script as 1_create_files.py : from fpdf import FPDF import json import os # Base directory to save content and metadata files inside content_and_metadata folder #base_dir = os.path.join(os.getcwd(), 'content_and_metadata') #os.makedirs(base_dir, exist_ok=True) # Base directory to save content and metadata files inside a particular folder # base_dir = r'C:\\Users\\dwaip\\Desktop\\mongoDB_CMS' # Base directory, to save the content and metadata in the current folder base_dir = os.getcwd() # Get the current working directory # Generate 10 PDF files and corresponding metadata files metadata_list = [ {\"title\": f\"Document {i}\", \"description\": f\"Description for document {i}\", \"file_name\": f\"document{i}.pdf\"} for i in range(1, 11) ] for i, metadata in enumerate(metadata_list, 1): # Create PDF file pdf = FPDF() pdf.add_page() pdf.set_font(\"Arial\", size=12) pdf.cell(200, 10, txt=f\"Document {i}\", ln=True, align='C') pdf.cell(200, 10, txt=\"This is a sample PDF file.\", ln=True, align='C') pdf_file_path = os.path.join(base_dir, f\"document{i}.pdf\") pdf.output(pdf_file_path) # Create metadata file metadata_file_path = os.path.join(base_dir, f'metadata{i}.json') with open(metadata_file_path, 'w') as f: json.dump(metadata, f, indent=4) print(f\"PDF and metadata files created successfully inside: {base_dir}\") Navigate to the folder and run the script: python 1_create_files.py","title":"Script to Create Metadata and PDF Files"},{"location":"DE-Projects/Project_MongoCMS/#upload-the-content-and-metadata-to-mongodb","text":"Save the following script as 2_upload_to_mongodb.py : from pymongo import MongoClient import json import gridfs import os # Connect to MongoDB client = MongoClient('mongodb://localhost:27017/') db = client['cms_db'] fs = gridfs.GridFS(db) # Directory path base_dir = os.getcwd() # Get the current working directory # base_dir = r'C:\\Users\\dwaip\\Desktop\\mongoDB_CMS' #For any custom directory # Read and upload metadata and content files for file in os.listdir(base_dir): if file.endswith('.json'): # Read metadata file metadata_path = os.path.join(base_dir, file) with open(metadata_path, 'r') as f: metadata = json.load(f) file_name = metadata['file_name'] file_path = os.path.join(base_dir, file_name) # Check if the content file exists if os.path.exists(file_path): # Upload file to GridFS with open(file_path, 'rb') as content_file: file_id = fs.put(content_file, filename=file_name) # Add file_id to metadata metadata['file_id'] = str(file_id) # Convert ObjectId to string # Insert metadata into MongoDB db.metadata.insert_one(metadata) else: print(f\"Content file {file_name} not found. Skipping...\") print(f\"Files and metadata uploaded successfully in: {base_dir}\") Using cmd, go to the folder and run the script: python 2_upload_to_mongodb.py","title":"Upload the Content and Metadata to MongoDB"},{"location":"DE-Projects/Project_MongoCMS/#create-and-run-the-flask-application-to-search-cms","text":"Create the folder structure mongoDB_CMS\\FlaskApp\\templates and create two HTML files inside the templates folder.","title":"Create and Run the Flask Application to Search CMS"},{"location":"DE-Projects/Project_MongoCMS/#indexhtml","text":"<!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <title>PDF CMS</title> </head> <body> <h1>Welcome to PDF CMS</h1> <form action=\"/search\" method=\"post\"> <input type=\"text\" name=\"query\" placeholder=\"Search for PDFs...\"> <button type=\"submit\">Search</button> </form> </body> </html>","title":"index.html"},{"location":"DE-Projects/Project_MongoCMS/#searchhtml","text":"<!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <title>Search Results</title> </head> <body> <h1>Search Results for \"{{ query }}\"</h1> <ul> {% for result in results %} <li> <strong>{{ result['title'] }}</strong><br> {{ result['description'] }}<br> <a href=\"/download/{{ result['file_id'] }}\">Download PDF</a> </li> {% else %} <li>No results found.</li> {% endfor %} </ul> <a href=\"/\">Go back</a> </body> </html>","title":"search.html"},{"location":"DE-Projects/Project_MongoCMS/#apppy","text":"Save the following script as app.py inside mongoDB_CMS\\FlaskApp\\ : from flask import Flask, render_template, request, send_file, jsonify from pymongo import MongoClient from gridfs import GridFS from bson.objectid import ObjectId import io app = Flask(__name__) # Connect to MongoDB client = MongoClient('mongodb://localhost:27017/') db = client['cms_db'] fs = GridFS(db) # Create text index for searching db.metadata.create_index([(\"$**\", \"text\")]) @app.route('/') def index(): return render_template('index.html') @app.route('/search', methods=['GET', 'POST']) def search(): query = request.form.get('query', '') print(f\"Search query: {query}\") if query: results = db.metadata.find({\"$text\": {\"$search\": query}}) results_list = list(results) print(f\"Search results: {results_list}\") else: results_list = [] return render_template('search.html', results=results_list, query=query) @app.route('/download/<file_id>') def download(file_id): file_id = ObjectId(file_id) file_data = fs.get(file_id) return send_file(io.BytesIO(file_data.read()), download_name=file_data.filename, as_attachment=True) @app.route('/api/search', methods=['POST']) def api_search(): query = request.json.get('query', '') results = db.metadata.find({\"$text\": {\"$search\": query}}) return jsonify([result for result in results]) if __name__ == '__main__': app.run(debug=True) Using CMD go the to the mongoDB_CMS\\FlaskApp\\ folder and run the Flask application: python app.py","title":"app.py"},{"location":"DE-Projects/Project_MongoCMS/#use-the-search-application","text":"Navigate to http://127.0.0.1:5000/ to access the application. You will see a search interface where you can search for PDF documents. The search results will display the documents with a link to download the PDFs.","title":"Use the search application"},{"location":"DE-Projects/Project_MongoCMS/#appendix","text":"","title":"Appendix"},{"location":"DE-Projects/Project_MongoCMS/#how-the-pdf-is-uploaded-and-linked-with-the-metadata","text":"The script iterates over .json files in the directory, reading each metadata file. python for file in os.listdir(base_dir): if file.endswith('.json'): with open(os.path.join(base_dir, file), 'r') as f: metadata = json.load(f) - It extracts the file_name from the metadata to locate the PDF file. python file_name = metadata['file_name'] file_path = os.path.join(base_dir, file_name) - If the PDF file exists, it is uploaded to GridFS, generating a unique file_id . python if os.path.exists(file_path): with open(file_path, 'rb') as content_file: file_id = fs.put(content_file, filename=file_name) - The file_id is added to the metadata and stored in the metadata collection in MongoDB. python metadata['file_id'] = str(file_id) db.metadata.insert_one(metadata)","title":"How the pdf is uploaded and linked with the metadata"},{"location":"DE-Projects/Project_MongoCMS/#how-mongodb-stores-files-and-metadata","text":"Files : Files are stored in MongoDB using GridFS, which breaks each file into 255 KB chunks. These chunks are stored as separate documents in the fs.chunks collection. The fs.files collection stores information about each file, like the filename, upload date, file length, and references to the chunks that make up the file. Metadata : Metadata for each PDF is stored in a separate collection called cms_db.metadata . This includes details like the title, description, and filename of the PDF. Each metadata document has a file_id field, which stores the ObjectId of the corresponding file in GridFS. This file_id links the metadata to the actual file, allowing the application to find and serve the file based on user requests.","title":"How MongoDB Stores Files and Metadata"},{"location":"DE-Projects/Project_MongoCMS/#default-metadata-gridfs-metadata","text":"GridFS automatically handles the basic information about each file. It stores files in chunks of 255 KB in the fs.chunks collection. The fs.files collection holds metadata like the filename, upload date, file length, chunk size, MD5 hash for checking file integrity, and references to the file chunks. This ensures that files are stored and retrieved correctly.","title":"Default Metadata (GridFS Metadata)"},{"location":"DE-Projects/Project_MongoCMS/#custom-metadata","text":"Apart from the default metadata, custom metadata is stored in a separate collection (e.g., cms_db.metadata ). This includes details specific to the application, such as the PDF title, description, and the user-friendly filename. Each custom metadata document also has a file_id field that links to the corresponding file in GridFS. This makes it easy for the application to find and serve the file when needed.","title":"Custom Metadata"},{"location":"DE-Projects/Project_MongoCMS/#conclusion","text":"Using MongoDB for Enterprise Content Management can be a good choice if you have specific needs, but traditional ECM systems are often better for several reasons. Traditional ECM systems like SharePoint, Alfresco, and Documentum come with built-in features such as document libraries, version control, metadata management, workflow automation, compliance tools, and user access controls. MongoDB, on the other hand, does not offer these features out-of-the-box. Additionally, ECM systems have user-friendly interfaces designed for document management, requiring less customization compared to MongoDB. ECM systems also help meet regulatory requirements for document retention, data privacy, and security by providing built-in tools for audit trails, access controls, and document lifecycle management. They offer strong governance features to manage documents according to company policies. Moreover, ECM solutions integrate smoothly with other enterprise applications like Microsoft 365 and ERP systems, whereas MongoDB requires more development effort for similar integration. ECM systems come with ready-to-use APIs and connectors, reducing the need for custom development. Another advantage of ECM systems is their quick deployment with minimal setup effort. Using MongoDB would require significant custom development to achieve the same functionalities. Finally, ECM solutions come with support and maintenance services to keep the system up-to-date and secure.","title":"COnclusion:"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint/","text":"Setting Up SSRS Reports in SharePoint for a Client with 100+ Field Offices Background The company has over 100 field offices, from which data is collected daily using different ETL workflows. Each workflow represents a specific lending process. The data includes both documents and metadata. The operational documents are stored in a Document warehouse, while the related metadata is kept in MSSQL tables. The analytical data for reporting is stored in MSSQL tables. At the end of each month, the company generates reports to ensure they include the latest data. Goal Create an end-to-end workflow using SSRS and SharePoint to automate the report generation process using the latest data from the MSSQL Server. Step-by-Step Guide MSSQL Database Design First, the tables for storing the analytical data were created. Each table represented one process like: - CustomerApplication - CreditEvaluation - LoanApproval - Disbursement - RepaymentTracking There were over 15 such processes. Creation of ETL Processes Field office staff captured data at their office and pushed it to a shared network folder. For each batch, one JSON file was created. This contained the metadata which needed to be flattened and stored in MSSQL. For this, SSIS was used. Configuring SharePoint Under the root site for Loan Origination and Processing, a subsite was created for managing the reports: https://<root_site_for_Loan_Accounting>/sites/MonthlyReports . Setting Up SSRS in SharePoint SQL Server Reporting Services (SSRS) was installed in SharePoint integrated mode on the SharePoint farm. SSRS was configured to run with our SharePoint instance. Creation of Report Libraries in SharePoint Report libraries were created in the SharePoint site's Site Contents. Following is an overview of the steps: Go to your SharePoint site. Click on the Settings symbol (gear icon) and select \"Site Contents.\" Click \"Add an app\" and search for \"Report Library.\" Name the app (e.g., \"MonthlyReport1_LAC\") and click \"Create.\" Click on the three dots (...) next to your Report Library and select \"App Settings.\" Click \"Add from existing site content types\" and add the \"SQL Server Reporting Service\" content type. Setting Up Data Sources and Datasets in SSRS After creating the report libraries, we created the required data sources. Following is an overview of the steps: Create Data Source In your Report Library, create a new data source. Enter the connection string to your MSSQL database, including server name, database name, and authentication details. Example: Server=myServerAddress;Database=myDataBase;User Id=myUsername;Password=myPassword; Create Datasets Use the created data source to define datasets that will pull the required data from your MSSQL tables. Example query for dataset: sql SELECT * FROM Disbursement WHERE Date BETWEEN @StartDate AND @EndDate Set parameters ( @StartDate , @EndDate ) to filter data for the month. Designing and Deploying Reports Once the setup was done, we collaborated with higher management for the report designing. SSDT was used for designing. Here is an overview of the steps: Design Reports Using Report Builder or SSDT Open Report Builder or SQL Server Data Tools (SSDT). Create a new report, connect it to the data source and dataset created earlier. Design the layout of the report (tables, charts, etc.). Deploy Reports to SharePoint Save the report as an RDL file. Upload the RDL file to the Report Library in SharePoint. Scheduling and Automation After designing and creating the reports, we scheduled them so that they automatically reach the designated recipients every month. Following is an overview of the steps: Set Up Report Subscriptions - In SharePoint, go to the Report Library and find the uploaded report. - Set up a subscription to automatically generate the report at the end of each month. - Configure the subscription to deliver the report to specific users via email or save it to a document library. Access and Permissions We provided necessary permissions to the libraries and subsites so that the reports are not accessible to everyone, only to the people maintaining or upgrading them in the future. Also, some users were given rights to navigate to the SharePoint site and go to the Report Library to view the latest reports directly.","title":"Project - ETL and Reporting Lending Org"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint/#setting-up-ssrs-reports-in-sharepoint-for-a-client-with-100-field-offices","text":"","title":"Setting Up SSRS Reports in SharePoint for a Client with 100+ Field Offices"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint/#background","text":"The company has over 100 field offices, from which data is collected daily using different ETL workflows. Each workflow represents a specific lending process. The data includes both documents and metadata. The operational documents are stored in a Document warehouse, while the related metadata is kept in MSSQL tables. The analytical data for reporting is stored in MSSQL tables. At the end of each month, the company generates reports to ensure they include the latest data.","title":"Background"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint/#goal","text":"Create an end-to-end workflow using SSRS and SharePoint to automate the report generation process using the latest data from the MSSQL Server.","title":"Goal"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint/#step-by-step-guide","text":"","title":"Step-by-Step Guide"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint/#mssql-database-design","text":"First, the tables for storing the analytical data were created. Each table represented one process like: - CustomerApplication - CreditEvaluation - LoanApproval - Disbursement - RepaymentTracking There were over 15 such processes.","title":"MSSQL Database Design"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint/#creation-of-etl-processes","text":"Field office staff captured data at their office and pushed it to a shared network folder. For each batch, one JSON file was created. This contained the metadata which needed to be flattened and stored in MSSQL. For this, SSIS was used.","title":"Creation of ETL Processes"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint/#configuring-sharepoint","text":"Under the root site for Loan Origination and Processing, a subsite was created for managing the reports: https://<root_site_for_Loan_Accounting>/sites/MonthlyReports .","title":"Configuring SharePoint"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint/#setting-up-ssrs-in-sharepoint","text":"SQL Server Reporting Services (SSRS) was installed in SharePoint integrated mode on the SharePoint farm. SSRS was configured to run with our SharePoint instance.","title":"Setting Up SSRS in SharePoint"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint/#creation-of-report-libraries-in-sharepoint","text":"Report libraries were created in the SharePoint site's Site Contents. Following is an overview of the steps: Go to your SharePoint site. Click on the Settings symbol (gear icon) and select \"Site Contents.\" Click \"Add an app\" and search for \"Report Library.\" Name the app (e.g., \"MonthlyReport1_LAC\") and click \"Create.\" Click on the three dots (...) next to your Report Library and select \"App Settings.\" Click \"Add from existing site content types\" and add the \"SQL Server Reporting Service\" content type.","title":"Creation of Report Libraries in SharePoint"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint/#setting-up-data-sources-and-datasets-in-ssrs","text":"After creating the report libraries, we created the required data sources. Following is an overview of the steps: Create Data Source In your Report Library, create a new data source. Enter the connection string to your MSSQL database, including server name, database name, and authentication details. Example: Server=myServerAddress;Database=myDataBase;User Id=myUsername;Password=myPassword; Create Datasets Use the created data source to define datasets that will pull the required data from your MSSQL tables. Example query for dataset: sql SELECT * FROM Disbursement WHERE Date BETWEEN @StartDate AND @EndDate Set parameters ( @StartDate , @EndDate ) to filter data for the month.","title":"Setting Up Data Sources and Datasets in SSRS"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint/#designing-and-deploying-reports","text":"Once the setup was done, we collaborated with higher management for the report designing. SSDT was used for designing. Here is an overview of the steps: Design Reports Using Report Builder or SSDT Open Report Builder or SQL Server Data Tools (SSDT). Create a new report, connect it to the data source and dataset created earlier. Design the layout of the report (tables, charts, etc.). Deploy Reports to SharePoint Save the report as an RDL file. Upload the RDL file to the Report Library in SharePoint.","title":"Designing and Deploying Reports"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint/#scheduling-and-automation","text":"After designing and creating the reports, we scheduled them so that they automatically reach the designated recipients every month. Following is an overview of the steps: Set Up Report Subscriptions - In SharePoint, go to the Report Library and find the uploaded report. - Set up a subscription to automatically generate the report at the end of each month. - Configure the subscription to deliver the report to specific users via email or save it to a document library.","title":"Scheduling and Automation"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint/#access-and-permissions","text":"We provided necessary permissions to the libraries and subsites so that the reports are not accessible to everyone, only to the people maintaining or upgrading them in the future. Also, some users were given rights to navigate to the SharePoint site and go to the Report Library to view the latest reports directly.","title":"Access and Permissions"},{"location":"DE-Projects/Raw-Json-To-Hive/","text":"","title":"Raw Json To Hive"},{"location":"DE-Projects/SPY_ETF_Buy_Recommender/","text":"Requirement Document: ETF SPY Recommendation App Objective: I want to create an app that provides buy or not recommendations specifically for the ETF SPY, based on real-time data analysis and AI-driven insights. Key Requirements: Data Sources: ETF SPY Data : The app should collect historical and real-time data for SPY, including prices, trading volume, dividend history, and expense ratios. Market Sentiment Data : The app should gather sentiment data from financial news, social media platforms like Twitter, and forums like Reddit to gauge market mood towards SPY. Macroeconomic Data : The app should also consider macroeconomic indicators like interest rates, inflation, and GDP growth that could impact SPY\u2019s performance. Data Processing: Feature Engineering : Extract relevant features from the data, such as moving averages, RSI, sentiment scores, and other technical indicators. Data Storage : The app should store collected data in a database, ensuring it is updated in real-time. AI Model Development: Model Selection : The app should use machine learning models, like Random Forests or Neural Networks, to analyze the data and make predictions. Training and Evaluation : The model should be trained on historical data and regularly updated with new data. It should also be evaluated for accuracy using backtesting techniques. Recommendation Logic: Buy or Not Decision : Based on the AI model\u2019s predictions and market sentiment analysis, the app should provide a simple buy or not recommendation. User Personalization : Allow users to input their risk tolerance and investment goals, which will tailor the recommendations. User Interface: Dashboard : The app should have a clean and user-friendly dashboard displaying SPY\u2019s current status, AI-driven recommendations, and key metrics. Alerts : Users should receive notifications or alerts when a significant change in recommendation occurs. Deployment: Cloud Hosting : The app should be hosted on a cloud platform like AWS or Azure to ensure scalability and reliability. Real-Time Updates : The app should integrate with data sources via APIs to provide real-time updates and recommendations. Solutioning Document: ETF SPY Recommendation App 1. Data Collection: APIs : I will use Yahoo Finance API for fetching SPY\u2019s financial data, including prices, volume, and dividends. For market sentiment, I\u2019ll use the Twitter API and NewsAPI to gather social media and news sentiment on SPY. Data Pipeline : I\u2019ll set up a data pipeline using Apache Kafka to stream real-time data from these sources into my app\u2019s backend. 2. Data Processing: Feature Engineering : I\u2019ll extract technical indicators like moving averages, RSI, and volatility from SPY\u2019s historical data. Sentiment analysis will be performed on the collected news and social media data using NLP tools. Database : I\u2019ll store all processed data in a PostgreSQL database, ensuring real-time updates with proper indexing for quick access. 3. AI Model Development: Model Selection : I\u2019ll start with a Random Forest model for its robustness and ability to handle different types of data. I might also experiment with LSTM networks for better time-series predictions. Training and Evaluation : I\u2019ll use historical SPY data to train the model and backtest it against past market conditions to ensure accuracy. 4. Recommendation Logic: Buy or Not Decision : The model\u2019s output, combined with sentiment scores, will feed into a decision-making algorithm. If the model predicts positive returns and sentiment is high, the app will recommend a buy; otherwise, it will advise holding off. User Personalization : I\u2019ll add options for users to set their risk levels and investment preferences, which will adjust the final recommendation accordingly. 5. User Interface: Dashboard Design : I\u2019ll create a dashboard using React for the frontend, showing SPY\u2019s current data, AI predictions, and a clear buy/not recommendation. I\u2019ll also include charts and graphs for better visualization. Alerts : I\u2019ll implement real-time notifications using Firebase Cloud Messaging to alert users when the recommendation changes. 6. Deployment: Cloud Hosting : I\u2019ll deploy the app on AWS , using services like EC2 for hosting the application and RDS for the PostgreSQL database. Real-Time Integration : The app will be connected to all data sources via APIs to ensure it provides up-to-the-minute recommendations.","title":"SPY ETF Buy Recommender"},{"location":"DE-Projects/SPY_ETF_Buy_Recommender/#requirement-document-etf-spy-recommendation-app","text":"","title":"Requirement Document: ETF SPY Recommendation App"},{"location":"DE-Projects/SPY_ETF_Buy_Recommender/#objective","text":"I want to create an app that provides buy or not recommendations specifically for the ETF SPY, based on real-time data analysis and AI-driven insights.","title":"Objective:"},{"location":"DE-Projects/SPY_ETF_Buy_Recommender/#key-requirements","text":"Data Sources: ETF SPY Data : The app should collect historical and real-time data for SPY, including prices, trading volume, dividend history, and expense ratios. Market Sentiment Data : The app should gather sentiment data from financial news, social media platforms like Twitter, and forums like Reddit to gauge market mood towards SPY. Macroeconomic Data : The app should also consider macroeconomic indicators like interest rates, inflation, and GDP growth that could impact SPY\u2019s performance. Data Processing: Feature Engineering : Extract relevant features from the data, such as moving averages, RSI, sentiment scores, and other technical indicators. Data Storage : The app should store collected data in a database, ensuring it is updated in real-time. AI Model Development: Model Selection : The app should use machine learning models, like Random Forests or Neural Networks, to analyze the data and make predictions. Training and Evaluation : The model should be trained on historical data and regularly updated with new data. It should also be evaluated for accuracy using backtesting techniques. Recommendation Logic: Buy or Not Decision : Based on the AI model\u2019s predictions and market sentiment analysis, the app should provide a simple buy or not recommendation. User Personalization : Allow users to input their risk tolerance and investment goals, which will tailor the recommendations. User Interface: Dashboard : The app should have a clean and user-friendly dashboard displaying SPY\u2019s current status, AI-driven recommendations, and key metrics. Alerts : Users should receive notifications or alerts when a significant change in recommendation occurs. Deployment: Cloud Hosting : The app should be hosted on a cloud platform like AWS or Azure to ensure scalability and reliability. Real-Time Updates : The app should integrate with data sources via APIs to provide real-time updates and recommendations.","title":"Key Requirements:"},{"location":"DE-Projects/SPY_ETF_Buy_Recommender/#solutioning-document-etf-spy-recommendation-app","text":"","title":"Solutioning Document: ETF SPY Recommendation App"},{"location":"DE-Projects/SPY_ETF_Buy_Recommender/#1-data-collection","text":"APIs : I will use Yahoo Finance API for fetching SPY\u2019s financial data, including prices, volume, and dividends. For market sentiment, I\u2019ll use the Twitter API and NewsAPI to gather social media and news sentiment on SPY. Data Pipeline : I\u2019ll set up a data pipeline using Apache Kafka to stream real-time data from these sources into my app\u2019s backend.","title":"1. Data Collection:"},{"location":"DE-Projects/SPY_ETF_Buy_Recommender/#2-data-processing","text":"Feature Engineering : I\u2019ll extract technical indicators like moving averages, RSI, and volatility from SPY\u2019s historical data. Sentiment analysis will be performed on the collected news and social media data using NLP tools. Database : I\u2019ll store all processed data in a PostgreSQL database, ensuring real-time updates with proper indexing for quick access.","title":"2. Data Processing:"},{"location":"DE-Projects/SPY_ETF_Buy_Recommender/#3-ai-model-development","text":"Model Selection : I\u2019ll start with a Random Forest model for its robustness and ability to handle different types of data. I might also experiment with LSTM networks for better time-series predictions. Training and Evaluation : I\u2019ll use historical SPY data to train the model and backtest it against past market conditions to ensure accuracy.","title":"3. AI Model Development:"},{"location":"DE-Projects/SPY_ETF_Buy_Recommender/#4-recommendation-logic","text":"Buy or Not Decision : The model\u2019s output, combined with sentiment scores, will feed into a decision-making algorithm. If the model predicts positive returns and sentiment is high, the app will recommend a buy; otherwise, it will advise holding off. User Personalization : I\u2019ll add options for users to set their risk levels and investment preferences, which will adjust the final recommendation accordingly.","title":"4. Recommendation Logic:"},{"location":"DE-Projects/SPY_ETF_Buy_Recommender/#5-user-interface","text":"Dashboard Design : I\u2019ll create a dashboard using React for the frontend, showing SPY\u2019s current data, AI predictions, and a clear buy/not recommendation. I\u2019ll also include charts and graphs for better visualization. Alerts : I\u2019ll implement real-time notifications using Firebase Cloud Messaging to alert users when the recommendation changes.","title":"5. User Interface:"},{"location":"DE-Projects/SPY_ETF_Buy_Recommender/#6-deployment","text":"Cloud Hosting : I\u2019ll deploy the app on AWS , using services like EC2 for hosting the application and RDS for the PostgreSQL database. Real-Time Integration : The app will be connected to all data sources via APIs to ensure it provides up-to-the-minute recommendations.","title":"6. Deployment:"},{"location":"DE-Projects/AzureSkyWeather/HomeProjectAzureSkyWeather/","text":"Overview Objective: Our goal is to develop an end-to-end data engineering solution using the Microsoft technology stack. We're mainly aiming to create a future-proof and efficient storage solution to store weather data sourced from web APIs for data science and analytics, . Project Components and Workflow: Data Ingestion Techniques: Step1A: Here, we'll use an HTTP-triggered Azure Function to pull in data from a Web API. The whole activity will be scheduled via Azure Logic Apps. Step1B: We'll also see how a timer-triggered Azure Function can do the same job, making additional services unnecessary. Ways to Sort Data: Step2A: We'll learn how to organise the weather files into directories like year=yy/month=mm/day=dd/hour=h using Local Spark and Hadoop Jars. Step2B: As an alternative, we'll use Python combined with Azure's Python SDK. Plus, I'll show how this can be set up in a timer-based Azure Function for regular execution. Cleaning and Transforming Data: Step3A: Let's explore data cleaning and its conversion to parquet format using standalone Spark and Hadoop jars. Step3B: We'll switch to Azure Databricks to see its advantages and how it simplifies our work. Advanced Processing: The following steps will dive deeper into data science and analytics tasks. Thought Behind This: Nowadays, many professionals prefer tools like Azure Data Factory, Azure Databricks, and Azure Synapse Analytics because of their all-in-one solutions. But, through this project, my idea is to show alternate methods. By understanding various alternatives, will strengthen your knowledge of data in the microsoft domain. Also knowing many methods will make you more confident in your choce of products. Step 1A: Data ingestion using Azure HTTP-Triggered Function And Azure Logic Apps This is the first step of our project, focusing on data ingestion. In this segment, I'll show how we can fetch data from an API using an HTTP-triggered Azure Function, developed via VS Code. To schedule our function, we'll make use of Azure Logic Apps. By the end of this section, you'll have a comprehensive understanding of creating Azure functions. Read more... Step 1B: Data ingestion using Just Azure Timer-Triggered Function Now, I'll show a different way. We'll use a Timer-Triggered Azure function that already has a built-in timer. This is another option. Choosing the best way can depend on things like cost. Azure Logic Apps can cost more than Timer-Triggered functions. I've kept this article short because many steps for both functions are the same. Read more... Step 2A: Organize the weather files by timestamp using Plain Python Here, I will show you how can can organize the weather files into directories like year=yy/month=mm/day=dd/hour=h using just Plain python. The code we will create can be easily incoporated into a Timer-trigger Azure function. That way we can schedule it to sort our files at a definite time. Read more... Step 2B: Organize the weather files by timestamp using Spark and Hadoop Now, I will show you how do the same using Standalone Spark and Hadoop Jars. This will give you a good idea about connecting using Spark with Hadoop Jars to work with Azure blob storage Read more... \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Project AzureSkyWeather"},{"location":"DE-Projects/AzureSkyWeather/HomeProjectAzureSkyWeather/#overview","text":"Objective: Our goal is to develop an end-to-end data engineering solution using the Microsoft technology stack. We're mainly aiming to create a future-proof and efficient storage solution to store weather data sourced from web APIs for data science and analytics, . Project Components and Workflow: Data Ingestion Techniques: Step1A: Here, we'll use an HTTP-triggered Azure Function to pull in data from a Web API. The whole activity will be scheduled via Azure Logic Apps. Step1B: We'll also see how a timer-triggered Azure Function can do the same job, making additional services unnecessary. Ways to Sort Data: Step2A: We'll learn how to organise the weather files into directories like year=yy/month=mm/day=dd/hour=h using Local Spark and Hadoop Jars. Step2B: As an alternative, we'll use Python combined with Azure's Python SDK. Plus, I'll show how this can be set up in a timer-based Azure Function for regular execution. Cleaning and Transforming Data: Step3A: Let's explore data cleaning and its conversion to parquet format using standalone Spark and Hadoop jars. Step3B: We'll switch to Azure Databricks to see its advantages and how it simplifies our work. Advanced Processing: The following steps will dive deeper into data science and analytics tasks. Thought Behind This: Nowadays, many professionals prefer tools like Azure Data Factory, Azure Databricks, and Azure Synapse Analytics because of their all-in-one solutions. But, through this project, my idea is to show alternate methods. By understanding various alternatives, will strengthen your knowledge of data in the microsoft domain. Also knowing many methods will make you more confident in your choce of products.","title":"Overview"},{"location":"DE-Projects/AzureSkyWeather/HomeProjectAzureSkyWeather/#step-1a-data-ingestion-using-azure-http-triggered-function-and-azure-logic-apps","text":"This is the first step of our project, focusing on data ingestion. In this segment, I'll show how we can fetch data from an API using an HTTP-triggered Azure Function, developed via VS Code. To schedule our function, we'll make use of Azure Logic Apps. By the end of this section, you'll have a comprehensive understanding of creating Azure functions. Read more...","title":"Step 1A: Data ingestion using Azure HTTP-Triggered Function And Azure Logic Apps"},{"location":"DE-Projects/AzureSkyWeather/HomeProjectAzureSkyWeather/#step-1b-data-ingestion-using-just-azure-timer-triggered-function","text":"Now, I'll show a different way. We'll use a Timer-Triggered Azure function that already has a built-in timer. This is another option. Choosing the best way can depend on things like cost. Azure Logic Apps can cost more than Timer-Triggered functions. I've kept this article short because many steps for both functions are the same. Read more...","title":"Step 1B: Data ingestion using Just Azure Timer-Triggered Function"},{"location":"DE-Projects/AzureSkyWeather/HomeProjectAzureSkyWeather/#step-2a-organize-the-weather-files-by-timestamp-using-plain-python","text":"Here, I will show you how can can organize the weather files into directories like year=yy/month=mm/day=dd/hour=h using just Plain python. The code we will create can be easily incoporated into a Timer-trigger Azure function. That way we can schedule it to sort our files at a definite time. Read more...","title":"Step 2A: Organize the weather files by timestamp using Plain Python"},{"location":"DE-Projects/AzureSkyWeather/HomeProjectAzureSkyWeather/#step-2b-organize-the-weather-files-by-timestamp-using-spark-and-hadoop","text":"Now, I will show you how do the same using Standalone Spark and Hadoop Jars. This will give you a good idea about connecting using Spark with Hadoop Jars to work with Azure blob storage Read more... \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Step 2B: Organize the weather files by timestamp using Spark and Hadoop"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/","text":"Table of Contents Project AzureSkyWeather. Part 1A: Using Azure HTTP-Triggered Function Overview Choice of Azure Services For Data Ingestion Let's ready the prerequisites Get the Weather API Set Up Azure Blob(ADLSG2) Storage Create the Azure Function App Prepare VS Code and local enviroment for Azure Function Development and Testing Azure Functions Extension For Visual Studio Code Python Extension for Visual Studio Code Azurite azure-functions SDK azure-storage-blob library for local testing Development and Deploymment Create the Python-based Azure Function Write our custom logic in the Azure Function Include azure-storage-blob in requirements.txt Test Our Azure Function Create Azure Function App Deploy the Azure Function To The App Schedule the Function using Azure Logic Apps Post-deployment Monitoring Common Errors \"AzureWebJobsStorage\" app setting is not present Logic Apps while trying to add azure function - No resources of this type found under this subscription Logic Apps Job shows Running status indefinately Appendix Detailed Project Summary Choice of storage. Why Azure Data Lake? Creating an Azure Functions App Using Azure Portal Azure Function VS-Code Project Folder Structure Structure of the function function_app.py. Some details of azure-functions library Azurite Extension Calculate Azure Logic Apps Cost Project AzureSkyWeather. Part 1A: Using Azure HTTP-Triggered Function Overview In this project we use an Azure HTTP Function to get the current weather data from weatherapi.com . Developed in Visual Studio Code using the Python V2 programming model , it stores data as hourly-named JSON files in Azure Data Lake . It's HTTP-triggered , with Azure Logic Apps managing the periodic fetch schedule. The function logs activities and handles errors, and if provided a 'name' in a request, it sends a personalized greeting . For a detailed overview, please see the Comprehensive Project Summary in the appendix. Choice of Azure Services For Data Ingestion We developed the Azure Function in Visual Studio Code on Windows using the Python V2 programming model . The V2 model simplifies Azure Function app development by allowing scheduling, binding, and the entire logic to be written in a main Python file, contrasting with V1 which requires additional setup in a JSON file. We sourced our weather data from weatherapi.com , a cost-free option that doesn't require credit card and lets you call the api almost indefinately. HTTP-triggered functions need scheduling, unlike Timer-triggered ones. We managed this with Azure Logic Apps. In terms of cost, the Azure function with a Timer-trigger is the most economical, followed by the HTTP-Triggered Azure function paired with Azure Logic App-based scheduling. For a sample cost breakdown of the Logic App, please refer to the section Calculate Azure Logic Apps Cost . Even so, for a simple hourly workflow, the expense remains minimal. It's worth noting that other options, especially Databricks, can be considerably pricier. Similarly, ADF and Synapse can also come at a higher cost. Here's a brief overview of the available options for creating an app like this: Azure Functions (Python V2): These are ideal for small to moderate workloads and straightforward data retrieval tasks. Most of the logic is self-contained within the main Python file[V2 model], making it a cost-effective, easy-to-build choice. Azure Data Factory or Azure Databricks: For more complex workflows and efficient handling of larger data volumes, Azure Data Factory and Azure Databricks offer powerful options. However, they may incur higher operational costs due to their capabilities and scalability. Azure Logic Apps: Positioned between the other options, Azure Logic Apps are suitable for uncomplicated workflows with moderate data volumes, offering a visual workflow designer for easy setup. Their cost varies based on the number of steps. While here we use Azure Functions for core logic and Logic Apps for scheduling, some opt for a single solution, like a timer-triggered Azure App or fully utilizing an Azure Logic App. Let's ready the prerequisites Get the Weather API First, create an account at weatherapi.com . Then, navigate to My Account . Copy the API key and keep it handy for later use. We chose weatherapi.com because it doesn't require sign-ups, and it offers most of the features we needed. Set Up Azure Blob(ADLSG2) Storage In the Azure portal, create a resource group, a storage account, and a blob container named weather-http . This is where the Azure Function will store the .json weather files. I've omitted the detailed steps here to focus on the main process. Remember: Enable Hierarchical Namespace . This will turn your storage into a Data Lake. Create the Azure Function App Azure Function App is a container for Azure Functions. It can be set up before or during the function deployment. I recommend deploying during development and creating a distinct Function App for each function to prevent overwriting. I will share 'run-time' deployment steps later in this tutorial. Sneak Peek: Deploying a Function App via VS Code. Prepare VS Code and local enviroment for Azure Function Development and Testing Install the following VS studio extensions: Azure Functions Extension For Visual Studio Code Python Extension for Visual Studio Code Note: This is just the extension, not the actual Python interpreter. I assume you already have Python installed. For my work, I use the Conda distribution of Python. Azurite azure-functions SDK When testing our Azure Function locally, the system will look for the azure-functions SDK/package/library in the local system. This package must be present in the Python environment you're using; otherwise, you might encounter errors like: As I use the Conda version of Python, I executed pip install azure-functions in the Anaconda prompt (with admin privileges). azure-storage-blob library for local testing As we are using Azure Blob Functions, we need to ensure that the Azure Blob Storage library/package/sdk is available during local testing . To install it locally using pip install azure-storage-blob . Development and Deploymment Create the Python-based Azure Function To create an Azure Function in Visual Studio Code, follow these steps: Open Visual Studio Code and access the Azure icon in the Activity bar. In the Workspace (local) area, click the thunder button, and select Create New Project . Choose a local folder location for our project. Select Python as the programming language. Opt for Model V2 as the programming model. Choose the Python environment that you intend to use. Make sure to select the correct environment with all the required dependencies and packages e.g. Azure Functions. Select HTTP trigger Provide a unique name for our function. VS Code will generate a complete project structure, including all the necessary components for developing our function. Write our custom logic in the Azure Function Let's dive into our weather code! Replace function_app.py with the following. Remember to fill in <your_connection_string> and <weather_api_endpoint> with the actual details. # Import necessary libraries and modules import logging, requests # logging for log management, requests to make HTTP requests from datetime import datetime # datetime for handling date and time operations import azure.functions as func # Azure functions library from azure.storage.blob import BlobServiceClient # Azure blob storage client library # Set the authentication level for the Azure function app = func.FunctionApp(http_auth_level=func.AuthLevel.FUNCTION) # Define the route for the Azure HTTP function @app.route(route=\"FetchWeatherHTTP\") def FetchWeatherHTTP(req: func.HttpRequest) -> func.HttpResponse: # Log that the function is being processed logging.info('Python HTTP trigger function processed a request.') # Generate a filename for the weather data based on the current hour. This helps organize and retrieve data efficiently. weatherFileName = datetime.now().strftime(\"%Y-%m-%d-%H.json\") # Create a connection to Azure Blob Storage using the connection string # You can obtain the connection string from the Azure Portal: Storage Account -> Access Keys -> key1 -> Connection String connection_string = \"<your_connection_string>\" blob_service_client = BlobServiceClient.from_connection_string(connection_string) blob_client = blob_service_client.get_blob_client(container=\"weather-http\", blob=weatherFileName) # Fetch weather data using the WeatherAPI endpoint. Replace 'YOUR_API_KEY' with your actual API key and 'LOCATION' with your desired location. response = requests.get(\"<weather_api_endpoint>\") # Check if the data fetch was successful. If not, log an error and return a failure response. if response.status_code != 200: logging.error('Failed to fetch weather data.') return func.HttpResponse(\"Failed to fetch weather data.\", status_code=500) # Parse the response content into JSON format data = response.json() # Try to store the fetched weather data into Azure Blob Storage. # If there's an error (e.g., network issue, permission issue), it'll log the error and return a failure response. try: blob_client.upload_blob(str(data), overwrite=True) except Exception as e: logging.error(f\"Error uploading data to Blob Storage: {e}\") return func.HttpResponse(\"Error storing weather data.\", status_code=500) # Check if there's a 'name' parameter in the request. # This is a feature that was initially scaffolded by VS Code to demonstrate how to handle request parameters. name = req.params.get('name') if not name: # If 'name' isn't in the request's parameters, check the request body. try: req_body = req.get_json() except ValueError: pass else: name = req_body.get('name') # If a name is provided, return a personalized greeting. # If not, return a generic success message. if name: return func.HttpResponse(f\"Hello, {name}. Weather data fetched and stored successfully.\") else: return func.HttpResponse( \"Weather data fetched and stored successfully. Pass a name in the query string or in the request body for a personalized response.\", status_code=200 ) Include azure-storage-blob in requirements.txt We've wrapped up the main coding. Now, just add azure-storage-blob to requirements.txt to instruct the deployment to install the library in Azure. Test Our Azure Function It's time to test our Azure Function. Follow these steps: Open Visual Studio Code and press Ctrl+Shift+P . Select Azurite: Start . This action starts the Azurite Blob Service. You can check the status at the bottom right corner of VS Code, where you should see something like this: Start debugging by pressing F5 . Then, on the left, click the Azure icon. Navigate to the workspace and locate our function (refresh if needed). Right-click the function and select Execute . If the execution is successful, you'll see an output similar to this: Additionally, a JSON file will be created in our container: Create Azure Function App For Azure Functions, we need a Function App\u2014a container for them. Instead of setting this up on the Azure Portal, I like using Visual Studio Code for its simplicity during initial deployment. Here's how to set it up with VS Code: Launch Setup in Visual Studio : Click the Azure icon, then select the thunder icon in the workspace. Choose Create Function app in Azure..(Advanced) . Comment : The icon might not always be visible; ensure your project is 'loaded' properly. Name Your Function App : Assign a unique name to your function app. Select the Right Runtime : If you're working on an Azure Function in Python, ensure you set the runtime environment to Python. Configure the resource group : Decide on using an existing resource group or create a new one. Ensure consistency in the chosen region. Choose the hosting Plan : Carefully select the hosting plan. If you're budget-conscious, consider the Consumption-based plan. Comment : Based on my experience, running our current project, 24 executions for around seven days costed me few dollars. But, you case cuold be different. Overall, I found Choose Storage account : Allocate a storage account for the Azure Function App. Using separate storage accounts for each function app simplifies the structure. Opt for Enhanced Monitoring : Incorporate an Application Insights resource for detailed insights and improved monitoring. After these steps, your Azure Function App is set up. The next phase involves deploying your Azure Function to this newly created app. Deploy the Azure Function To The App The deployment process is straightforward. In the workspace, click the thunder icon and choose Deploy to Function App . Visual Studio Code will display the Function App where you can deploy our Azure Function. Select the Function App. Click Deploy Note: This will overwrite ANY function present in the Azure Func app. After successful deployment, you will see output like the following in the console: And you can see the Function inside the Azure Function App: Schedule the Function using Azure Logic Apps Create a Logic App in the Azure Portal and name it Anything-Meaningful-Haha . Go to the Logic App Designer to design your workflow. Search for \"Recurrence\" and add this step. For the \"Interval\", enter \"1\". For \"Frequency\", select \"Hour\". Choose your time zone. Leave the \"Start time\" empty. Next, add an action. This step is crucial. We have two options: HTTP Action and HTTP Webhook Action. For this scenario, choose the HTTP Action . Here's why: HTTP Action : Use this when you simply want to call an HTTP endpoint (in your case, the Azure Function) without waiting for a prolonged period or any kind of asynchronous processing. The Logic App will receive the immediate response from the Azure Function and then proceed to the next action or finish the workflow. HTTP Webhook Action : Use this when there's a need to wait for an asynchronous process to complete before moving on in the Logic App workflow. The Logic App will pause until it receives a \"callback\" from the endpoint, signaling that the task is done. More complex and often used in scenarios where there might be a long delay or wait time between triggering an action and its completion. For our scenario, the HTTP Action is a simpler and more suitable choice. It will trigger the Azure Function, and once the Function responds (which it does after fetching and storing the weather data), the Logic App will consider the action completed. There is no need to wait for the Azure Function to complete its task and send a callback. Here is a real-world example of the output from choosing each scenario: In the HTTP Action , provide the URI and Method as GET. To obtain the URI from our Azure HTTP-Triggered Function, follow these steps: Open the Azure App hosting the Azure Function. Click the Azure Function , navigate to Overview , then at the top, click Get Function URL and copy the URL. The workflow creation is complete . Save the workflow and wait for the next hour. In the Overview section of the Logic App, under Runs history , check the runs to see if the desired action (in our case, the creation of the weather JSON in the blob storage) is being performed. Post-deployment Monitoring The purpose of this http-triggered logic-app scheduled Azure Function is to periodically fetch weather data and store it as a JSON file in an Azure Blob container. After deploying the function, you can monitor its performance and analyze invocations by following these steps: Open the Azure Function in the Azure portal. Go to the \"Monitor\" section to access detailed information about function invocations. Check the Azure Blob container to verify if the JSON files are being created as expected. Common Errors \"AzureWebJobsStorage\" app setting is not present The \"AzureWebJobsStorage\" app setting error indicates that our Azure Functions app is missing a crucial configuration related to the storage account connection string. This could also be realted to the following deployment failure message 12:23:58 PM FetchWeatherAzFunc: Deployment Log file does not exist in /tmp/oryx-build.log 12:23:58 PM FetchWeatherAzFunc: The logfile at /tmp/oryx-build.log is empty. Unable to fetch the summary of build 12:23:58 PM FetchWeatherAzFunc: Deployment Failed. deployer = ms-azuretools-vscode deploymentPath = Functions App ZipDeploy. Extract zip. Remote build. 12:24:00 PM FetchWeatherAzFunc: Deployment failed. 12:35:57 PM FetchWeatherAzFunc: Starting deployment. To resolve this: Create or Identify a Storage Account : If you don't already have an Azure Storage account, create one in the same region as our function app. Get the Storage Account Connection String : Navigate to the Azure Storage account in the Azure Portal. Under the \"Settings\" section, click on \"Access keys.\" Here, you'll find two keys (key1 and key2) each with a connection string. You can use either of these connection strings for the next step. Update Function App Settings : Navigate to our Azure Functions app in the Azure Portal. Under the \"Settings\" section, click on \"Configuration.\" In the \"Application settings\" tab, locate or create the AzureWebJobsStorage setting. Add the setting: Name: AzureWebJobsStorage Value: [our Azure Storage Account connection string from step 2] Click \"OK\" or \"Save\" to add the setting and save our changes on the main configuration page. Restart our Function App : After adding the necessary setting, restart our function app for the changes to take effect. Following these steps will resolve the error related to the \"AzureWebJobsStorage\" app setting. Logic Apps while trying to add azure function - No resources of this type found under this subscription Ideally, if you want to add an Azure Function to a Logic App, it should appear like this: However, sometimes you may encounter the main Azure App, but not the bespoke function you created. Instead, you might see an error message such as \"No resources of this type found under this subscription.\" One of the reason for this could be realted to the storage account which the Azure Function uses. There could be other reason's too like the different regions. Logic Apps Job shows Running status indefinately If your Logic App consistently displays a \"Running\" status for a prolonged duration, several factors might be at play. A common cause is when an HTTP Webhook action is used; the Logic App waits for the webhook's response. It's crucial to verify that the webhook returns a response. Otherwise, the Logic App's status remains \"Running\" until a timeout or until a response is received. A solution is to use a straightforward HTTP action . In this method, the step won't await any response; it marks itself as successful upon receiving an output. Appendix Detailed Project Summary In the 'Overview' and 'Solutions Summary Approach' sections, I've sketched out a basic outline of our project. To dive deeper into the details, refer to the content below Objective The objective of the Azure function is to fetch current weather data periodically and store it efficiently for later use. Deployment and Environment: - The function is developed using Visual Studio Code on Windows. - It utilizes the Python V2 programming model for Azure Functions, simplifying the development process. Data Retrieval: - The function sources weather data from weatherapi.com . - It fetches current weather information without the need for credit card details, with data availability spanning up to 15 days. Data Storage: - Weather data is saved as JSON files in an Azure Blob Storage container. - Filenames are generated based on the current hour, ensuring organized and chronological storage of data. Trigger Mechanism: - This Azure Function is HTTP-triggered, requiring external scheduling. - The scheduling is managed through Azure Logic Apps, allowing for regular, automated data retrieval. Error Handling and Logging: - The function contains robust logging, capturing both successful data retrievals and potential issues. - It has mechanisms in place to handle errors, especially when fetching data from the API or while uploading to Blob Storage. Response Mechanism: - Beyond its primary data retrieval and storage task, the function can provide a personalized greeting if a 'name' parameter is passed in the request. Choice of storage. Why Azure Data Lake? We've selected Azure Data Lake Storage Gen2 (ADLS G2) for storing our weather data. This decision was largely driven by its seamless integration with Azure Functions, which streamlines our data collection process. Apart form this ADLS G2 also has many other advantages, like: Hierarchical Organization : ADLS G2 allows us to neatly structure our data, similar to a directory system. We can store our weather data grouped by year, month day etc. Enhanced Security : The Active Directory permission can be applied to the folders and files. This makes the storage as secure as it can get. Analytics Performance : ADLS G2 is optimized for heavy-duty data analytics, ensuring efficient query operations over vast datasets. In the next stage, we decided to convert our JSON-formatted data into Parquet. Here's why: Analytical Efficiency : Parquet, with its columnar storage design, streamlines analytical queries, letting us access specific data without scanning the entire dataset. Storage Efficiency : Parquet compresses data effectively, optimizing storage use and potentially lowering costs. Adaptable Structure : Parquet inherently carries schema information, allowing for alterations in data structure without disrupting existing data. A noteworthy advantage of Parquet is its compatibility with various storage systems, including NoSQL databases, data warehouses, and lakehouses. This ensures that we can migrate or integrate our data effortlessly if our analytical demands change. To summarize, our storage strategy not only prepares our data for immediate analysis but we can also easily move it to other storage solution in future. Creating an Azure Functions App Using Azure Portal An Azure Functions App acts as a host for one or more Azure Functions. In essence, while an individual Azure Function is a piece of code that runs in response to an event, the Function App serves as the hosting and management unit for that function (or multiple functions). To create an Azure Function, click \"Create\" under \"Function App\". Fill in the required details, including Subscription, Resource Group, Function App name, etc. For the runtime, choose Python. Click \"Review + create\" and then \"Create\". Azure Function VS-Code Project Folder Structure Visual Studio Creates the following project structure for Azure Functions Project: The main Project Folder Contents: .venv/: (Optional) Python virtual environment for local development. .vscode/: (Optional) Visual Studio Code configuration. function_app.py: Main entry point. Default location for functions, triggers, and bindings. additional_functions.py: (Optional) Additional Python files for logical grouping and reference in function_app.py. tests/: (Optional) Test cases for our function app. .funcignore: (Optional) Declares files not to be published to Azure, often including .vscode/ for editor settings, .venv/ for local Python virtual environment, tests/ for test cases, and local.settings.json to prevent local app settings from being published. host.json: Contains configuration options affecting all functions in a function app. Gets published to Azure. local.settings.json: Stores app settings and connection strings for local development, not published to Azure. requirements.txt: Lists Python packages installed when publishing to Azure. Dockerfile: (Optional) Used for custom container publishing. Structure of the function function_app.py. The function_app.py file is the key entry point. It resides at the root directory and serves as a Python script containing the definitions of the functions within the Function App. Each function defined in function_app.py must specify several crucial elements: Function Name : It should have a unique and descriptive name. Function Trigger : This defines what activates the function. For example, it could be triggered by an HTTP request or based on a timer schedule. Function Code : This is where you place the actual logic for the function. Note, that the structure of function_app.py varies a little bit based on the type of trigger used. E.g., when creating a Timer-triggered function, a specific template code is generated VS Code. Some details of azure-functions library The command pip install azure-functions adds the Azure Functions SDK to your Python environment. This SDK facilitates local development, testing, and execution of Azure Functions. It includes tools that allow functions to interact with Azure and other services, such as responding to an HTTP request or storing data in Azure Blob Storage. Azurite Extension Azurite is mainly used for local development and testing of applications that interact with Azure Storage services. It provides an emulation of Azure Storage services, including Blob, Queue, and Table services, in a local environment. (CTRL+SHIFT+P)\"Azurite: Start\" is a command that starts the Azurite server for local debugging etc. After that you typically use F5 to start debugging. Calculate Azure Logic Apps Cost Azure Logic Apps pricing primarily hinges on the total executions and the combined action and connector executions. To illustrate, for an hourly-triggered Azure Function: The Logic App runs hourly, translating to 24 daily executions. Every execution typically involves at least two action/connector executions: one for the recurring hourly trigger and another for invoking the Azure Function. Considering rates of approximately $0.000025 per execution and $0.000125 per action or connector execution (though rates can differ based on the region or Microsoft's adjustments): Estimated Monthly Cost = 24 x 30 x (0.000025 + 2 x 0.000125) \u2248 $0.198 So, running this Azure Logic App with an hourly trigger might cost about 20 cents monthly. Yet, it's crucial to: I wrote this article in 2022. So, confirm current prices on the Azure Logic Apps pricing page . Use Azure's monitoring tools to observe your actual costs. Remember Azure's Logic Apps free tier, which offers a set number of free runs and actions monthly, potentially lowering costs if you stay within these free tier boundaries. Overall, for straightforward hourly triggers, Azure Logic Apps offer an affordable solution to schedule tasks without added coding.","title":"Part 1A - Using Azure HTTP-Triggered Function"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#table-of-contents","text":"Project AzureSkyWeather. Part 1A: Using Azure HTTP-Triggered Function Overview Choice of Azure Services For Data Ingestion Let's ready the prerequisites Get the Weather API Set Up Azure Blob(ADLSG2) Storage Create the Azure Function App Prepare VS Code and local enviroment for Azure Function Development and Testing Azure Functions Extension For Visual Studio Code Python Extension for Visual Studio Code Azurite azure-functions SDK azure-storage-blob library for local testing Development and Deploymment Create the Python-based Azure Function Write our custom logic in the Azure Function Include azure-storage-blob in requirements.txt Test Our Azure Function Create Azure Function App Deploy the Azure Function To The App Schedule the Function using Azure Logic Apps Post-deployment Monitoring Common Errors \"AzureWebJobsStorage\" app setting is not present Logic Apps while trying to add azure function - No resources of this type found under this subscription Logic Apps Job shows Running status indefinately Appendix Detailed Project Summary Choice of storage. Why Azure Data Lake? Creating an Azure Functions App Using Azure Portal Azure Function VS-Code Project Folder Structure Structure of the function function_app.py. Some details of azure-functions library Azurite Extension Calculate Azure Logic Apps Cost","title":"Table of Contents"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#project-azureskyweather-part-1a-using-azure-http-triggered-function","text":"","title":"Project AzureSkyWeather. Part 1A: Using Azure HTTP-Triggered Function"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#overview","text":"In this project we use an Azure HTTP Function to get the current weather data from weatherapi.com . Developed in Visual Studio Code using the Python V2 programming model , it stores data as hourly-named JSON files in Azure Data Lake . It's HTTP-triggered , with Azure Logic Apps managing the periodic fetch schedule. The function logs activities and handles errors, and if provided a 'name' in a request, it sends a personalized greeting . For a detailed overview, please see the Comprehensive Project Summary in the appendix.","title":"Overview"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#choice-of-azure-services-for-data-ingestion","text":"We developed the Azure Function in Visual Studio Code on Windows using the Python V2 programming model . The V2 model simplifies Azure Function app development by allowing scheduling, binding, and the entire logic to be written in a main Python file, contrasting with V1 which requires additional setup in a JSON file. We sourced our weather data from weatherapi.com , a cost-free option that doesn't require credit card and lets you call the api almost indefinately. HTTP-triggered functions need scheduling, unlike Timer-triggered ones. We managed this with Azure Logic Apps. In terms of cost, the Azure function with a Timer-trigger is the most economical, followed by the HTTP-Triggered Azure function paired with Azure Logic App-based scheduling. For a sample cost breakdown of the Logic App, please refer to the section Calculate Azure Logic Apps Cost . Even so, for a simple hourly workflow, the expense remains minimal. It's worth noting that other options, especially Databricks, can be considerably pricier. Similarly, ADF and Synapse can also come at a higher cost. Here's a brief overview of the available options for creating an app like this: Azure Functions (Python V2): These are ideal for small to moderate workloads and straightforward data retrieval tasks. Most of the logic is self-contained within the main Python file[V2 model], making it a cost-effective, easy-to-build choice. Azure Data Factory or Azure Databricks: For more complex workflows and efficient handling of larger data volumes, Azure Data Factory and Azure Databricks offer powerful options. However, they may incur higher operational costs due to their capabilities and scalability. Azure Logic Apps: Positioned between the other options, Azure Logic Apps are suitable for uncomplicated workflows with moderate data volumes, offering a visual workflow designer for easy setup. Their cost varies based on the number of steps. While here we use Azure Functions for core logic and Logic Apps for scheduling, some opt for a single solution, like a timer-triggered Azure App or fully utilizing an Azure Logic App.","title":"Choice of Azure Services For Data Ingestion"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#lets-ready-the-prerequisites","text":"","title":"Let's ready the prerequisites"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#get-the-weather-api","text":"First, create an account at weatherapi.com . Then, navigate to My Account . Copy the API key and keep it handy for later use. We chose weatherapi.com because it doesn't require sign-ups, and it offers most of the features we needed.","title":"Get the Weather API"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#set-up-azure-blobadlsg2-storage","text":"In the Azure portal, create a resource group, a storage account, and a blob container named weather-http . This is where the Azure Function will store the .json weather files. I've omitted the detailed steps here to focus on the main process. Remember: Enable Hierarchical Namespace . This will turn your storage into a Data Lake.","title":"Set Up Azure Blob(ADLSG2) Storage"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#create-the-azure-function-app","text":"Azure Function App is a container for Azure Functions. It can be set up before or during the function deployment. I recommend deploying during development and creating a distinct Function App for each function to prevent overwriting. I will share 'run-time' deployment steps later in this tutorial. Sneak Peek: Deploying a Function App via VS Code.","title":"Create the Azure Function App"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#prepare-vs-code-and-local-enviroment-for-azure-function-development-and-testing","text":"Install the following VS studio extensions:","title":"Prepare VS Code and local enviroment for Azure Function Development and Testing"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#azure-functions-extension-for-visual-studio-code","text":"","title":"Azure Functions Extension For Visual Studio Code"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#python-extension-for-visual-studio-code","text":"Note: This is just the extension, not the actual Python interpreter. I assume you already have Python installed. For my work, I use the Conda distribution of Python.","title":"Python Extension for Visual Studio Code"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#azurite","text":"","title":"Azurite"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#azure-functions-sdk","text":"When testing our Azure Function locally, the system will look for the azure-functions SDK/package/library in the local system. This package must be present in the Python environment you're using; otherwise, you might encounter errors like: As I use the Conda version of Python, I executed pip install azure-functions in the Anaconda prompt (with admin privileges).","title":"azure-functions SDK"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#azure-storage-blob-library-for-local-testing","text":"As we are using Azure Blob Functions, we need to ensure that the Azure Blob Storage library/package/sdk is available during local testing . To install it locally using pip install azure-storage-blob .","title":"azure-storage-blob library for local testing"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#development-and-deploymment","text":"","title":"Development and Deploymment"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#create-the-python-based-azure-function","text":"To create an Azure Function in Visual Studio Code, follow these steps: Open Visual Studio Code and access the Azure icon in the Activity bar. In the Workspace (local) area, click the thunder button, and select Create New Project . Choose a local folder location for our project. Select Python as the programming language. Opt for Model V2 as the programming model. Choose the Python environment that you intend to use. Make sure to select the correct environment with all the required dependencies and packages e.g. Azure Functions. Select HTTP trigger Provide a unique name for our function. VS Code will generate a complete project structure, including all the necessary components for developing our function.","title":"Create the Python-based Azure Function"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#write-our-custom-logic-in-the-azure-function","text":"Let's dive into our weather code! Replace function_app.py with the following. Remember to fill in <your_connection_string> and <weather_api_endpoint> with the actual details. # Import necessary libraries and modules import logging, requests # logging for log management, requests to make HTTP requests from datetime import datetime # datetime for handling date and time operations import azure.functions as func # Azure functions library from azure.storage.blob import BlobServiceClient # Azure blob storage client library # Set the authentication level for the Azure function app = func.FunctionApp(http_auth_level=func.AuthLevel.FUNCTION) # Define the route for the Azure HTTP function @app.route(route=\"FetchWeatherHTTP\") def FetchWeatherHTTP(req: func.HttpRequest) -> func.HttpResponse: # Log that the function is being processed logging.info('Python HTTP trigger function processed a request.') # Generate a filename for the weather data based on the current hour. This helps organize and retrieve data efficiently. weatherFileName = datetime.now().strftime(\"%Y-%m-%d-%H.json\") # Create a connection to Azure Blob Storage using the connection string # You can obtain the connection string from the Azure Portal: Storage Account -> Access Keys -> key1 -> Connection String connection_string = \"<your_connection_string>\" blob_service_client = BlobServiceClient.from_connection_string(connection_string) blob_client = blob_service_client.get_blob_client(container=\"weather-http\", blob=weatherFileName) # Fetch weather data using the WeatherAPI endpoint. Replace 'YOUR_API_KEY' with your actual API key and 'LOCATION' with your desired location. response = requests.get(\"<weather_api_endpoint>\") # Check if the data fetch was successful. If not, log an error and return a failure response. if response.status_code != 200: logging.error('Failed to fetch weather data.') return func.HttpResponse(\"Failed to fetch weather data.\", status_code=500) # Parse the response content into JSON format data = response.json() # Try to store the fetched weather data into Azure Blob Storage. # If there's an error (e.g., network issue, permission issue), it'll log the error and return a failure response. try: blob_client.upload_blob(str(data), overwrite=True) except Exception as e: logging.error(f\"Error uploading data to Blob Storage: {e}\") return func.HttpResponse(\"Error storing weather data.\", status_code=500) # Check if there's a 'name' parameter in the request. # This is a feature that was initially scaffolded by VS Code to demonstrate how to handle request parameters. name = req.params.get('name') if not name: # If 'name' isn't in the request's parameters, check the request body. try: req_body = req.get_json() except ValueError: pass else: name = req_body.get('name') # If a name is provided, return a personalized greeting. # If not, return a generic success message. if name: return func.HttpResponse(f\"Hello, {name}. Weather data fetched and stored successfully.\") else: return func.HttpResponse( \"Weather data fetched and stored successfully. Pass a name in the query string or in the request body for a personalized response.\", status_code=200 )","title":"Write our custom logic in the Azure Function"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#include-azure-storage-blob-in-requirementstxt","text":"We've wrapped up the main coding. Now, just add azure-storage-blob to requirements.txt to instruct the deployment to install the library in Azure.","title":"Include azure-storage-blob in requirements.txt"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#test-our-azure-function","text":"It's time to test our Azure Function. Follow these steps: Open Visual Studio Code and press Ctrl+Shift+P . Select Azurite: Start . This action starts the Azurite Blob Service. You can check the status at the bottom right corner of VS Code, where you should see something like this: Start debugging by pressing F5 . Then, on the left, click the Azure icon. Navigate to the workspace and locate our function (refresh if needed). Right-click the function and select Execute . If the execution is successful, you'll see an output similar to this: Additionally, a JSON file will be created in our container:","title":"Test Our Azure Function"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#create-azure-function-app","text":"For Azure Functions, we need a Function App\u2014a container for them. Instead of setting this up on the Azure Portal, I like using Visual Studio Code for its simplicity during initial deployment. Here's how to set it up with VS Code: Launch Setup in Visual Studio : Click the Azure icon, then select the thunder icon in the workspace. Choose Create Function app in Azure..(Advanced) . Comment : The icon might not always be visible; ensure your project is 'loaded' properly. Name Your Function App : Assign a unique name to your function app. Select the Right Runtime : If you're working on an Azure Function in Python, ensure you set the runtime environment to Python. Configure the resource group : Decide on using an existing resource group or create a new one. Ensure consistency in the chosen region. Choose the hosting Plan : Carefully select the hosting plan. If you're budget-conscious, consider the Consumption-based plan. Comment : Based on my experience, running our current project, 24 executions for around seven days costed me few dollars. But, you case cuold be different. Overall, I found Choose Storage account : Allocate a storage account for the Azure Function App. Using separate storage accounts for each function app simplifies the structure. Opt for Enhanced Monitoring : Incorporate an Application Insights resource for detailed insights and improved monitoring. After these steps, your Azure Function App is set up. The next phase involves deploying your Azure Function to this newly created app.","title":"Create Azure Function App"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#deploy-the-azure-function-to-the-app","text":"The deployment process is straightforward. In the workspace, click the thunder icon and choose Deploy to Function App . Visual Studio Code will display the Function App where you can deploy our Azure Function. Select the Function App. Click Deploy Note: This will overwrite ANY function present in the Azure Func app. After successful deployment, you will see output like the following in the console: And you can see the Function inside the Azure Function App:","title":"Deploy the Azure Function To The App"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#schedule-the-function-using-azure-logic-apps","text":"Create a Logic App in the Azure Portal and name it Anything-Meaningful-Haha . Go to the Logic App Designer to design your workflow. Search for \"Recurrence\" and add this step. For the \"Interval\", enter \"1\". For \"Frequency\", select \"Hour\". Choose your time zone. Leave the \"Start time\" empty. Next, add an action. This step is crucial. We have two options: HTTP Action and HTTP Webhook Action. For this scenario, choose the HTTP Action . Here's why: HTTP Action : Use this when you simply want to call an HTTP endpoint (in your case, the Azure Function) without waiting for a prolonged period or any kind of asynchronous processing. The Logic App will receive the immediate response from the Azure Function and then proceed to the next action or finish the workflow. HTTP Webhook Action : Use this when there's a need to wait for an asynchronous process to complete before moving on in the Logic App workflow. The Logic App will pause until it receives a \"callback\" from the endpoint, signaling that the task is done. More complex and often used in scenarios where there might be a long delay or wait time between triggering an action and its completion. For our scenario, the HTTP Action is a simpler and more suitable choice. It will trigger the Azure Function, and once the Function responds (which it does after fetching and storing the weather data), the Logic App will consider the action completed. There is no need to wait for the Azure Function to complete its task and send a callback. Here is a real-world example of the output from choosing each scenario: In the HTTP Action , provide the URI and Method as GET. To obtain the URI from our Azure HTTP-Triggered Function, follow these steps: Open the Azure App hosting the Azure Function. Click the Azure Function , navigate to Overview , then at the top, click Get Function URL and copy the URL. The workflow creation is complete . Save the workflow and wait for the next hour. In the Overview section of the Logic App, under Runs history , check the runs to see if the desired action (in our case, the creation of the weather JSON in the blob storage) is being performed.","title":"Schedule the Function using Azure Logic Apps"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#post-deployment-monitoring","text":"The purpose of this http-triggered logic-app scheduled Azure Function is to periodically fetch weather data and store it as a JSON file in an Azure Blob container. After deploying the function, you can monitor its performance and analyze invocations by following these steps: Open the Azure Function in the Azure portal. Go to the \"Monitor\" section to access detailed information about function invocations. Check the Azure Blob container to verify if the JSON files are being created as expected.","title":"Post-deployment Monitoring"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#common-errors","text":"","title":"Common Errors"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#azurewebjobsstorage-app-setting-is-not-present","text":"The \"AzureWebJobsStorage\" app setting error indicates that our Azure Functions app is missing a crucial configuration related to the storage account connection string. This could also be realted to the following deployment failure message 12:23:58 PM FetchWeatherAzFunc: Deployment Log file does not exist in /tmp/oryx-build.log 12:23:58 PM FetchWeatherAzFunc: The logfile at /tmp/oryx-build.log is empty. Unable to fetch the summary of build 12:23:58 PM FetchWeatherAzFunc: Deployment Failed. deployer = ms-azuretools-vscode deploymentPath = Functions App ZipDeploy. Extract zip. Remote build. 12:24:00 PM FetchWeatherAzFunc: Deployment failed. 12:35:57 PM FetchWeatherAzFunc: Starting deployment. To resolve this: Create or Identify a Storage Account : If you don't already have an Azure Storage account, create one in the same region as our function app. Get the Storage Account Connection String : Navigate to the Azure Storage account in the Azure Portal. Under the \"Settings\" section, click on \"Access keys.\" Here, you'll find two keys (key1 and key2) each with a connection string. You can use either of these connection strings for the next step. Update Function App Settings : Navigate to our Azure Functions app in the Azure Portal. Under the \"Settings\" section, click on \"Configuration.\" In the \"Application settings\" tab, locate or create the AzureWebJobsStorage setting. Add the setting: Name: AzureWebJobsStorage Value: [our Azure Storage Account connection string from step 2] Click \"OK\" or \"Save\" to add the setting and save our changes on the main configuration page. Restart our Function App : After adding the necessary setting, restart our function app for the changes to take effect. Following these steps will resolve the error related to the \"AzureWebJobsStorage\" app setting.","title":"\"AzureWebJobsStorage\" app setting is not present"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#logic-apps-while-trying-to-add-azure-function-no-resources-of-this-type-found-under-this-subscription","text":"Ideally, if you want to add an Azure Function to a Logic App, it should appear like this: However, sometimes you may encounter the main Azure App, but not the bespoke function you created. Instead, you might see an error message such as \"No resources of this type found under this subscription.\" One of the reason for this could be realted to the storage account which the Azure Function uses. There could be other reason's too like the different regions.","title":"Logic Apps while trying to add azure function - No resources of this type found under this subscription"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#logic-apps-job-shows-running-status-indefinately","text":"If your Logic App consistently displays a \"Running\" status for a prolonged duration, several factors might be at play. A common cause is when an HTTP Webhook action is used; the Logic App waits for the webhook's response. It's crucial to verify that the webhook returns a response. Otherwise, the Logic App's status remains \"Running\" until a timeout or until a response is received. A solution is to use a straightforward HTTP action . In this method, the step won't await any response; it marks itself as successful upon receiving an output.","title":"Logic Apps Job shows Running status indefinately"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#appendix","text":"","title":"Appendix"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#detailed-project-summary","text":"In the 'Overview' and 'Solutions Summary Approach' sections, I've sketched out a basic outline of our project. To dive deeper into the details, refer to the content below Objective The objective of the Azure function is to fetch current weather data periodically and store it efficiently for later use. Deployment and Environment: - The function is developed using Visual Studio Code on Windows. - It utilizes the Python V2 programming model for Azure Functions, simplifying the development process. Data Retrieval: - The function sources weather data from weatherapi.com . - It fetches current weather information without the need for credit card details, with data availability spanning up to 15 days. Data Storage: - Weather data is saved as JSON files in an Azure Blob Storage container. - Filenames are generated based on the current hour, ensuring organized and chronological storage of data. Trigger Mechanism: - This Azure Function is HTTP-triggered, requiring external scheduling. - The scheduling is managed through Azure Logic Apps, allowing for regular, automated data retrieval. Error Handling and Logging: - The function contains robust logging, capturing both successful data retrievals and potential issues. - It has mechanisms in place to handle errors, especially when fetching data from the API or while uploading to Blob Storage. Response Mechanism: - Beyond its primary data retrieval and storage task, the function can provide a personalized greeting if a 'name' parameter is passed in the request.","title":"Detailed Project Summary"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#choice-of-storage-why-azure-data-lake","text":"We've selected Azure Data Lake Storage Gen2 (ADLS G2) for storing our weather data. This decision was largely driven by its seamless integration with Azure Functions, which streamlines our data collection process. Apart form this ADLS G2 also has many other advantages, like: Hierarchical Organization : ADLS G2 allows us to neatly structure our data, similar to a directory system. We can store our weather data grouped by year, month day etc. Enhanced Security : The Active Directory permission can be applied to the folders and files. This makes the storage as secure as it can get. Analytics Performance : ADLS G2 is optimized for heavy-duty data analytics, ensuring efficient query operations over vast datasets. In the next stage, we decided to convert our JSON-formatted data into Parquet. Here's why: Analytical Efficiency : Parquet, with its columnar storage design, streamlines analytical queries, letting us access specific data without scanning the entire dataset. Storage Efficiency : Parquet compresses data effectively, optimizing storage use and potentially lowering costs. Adaptable Structure : Parquet inherently carries schema information, allowing for alterations in data structure without disrupting existing data. A noteworthy advantage of Parquet is its compatibility with various storage systems, including NoSQL databases, data warehouses, and lakehouses. This ensures that we can migrate or integrate our data effortlessly if our analytical demands change. To summarize, our storage strategy not only prepares our data for immediate analysis but we can also easily move it to other storage solution in future.","title":"Choice of storage. Why Azure Data Lake?"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#creating-an-azure-functions-app-using-azure-portal","text":"An Azure Functions App acts as a host for one or more Azure Functions. In essence, while an individual Azure Function is a piece of code that runs in response to an event, the Function App serves as the hosting and management unit for that function (or multiple functions). To create an Azure Function, click \"Create\" under \"Function App\". Fill in the required details, including Subscription, Resource Group, Function App name, etc. For the runtime, choose Python. Click \"Review + create\" and then \"Create\".","title":"Creating an Azure Functions App Using Azure Portal"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#azure-function-vs-code-project-folder-structure","text":"Visual Studio Creates the following project structure for Azure Functions Project: The main Project Folder Contents: .venv/: (Optional) Python virtual environment for local development. .vscode/: (Optional) Visual Studio Code configuration. function_app.py: Main entry point. Default location for functions, triggers, and bindings. additional_functions.py: (Optional) Additional Python files for logical grouping and reference in function_app.py. tests/: (Optional) Test cases for our function app. .funcignore: (Optional) Declares files not to be published to Azure, often including .vscode/ for editor settings, .venv/ for local Python virtual environment, tests/ for test cases, and local.settings.json to prevent local app settings from being published. host.json: Contains configuration options affecting all functions in a function app. Gets published to Azure. local.settings.json: Stores app settings and connection strings for local development, not published to Azure. requirements.txt: Lists Python packages installed when publishing to Azure. Dockerfile: (Optional) Used for custom container publishing.","title":"Azure Function VS-Code Project Folder Structure"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#structure-of-the-function-function_apppy","text":"The function_app.py file is the key entry point. It resides at the root directory and serves as a Python script containing the definitions of the functions within the Function App. Each function defined in function_app.py must specify several crucial elements: Function Name : It should have a unique and descriptive name. Function Trigger : This defines what activates the function. For example, it could be triggered by an HTTP request or based on a timer schedule. Function Code : This is where you place the actual logic for the function. Note, that the structure of function_app.py varies a little bit based on the type of trigger used. E.g., when creating a Timer-triggered function, a specific template code is generated VS Code.","title":"Structure of the function function_app.py."},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#some-details-of-azure-functions-library","text":"The command pip install azure-functions adds the Azure Functions SDK to your Python environment. This SDK facilitates local development, testing, and execution of Azure Functions. It includes tools that allow functions to interact with Azure and other services, such as responding to an HTTP request or storing data in Azure Blob Storage.","title":"Some details of azure-functions library"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#azurite-extension","text":"Azurite is mainly used for local development and testing of applications that interact with Azure Storage services. It provides an emulation of Azure Storage services, including Blob, Queue, and Table services, in a local environment. (CTRL+SHIFT+P)\"Azurite: Start\" is a command that starts the Azurite server for local debugging etc. After that you typically use F5 to start debugging.","title":"Azurite Extension"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/#calculate-azure-logic-apps-cost","text":"Azure Logic Apps pricing primarily hinges on the total executions and the combined action and connector executions. To illustrate, for an hourly-triggered Azure Function: The Logic App runs hourly, translating to 24 daily executions. Every execution typically involves at least two action/connector executions: one for the recurring hourly trigger and another for invoking the Azure Function. Considering rates of approximately $0.000025 per execution and $0.000125 per action or connector execution (though rates can differ based on the region or Microsoft's adjustments): Estimated Monthly Cost = 24 x 30 x (0.000025 + 2 x 0.000125) \u2248 $0.198 So, running this Azure Logic App with an hourly trigger might cost about 20 cents monthly. Yet, it's crucial to: I wrote this article in 2022. So, confirm current prices on the Azure Logic Apps pricing page . Use Azure's monitoring tools to observe your actual costs. Remember Azure's Logic Apps free tier, which offers a set number of free runs and actions monthly, potentially lowering costs if you stay within these free tier boundaries. Overall, for straightforward hourly triggers, Azure Logic Apps offer an affordable solution to schedule tasks without added coding.","title":"Calculate Azure Logic Apps Cost"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/","text":"Table of Contents Azure Functions Quickstart - Create, Debug, Deploy, Monitor Create Create the Azure Function Project Add Python packages to requirements.txt Debug Test/Debug the Azure Function Deploy Create an Azure Function App Deploy the Azure Function To The Azure Function App Monitor Monitor the function post deployment Appendix Key takeways When to choose Azure Functions Advantages: Challenges: Azure Functions: V2 Python Programming Model Vs V1 The folder structure of Azure Functions(V2) What is this Azurite What happens during debugging Azure Function Core Tools Warms Up User Clicks Execute Function Now.. Common Errors \"AzureWebJobsStorage\" app setting is not present Azure Functions Quickstart - Create, Debug, Deploy, Monitor Here, we'll learn quickly about Azure Functions. It's a cloud service by Azure where you can run pieces of code without worry about server and hosting. We'll start by setting up our workspace in VS Code and adding it with essential extensions. Then, we'll test our function. After debugging, we'll deploy it to Azure. Then, we'll monitor its performance. If you want to know more, I've put some essential info in the appendix section. Create Let's start by installing the following three Visual Studio Extensions and One Command Line Tool: Extension Name Why Install? How to Install? Azure Tools for Visual Studio Code This extension holds a collection of extensions, including the Azure Functions extension. While you can install just the Azure Functions extension in this 'pack', having the full collection is never bad Ctrl+Shift+X , \"Azure Tools for Visual Studio Code\", Install Python extension for Visual Studio Code Provides extensive features for Python development, enabling linting, debugging, and IntelliSense for Azure Functions written in Python. Ctrl+Shift+X , \"Python\", Install Azurite An Azure Storage emulator, crucial for local testing and debugging of Azure Functions. Ctrl+Shift+X , \"Azurite\", Install . Azure Functions Core Tools Command-line tools essential for local development and testing of Azure Functions. These tools enable local function runtime, debugging, and deployment capabilities. Install via npm with the command: npm install -g azure-functions-core-tools@3 --unsafe-perm true (for version 3.x) Or using GUI Create the Azure Function Project Open Visual Studio and Click the Azure Icon on the Left In the Workspace (local) area, click the thunder button, and select Create New Project . Choose a folder location for the project Select Python as the programming language. Opt for Model V2 as the programming model. Choose the Python environment. Refer to the Appendix section for more details. Select HTTP trigger . Refer to the appendix section below for more details. Provide a unique name for our function. VS Code will generate a complete project structure like shown below Write your custom code, say you want to perform some blob operations, in function_app.py . This is the main/entry point function to the Fnction app. Add Python packages to requirements.txt Add library names of Python packages you imported in your script, like numpy , in requirements.txt . When you start you local debugging VS Code will install the requirements.txt packages to your local python virtual enviornment's .venv . - During actual deployment, VS Code will install the packages to Azure cloud. Debug Now, I will show you how to debug the azure function: Test/Debug the Azure Function With your function_app.py open press Ctrl+Shift+P . Select Azurite: Start . This action starts the Azurite storage Emulator. You can check the status at the bottom right corner of VS Code, where you should see something like this: Press F5 . Then, under Workspace Right-click the function and select Execute . If the execution is successful, the output will be similar to this: Deploy Here I will show you how to deploy the function to Azure. Create an Azure Function App Now, our function is ready and we need to deploy it to Azure. To deploy an azure function we need Azure Function App. This is like a container for the function. You can create the Azure Function app from the portal. But, here I will show you how to do it right from VS code. Click the Azure icon, then select the thunder icon in the workspace. Choose Create Function app in Azure..(Advanced) . Assign a unique name to your function app. If you're working on an Azure Function in Python, ensure you set the runtime environment to Python. Decide on using an existing resource group or create a new one. Ensure consistency in the chosen region. Carefully select the hosting plan. If you're budget-conscious, consider the Consumption-based plan. Allocate a storage account for the Azure Function App. Using separate storage accounts for each function app simplifies the structure. Incorporate an Application Insights resource for detailed insights and improved monitoring. After these steps, your Azure Function App is set up. The next phase involves deploying your Azure Function to this newly created app. Deploy the Azure Function To The Azure Function App The deployment process is straightforward. In the workspace, click the thunder icon and choose Deploy to Function App . Visual Studio Code will display the Function App where you can deploy our Azure Function. Select the Function App. Click Deploy Note: This will overwrite ANY function present in the Azure Func app. - After successful deployment, you will see output like the following in the console: - And you can see the Function inside the Azure Function App: Monitor Afer deploying the function, I will show you how to monitor it in the portal. Monitor the function post deployment Open the Azure Function in the Azure portal. Go to the \"Monitor\" section to access detailed information about function invocations. Appendix Key takeways Azure function is different from Azure Function App. Azure Function app is the container which holds Azure Functions. Azure functions can be developed using Python V2 Programming model, which uses decorators, lesser files, less-complex folder structure and a function_app.py HTTP-triggered functions and Timer-triggered functions are common in Function apps. Timer-triggered function have in-built trigger mechanism. When to choose Azure Functions Imagine you're thinking of using Azure Functions to convert JSON files to Parquet. Should you just use simple Python code in Azure Functions or go for Databricks? Here are some advantages and challenges of Azure Functions to help you decide: Advantages: Auto-scaling : The function can scale up and down. This means you don't have to worry about resources if the workload increases, and you don't have to be concerned about costs if it decreases. Pay-as-long-as-you-use : You only pay for the actual time your code runs, making it very cost-efficient. Triggers: It offers numerous event triggers and has a built-in timer for automatic scheduling. Serverless : There's no need to fret about server infrastructure. Just focus on writing the correct code. The V2 programming model makes it easier and to create Azure functions. See section below. Challenges: Time Limit: There's a limit to how long Azure Functions can run. If you have a big file or slow processing, it might not finish in time. Not for Heavy Work: Azure Functions is good for small tasks. If you're doing a lot of heavy calculations or have very big files, it might not be the best choice. Slow Start: If your function is not used for a while and then suddenly starts, it might take a bit more time to begin, which can delay your processing. For tasks that involve heavy data manipulation, transformation, and analysis, Databricks often becomes a preferred choice due to its scale-out architecture, optimized data processing capabilities, advanced dataframe support, built-in data cleansing tools, integrated machine learning libraries, and robust resource management. In contrast, for simpler tasks like just converting files, and when the data volume isn't immense, Azure Functions can offer a speedy and cost-effective approach. Azure Functions: V2 Python Programming Model Vs V1 The V2 programming model for Python, gives more Python-centric development experience for Azure Functions. Here are some key points about the V2 model: Need fewer files for a function app, so you can have many functions in one file. Decorators are used instead of the function.json file for triggers and things. Blueprints are a new thing in V2. They help group functions in an app. With blueprints, functions aren't directly indexed. They need to be registered first. All functions go in one function_app.py file, no need for many folders. No need for the function.json file now. Decorators in the function_app.py file do the job. The folder structure of Azure Functions(V2) This is how the project folder structure looks like: Here is what each item means: <project_root>/ \u2502 \u251c\u2500\u2500 \ud83d\udcc1 .venv/ - (Optional) Python virtual environment. \u2502 \u251c\u2500\u2500 \ud83d\udee0 .venv/pyvenv.cfg - Local python, version, command. \u2502 \u251c\u2500\u2500 \ud83d\udcc1 .vscode/ - (Optional) VS Code config. \u2502 \u251c\u2500\u2500 \ud83d\udc0d function_app.py - Default location for functions. \u2502 \u251c\u2500\u2500 \ud83d\udc0d additional_functions.py - (Optional) Additional functions. \u2502 \u251c\u2500\u2500 \ud83d\udcc1 tests/ - (Optional) Test cases. \u2502 \u251c\u2500\u2500 \ud83d\udee0 .funcignore - (Optional) Declares ignored files. \u2502 \u251c\u2500\u2500 \ud83c\udf10 host.json - Configuration options. \u2502 \u251c\u2500\u2500 \ud83c\udfe0 local.settings.json - Local app settings. \u2502 \u251c\u2500\u2500 \ud83d\udcc4 requirements.txt - Python packages for Azure. \u2502 \u2514\u2500\u2500 \ud83d\udc33 Dockerfile - (Optional) Custom container. I will try to modify the section later to give you a better understanding of the project structure. What is this Azurite When you click F5 you will see a message which looks like the one below. This is where Azurite comes into play. Azurite is a free tool to mimic Azure Storage on your computer. It helps in testing Azure storage without actually using the real Azure services. It saves money,can work offline, its safe, and quick. What happens during debugging Here are the events that take place when you debug an Azure Function using VS Code: Azure Function Core Tools Warms Up The Azure Functions Core Tools will set up the libraries mentioned in the requirements.txt file to the virtual environment's .venv\\lib\\site-packages using the command: Executing task: .venv\\Scripts\\python -m pip install -r requirements.txt Then the virtual environment is activated with .venv\\Scripts\\activate . It starts the debugger using func host start Then it attaches to the Azure Function runtime, loads the Azure Function app and set a breakpoint at the first line of the code. You'll notice two main things in the output: Functions: This lists down all functions in your Azure Function app. A line like [2023-10-25T04:16:47.402Z] Host lock lease acquired by instance ID '0000000000000000000000002B26484C' , tells that the debugger has locked the Azure Function host. This lock prevents the Azure Function host from being restarted by another process while the debugger is attached. User Clicks Execute Function Now.. Now the user right-clicks on the function and clicks Execute function Now.. . This executes the function. The rest is stepping through the function and checking if all is working fine. And, finally the debugging completes. Common Errors \"AzureWebJobsStorage\" app setting is not present The \"AzureWebJobsStorage\" app setting error indicates that our Azure Functions app is missing a crucial configuration related to the storage account connection string. This could also be realted to the following deployment failure message 12:23:58 PM FetchWeatherAzFunc: Deployment Log file does not exist in /tmp/oryx-build.log 12:23:58 PM FetchWeatherAzFunc: The logfile at /tmp/oryx-build.log is empty. Unable to fetch the summary of build 12:23:58 PM FetchWeatherAzFunc: Deployment Failed. deployer = ms-azuretools-vscode deploymentPath = Functions App ZipDeploy. Extract zip. Remote build. 12:24:00 PM FetchWeatherAzFunc: Deployment failed. 12:35:57 PM FetchWeatherAzFunc: Starting deployment. To resolve this: Create or Identify a Storage Account : If you don't already have an Azure Storage account, create one in the same region as our function app. Get the Storage Account Connection String : Navigate to the Azure Storage account in the Azure Portal. Under the \"Settings\" section, click on \"Access keys.\" Here, you'll find two keys (key1 and key2) each with a connection string. You can use either of these connection strings for the next step. Update Function App Settings : Navigate to our Azure Functions app in the Azure Portal. Under the \"Settings\" section, click on \"Configuration.\" In the \"Application settings\" tab, locate or create the AzureWebJobsStorage setting. Add the setting: Name: AzureWebJobsStorage Value: [our Azure Storage Account connection string from step 2] Click \"OK\" or \"Save\" to add the setting and save our changes on the main configuration page. Restart our Function App : After adding the necessary setting, restart our function app for the changes to take effect. Following these steps will resolve the error related to the \"AzureWebJobsStorage\" app setting. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Azure Functions Quickstart"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#table-of-contents","text":"Azure Functions Quickstart - Create, Debug, Deploy, Monitor Create Create the Azure Function Project Add Python packages to requirements.txt Debug Test/Debug the Azure Function Deploy Create an Azure Function App Deploy the Azure Function To The Azure Function App Monitor Monitor the function post deployment Appendix Key takeways When to choose Azure Functions Advantages: Challenges: Azure Functions: V2 Python Programming Model Vs V1 The folder structure of Azure Functions(V2) What is this Azurite What happens during debugging Azure Function Core Tools Warms Up User Clicks Execute Function Now.. Common Errors \"AzureWebJobsStorage\" app setting is not present","title":"Table of Contents"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#azure-functions-quickstart-create-debug-deploy-monitor","text":"Here, we'll learn quickly about Azure Functions. It's a cloud service by Azure where you can run pieces of code without worry about server and hosting. We'll start by setting up our workspace in VS Code and adding it with essential extensions. Then, we'll test our function. After debugging, we'll deploy it to Azure. Then, we'll monitor its performance. If you want to know more, I've put some essential info in the appendix section.","title":"Azure Functions Quickstart - Create, Debug, Deploy, Monitor"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#create","text":"Let's start by installing the following three Visual Studio Extensions and One Command Line Tool: Extension Name Why Install? How to Install? Azure Tools for Visual Studio Code This extension holds a collection of extensions, including the Azure Functions extension. While you can install just the Azure Functions extension in this 'pack', having the full collection is never bad Ctrl+Shift+X , \"Azure Tools for Visual Studio Code\", Install Python extension for Visual Studio Code Provides extensive features for Python development, enabling linting, debugging, and IntelliSense for Azure Functions written in Python. Ctrl+Shift+X , \"Python\", Install Azurite An Azure Storage emulator, crucial for local testing and debugging of Azure Functions. Ctrl+Shift+X , \"Azurite\", Install . Azure Functions Core Tools Command-line tools essential for local development and testing of Azure Functions. These tools enable local function runtime, debugging, and deployment capabilities. Install via npm with the command: npm install -g azure-functions-core-tools@3 --unsafe-perm true (for version 3.x) Or using GUI","title":"Create"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#create-the-azure-function-project","text":"Open Visual Studio and Click the Azure Icon on the Left In the Workspace (local) area, click the thunder button, and select Create New Project . Choose a folder location for the project Select Python as the programming language. Opt for Model V2 as the programming model. Choose the Python environment. Refer to the Appendix section for more details. Select HTTP trigger . Refer to the appendix section below for more details. Provide a unique name for our function. VS Code will generate a complete project structure like shown below Write your custom code, say you want to perform some blob operations, in function_app.py . This is the main/entry point function to the Fnction app.","title":"Create the Azure Function Project"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#add-python-packages-to-requirementstxt","text":"Add library names of Python packages you imported in your script, like numpy , in requirements.txt . When you start you local debugging VS Code will install the requirements.txt packages to your local python virtual enviornment's .venv . - During actual deployment, VS Code will install the packages to Azure cloud.","title":"Add Python packages to requirements.txt"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#debug","text":"Now, I will show you how to debug the azure function:","title":"Debug"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#testdebug-the-azure-function","text":"With your function_app.py open press Ctrl+Shift+P . Select Azurite: Start . This action starts the Azurite storage Emulator. You can check the status at the bottom right corner of VS Code, where you should see something like this: Press F5 . Then, under Workspace Right-click the function and select Execute . If the execution is successful, the output will be similar to this:","title":"Test/Debug the Azure Function"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#deploy","text":"Here I will show you how to deploy the function to Azure.","title":"Deploy"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#create-an-azure-function-app","text":"Now, our function is ready and we need to deploy it to Azure. To deploy an azure function we need Azure Function App. This is like a container for the function. You can create the Azure Function app from the portal. But, here I will show you how to do it right from VS code. Click the Azure icon, then select the thunder icon in the workspace. Choose Create Function app in Azure..(Advanced) . Assign a unique name to your function app. If you're working on an Azure Function in Python, ensure you set the runtime environment to Python. Decide on using an existing resource group or create a new one. Ensure consistency in the chosen region. Carefully select the hosting plan. If you're budget-conscious, consider the Consumption-based plan. Allocate a storage account for the Azure Function App. Using separate storage accounts for each function app simplifies the structure. Incorporate an Application Insights resource for detailed insights and improved monitoring. After these steps, your Azure Function App is set up. The next phase involves deploying your Azure Function to this newly created app.","title":"Create an Azure Function App"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#deploy-the-azure-function-to-the-azure-function-app","text":"The deployment process is straightforward. In the workspace, click the thunder icon and choose Deploy to Function App . Visual Studio Code will display the Function App where you can deploy our Azure Function. Select the Function App. Click Deploy Note: This will overwrite ANY function present in the Azure Func app. - After successful deployment, you will see output like the following in the console: - And you can see the Function inside the Azure Function App:","title":"Deploy the Azure Function To The Azure Function App"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#monitor","text":"Afer deploying the function, I will show you how to monitor it in the portal.","title":"Monitor"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#monitor-the-function-post-deployment","text":"Open the Azure Function in the Azure portal. Go to the \"Monitor\" section to access detailed information about function invocations.","title":"Monitor the function post deployment"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#appendix","text":"","title":"Appendix"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#key-takeways","text":"Azure function is different from Azure Function App. Azure Function app is the container which holds Azure Functions. Azure functions can be developed using Python V2 Programming model, which uses decorators, lesser files, less-complex folder structure and a function_app.py HTTP-triggered functions and Timer-triggered functions are common in Function apps. Timer-triggered function have in-built trigger mechanism.","title":"Key takeways"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#when-to-choose-azure-functions","text":"Imagine you're thinking of using Azure Functions to convert JSON files to Parquet. Should you just use simple Python code in Azure Functions or go for Databricks? Here are some advantages and challenges of Azure Functions to help you decide:","title":"When to choose Azure Functions"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#advantages","text":"Auto-scaling : The function can scale up and down. This means you don't have to worry about resources if the workload increases, and you don't have to be concerned about costs if it decreases. Pay-as-long-as-you-use : You only pay for the actual time your code runs, making it very cost-efficient. Triggers: It offers numerous event triggers and has a built-in timer for automatic scheduling. Serverless : There's no need to fret about server infrastructure. Just focus on writing the correct code. The V2 programming model makes it easier and to create Azure functions. See section below.","title":"Advantages:"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#challenges","text":"Time Limit: There's a limit to how long Azure Functions can run. If you have a big file or slow processing, it might not finish in time. Not for Heavy Work: Azure Functions is good for small tasks. If you're doing a lot of heavy calculations or have very big files, it might not be the best choice. Slow Start: If your function is not used for a while and then suddenly starts, it might take a bit more time to begin, which can delay your processing. For tasks that involve heavy data manipulation, transformation, and analysis, Databricks often becomes a preferred choice due to its scale-out architecture, optimized data processing capabilities, advanced dataframe support, built-in data cleansing tools, integrated machine learning libraries, and robust resource management. In contrast, for simpler tasks like just converting files, and when the data volume isn't immense, Azure Functions can offer a speedy and cost-effective approach.","title":"Challenges:"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#azure-functions-v2-python-programming-model-vs-v1","text":"The V2 programming model for Python, gives more Python-centric development experience for Azure Functions. Here are some key points about the V2 model: Need fewer files for a function app, so you can have many functions in one file. Decorators are used instead of the function.json file for triggers and things. Blueprints are a new thing in V2. They help group functions in an app. With blueprints, functions aren't directly indexed. They need to be registered first. All functions go in one function_app.py file, no need for many folders. No need for the function.json file now. Decorators in the function_app.py file do the job.","title":"Azure Functions: V2 Python Programming Model Vs V1"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#the-folder-structure-of-azure-functionsv2","text":"This is how the project folder structure looks like: Here is what each item means: <project_root>/ \u2502 \u251c\u2500\u2500 \ud83d\udcc1 .venv/ - (Optional) Python virtual environment. \u2502 \u251c\u2500\u2500 \ud83d\udee0 .venv/pyvenv.cfg - Local python, version, command. \u2502 \u251c\u2500\u2500 \ud83d\udcc1 .vscode/ - (Optional) VS Code config. \u2502 \u251c\u2500\u2500 \ud83d\udc0d function_app.py - Default location for functions. \u2502 \u251c\u2500\u2500 \ud83d\udc0d additional_functions.py - (Optional) Additional functions. \u2502 \u251c\u2500\u2500 \ud83d\udcc1 tests/ - (Optional) Test cases. \u2502 \u251c\u2500\u2500 \ud83d\udee0 .funcignore - (Optional) Declares ignored files. \u2502 \u251c\u2500\u2500 \ud83c\udf10 host.json - Configuration options. \u2502 \u251c\u2500\u2500 \ud83c\udfe0 local.settings.json - Local app settings. \u2502 \u251c\u2500\u2500 \ud83d\udcc4 requirements.txt - Python packages for Azure. \u2502 \u2514\u2500\u2500 \ud83d\udc33 Dockerfile - (Optional) Custom container. I will try to modify the section later to give you a better understanding of the project structure.","title":"The folder structure of Azure Functions(V2)"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#what-is-this-azurite","text":"When you click F5 you will see a message which looks like the one below. This is where Azurite comes into play. Azurite is a free tool to mimic Azure Storage on your computer. It helps in testing Azure storage without actually using the real Azure services. It saves money,can work offline, its safe, and quick.","title":"What is this Azurite"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#what-happens-during-debugging","text":"Here are the events that take place when you debug an Azure Function using VS Code:","title":"What happens during debugging"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#azure-function-core-tools-warms-up","text":"The Azure Functions Core Tools will set up the libraries mentioned in the requirements.txt file to the virtual environment's .venv\\lib\\site-packages using the command: Executing task: .venv\\Scripts\\python -m pip install -r requirements.txt Then the virtual environment is activated with .venv\\Scripts\\activate . It starts the debugger using func host start Then it attaches to the Azure Function runtime, loads the Azure Function app and set a breakpoint at the first line of the code. You'll notice two main things in the output: Functions: This lists down all functions in your Azure Function app. A line like [2023-10-25T04:16:47.402Z] Host lock lease acquired by instance ID '0000000000000000000000002B26484C' , tells that the debugger has locked the Azure Function host. This lock prevents the Azure Function host from being restarted by another process while the debugger is attached.","title":"Azure Function Core Tools Warms Up"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#user-clicks-execute-function-now","text":"Now the user right-clicks on the function and clicks Execute function Now.. . This executes the function. The rest is stepping through the function and checking if all is working fine. And, finally the debugging completes.","title":"User Clicks Execute Function Now.."},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#common-errors","text":"","title":"Common Errors"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/#azurewebjobsstorage-app-setting-is-not-present","text":"The \"AzureWebJobsStorage\" app setting error indicates that our Azure Functions app is missing a crucial configuration related to the storage account connection string. This could also be realted to the following deployment failure message 12:23:58 PM FetchWeatherAzFunc: Deployment Log file does not exist in /tmp/oryx-build.log 12:23:58 PM FetchWeatherAzFunc: The logfile at /tmp/oryx-build.log is empty. Unable to fetch the summary of build 12:23:58 PM FetchWeatherAzFunc: Deployment Failed. deployer = ms-azuretools-vscode deploymentPath = Functions App ZipDeploy. Extract zip. Remote build. 12:24:00 PM FetchWeatherAzFunc: Deployment failed. 12:35:57 PM FetchWeatherAzFunc: Starting deployment. To resolve this: Create or Identify a Storage Account : If you don't already have an Azure Storage account, create one in the same region as our function app. Get the Storage Account Connection String : Navigate to the Azure Storage account in the Azure Portal. Under the \"Settings\" section, click on \"Access keys.\" Here, you'll find two keys (key1 and key2) each with a connection string. You can use either of these connection strings for the next step. Update Function App Settings : Navigate to our Azure Functions app in the Azure Portal. Under the \"Settings\" section, click on \"Configuration.\" In the \"Application settings\" tab, locate or create the AzureWebJobsStorage setting. Add the setting: Name: AzureWebJobsStorage Value: [our Azure Storage Account connection string from step 2] Click \"OK\" or \"Save\" to add the setting and save our changes on the main configuration page. Restart our Function App : After adding the necessary setting, restart our function app for the changes to take effect. Following these steps will resolve the error related to the \"AzureWebJobsStorage\" app setting. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"\"AzureWebJobsStorage\" app setting is not present"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc/","text":"Table of contents Project AzureSkyWeather. Part 1B: Using Azure Timer-Triggered Function Overview Setting the pre-requisites Create the Python-based Azure Function Write our custom logic in the Azure Function Include azure-storage-blob in requirements.txt Test Our Azure Function Deployment and rest of the steps Project AzureSkyWeather. Part 1B: Using Azure Timer-Triggered Function Overview In Part 1A, we used Azure HTTP Function to get the weather from weatherapi.com . We also used Azure Logic Apps to set when to fetch this weather data. Now, I'll show a different way using just Azure Functions with a timer. This method is simpler because everything is in one place, and it might save some money compared to using both Azure Functions and Logic Apps. Setting the pre-requisites In Part 1A, we explained how to set up the VS environment and other starting tasks. We'll skip those since they're the same. Now, we'll focus on coding. Both HTTP-Triggered and Timer-Triggered functions have the same project structure, but the content in the function_app.py file is different. Create the Python-based Azure Function To create an Timer-Triggered Azure Function in Visual Studio Code, follow these steps: Open Visual Studio Code and access the Azure icon in the Activity bar. In the Workspace (local) area, click the thunder button, and select Create New Project . Choose a local folder location for our project. Select Python as the programming language. Opt for Model V2 as the programming model. Choose the Python environment that you intend to use. Make sure to select the correct environment with all the required dependencies and packages e.g. Azure Functions. Select Timer trigger Provide a unique name for our function. VS Code will generate a complete project structure, including all the necessary components for developing our function. Write our custom logic in the Azure Function Now, update function_app.py with the code below. Ensure you input <your_connection_string_to_Azure_storage> and <weather_api_endpoint> details. \"\"\" ------------------------------------------------------------------------------- Author: Das Date: Oct 2023 Description: This script contains an Azure Function designed to fetch the current weather data for London from the Weather API. The acquired data is subsequently stored in Azure Blob Storage, with each hour generating a distinct file based on the current weather conditions. NOTES: - Do not adjust the import structure to: - Avoid \"module 'azure.storage.blob' has no attribute 'from_connection_string'.\" - Ensure 'from datetime import datetime' remains, otherwise AttributeError: module 'datetime' has no attribute 'now' ------------------------------------------------------------------------------- \"\"\" import logging, requests from datetime import datetime import azure.functions as func from azure.storage.blob import BlobServiceClient app = func.FunctionApp() # schedule=\"0 */5 * * * *\" triggers every five minutes. Tested and working. # schedule=\"0 0 * * * *\" triggers every hour. # schedule=\"0 3 * * * *\" triggers every day at 3 AM. @app.schedule(schedule=\"0 0 * * * *\", arg_name=\"myTimer\", run_on_startup=True, use_monitor=False) def FetchWeatherTimerTriggered(myTimer: func.TimerRequest) -> None: if myTimer.past_due: logging.info('The timer is past due!') logging.info('Python timer trigger function executed.') # Custom code - Start # Generate a filename based on the current hour to separate hourly weather data. # The file will be overwritten, overwrite=True below, if the function is triggered multiple times within the same hour. weatherFileName = datetime.now().strftime(\"%Y-%m-%d-%H.json\") # Initialize BlobServiceClient using the given connection string. # Storage Act, Access Keys, key1 -> Connection string(Copy) # Note, here we are using a different container, \"weather-timer\". Just to separate it from HTTP-Triggered code. connection_string = \"<your_connection_string_to_Azure_storage>\" blob_service_client = BlobServiceClient.from_connection_string(connection_string) blob_client = blob_service_client.get_blob_client(container=\"weather-timer\", blob=weatherFileName) \"\"\" As per the website. Their API Endpoint URL structure is like: http://api.weatherapi.com/v1/current.json?key=YOUR_API_KEY&q=LOCATION \"\"\" # Use the requests library in Python to fetch data from the constructed endpoint: response = requests.get(\"<weather_api_endpoint>\") if response.status_code != 200: logging.error('Failed to fetch weather data.') return func.HttpResponse(\"Failed to fetch weather data.\", status_code=500) data = response.json() # Attempt to store the fetched weather data in Azure Blob Storage. try: blob_client.upload_blob(str(data), overwrite=True) except Exception as e: logging.error(f\"Error uploading data to Blob Storage: {e}\") return func.HttpResponse(\"Error storing weather data.\", status_code=500) # Custom code - End Include azure-storage-blob in requirements.txt We've wrapped up the main coding. Now, just add azure-storage-blob to requirements.txt to instruct the deployment to install the library in Azure like we did in Part 1A. Test Our Azure Function It's time to test our Azure Function. Follow these steps: Open Visual Studio Code and press Ctrl+Shift+P . Select Azurite: Start . This action starts the Azurite Blob Service. You can check the status at the bottom right corner of VS Code, where you should see something like this: Start debugging by pressing F5 . Then, on the left, click the Azure icon. Navigate to the workspace and locate our function (refresh if needed). Right-click the function and select Execute . If the execution is successful, you'll see an output similar to this: Additionally, a JSON file will be created in our container: Deployment and rest of the steps The next steps are like Part 1A. Check there for more details. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Part 1B - Using Azure Timer-Triggered Function"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc/#table-of-contents","text":"Project AzureSkyWeather. Part 1B: Using Azure Timer-Triggered Function Overview Setting the pre-requisites Create the Python-based Azure Function Write our custom logic in the Azure Function Include azure-storage-blob in requirements.txt Test Our Azure Function Deployment and rest of the steps","title":"Table of contents"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc/#project-azureskyweather-part-1b-using-azure-timer-triggered-function","text":"","title":"Project AzureSkyWeather. Part 1B: Using Azure Timer-Triggered Function"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc/#overview","text":"In Part 1A, we used Azure HTTP Function to get the weather from weatherapi.com . We also used Azure Logic Apps to set when to fetch this weather data. Now, I'll show a different way using just Azure Functions with a timer. This method is simpler because everything is in one place, and it might save some money compared to using both Azure Functions and Logic Apps.","title":"Overview"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc/#setting-the-pre-requisites","text":"In Part 1A, we explained how to set up the VS environment and other starting tasks. We'll skip those since they're the same. Now, we'll focus on coding. Both HTTP-Triggered and Timer-Triggered functions have the same project structure, but the content in the function_app.py file is different.","title":"Setting the pre-requisites"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc/#create-the-python-based-azure-function","text":"To create an Timer-Triggered Azure Function in Visual Studio Code, follow these steps: Open Visual Studio Code and access the Azure icon in the Activity bar. In the Workspace (local) area, click the thunder button, and select Create New Project . Choose a local folder location for our project. Select Python as the programming language. Opt for Model V2 as the programming model. Choose the Python environment that you intend to use. Make sure to select the correct environment with all the required dependencies and packages e.g. Azure Functions. Select Timer trigger Provide a unique name for our function. VS Code will generate a complete project structure, including all the necessary components for developing our function.","title":"Create the Python-based Azure Function"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc/#write-our-custom-logic-in-the-azure-function","text":"Now, update function_app.py with the code below. Ensure you input <your_connection_string_to_Azure_storage> and <weather_api_endpoint> details. \"\"\" ------------------------------------------------------------------------------- Author: Das Date: Oct 2023 Description: This script contains an Azure Function designed to fetch the current weather data for London from the Weather API. The acquired data is subsequently stored in Azure Blob Storage, with each hour generating a distinct file based on the current weather conditions. NOTES: - Do not adjust the import structure to: - Avoid \"module 'azure.storage.blob' has no attribute 'from_connection_string'.\" - Ensure 'from datetime import datetime' remains, otherwise AttributeError: module 'datetime' has no attribute 'now' ------------------------------------------------------------------------------- \"\"\" import logging, requests from datetime import datetime import azure.functions as func from azure.storage.blob import BlobServiceClient app = func.FunctionApp() # schedule=\"0 */5 * * * *\" triggers every five minutes. Tested and working. # schedule=\"0 0 * * * *\" triggers every hour. # schedule=\"0 3 * * * *\" triggers every day at 3 AM. @app.schedule(schedule=\"0 0 * * * *\", arg_name=\"myTimer\", run_on_startup=True, use_monitor=False) def FetchWeatherTimerTriggered(myTimer: func.TimerRequest) -> None: if myTimer.past_due: logging.info('The timer is past due!') logging.info('Python timer trigger function executed.') # Custom code - Start # Generate a filename based on the current hour to separate hourly weather data. # The file will be overwritten, overwrite=True below, if the function is triggered multiple times within the same hour. weatherFileName = datetime.now().strftime(\"%Y-%m-%d-%H.json\") # Initialize BlobServiceClient using the given connection string. # Storage Act, Access Keys, key1 -> Connection string(Copy) # Note, here we are using a different container, \"weather-timer\". Just to separate it from HTTP-Triggered code. connection_string = \"<your_connection_string_to_Azure_storage>\" blob_service_client = BlobServiceClient.from_connection_string(connection_string) blob_client = blob_service_client.get_blob_client(container=\"weather-timer\", blob=weatherFileName) \"\"\" As per the website. Their API Endpoint URL structure is like: http://api.weatherapi.com/v1/current.json?key=YOUR_API_KEY&q=LOCATION \"\"\" # Use the requests library in Python to fetch data from the constructed endpoint: response = requests.get(\"<weather_api_endpoint>\") if response.status_code != 200: logging.error('Failed to fetch weather data.') return func.HttpResponse(\"Failed to fetch weather data.\", status_code=500) data = response.json() # Attempt to store the fetched weather data in Azure Blob Storage. try: blob_client.upload_blob(str(data), overwrite=True) except Exception as e: logging.error(f\"Error uploading data to Blob Storage: {e}\") return func.HttpResponse(\"Error storing weather data.\", status_code=500) # Custom code - End","title":"Write our custom logic in the Azure Function"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc/#include-azure-storage-blob-in-requirementstxt","text":"We've wrapped up the main coding. Now, just add azure-storage-blob to requirements.txt to instruct the deployment to install the library in Azure like we did in Part 1A.","title":"Include azure-storage-blob in requirements.txt"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc/#test-our-azure-function","text":"It's time to test our Azure Function. Follow these steps: Open Visual Studio Code and press Ctrl+Shift+P . Select Azurite: Start . This action starts the Azurite Blob Service. You can check the status at the bottom right corner of VS Code, where you should see something like this: Start debugging by pressing F5 . Then, on the left, click the Azure icon. Navigate to the workspace and locate our function (refresh if needed). Right-click the function and select Execute . If the execution is successful, you'll see an output similar to this: Additionally, a JSON file will be created in our container:","title":"Test Our Azure Function"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc/#deployment-and-rest-of-the-steps","text":"The next steps are like Part 1A. Check there for more details. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Deployment and rest of the steps"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/","text":"Perform Data Validation and Quality Checks Before you organize your data into a specific model or format, it's crucial to ensure the data is correct and of high quality. Validation includes checking for correct data types, missing or null values, adherence to a predefined schema, and other business rule validations. This step is essential to avoid the \"garbage in, garbage out\" problem. Data Validation and Quality Check Strategy Validation Points: Schema Validation: Ensure fields like temp_c , lat , lon , etc., are of correct data types (e.g., float, integer). Range Checks: Validate values within expected ranges, e.g., latitude between -90 and 90. Consistency Checks: Cross-check related data points (e.g., temp_c and temp_f should match in Celsius/Fahrenheit conversions). Temporal Consistency: Check for correct time zone conversions and alignment of localtime . Missing Data: Look for nulls or missing values where not expected. Duplication Checks: Ensure there are no duplicate records. Script Overview This script will use a combination of great-expectations , Pandera , and Pydantic for data validation, and is designed to be run in a Spark environment. The objective is to use of these popular validation libraries. Data Reading : Leverage Spark to read JSON files from ADLS. Validation : Use Pydantic for schema validation. Implement Pandera for range and consistency checks. Utilize great-expectations for more complex validations like temporal consistency, missing data, and duplication checks. Data Modeling : Define a data model, perhaps a star schema for data warehousing. Parquet Conversion : Prepare for converting the validated and modeled data into Parquet (to be detailed later). Python Script Skeleton from pyspark.sql import SparkSession from pydantic import BaseModel, validator from pandera import DataFrameSchema, Column, Check import great_expectations as ge import pandas as pd # Part 1: Spark Session Initialization spark = SparkSession.builder.appName(\"WeatherDataProcessor\").getOrCreate() # Part 2: Read JSON files from ADLS (based on your ADLSSorter script) # [ ... Existing ADLSSorter code to fetch files ... ] # Pydantic model for schema validation class WeatherData(BaseModel): lat: float lon: float temp_c: float temp_f: float localtime_epoch: int @validator('temp_c') def temp_celsius_fahrenheit_match(cls, v, values, **kwargs): # Validate temp_c and temp_f conversion temp_f = values.get('temp_f') if temp_f and abs(temp_f - (v * 9/5 + 32)) > 0.1: raise ValueError('temp_c and temp_f do not match') return v # Pandera schema for range and consistency checks weather_schema = DataFrameSchema({ \"lat\": Column(float, Check(lambda x: -90 <= x <= 90)), \"lon\": Column(float, Check(lambda x: -180 <= x <= 180)), # ... additional range checks ... }) # Great Expectations for more advanced checks def validate_with_great_expectations(df): # Define expectations (temporal consistency, missing data, etc.) # ge_df = ge.from_pandas(df) # ge_df.expect_column_values_to_be_between(...) # ... define other expectations ... pass # Main processing loop for file_path in old_files: # Read data df = spark.read.json(file_path) pd_df = df.toPandas() # Validate data # Pydantic try: WeatherData(**pd_df.to_dict(orient=\"list\")) except ValidationError as e: print(f\"Validation error: {e}\") # Pandera try: weather_schema.validate(pd_df) except SchemaError as e: print(f\"Schema validation error: {e}\") # Great Expectations validate_with_great_expectations(pd_df) # Data Modeling and Transformation (to be defined) # ... # Note: Conversion to Parquet will be handled in the next phase. Create a Data Model Once you're confident in your data quality, the next step is to structure or model the data. This could mean transforming raw data into a more meaningful and usable format, aligning it with a dimensional model (like a star schema), or preparing it for specific analytical needs. This stage is where you'd typically perform operations like filtering, grouping, aggregating, or joining different data sets. Benefits of a data model Storing files as Parquet without a specific schema or structure is okay if you're merely archiving data or doing simple, infrequent analytics. However, for more complex analytics and reporting, creating a proper data model can have several benefits, like: a) Faster Query Performance : - By organizing data into a structured model (like star or snowflake schema), you can optimize query performance. The data model reduces the amount of data scanned during queries. b) Understandability and Consistency : - Having a defined schema makes it easier for data scientists, analysts, and other stakeholders to understand the data. It ensures everyone is working with data in a consistent manner. c) Joining Multiple Data Sources : - If you have (or plan to have) multiple data sources, a structured data model simplifies joining them. For instance, weather data can be enriched with location data, demographic data, etc. d) Data Integrity : - A structured data model, especially when coupled with a database or data warehouse, can ensure data integrity with primary and foreign key constraints. e) Improved Data Quality : - Data models can have defined constraints, ensuring that incoming data meets specific quality standards. Star Schema : - A commonly used schema in data warehousing. It includes a central fact table (e.g., hourly weather measurements) and surrounding dimension tables (e.g., location, date, time). It's simple and often results in fast query performances. Snowflake Schema : - A normalized version of the star schema. It can save storage but might result in more complex queries. Consideration When deciding on whether to structure your Parquet files according to a data model, consider: - The types of queries you'll be running. - The expected volume of data. - The frequency of data access. - Whether you plan to integrate with other data sources in the future. Conversion to Parquet : After the data is validated and properly modeled, converting it into an efficient storage format like Parquet is the final step. Parquet is a columnar storage format, offering efficient data compression and encoding schemes. This format is optimal for analytic querying performance and works well with big data technologies. Converting to Parquet after validation and modeling ensures that you're storing high-quality, well-structured data, making your analytics processes more efficient. Conversion to parquet Strategy: When considering a robust data storage strategy, especially for data analytics and long-term storage, the structure and features of Parquet come into play. Parquet is a columnar storage file format, which is optimized for use with big data processing tools like Apache Spark, Apache Hive, and many others. Here's what you should consider for the best Parquet structure: Columnar Storage : Take advantage of Parquet's columnar storage format. This means when querying specific columns, only those columns' data will be read, resulting in improved performance and reduced I/O. Schema Evolution : One of the significant advantages of Parquet is its ability to handle schema evolution. Make sure your solution can accommodate changes to the schema over time without affecting the existing data. Compression : Parquet supports various compression techniques like SNAPPY, GZIP, and more. Depending on your analytics use-case, select the compression method that offers a good trade-off between storage cost and query performance. Partitioning : For your use-case, since you are already organizing by year, month, day, and hour, you should partition the Parquet files this way. This will speed up query times since only the relevant partitions need to be read. Example: /year=2023/month=10/day=17/hour=13/data.parquet Row Group Size : Parquet organizes data into smaller row groups, allowing for more efficient column pruning. Adjusting the size of row groups can have a performance impact. The default is typically 128MB, but you might want to adjust based on your typical query patterns. Metadata : Parquet files store metadata about the data they contain, which helps in understanding the schema and optimizing queries. Ensure this metadata is kept up-to-date. Consistent Schema : Ensure that the schema for your Parquet files remains consistent, especially if you're ingesting data regularly. Any changes in the incoming JSON schema should be handled gracefully. Data Lake Integration : Since you're using Azure Data Lake Storage, ensure that the tools you're using for analytics are well-integrated with ADLS and can take full advantage of the features both ADLS and Parquet provide. Regular Compaction : Over time, as data gets updated or deleted, you might end up with many small Parquet files. This is suboptimal for query performance. Implement a regular compaction process to combine these smaller files into larger, more efficient Parquet files. Avoiding Too Many Small Files : If your ingestion process creates too many small files, it can degrade performance. Consider batching incoming data to create larger Parquet files. Given the JSON structure you provided, you might want to flatten it out a bit for more effective columnar storage, unless you're often querying multiple subfields of location or current together. The columnar nature of Parquet means that nesting can sometimes reduce performance benefits, especially if the data is queried column-by-column. Appendix Data Processing Libraries Overview Data Validation Libraries Library Why Use Cases Great Expectations Provides a robust suite for JSON data testing, documentation, and validation, ideal for complex structures with clear, definable rules and expectations, ensuring type, range, structure, and content accuracy. Works with both spark and pandas dataframes. Extensive data quality checks, complex validation rules, data documentation. Pandera Provides a flexible and expressive API for pandas DataFrame validation, allowing for easy statistical checks, type validation, and more. When working with Pandas for data manipulation and needing validation tightly coupled with these operations. Pydantic Used primarily for data parsing and validation with a strong emphasis on strict type validations via Python type annotations. Best for scenarios where you are dealing with JSON-like data structures, needing strong type checks and simple data validation. Data Modeling Libraries Library Why Use Cases Pandas Extremely popular for data manipulation and analysis in Python, with a very straightforward, user-friendly API. Pandas is a Python library for data manipulation and analysis. It is well-suited for working with small to medium-sized datasets on a single machine. PySpark Best for large-scale data processing. It can handle very large datasets that don't fit into a single machine's memory. Large datasets, needing distributed computing, or integrating with other components in a big data ecosystem. Data Format Conversion Libraries Library Why Use Cases PyArrow PyArrow provides a bridge between the columnar storage format Parquet and Python data analysis libraries. It's fast and efficient. High-speed data serialization/deserialization, working with Parquet files, large datasets. Pandas Directly supports reading and writing Parquet files (though it uses PyArrow or fastparquet under the hood). If you're already using Pandas for data manipulation, converting to/from Parquet is very straightforward. Conclusion Great Expectations was chosen for its comprehensive data validation and documentation capabilities, key for ensuring data quality in analytics and reporting pipelines. PyArrow is recommended for handling large data volumes and efficient Parquet format conversion due to its performance and direct support for the format. Pandas can be a streamlined choice for moderate-sized data, offering both data manipulation and validation (with Pandera for added validation support), and easy Parquet conversion. Choice of Platform Stage Objective Azure Product Data Validation Ensure JSON consistency, completeness, and data types. Azure Databricks (Use PySpark for large-scale data processing and validation) Conversion to Parquet Convert JSON to Parquet for efficient storage/querying. Azure Databricks (Native support for JSON & Parquet via PySpark) Organize Data into a Data Model Structure Parquet files for optimal analytics/reporting. Azure Data Factory (For complex transformation logic) Azure Databricks (PySpark DataFrame API for reshaping data) Storage Store transformed Parquet files securely and efficiently. Azure Data Lake Storage Gen2 (Optimized for Azure analytics platforms) Analytics & Querying Run analytics and queries on data. Azure Synapse Analytics (For massive parallel processing) Azure Databricks (For deeper analytics/ML) Monitoring and Maintenance Monitor pipeline health and performance. Azure Monitor and Azure Log Analytics (Full-stack monitoring, advanced analytics) Data Security Secure data at rest and in transit. Azure Data Lake Storage Gen2 (Encryption at rest) Azure Key Vault (Manage cryptographic keys/secrets) Automation and Scheduling Automate the pipeline processes. Azure Data Factory (Define and orchestrate data-driven workflows) \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Solution Details"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/#perform-data-validation-and-quality-checks","text":"Before you organize your data into a specific model or format, it's crucial to ensure the data is correct and of high quality. Validation includes checking for correct data types, missing or null values, adherence to a predefined schema, and other business rule validations. This step is essential to avoid the \"garbage in, garbage out\" problem.","title":"Perform Data Validation and Quality Checks"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/#data-validation-and-quality-check-strategy","text":"","title":"Data Validation and Quality Check Strategy"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/#validation-points","text":"Schema Validation: Ensure fields like temp_c , lat , lon , etc., are of correct data types (e.g., float, integer). Range Checks: Validate values within expected ranges, e.g., latitude between -90 and 90. Consistency Checks: Cross-check related data points (e.g., temp_c and temp_f should match in Celsius/Fahrenheit conversions). Temporal Consistency: Check for correct time zone conversions and alignment of localtime . Missing Data: Look for nulls or missing values where not expected. Duplication Checks: Ensure there are no duplicate records.","title":"Validation Points:"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/#script-overview","text":"This script will use a combination of great-expectations , Pandera , and Pydantic for data validation, and is designed to be run in a Spark environment. The objective is to use of these popular validation libraries. Data Reading : Leverage Spark to read JSON files from ADLS. Validation : Use Pydantic for schema validation. Implement Pandera for range and consistency checks. Utilize great-expectations for more complex validations like temporal consistency, missing data, and duplication checks. Data Modeling : Define a data model, perhaps a star schema for data warehousing. Parquet Conversion : Prepare for converting the validated and modeled data into Parquet (to be detailed later).","title":"Script Overview"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/#python-script-skeleton","text":"from pyspark.sql import SparkSession from pydantic import BaseModel, validator from pandera import DataFrameSchema, Column, Check import great_expectations as ge import pandas as pd # Part 1: Spark Session Initialization spark = SparkSession.builder.appName(\"WeatherDataProcessor\").getOrCreate() # Part 2: Read JSON files from ADLS (based on your ADLSSorter script) # [ ... Existing ADLSSorter code to fetch files ... ] # Pydantic model for schema validation class WeatherData(BaseModel): lat: float lon: float temp_c: float temp_f: float localtime_epoch: int @validator('temp_c') def temp_celsius_fahrenheit_match(cls, v, values, **kwargs): # Validate temp_c and temp_f conversion temp_f = values.get('temp_f') if temp_f and abs(temp_f - (v * 9/5 + 32)) > 0.1: raise ValueError('temp_c and temp_f do not match') return v # Pandera schema for range and consistency checks weather_schema = DataFrameSchema({ \"lat\": Column(float, Check(lambda x: -90 <= x <= 90)), \"lon\": Column(float, Check(lambda x: -180 <= x <= 180)), # ... additional range checks ... }) # Great Expectations for more advanced checks def validate_with_great_expectations(df): # Define expectations (temporal consistency, missing data, etc.) # ge_df = ge.from_pandas(df) # ge_df.expect_column_values_to_be_between(...) # ... define other expectations ... pass # Main processing loop for file_path in old_files: # Read data df = spark.read.json(file_path) pd_df = df.toPandas() # Validate data # Pydantic try: WeatherData(**pd_df.to_dict(orient=\"list\")) except ValidationError as e: print(f\"Validation error: {e}\") # Pandera try: weather_schema.validate(pd_df) except SchemaError as e: print(f\"Schema validation error: {e}\") # Great Expectations validate_with_great_expectations(pd_df) # Data Modeling and Transformation (to be defined) # ... # Note: Conversion to Parquet will be handled in the next phase.","title":"Python Script Skeleton"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/#create-a-data-model","text":"Once you're confident in your data quality, the next step is to structure or model the data. This could mean transforming raw data into a more meaningful and usable format, aligning it with a dimensional model (like a star schema), or preparing it for specific analytical needs. This stage is where you'd typically perform operations like filtering, grouping, aggregating, or joining different data sets.","title":"Create a Data Model"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/#benefits-of-a-data-model","text":"Storing files as Parquet without a specific schema or structure is okay if you're merely archiving data or doing simple, infrequent analytics. However, for more complex analytics and reporting, creating a proper data model can have several benefits, like: a) Faster Query Performance : - By organizing data into a structured model (like star or snowflake schema), you can optimize query performance. The data model reduces the amount of data scanned during queries. b) Understandability and Consistency : - Having a defined schema makes it easier for data scientists, analysts, and other stakeholders to understand the data. It ensures everyone is working with data in a consistent manner. c) Joining Multiple Data Sources : - If you have (or plan to have) multiple data sources, a structured data model simplifies joining them. For instance, weather data can be enriched with location data, demographic data, etc. d) Data Integrity : - A structured data model, especially when coupled with a database or data warehouse, can ensure data integrity with primary and foreign key constraints. e) Improved Data Quality : - Data models can have defined constraints, ensuring that incoming data meets specific quality standards. Star Schema : - A commonly used schema in data warehousing. It includes a central fact table (e.g., hourly weather measurements) and surrounding dimension tables (e.g., location, date, time). It's simple and often results in fast query performances. Snowflake Schema : - A normalized version of the star schema. It can save storage but might result in more complex queries.","title":"Benefits of a data model"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/#consideration","text":"When deciding on whether to structure your Parquet files according to a data model, consider: - The types of queries you'll be running. - The expected volume of data. - The frequency of data access. - Whether you plan to integrate with other data sources in the future.","title":"Consideration"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/#conversion-to-parquet","text":"After the data is validated and properly modeled, converting it into an efficient storage format like Parquet is the final step. Parquet is a columnar storage format, offering efficient data compression and encoding schemes. This format is optimal for analytic querying performance and works well with big data technologies. Converting to Parquet after validation and modeling ensures that you're storing high-quality, well-structured data, making your analytics processes more efficient.","title":"Conversion to Parquet:"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/#conversion-to-parquet-strategy","text":"When considering a robust data storage strategy, especially for data analytics and long-term storage, the structure and features of Parquet come into play. Parquet is a columnar storage file format, which is optimized for use with big data processing tools like Apache Spark, Apache Hive, and many others. Here's what you should consider for the best Parquet structure: Columnar Storage : Take advantage of Parquet's columnar storage format. This means when querying specific columns, only those columns' data will be read, resulting in improved performance and reduced I/O. Schema Evolution : One of the significant advantages of Parquet is its ability to handle schema evolution. Make sure your solution can accommodate changes to the schema over time without affecting the existing data. Compression : Parquet supports various compression techniques like SNAPPY, GZIP, and more. Depending on your analytics use-case, select the compression method that offers a good trade-off between storage cost and query performance. Partitioning : For your use-case, since you are already organizing by year, month, day, and hour, you should partition the Parquet files this way. This will speed up query times since only the relevant partitions need to be read. Example: /year=2023/month=10/day=17/hour=13/data.parquet Row Group Size : Parquet organizes data into smaller row groups, allowing for more efficient column pruning. Adjusting the size of row groups can have a performance impact. The default is typically 128MB, but you might want to adjust based on your typical query patterns. Metadata : Parquet files store metadata about the data they contain, which helps in understanding the schema and optimizing queries. Ensure this metadata is kept up-to-date. Consistent Schema : Ensure that the schema for your Parquet files remains consistent, especially if you're ingesting data regularly. Any changes in the incoming JSON schema should be handled gracefully. Data Lake Integration : Since you're using Azure Data Lake Storage, ensure that the tools you're using for analytics are well-integrated with ADLS and can take full advantage of the features both ADLS and Parquet provide. Regular Compaction : Over time, as data gets updated or deleted, you might end up with many small Parquet files. This is suboptimal for query performance. Implement a regular compaction process to combine these smaller files into larger, more efficient Parquet files. Avoiding Too Many Small Files : If your ingestion process creates too many small files, it can degrade performance. Consider batching incoming data to create larger Parquet files. Given the JSON structure you provided, you might want to flatten it out a bit for more effective columnar storage, unless you're often querying multiple subfields of location or current together. The columnar nature of Parquet means that nesting can sometimes reduce performance benefits, especially if the data is queried column-by-column.","title":"Conversion to parquet Strategy:"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/#appendix","text":"","title":"Appendix"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/#data-processing-libraries-overview","text":"","title":"Data Processing Libraries Overview"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/#data-validation-libraries","text":"Library Why Use Cases Great Expectations Provides a robust suite for JSON data testing, documentation, and validation, ideal for complex structures with clear, definable rules and expectations, ensuring type, range, structure, and content accuracy. Works with both spark and pandas dataframes. Extensive data quality checks, complex validation rules, data documentation. Pandera Provides a flexible and expressive API for pandas DataFrame validation, allowing for easy statistical checks, type validation, and more. When working with Pandas for data manipulation and needing validation tightly coupled with these operations. Pydantic Used primarily for data parsing and validation with a strong emphasis on strict type validations via Python type annotations. Best for scenarios where you are dealing with JSON-like data structures, needing strong type checks and simple data validation.","title":"Data Validation Libraries"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/#data-modeling-libraries","text":"Library Why Use Cases Pandas Extremely popular for data manipulation and analysis in Python, with a very straightforward, user-friendly API. Pandas is a Python library for data manipulation and analysis. It is well-suited for working with small to medium-sized datasets on a single machine. PySpark Best for large-scale data processing. It can handle very large datasets that don't fit into a single machine's memory. Large datasets, needing distributed computing, or integrating with other components in a big data ecosystem.","title":"Data Modeling Libraries"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/#data-format-conversion-libraries","text":"Library Why Use Cases PyArrow PyArrow provides a bridge between the columnar storage format Parquet and Python data analysis libraries. It's fast and efficient. High-speed data serialization/deserialization, working with Parquet files, large datasets. Pandas Directly supports reading and writing Parquet files (though it uses PyArrow or fastparquet under the hood). If you're already using Pandas for data manipulation, converting to/from Parquet is very straightforward.","title":"Data Format Conversion Libraries"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/#conclusion","text":"Great Expectations was chosen for its comprehensive data validation and documentation capabilities, key for ensuring data quality in analytics and reporting pipelines. PyArrow is recommended for handling large data volumes and efficient Parquet format conversion due to its performance and direct support for the format. Pandas can be a streamlined choice for moderate-sized data, offering both data manipulation and validation (with Pandera for added validation support), and easy Parquet conversion.","title":"Conclusion"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/#choice-of-platform","text":"Stage Objective Azure Product Data Validation Ensure JSON consistency, completeness, and data types. Azure Databricks (Use PySpark for large-scale data processing and validation) Conversion to Parquet Convert JSON to Parquet for efficient storage/querying. Azure Databricks (Native support for JSON & Parquet via PySpark) Organize Data into a Data Model Structure Parquet files for optimal analytics/reporting. Azure Data Factory (For complex transformation logic) Azure Databricks (PySpark DataFrame API for reshaping data) Storage Store transformed Parquet files securely and efficiently. Azure Data Lake Storage Gen2 (Optimized for Azure analytics platforms) Analytics & Querying Run analytics and queries on data. Azure Synapse Analytics (For massive parallel processing) Azure Databricks (For deeper analytics/ML) Monitoring and Maintenance Monitor pipeline health and performance. Azure Monitor and Azure Log Analytics (Full-stack monitoring, advanced analytics) Data Security Secure data at rest and in transit. Azure Data Lake Storage Gen2 (Encryption at rest) Azure Key Vault (Manage cryptographic keys/secrets) Automation and Scheduling Automate the pipeline processes. Azure Data Factory (Define and orchestrate data-driven workflows) \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Choice of Platform"},{"location":"DE-Projects/JsonValidator/AzureFunction-ValidateJSOns/","text":"Clean and Validate JSON Using Azure Functions Introduction In a project, I encountered numerous ASCII-formatted JSON files with single quotes stored in a ADLS Container, rendering the format invalid. My task involved converting these files to UTF-8, replacing the quotes, validating them against a JSON schema, and relocating them to different containers. For this, I used Azure Functions, which is efficient for smaller datasets, but for larger ones, I recommend using Spark or Databricks. Pre-requisites Create the JSON Schema : For well-strucuctured JSONs we can create the schema easily. You can take help from sites liks transform.tools . Azure Function Setup : To create an auto-scheduled function you will need to create a timer-triggered Azure function. You can refer to my article for details. Data Lake Storage Configuration : My example works with ADLS Gen2. The code should also work with simple azure blob storage. The Code Here's the code for function_app.py . Please enter your details as required in the placeholders. import requests, logging, json from jsonschema import validate from jsonschema.exceptions import ValidationError from datetime import datetime import azure.functions as func from azure.storage.blob import BlobServiceClient def is_valid_json(data, schema): try: validate(instance=data, schema=schema) return True except ValidationError as e: logging.error(f\"Validation error: {e}\") return False app = func.FunctionApp() @app.schedule(schedule=\"<your CRON, e.g. 0 0 * * * *>\", arg_name=\"myTimer\", run_on_startup=True, use_monitor=False) def AzFuncCheckNMoveJson(myTimer: func.TimerRequest) -> None: if myTimer.past_due: logging.info('The timer is past due!') logging.info('Python timer trigger function executed.') \"\"\" Initialize BlobServiceClient using the given connection string. Storage Act, Access Keys, key1 -> Connection string(Copy) \"\"\" blob_service_client = BlobServiceClient.from_connection_string(\"DefaultEndpointsProtocol=https;AccountName=<your_storage_act_name>;AccountKey=<your_account_key>;EndpointSuffix=core.windows.net\") # Fetch the schema schema_blob_client = blob_service_client.get_blob_client(container=\"schema\", blob=\"JSON_schema.json\") try: schema_json = json.loads(schema_blob_client.download_blob().readall()) except Exception as e: logging.error(f\"Error fetching schema: {e}\") # Iterate over blobs in the \"weather-http\" container container_client = blob_service_client.get_container_client(\"<raw_container>\") for blob in container_client.list_blobs(): try: blob_client = blob_service_client.get_blob_client(container=\"weather-http\", blob=blob.name) data_str = blob_client.download_blob().readall().decode('utf-8') data_str = data_str.replace(\"'\", '\"') try: data = json.loads(data_str) except json.JSONDecodeError: data = json.loads(blob_client.download_blob().readall().decode('utf-8')) # Validate the JSON data if is_valid_json(data, schema_json): target_container = \"silver\" else: target_container = \"error\" # Move blob to the target container target_blob_client = blob_service_client.get_blob_client(container=target_container, blob=blob.name) target_blob_client.upload_blob(json.dumps(data), overwrite=True) blob_client.delete_blob() # Delete the original blob after moving except Exception as e: logging.error(f\"Error processing blob {blob.name}: {e}\") logging.info(\"Processing complete.\") requirements.txt The following entries should be there in requirements.txt file azure-functions requests azure-storage-blob jsonschema Conclusion The use of jsonschema in this project proved invaluable for efficient JSON validation, eliminating the need for iterative item-by-item examination. This method enhanced both speed and accuracy. While Azure Functions were a good choice for our dataset size, they're best for smaller datasets. For larger volumes, solutions like Spark with Azure Data Lake Storage (ADLS) are recommended.","title":"Azure Functions - Validate JSONs"},{"location":"DE-Projects/JsonValidator/AzureFunction-ValidateJSOns/#clean-and-validate-json-using-azure-functions","text":"","title":"Clean and Validate JSON Using Azure Functions"},{"location":"DE-Projects/JsonValidator/AzureFunction-ValidateJSOns/#introduction","text":"In a project, I encountered numerous ASCII-formatted JSON files with single quotes stored in a ADLS Container, rendering the format invalid. My task involved converting these files to UTF-8, replacing the quotes, validating them against a JSON schema, and relocating them to different containers. For this, I used Azure Functions, which is efficient for smaller datasets, but for larger ones, I recommend using Spark or Databricks.","title":"Introduction"},{"location":"DE-Projects/JsonValidator/AzureFunction-ValidateJSOns/#pre-requisites","text":"Create the JSON Schema : For well-strucuctured JSONs we can create the schema easily. You can take help from sites liks transform.tools . Azure Function Setup : To create an auto-scheduled function you will need to create a timer-triggered Azure function. You can refer to my article for details. Data Lake Storage Configuration : My example works with ADLS Gen2. The code should also work with simple azure blob storage.","title":"Pre-requisites"},{"location":"DE-Projects/JsonValidator/AzureFunction-ValidateJSOns/#the-code","text":"Here's the code for function_app.py . Please enter your details as required in the placeholders. import requests, logging, json from jsonschema import validate from jsonschema.exceptions import ValidationError from datetime import datetime import azure.functions as func from azure.storage.blob import BlobServiceClient def is_valid_json(data, schema): try: validate(instance=data, schema=schema) return True except ValidationError as e: logging.error(f\"Validation error: {e}\") return False app = func.FunctionApp() @app.schedule(schedule=\"<your CRON, e.g. 0 0 * * * *>\", arg_name=\"myTimer\", run_on_startup=True, use_monitor=False) def AzFuncCheckNMoveJson(myTimer: func.TimerRequest) -> None: if myTimer.past_due: logging.info('The timer is past due!') logging.info('Python timer trigger function executed.') \"\"\" Initialize BlobServiceClient using the given connection string. Storage Act, Access Keys, key1 -> Connection string(Copy) \"\"\" blob_service_client = BlobServiceClient.from_connection_string(\"DefaultEndpointsProtocol=https;AccountName=<your_storage_act_name>;AccountKey=<your_account_key>;EndpointSuffix=core.windows.net\") # Fetch the schema schema_blob_client = blob_service_client.get_blob_client(container=\"schema\", blob=\"JSON_schema.json\") try: schema_json = json.loads(schema_blob_client.download_blob().readall()) except Exception as e: logging.error(f\"Error fetching schema: {e}\") # Iterate over blobs in the \"weather-http\" container container_client = blob_service_client.get_container_client(\"<raw_container>\") for blob in container_client.list_blobs(): try: blob_client = blob_service_client.get_blob_client(container=\"weather-http\", blob=blob.name) data_str = blob_client.download_blob().readall().decode('utf-8') data_str = data_str.replace(\"'\", '\"') try: data = json.loads(data_str) except json.JSONDecodeError: data = json.loads(blob_client.download_blob().readall().decode('utf-8')) # Validate the JSON data if is_valid_json(data, schema_json): target_container = \"silver\" else: target_container = \"error\" # Move blob to the target container target_blob_client = blob_service_client.get_blob_client(container=target_container, blob=blob.name) target_blob_client.upload_blob(json.dumps(data), overwrite=True) blob_client.delete_blob() # Delete the original blob after moving except Exception as e: logging.error(f\"Error processing blob {blob.name}: {e}\") logging.info(\"Processing complete.\")","title":"The Code"},{"location":"DE-Projects/JsonValidator/AzureFunction-ValidateJSOns/#requirementstxt","text":"The following entries should be there in requirements.txt file azure-functions requests azure-storage-blob jsonschema","title":"requirements.txt"},{"location":"DE-Projects/JsonValidator/AzureFunction-ValidateJSOns/#conclusion","text":"The use of jsonschema in this project proved invaluable for efficient JSON validation, eliminating the need for iterative item-by-item examination. This method enhanced both speed and accuracy. While Azure Functions were a good choice for our dataset size, they're best for smaller datasets. For larger volumes, solutions like Spark with Azure Data Lake Storage (ADLS) are recommended.","title":"Conclusion"},{"location":"DE-Projects/JsonValidator/Python-ValidateJSONs/","text":"Table of Contents Validating JSON Data with Python and JSON Schema Introduction Prerequisites How to Run the Script Code Overview Conclusion Validating JSON Data with Python and JSON Schema Introduction This is a short tutorial on how to use Python to validate JSON file data. Rather than iterating through all the fields in the JSON, usage of libraries like JSONschema etc. are considered more efficient. Hence, I thought of sharing my experience. Prerequisites Python Installation : Ensure Python is installed on your system. If not, download and install it from the official Python website . JSON Files : Weather Data File ( weather.json ) : A JSON file containing weather data, typically obtained from weather APIs e.g. weatherapi . JSON Schema File ( weather_schema.json ) : A JSON schema file that defines the expected structure, required fields, and data types of the weather data JSON. Tools like Quicktype or JSON Schema Tool can help generate a schema from a JSON example. jsonschema Library : This Python library is used for validating JSON data against a schema. Install it using pip: pip install jsonschema How to Run the Script Prepare Your JSON Files : Ensure your weather.json and weather_schema.json are placed in the same directory as your script. Run the Script : Open Visual Studio Code, create a Python file or a Jupyter notebook And copy-paste the code below. Adjust as needed. ```python import json from jsonschema import validate, ValidationError Load the JSON data and schema with open('weather.json', 'r') as file: weather_data = json.load(file) with open('weather_schema.json', 'r') as file: schema = json.load(file) Function to validate JSON def is_valid_json(data, schema): try: validate(instance=data, schema=schema) return True except ValidationError as ve: print(f\"Validation error: {ve}\") return False Validate the weather data if is_valid_json(weather_data, schema): print(\"Validation Successful!\") else: print(\"Validation Failed!\") ``` Run and debug using the editor. Code Overview The script consists of the following components: Loading JSON Data and Schema : We use Python's built-in json module to load the weather data and the schema from their respective files. Validation Logic : Utilizing the jsonschema.validate() function, we check if the JSON data adheres to the schema, capturing any validation errors that might indicate discrepancies. Error Handling : The script identifies and prints specific validation errors, making it easier to pinpoint issues in the data or the schema. Conclusion This code can be further enhanced to be included in an Azure Function. For example, if you are fetching data as JSON format from a web API, this code can be added there to perform data validation. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Validate JSON using Python"},{"location":"DE-Projects/JsonValidator/Python-ValidateJSONs/#table-of-contents","text":"Validating JSON Data with Python and JSON Schema Introduction Prerequisites How to Run the Script Code Overview Conclusion","title":"Table of Contents"},{"location":"DE-Projects/JsonValidator/Python-ValidateJSONs/#validating-json-data-with-python-and-json-schema","text":"","title":"Validating JSON Data with Python and JSON Schema"},{"location":"DE-Projects/JsonValidator/Python-ValidateJSONs/#introduction","text":"This is a short tutorial on how to use Python to validate JSON file data. Rather than iterating through all the fields in the JSON, usage of libraries like JSONschema etc. are considered more efficient. Hence, I thought of sharing my experience.","title":"Introduction"},{"location":"DE-Projects/JsonValidator/Python-ValidateJSONs/#prerequisites","text":"Python Installation : Ensure Python is installed on your system. If not, download and install it from the official Python website . JSON Files : Weather Data File ( weather.json ) : A JSON file containing weather data, typically obtained from weather APIs e.g. weatherapi . JSON Schema File ( weather_schema.json ) : A JSON schema file that defines the expected structure, required fields, and data types of the weather data JSON. Tools like Quicktype or JSON Schema Tool can help generate a schema from a JSON example. jsonschema Library : This Python library is used for validating JSON data against a schema. Install it using pip: pip install jsonschema","title":"Prerequisites"},{"location":"DE-Projects/JsonValidator/Python-ValidateJSONs/#how-to-run-the-script","text":"Prepare Your JSON Files : Ensure your weather.json and weather_schema.json are placed in the same directory as your script. Run the Script : Open Visual Studio Code, create a Python file or a Jupyter notebook And copy-paste the code below. Adjust as needed. ```python import json from jsonschema import validate, ValidationError","title":"How to Run the Script"},{"location":"DE-Projects/JsonValidator/Python-ValidateJSONs/#load-the-json-data-and-schema","text":"with open('weather.json', 'r') as file: weather_data = json.load(file) with open('weather_schema.json', 'r') as file: schema = json.load(file)","title":"Load the JSON data and schema"},{"location":"DE-Projects/JsonValidator/Python-ValidateJSONs/#function-to-validate-json","text":"def is_valid_json(data, schema): try: validate(instance=data, schema=schema) return True except ValidationError as ve: print(f\"Validation error: {ve}\") return False","title":"Function to validate JSON"},{"location":"DE-Projects/JsonValidator/Python-ValidateJSONs/#validate-the-weather-data","text":"if is_valid_json(weather_data, schema): print(\"Validation Successful!\") else: print(\"Validation Failed!\") ``` Run and debug using the editor.","title":"Validate the weather data"},{"location":"DE-Projects/JsonValidator/Python-ValidateJSONs/#code-overview","text":"The script consists of the following components: Loading JSON Data and Schema : We use Python's built-in json module to load the weather data and the schema from their respective files. Validation Logic : Utilizing the jsonschema.validate() function, we check if the JSON data adheres to the schema, capturing any validation errors that might indicate discrepancies. Error Handling : The script identifies and prints specific validation errors, making it easier to pinpoint issues in the data or the schema.","title":"Code Overview"},{"location":"DE-Projects/JsonValidator/Python-ValidateJSONs/#conclusion","text":"This code can be further enhanced to be included in an Azure Function. For example, if you are fetching data as JSON format from a web API, this code can be added there to perform data validation. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Conclusion"},{"location":"DE-Projects/Sparkzure/HomeProjectSparkzure/","text":"Overview How to Connect an local/on-premise Pyspark Setup with Azure Data Lake Method 1: How to rearrange items in Blob storage using an local/on-premise Pyspark \\& Hadoop Jars Method 2: How to rearrange items in Blob storage using an local/on-premise Python and Azure Python libraries(SDK) How to schedule our Blob file organizer Python script usign Azure Timer-Trigger Function Overview In this article I will show you how you can connect to Azure Storage and perform blob operations from a Standlone Pyspark Setup. We will make use of Hadoop jars to perform the same. How to Connect an local/on-premise Pyspark Setup with Azure Data Lake We all know that Azure Databricks easily connects with ADLS because they share the same Azure setup. But if you're using a basic Spark setup, it gets a bit tricky because you have to carefully manage some JAR files. In this guide, we'll start our journey by making a simple connection to Azure from a standalone Spark and fetching some basic info. I'll also show you how to set up this connection inside a Docker container. We'll also learn how to connect Visual Studio Code to this Docker, so you can run PySpark easily! Read more... Method 1: How to rearrange items in Blob storage using an local/on-premise Pyspark & Hadoop Jars In this article, we'll see how to sort files in an Azure Data Lake Container by using a Standalone Spark application. You could use Azure Data Factory, Databricks, or Azure Logic Apps, but this method stands out. It's an alternative and often much cheaper than the other mentioned Azure services. This is a real-world requirement; having a structure like this can make partition pruning more efficient during query time, especially if you're using a system like Apache Hive or Delta Lake. Read more... Method 2: How to rearrange items in Blob storage using an local/on-premise Python and Azure Python libraries(SDK) I the previous article I showed you how to sort files using Spark and Hadoop Jars. Here I will show you how using just straightforward python method in a local setup we can achieve teh same output, i.e, sort the files inside our Azure blob container. This is a real-world-scenario and such well-partioned data structures are required for analysis and Migration. Read more... How to schedule our Blob file organizer Python script usign Azure Timer-Trigger Function In the last two articles I showed you and artisinal-appreaoch to organize content inside Azure blob storage. But, we may require to schedule such function. Here I will show you how I converted my Python script[Method2] to a timer-triggered Azure function which runs everyday at 11.30 PM. Read more... \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Project Sparkzure"},{"location":"DE-Projects/Sparkzure/HomeProjectSparkzure/#overview","text":"In this article I will show you how you can connect to Azure Storage and perform blob operations from a Standlone Pyspark Setup. We will make use of Hadoop jars to perform the same.","title":"Overview"},{"location":"DE-Projects/Sparkzure/HomeProjectSparkzure/#how-to-connect-an-localon-premise-pyspark-setup-with-azure-data-lake","text":"We all know that Azure Databricks easily connects with ADLS because they share the same Azure setup. But if you're using a basic Spark setup, it gets a bit tricky because you have to carefully manage some JAR files. In this guide, we'll start our journey by making a simple connection to Azure from a standalone Spark and fetching some basic info. I'll also show you how to set up this connection inside a Docker container. We'll also learn how to connect Visual Studio Code to this Docker, so you can run PySpark easily! Read more...","title":"How to Connect an local/on-premise Pyspark Setup with Azure Data Lake"},{"location":"DE-Projects/Sparkzure/HomeProjectSparkzure/#method-1-how-to-rearrange-items-in-blob-storage-using-an-localon-premise-pyspark-hadoop-jars","text":"In this article, we'll see how to sort files in an Azure Data Lake Container by using a Standalone Spark application. You could use Azure Data Factory, Databricks, or Azure Logic Apps, but this method stands out. It's an alternative and often much cheaper than the other mentioned Azure services. This is a real-world requirement; having a structure like this can make partition pruning more efficient during query time, especially if you're using a system like Apache Hive or Delta Lake. Read more...","title":"Method 1: How to rearrange items in Blob storage using an local/on-premise Pyspark &amp; Hadoop Jars"},{"location":"DE-Projects/Sparkzure/HomeProjectSparkzure/#method-2-how-to-rearrange-items-in-blob-storage-using-an-localon-premise-python-and-azure-python-librariessdk","text":"I the previous article I showed you how to sort files using Spark and Hadoop Jars. Here I will show you how using just straightforward python method in a local setup we can achieve teh same output, i.e, sort the files inside our Azure blob container. This is a real-world-scenario and such well-partioned data structures are required for analysis and Migration. Read more...","title":"Method 2: How to rearrange items in Blob storage using an local/on-premise Python and Azure Python libraries(SDK)"},{"location":"DE-Projects/Sparkzure/HomeProjectSparkzure/#how-to-schedule-our-blob-file-organizer-python-script-usign-azure-timer-trigger-function","text":"In the last two articles I showed you and artisinal-appreaoch to organize content inside Azure blob storage. But, we may require to schedule such function. Here I will show you how I converted my Python script[Method2] to a timer-triggered Azure function which runs everyday at 11.30 PM. Read more... \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"How to schedule our Blob file organizer Python script usign Azure Timer-Trigger Function"},{"location":"DE-Projects/StreamKraft/HomeProjectStreamKraft/","text":"StreamKraft: Real-Time Music Data Pipeline. Kafka. SparkStreaming. MongoDB. Project Setup Step 1 - Dataset prepartion for Fake-streaming Step 2 - Fake-streaming to Kafka Topic Step 3 - Reading from Kafka with Spark Streaming and saving to MongoDB Appendix StreamKraft: Real-Time Music Data Pipeline. Kafka. SparkStreaming. MongoDB. In this project, we'll be building a real-time data processing pipeline using the million songs dataset. We'll use a Python script along with the dataset to create a fake streaming that will be sent to a Kafka topic. Then, we'll use Spark streaming to receive data from the topic and store it in MongoDB. The entire project will be run in a containerized environment. Along the way, we'll learn how to connect the Spark cluster to the Kafka environment, how to stream data to and from a Kafka topic, and how to use Spark streaming to save data to MongoDB. Project Setup Here is a list of some key components of the project Category Software/Configuration Hardware Mac M1 RAM 16 GB Operating System macOS 14.2 Containerization Docker Spark Cluster Bitnami Spark 3.5 Cluster - 1 Master - 2 Workers Images 1. Bitnami Spark Cluster 2. Kafka Setup Message Broker Apache Kafka KRaft(Kafka Without Zookeper) (Docker Image: confluentinc/cp-kafka 7.5.1) Programming Language Python 3.11 Integrated Development Environment Visual Studio Code Step 1 - Dataset prepartion for Fake-streaming Download the million dollar summary dataset here Place it in a folder accessible to the code snippet below and run the following script import json import h5py import numpy as np def h5_to_json(h5_file_path, json_file_path): with h5py.File(h5_file_path, 'r') as h5_file: # Access the metadata group metadata = h5_file['metadata'] # Access the songs dataset within metadata songs = metadata['songs'] # Create a list to hold song data data_to_export = [] # Iterate over each entry in the songs dataset for song in songs: # Convert fields to the correct Python type, e.g., decode bytes to string song_data = { 'analyzer_version': song['analyzer_version'].decode('UTF-8') if song['analyzer_version'] else None, 'artist_7digitalid': song['artist_7digitalid'].item(), 'artist_familiarity': song['artist_familiarity'].item(), 'artist_hotttnesss': song['artist_hotttnesss'].item(), 'artist_id': song['artist_id'].decode('UTF-8'), 'artist_latitude': song['artist_latitude'].item() if song['artist_latitude'] else None, 'artist_location': song['artist_location'].decode('UTF-8') if song['artist_location'] else None, 'artist_longitude': song['artist_longitude'].item() if song['artist_longitude'] else None, 'artist_mbid': song['artist_mbid'].decode('UTF-8'), 'artist_name': song['artist_name'].decode('UTF-8'), 'artist_playmeid': song['artist_playmeid'].item(), 'idx_artist_terms': song['idx_artist_terms'].item(), 'idx_similar_artists': song['idx_similar_artists'].item(), 'release': song['release'].decode('UTF-8'), 'release_7digitalid': song['release_7digitalid'].item(), 'song_hotttnesss': song['song_hotttnesss'].item(), 'song_id': song['song_id'].decode('UTF-8'), 'title': song['title'].decode('UTF-8'), 'track_7digitalid': song['track_7digitalid'].item() } data_to_export.append(song_data) # Write the data to a JSON file with open(json_file_path, 'w') as json_file: json.dump(data_to_export, json_file, indent=4) # Replace with your actual file paths h5_to_json(r'C:\\Users\\dwaip\\OneDrive\\Work\\Projects\\AzureTuneStream\\dataset\\msd_summary_file.h5', r'C:\\Users\\dwaip\\OneDrive\\Work\\Projects\\AzureTuneStream\\dataset\\msd_summary_file.json') Step 2 - Fake-streaming to Kafka Topic Execute the script provided to simulate real-world live streaming to a Kafka topic using data from the million dollar dataset. Note: - Ensure all non-Kafka containers are part of the confluent-kafka_default network, automatically assigned to Kafka Clusters upon creation with docker-compose. - To add non-Kafka containers to the confluent-kafka_default network, use the following command: docker network connect confluent-kafka_default [container_name] If the other containres are not added, they will not be able to connect to the broker. \"\"\" This code streams the 700 MB streamify content to kafka. 1. The container running this code should be part of \"confluent-kafka_default\" network. To add, run this command: connect confluent-kafka_default [external-container-name-or-id] 2. Only this broker address works \"broker:29092\". 3. Topics were populated in Kafka and checked at http://localhost:9021 4. Takes 36 seconds to execute \"\"\" \"\"\" Frequently encountered error: AnalysisException: Failed to find data source: kafka. This error happens and then goes away. Sometimes happens all of a sudden. Repeteadly doing it resolves the error. Solution: Sometimes just restart solves it. Probably a connection problem. \"\"\" from pyspark.sql import SparkSession from pyspark.sql.functions import col, to_json, struct import json # Initialize a Spark session with enhanced memory settings spark = SparkSession \\ .builder \\ .appName(\"FakeEventStreamerSparkCluster\") \\ .master(\"spark://spark-master:7077\") \\ .config(\"spark.driver.memory\", \"15g\") \\ .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\ .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0')\\ .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\ .getOrCreate() # # AnalysisException: Failed to find data source: kafka. # Set log level to monitor execution spark.sparkContext.setLogLevel(\"INFO\") print(\"Spark session initialized.\") # Read the JSON file into a Spark DataFrame spark_df = spark.read.json('/opt/shared-data/dataset.json', multiLine=True) # Serialize the DataFrame into a JSON string serialized_df = spark_df.select(to_json(struct([col(c) for c in spark_df.columns])).alias('value')) # Kafka producer settings kafka_servers = \"broker:29092\" kafka_topic = \"tunestream\" # Write the DataFrame to Kafka serialized_df.write \\ .format(\"kafka\") \\ .option(\"kafka.bootstrap.servers\", kafka_servers) \\ .option(\"topic\", kafka_topic) \\ .save() print(\"Data sent to Kafka.\") spark.stop() Step 3 - Reading from Kafka with Spark Streaming and saving to MongoDB Once the data is successfully streaming through the Kafka topic, we can use Spark Streaming to analyze and process the incoming information. This shows a real-time data analysis. from pyspark.sql import SparkSession from pyspark.sql.functions import col, to_json, struct import json # Common errors: Caused by: java.lang.ClassNotFoundException: mongo.DefaultSource # Solution: No proper solution. Goes away by itself # https://mvnrepository.com/artifact/org.mongodb.spark/mongo-spark-connector # Initialize a Spark session with enhanced memory settings spark = SparkSession \\ .builder \\ .appName(\"FakeEventStreamerSparkCluster\") \\ .master(\"spark://spark-master:7077\") \\ .config(\"spark.driver.memory\", \"15g\") \\ .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\ .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\ .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\ .getOrCreate() spark.sparkContext.setLogLevel(\"INFO\") # for verbose comments # Read from Kafka df = spark \\ .readStream \\ .format(\"kafka\") \\ .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\ .option(\"subscribe\", \"tunestream\") \\ .load() df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") # Common error: UnsupportedOperationException: Data source mongo does not support streamed writing. # Write to MongoDB mongodb_uri = \"mongodb://my-mongodb:27017\" # \"mongodb://<mongoDBContainerNameOriPAddress>:27017\" database_name = \"tuneStreamDB\" # Spark will create, if not present collection_name = \"tuneStreamData\" # Spark will create, if not present # The snippet below gives rise to : UnsupportedOperationException: Data source mongo does not support streamed writing. Follow the workaround shown later. # df.writeStream \\ # .format(\"mongo\") \\ # .option(\"uri\", mongodb_uri) \\ # .option(\"database\", database_name) \\ # .option(\"collection\", collection_name) \\ # .trigger(processingTime=\"10 seconds\") \\ # .start() \\ # .awaitTermination() ''' MongoDB Spark Connector does not currently support streamed writing. As a workaround, we write data to a batch DataFrame and then to MongoDB. ''' # Use foreachBatch to write batch data to MongoDB query = df.writeStream \\ .trigger(processingTime=\"10 seconds\") \\ .foreachBatch(lambda df, epochId: df.write.format(\"mongo\") \\ .option(\"uri\", mongodb_uri) \\ .option(\"database\", database_name) \\ .option(\"collection\", collection_name) \\ .mode(\"append\") \\ .save()) \\ .start() \\ # Log the query execution plan query.explain() # Wait for the termination of the streaming query query.awaitTermination() spark.stop() Appendix Pyspark Structured Streaming . Streaming using a file-based dataset .","title":"Project StreamKraft"},{"location":"DE-Projects/StreamKraft/HomeProjectStreamKraft/#streamkraft-real-time-music-data-pipeline-kafka-sparkstreaming-mongodb","text":"In this project, we'll be building a real-time data processing pipeline using the million songs dataset. We'll use a Python script along with the dataset to create a fake streaming that will be sent to a Kafka topic. Then, we'll use Spark streaming to receive data from the topic and store it in MongoDB. The entire project will be run in a containerized environment. Along the way, we'll learn how to connect the Spark cluster to the Kafka environment, how to stream data to and from a Kafka topic, and how to use Spark streaming to save data to MongoDB.","title":"StreamKraft: Real-Time Music Data Pipeline. Kafka. SparkStreaming. MongoDB."},{"location":"DE-Projects/StreamKraft/HomeProjectStreamKraft/#project-setup","text":"Here is a list of some key components of the project Category Software/Configuration Hardware Mac M1 RAM 16 GB Operating System macOS 14.2 Containerization Docker Spark Cluster Bitnami Spark 3.5 Cluster - 1 Master - 2 Workers Images 1. Bitnami Spark Cluster 2. Kafka Setup Message Broker Apache Kafka KRaft(Kafka Without Zookeper) (Docker Image: confluentinc/cp-kafka 7.5.1) Programming Language Python 3.11 Integrated Development Environment Visual Studio Code","title":"Project Setup"},{"location":"DE-Projects/StreamKraft/HomeProjectStreamKraft/#step-1-dataset-prepartion-for-fake-streaming","text":"Download the million dollar summary dataset here Place it in a folder accessible to the code snippet below and run the following script import json import h5py import numpy as np def h5_to_json(h5_file_path, json_file_path): with h5py.File(h5_file_path, 'r') as h5_file: # Access the metadata group metadata = h5_file['metadata'] # Access the songs dataset within metadata songs = metadata['songs'] # Create a list to hold song data data_to_export = [] # Iterate over each entry in the songs dataset for song in songs: # Convert fields to the correct Python type, e.g., decode bytes to string song_data = { 'analyzer_version': song['analyzer_version'].decode('UTF-8') if song['analyzer_version'] else None, 'artist_7digitalid': song['artist_7digitalid'].item(), 'artist_familiarity': song['artist_familiarity'].item(), 'artist_hotttnesss': song['artist_hotttnesss'].item(), 'artist_id': song['artist_id'].decode('UTF-8'), 'artist_latitude': song['artist_latitude'].item() if song['artist_latitude'] else None, 'artist_location': song['artist_location'].decode('UTF-8') if song['artist_location'] else None, 'artist_longitude': song['artist_longitude'].item() if song['artist_longitude'] else None, 'artist_mbid': song['artist_mbid'].decode('UTF-8'), 'artist_name': song['artist_name'].decode('UTF-8'), 'artist_playmeid': song['artist_playmeid'].item(), 'idx_artist_terms': song['idx_artist_terms'].item(), 'idx_similar_artists': song['idx_similar_artists'].item(), 'release': song['release'].decode('UTF-8'), 'release_7digitalid': song['release_7digitalid'].item(), 'song_hotttnesss': song['song_hotttnesss'].item(), 'song_id': song['song_id'].decode('UTF-8'), 'title': song['title'].decode('UTF-8'), 'track_7digitalid': song['track_7digitalid'].item() } data_to_export.append(song_data) # Write the data to a JSON file with open(json_file_path, 'w') as json_file: json.dump(data_to_export, json_file, indent=4) # Replace with your actual file paths h5_to_json(r'C:\\Users\\dwaip\\OneDrive\\Work\\Projects\\AzureTuneStream\\dataset\\msd_summary_file.h5', r'C:\\Users\\dwaip\\OneDrive\\Work\\Projects\\AzureTuneStream\\dataset\\msd_summary_file.json')","title":"Step 1 - Dataset prepartion for Fake-streaming"},{"location":"DE-Projects/StreamKraft/HomeProjectStreamKraft/#step-2-fake-streaming-to-kafka-topic","text":"Execute the script provided to simulate real-world live streaming to a Kafka topic using data from the million dollar dataset. Note: - Ensure all non-Kafka containers are part of the confluent-kafka_default network, automatically assigned to Kafka Clusters upon creation with docker-compose. - To add non-Kafka containers to the confluent-kafka_default network, use the following command: docker network connect confluent-kafka_default [container_name] If the other containres are not added, they will not be able to connect to the broker. \"\"\" This code streams the 700 MB streamify content to kafka. 1. The container running this code should be part of \"confluent-kafka_default\" network. To add, run this command: connect confluent-kafka_default [external-container-name-or-id] 2. Only this broker address works \"broker:29092\". 3. Topics were populated in Kafka and checked at http://localhost:9021 4. Takes 36 seconds to execute \"\"\" \"\"\" Frequently encountered error: AnalysisException: Failed to find data source: kafka. This error happens and then goes away. Sometimes happens all of a sudden. Repeteadly doing it resolves the error. Solution: Sometimes just restart solves it. Probably a connection problem. \"\"\" from pyspark.sql import SparkSession from pyspark.sql.functions import col, to_json, struct import json # Initialize a Spark session with enhanced memory settings spark = SparkSession \\ .builder \\ .appName(\"FakeEventStreamerSparkCluster\") \\ .master(\"spark://spark-master:7077\") \\ .config(\"spark.driver.memory\", \"15g\") \\ .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\ .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0')\\ .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\ .getOrCreate() # # AnalysisException: Failed to find data source: kafka. # Set log level to monitor execution spark.sparkContext.setLogLevel(\"INFO\") print(\"Spark session initialized.\") # Read the JSON file into a Spark DataFrame spark_df = spark.read.json('/opt/shared-data/dataset.json', multiLine=True) # Serialize the DataFrame into a JSON string serialized_df = spark_df.select(to_json(struct([col(c) for c in spark_df.columns])).alias('value')) # Kafka producer settings kafka_servers = \"broker:29092\" kafka_topic = \"tunestream\" # Write the DataFrame to Kafka serialized_df.write \\ .format(\"kafka\") \\ .option(\"kafka.bootstrap.servers\", kafka_servers) \\ .option(\"topic\", kafka_topic) \\ .save() print(\"Data sent to Kafka.\") spark.stop()","title":"Step 2 - Fake-streaming to Kafka Topic"},{"location":"DE-Projects/StreamKraft/HomeProjectStreamKraft/#step-3-reading-from-kafka-with-spark-streaming-and-saving-to-mongodb","text":"Once the data is successfully streaming through the Kafka topic, we can use Spark Streaming to analyze and process the incoming information. This shows a real-time data analysis. from pyspark.sql import SparkSession from pyspark.sql.functions import col, to_json, struct import json # Common errors: Caused by: java.lang.ClassNotFoundException: mongo.DefaultSource # Solution: No proper solution. Goes away by itself # https://mvnrepository.com/artifact/org.mongodb.spark/mongo-spark-connector # Initialize a Spark session with enhanced memory settings spark = SparkSession \\ .builder \\ .appName(\"FakeEventStreamerSparkCluster\") \\ .master(\"spark://spark-master:7077\") \\ .config(\"spark.driver.memory\", \"15g\") \\ .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\ .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\ .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\ .getOrCreate() spark.sparkContext.setLogLevel(\"INFO\") # for verbose comments # Read from Kafka df = spark \\ .readStream \\ .format(\"kafka\") \\ .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\ .option(\"subscribe\", \"tunestream\") \\ .load() df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") # Common error: UnsupportedOperationException: Data source mongo does not support streamed writing. # Write to MongoDB mongodb_uri = \"mongodb://my-mongodb:27017\" # \"mongodb://<mongoDBContainerNameOriPAddress>:27017\" database_name = \"tuneStreamDB\" # Spark will create, if not present collection_name = \"tuneStreamData\" # Spark will create, if not present # The snippet below gives rise to : UnsupportedOperationException: Data source mongo does not support streamed writing. Follow the workaround shown later. # df.writeStream \\ # .format(\"mongo\") \\ # .option(\"uri\", mongodb_uri) \\ # .option(\"database\", database_name) \\ # .option(\"collection\", collection_name) \\ # .trigger(processingTime=\"10 seconds\") \\ # .start() \\ # .awaitTermination() ''' MongoDB Spark Connector does not currently support streamed writing. As a workaround, we write data to a batch DataFrame and then to MongoDB. ''' # Use foreachBatch to write batch data to MongoDB query = df.writeStream \\ .trigger(processingTime=\"10 seconds\") \\ .foreachBatch(lambda df, epochId: df.write.format(\"mongo\") \\ .option(\"uri\", mongodb_uri) \\ .option(\"database\", database_name) \\ .option(\"collection\", collection_name) \\ .mode(\"append\") \\ .save()) \\ .start() \\ # Log the query execution plan query.explain() # Wait for the termination of the streaming query query.awaitTermination() spark.stop()","title":"Step 3 - Reading from Kafka with Spark Streaming and saving to MongoDB"},{"location":"DE-Projects/StreamKraft/HomeProjectStreamKraft/#appendix","text":"Pyspark Structured Streaming . Streaming using a file-based dataset .","title":"Appendix"},{"location":"DevOps/1.1_Hello_GitHub_Actions_Workflow/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Background GitHub Actions Workflows are the core of GitHub CI/CD. You can set them up easily. VS Code has many extensions for GitHub. In this article, I will show how to create a simple GitHub Actions Workflow. It will help you understand the flow. Later using similar steps you can create complex Action Workflows. Let's get started. Create an End-to-End GitHub Actions Workflow Using VS Code Create a New Repository Using GitHub Desktop Open GitHub Desktop . Click on New repository . Provide a name for your repository, check the local path, and click on Create repository . Click on Publish repository . Then Click on Open in Visual Studio Code . In the VS Code you will be able to see your folder .gitattributes file Set Up the GitHub Actions Workflow in VS Code Create Folder Structure : In VS Code, click on the create folder icon and create a .github folder. Inside the .github folder, create another folder named workflows . Create Workflow File : Inside the workflows folder, create a file named hello-github-actions.yml . Add the following code to hello-github-actions.yml : yaml name: Hello GitHub Actions on: [push] jobs: say-hello: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v4 - name: Run hello.py run: python hello.py Create a Python Script In the root of your repository, create a new file named hello.py . Add the following code to hello.py : python print(\"Hello, GitHub Actions!\") Commit and Push the Changes Click on the source control icon in VS Code. Uncomment or Write a commit message (e.g., \"This is a message bla bla bla\"). Click on Commit to commit the changes. Click on Push to push the changes to GitHub. This will trigger the workflow. Check the Workflow Execution In VS Code, click on the GitHub Actions Extension in the left pane. This will show you the GitHub Actions Workflow that you created. You can click on the globe icon to see the workflow directly on GitHub. Summary Well, this was a hand-holding article. You can do a lot more with GitHub Actions workflows.","title":"Hello GitHub Actions Workflow"},{"location":"DevOps/1.1_Hello_GitHub_Actions_Workflow/#background","text":"GitHub Actions Workflows are the core of GitHub CI/CD. You can set them up easily. VS Code has many extensions for GitHub. In this article, I will show how to create a simple GitHub Actions Workflow. It will help you understand the flow. Later using similar steps you can create complex Action Workflows. Let's get started.","title":"Background"},{"location":"DevOps/1.1_Hello_GitHub_Actions_Workflow/#create-an-end-to-end-github-actions-workflow-using-vs-code","text":"","title":"Create an End-to-End GitHub Actions Workflow Using VS Code"},{"location":"DevOps/1.1_Hello_GitHub_Actions_Workflow/#create-a-new-repository-using-github-desktop","text":"Open GitHub Desktop . Click on New repository . Provide a name for your repository, check the local path, and click on Create repository . Click on Publish repository . Then Click on Open in Visual Studio Code . In the VS Code you will be able to see your folder .gitattributes file","title":"Create a New Repository Using GitHub Desktop"},{"location":"DevOps/1.1_Hello_GitHub_Actions_Workflow/#set-up-the-github-actions-workflow-in-vs-code","text":"Create Folder Structure : In VS Code, click on the create folder icon and create a .github folder. Inside the .github folder, create another folder named workflows . Create Workflow File : Inside the workflows folder, create a file named hello-github-actions.yml . Add the following code to hello-github-actions.yml : yaml name: Hello GitHub Actions on: [push] jobs: say-hello: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v4 - name: Run hello.py run: python hello.py","title":"Set Up the GitHub Actions Workflow in VS Code"},{"location":"DevOps/1.1_Hello_GitHub_Actions_Workflow/#create-a-python-script","text":"In the root of your repository, create a new file named hello.py . Add the following code to hello.py : python print(\"Hello, GitHub Actions!\")","title":"Create a Python Script"},{"location":"DevOps/1.1_Hello_GitHub_Actions_Workflow/#commit-and-push-the-changes","text":"Click on the source control icon in VS Code. Uncomment or Write a commit message (e.g., \"This is a message bla bla bla\"). Click on Commit to commit the changes. Click on Push to push the changes to GitHub. This will trigger the workflow.","title":"Commit and Push the Changes"},{"location":"DevOps/1.1_Hello_GitHub_Actions_Workflow/#check-the-workflow-execution","text":"In VS Code, click on the GitHub Actions Extension in the left pane. This will show you the GitHub Actions Workflow that you created. You can click on the globe icon to see the workflow directly on GitHub.","title":"Check the Workflow Execution"},{"location":"DevOps/1.1_Hello_GitHub_Actions_Workflow/#summary","text":"Well, this was a hand-holding article. You can do a lot more with GitHub Actions workflows.","title":"Summary"},{"location":"DevOps/1.1_Sample_Workflows/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Example 1: Basic CI Pipeline This example shows a simple CI pipeline that runs tests on every push to the main branch. Workflow YAML: name: CI Pipeline on: push: branches: - main jobs: build-and-test: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Set up Node.js uses: actions/setup-node@v2 with: node-version: '14' - name: Install dependencies run: npm install - name: Run tests run: npm test Explanation: - on: push: Triggers the workflow on pushes to the main branch. - jobs: Defines the job that will run. - steps: Lists the steps to be executed, such as checking out the code, setting up Node.js, installing dependencies, and running tests. Example 2: CI/CD Pipeline with Deployment This example shows a CI/CD pipeline that builds, tests, and deploys a Node.js application to an Azure App Service. Workflow YAML: name: CI/CD Pipeline on: push: branches: - main jobs: build-test-deploy: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Set up Node.js uses: actions/setup-node@v2 with: node-version: '14' - name: Install dependencies run: npm install - name: Run tests run: npm test - name: Build project run: npm run build - name: Deploy to Azure Web App uses: azure/webapps-deploy@v2 with: app-name: 'my-web-app' publish-profile: ${{ secrets.AZURE_WEBAPP_PUBLISH_PROFILE }} package: . Explanation: - on: push: Triggers the workflow on pushes to the main branch. - jobs: Defines the job that will run. - steps: Lists the steps to be executed, including checking out the code, setting up Node.js, installing dependencies, running tests, building the project, and deploying to Azure Web App. - secrets: Securely stores sensitive information like the Azure publish profile. Example 3: Scheduled Workflow This example demonstrates a workflow that runs tests every day at midnight. Workflow YAML: name: Scheduled Test Run on: schedule: - cron: '0 0 * * *' jobs: test: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Set up Python uses: actions/setup-python@v2 with: python-version: '3.8' - name: Install dependencies run: pip install -r requirements.txt - name: Run tests run: pytest Explanation: - on: schedule: Triggers the workflow based on a cron schedule (every day at midnight). - jobs: Defines the job that will run. - steps: Lists the steps to be executed, such as checking out the code, setting up Python, installing dependencies, and running tests. Example 4: Multi-Environment Deployment This example shows how to deploy to different environments (staging and production) based on the branch. Workflow YAML: name: Multi-Environment Deployment on: push: branches: - main - staging jobs: build-and-deploy: runs-on: ubuntu-latest strategy: matrix: environment: [staging, production] steps: - name: Checkout code uses: actions/checkout@v2 - name: Set up Node.js uses: actions/setup-node@v2 with: node-version: '14' - name: Install dependencies run: npm install - name: Run tests run: npm test - name: Build project run: npm run build - name: Deploy to Azure Web App uses: azure/webapps-deploy@v2 with: app-name: ${{ matrix.environment == 'production' && 'my-web-app-prod' || 'my-web-app-staging' }} publish-profile: ${{ secrets.AZURE_WEBAPP_PUBLISH_PROFILE_${{ matrix.environment | upper }} }} package: . Explanation: - on: push: Triggers the workflow on pushes to the main and staging branches. - strategy: matrix: Defines a matrix strategy to run the job for both staging and production environments. - steps: Lists the steps to be executed, including checking out the code, setting up Node.js, installing dependencies, running tests, building the project, and deploying to Azure Web App. - conditional logic: Uses conditional logic to deploy to the appropriate environment based on the branch.","title":"Sample Workflows"},{"location":"DevOps/1.1_Sample_Workflows/#example-1-basic-ci-pipeline","text":"This example shows a simple CI pipeline that runs tests on every push to the main branch. Workflow YAML: name: CI Pipeline on: push: branches: - main jobs: build-and-test: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Set up Node.js uses: actions/setup-node@v2 with: node-version: '14' - name: Install dependencies run: npm install - name: Run tests run: npm test Explanation: - on: push: Triggers the workflow on pushes to the main branch. - jobs: Defines the job that will run. - steps: Lists the steps to be executed, such as checking out the code, setting up Node.js, installing dependencies, and running tests.","title":"Example 1: Basic CI Pipeline"},{"location":"DevOps/1.1_Sample_Workflows/#example-2-cicd-pipeline-with-deployment","text":"This example shows a CI/CD pipeline that builds, tests, and deploys a Node.js application to an Azure App Service. Workflow YAML: name: CI/CD Pipeline on: push: branches: - main jobs: build-test-deploy: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Set up Node.js uses: actions/setup-node@v2 with: node-version: '14' - name: Install dependencies run: npm install - name: Run tests run: npm test - name: Build project run: npm run build - name: Deploy to Azure Web App uses: azure/webapps-deploy@v2 with: app-name: 'my-web-app' publish-profile: ${{ secrets.AZURE_WEBAPP_PUBLISH_PROFILE }} package: . Explanation: - on: push: Triggers the workflow on pushes to the main branch. - jobs: Defines the job that will run. - steps: Lists the steps to be executed, including checking out the code, setting up Node.js, installing dependencies, running tests, building the project, and deploying to Azure Web App. - secrets: Securely stores sensitive information like the Azure publish profile.","title":"Example 2: CI/CD Pipeline with Deployment"},{"location":"DevOps/1.1_Sample_Workflows/#example-3-scheduled-workflow","text":"This example demonstrates a workflow that runs tests every day at midnight. Workflow YAML: name: Scheduled Test Run on: schedule: - cron: '0 0 * * *' jobs: test: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Set up Python uses: actions/setup-python@v2 with: python-version: '3.8' - name: Install dependencies run: pip install -r requirements.txt - name: Run tests run: pytest Explanation: - on: schedule: Triggers the workflow based on a cron schedule (every day at midnight). - jobs: Defines the job that will run. - steps: Lists the steps to be executed, such as checking out the code, setting up Python, installing dependencies, and running tests.","title":"Example 3: Scheduled Workflow"},{"location":"DevOps/1.1_Sample_Workflows/#example-4-multi-environment-deployment","text":"This example shows how to deploy to different environments (staging and production) based on the branch. Workflow YAML: name: Multi-Environment Deployment on: push: branches: - main - staging jobs: build-and-deploy: runs-on: ubuntu-latest strategy: matrix: environment: [staging, production] steps: - name: Checkout code uses: actions/checkout@v2 - name: Set up Node.js uses: actions/setup-node@v2 with: node-version: '14' - name: Install dependencies run: npm install - name: Run tests run: npm test - name: Build project run: npm run build - name: Deploy to Azure Web App uses: azure/webapps-deploy@v2 with: app-name: ${{ matrix.environment == 'production' && 'my-web-app-prod' || 'my-web-app-staging' }} publish-profile: ${{ secrets.AZURE_WEBAPP_PUBLISH_PROFILE_${{ matrix.environment | upper }} }} package: . Explanation: - on: push: Triggers the workflow on pushes to the main and staging branches. - strategy: matrix: Defines a matrix strategy to run the job for both staging and production environments. - steps: Lists the steps to be executed, including checking out the code, setting up Node.js, installing dependencies, running tests, building the project, and deploying to Azure Web App. - conditional logic: Uses conditional logic to deploy to the appropriate environment based on the branch.","title":"Example 4: Multi-Environment Deployment"},{"location":"DevOps/1_GitHub-Concepts/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Getting started with GitHub Repositories, Branches, Push, Pull, Commit Repositories Branches Push Pull Commits Understanding GitHub Actions In this article, I'll explain the core concepts of GitHub Actions. Further reading. For the Busy Readers GitHub Actions is a tool to automate code building and deployment tasks. With GitHub Actions, you create workflows . These workflows are YAML files inside the .github/workflows folder in your project. Workflows contain jobs, and jobs contain steps. Steps can be simple commands, scripts, or pre-built actions from the GitHub community. Another explanation: GitHub Actions is a GitHub feature that lets you run tasks when certain events happen in your code repository. You can use it to trigger any part of a CI/CD pipeline using webhooks on GitHub. GitHub Actions offers over 13,000 pre-written and tested CI/CD workflows. You can also write your own workflows or customize existing ones using YAML files. GitHub Actions is not just for CI/CD pipelines. Here are some other uses: Running nightly builds in the master branch for testing. Cleaning up old issues or bug reports. Creating bots that respond to comments or commands on pull requests or issues. In short, GitHub Actions is the GitHub feature that automates CI/CD and more. GitHub Action is a tool. GitHub Actions Workflow is the output of the tool. Key Concepts in GitHub Actions Here are the key terms you will hear most of the time when dealing with GitHub Actions: Workflows : YAML files inside the .github/workflows folder. Jobs : Workflows contain jobs, which are sets of steps. Steps : Jobs are made up of steps that run commands, scripts, or actions. Runners : Servers that execute the jobs (can be GitHub-provided or self-hosted). How It Works: 1. Trigger : An event, like pushing code or creating a pull request, triggers the workflow. 2. Workflow Activation : The specified workflow for that trigger starts running. 3. Jobs Execution : Jobs within the workflow run, either independently or in sequence. 4. Runners : Jobs use virtual machines provided by GitHub or self-hosted machines to run. Example Workflow You commit code to your repository. The workflow is triggered. Your code is built, tested, and deployed automatically. GitHub provides virtual machines for Linux, Windows, and macOS. Anatomy of a GitHub Workflow A GitHub Actions workflow is written as a YAML file inside the .github/workflows directory of a project/repository. Each workflow is stored as a separate YAML file in your code repository. This is how a typical workflow YAML file looks: name: CI on: push: branches: [ main ] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Build run: make build name : This workflow is named \"CI\" (Continuous Integration). You can name it anything you like. on : This workflow runs when there is a push to the main branch. jobs : This workflow has one job named build . runs-on : This job runs on an ubuntu-latest virtual machine. steps : Checkout code : Uses actions/checkout@v3 to pull the code. Build : Runs the make build command to build the project.","title":"GitHub Concepts"},{"location":"DevOps/1_GitHub-Concepts/#getting-started-with-github","text":"","title":"Getting started with GitHub"},{"location":"DevOps/1_GitHub-Concepts/#repositories-branches-push-pull-commit","text":"Repositories Branches Push Pull Commits","title":"Repositories, Branches, Push, Pull, Commit"},{"location":"DevOps/1_GitHub-Concepts/#understanding-github-actions","text":"In this article, I'll explain the core concepts of GitHub Actions. Further reading.","title":"Understanding GitHub Actions"},{"location":"DevOps/1_GitHub-Concepts/#for-the-busy-readers","text":"GitHub Actions is a tool to automate code building and deployment tasks. With GitHub Actions, you create workflows . These workflows are YAML files inside the .github/workflows folder in your project. Workflows contain jobs, and jobs contain steps. Steps can be simple commands, scripts, or pre-built actions from the GitHub community. Another explanation: GitHub Actions is a GitHub feature that lets you run tasks when certain events happen in your code repository. You can use it to trigger any part of a CI/CD pipeline using webhooks on GitHub. GitHub Actions offers over 13,000 pre-written and tested CI/CD workflows. You can also write your own workflows or customize existing ones using YAML files. GitHub Actions is not just for CI/CD pipelines. Here are some other uses: Running nightly builds in the master branch for testing. Cleaning up old issues or bug reports. Creating bots that respond to comments or commands on pull requests or issues. In short, GitHub Actions is the GitHub feature that automates CI/CD and more. GitHub Action is a tool. GitHub Actions Workflow is the output of the tool.","title":"For the Busy Readers"},{"location":"DevOps/1_GitHub-Concepts/#key-concepts-in-github-actions","text":"Here are the key terms you will hear most of the time when dealing with GitHub Actions: Workflows : YAML files inside the .github/workflows folder. Jobs : Workflows contain jobs, which are sets of steps. Steps : Jobs are made up of steps that run commands, scripts, or actions. Runners : Servers that execute the jobs (can be GitHub-provided or self-hosted). How It Works: 1. Trigger : An event, like pushing code or creating a pull request, triggers the workflow. 2. Workflow Activation : The specified workflow for that trigger starts running. 3. Jobs Execution : Jobs within the workflow run, either independently or in sequence. 4. Runners : Jobs use virtual machines provided by GitHub or self-hosted machines to run.","title":"Key Concepts in GitHub Actions"},{"location":"DevOps/1_GitHub-Concepts/#example-workflow","text":"You commit code to your repository. The workflow is triggered. Your code is built, tested, and deployed automatically. GitHub provides virtual machines for Linux, Windows, and macOS.","title":"Example Workflow"},{"location":"DevOps/1_GitHub-Concepts/#anatomy-of-a-github-workflow","text":"A GitHub Actions workflow is written as a YAML file inside the .github/workflows directory of a project/repository. Each workflow is stored as a separate YAML file in your code repository. This is how a typical workflow YAML file looks: name: CI on: push: branches: [ main ] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Build run: make build name : This workflow is named \"CI\" (Continuous Integration). You can name it anything you like. on : This workflow runs when there is a push to the main branch. jobs : This workflow has one job named build . runs-on : This job runs on an ubuntu-latest virtual machine. steps : Checkout code : Uses actions/checkout@v3 to pull the code. Build : Runs the make build command to build the project.","title":"Anatomy of a GitHub Workflow"},{"location":"DevOps/2.1_Self-Hosted_Agent_Windows/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Installing a Self-Hosted Agent on Local Windows(No Container) Download and Configure the Agent Log on to the machine using the account for which you've prepared permissions. Sign In to Azure Pipelines Open your web browser and sign in to Azure DevOps . Navigate to your organization settings: https://dev.azure.com/{your-org} . Navigate to Agent Pools Choose Azure DevOps > Organization settings . Select Agent pools . Choose the Agent pools tab. Select the Default pool , go to the Agents tab, and choose New agent . Download the Agent In the Get the agent dialog box, choose Windows . On the left pane, select the processor architecture of your Windows OS (x64 for 64-bit Windows, x86 for 32-bit Windows). On the right pane, click the Download button. Unpack the Agent Follow the instructions on the page to download the agent. Unpack the agent into a directory without spaces in its path (e.g., C:\\agents ). Put the extracted contents directly inside the C:\\agents folder. Configure the Agent Open a Command Prompt in the C:\\agents directory. Run the config.cmd script. When setup asks for your server URL, enter https://dev.azure.com/{org-Name} (e.g., https://dev.azure.com/MOUMITA001/ ). Choose PAT (Personal Access Token) for authentication and enter your PAT. Starting an Agent To start the agent, run the run.cmd script in the C:\\agents directory. Removing an Agent To remove the agent, follow these steps: Stop the agent by closing the Command Prompt window running the agent. Open a new Command Prompt in the C:\\agents directory. Run the config.cmd remove command.","title":"Self-Hosted-Agent-Windows"},{"location":"DevOps/2.1_Self-Hosted_Agent_Windows/#installing-a-self-hosted-agent-on-local-windowsno-container","text":"","title":"Installing a Self-Hosted Agent on Local Windows(No Container)"},{"location":"DevOps/2.1_Self-Hosted_Agent_Windows/#download-and-configure-the-agent","text":"Log on to the machine using the account for which you've prepared permissions.","title":"Download and Configure the Agent"},{"location":"DevOps/2.1_Self-Hosted_Agent_Windows/#sign-in-to-azure-pipelines","text":"Open your web browser and sign in to Azure DevOps . Navigate to your organization settings: https://dev.azure.com/{your-org} .","title":"Sign In to Azure Pipelines"},{"location":"DevOps/2.1_Self-Hosted_Agent_Windows/#navigate-to-agent-pools","text":"Choose Azure DevOps > Organization settings . Select Agent pools . Choose the Agent pools tab. Select the Default pool , go to the Agents tab, and choose New agent .","title":"Navigate to Agent Pools"},{"location":"DevOps/2.1_Self-Hosted_Agent_Windows/#download-the-agent","text":"In the Get the agent dialog box, choose Windows . On the left pane, select the processor architecture of your Windows OS (x64 for 64-bit Windows, x86 for 32-bit Windows). On the right pane, click the Download button.","title":"Download the Agent"},{"location":"DevOps/2.1_Self-Hosted_Agent_Windows/#unpack-the-agent","text":"Follow the instructions on the page to download the agent. Unpack the agent into a directory without spaces in its path (e.g., C:\\agents ). Put the extracted contents directly inside the C:\\agents folder.","title":"Unpack the Agent"},{"location":"DevOps/2.1_Self-Hosted_Agent_Windows/#configure-the-agent","text":"Open a Command Prompt in the C:\\agents directory. Run the config.cmd script. When setup asks for your server URL, enter https://dev.azure.com/{org-Name} (e.g., https://dev.azure.com/MOUMITA001/ ). Choose PAT (Personal Access Token) for authentication and enter your PAT.","title":"Configure the Agent"},{"location":"DevOps/2.1_Self-Hosted_Agent_Windows/#starting-an-agent","text":"To start the agent, run the run.cmd script in the C:\\agents directory.","title":"Starting an Agent"},{"location":"DevOps/2.1_Self-Hosted_Agent_Windows/#removing-an-agent","text":"To remove the agent, follow these steps: Stop the agent by closing the Command Prompt window running the agent. Open a new Command Prompt in the C:\\agents directory. Run the config.cmd remove command.","title":"Removing an Agent"},{"location":"DevOps/2.2_Self-Hosted_Agent_Windows_Container/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Create a Self-Hosted Agent in Docker Windows Container You can run your Azure Pipeline agent in Windows , Windows Container , and Ubuntu Container . This guide will show you how to set up your Azure Pipeline agent in a Windows Container. Setup We'll use a Windows laptop/desktop with Docker Desktop installed. First, we'll switch Docker Desktop to Windows Mode (it runs in Linux Mode by default) because the container will be a Windows OS. Then we'll create the image, and the rest will be easy. Install Docker for Windows First, make sure Docker Desktop is installed on your Windows machine. Switch Docker to Use Windows Containers Note: Docker can run both Linux and Windows containers. By default, Docker runs in Linux mode. For more details, refer to this guide . In PowerShell, run this command: Note: If you have existing Linux containers, they will disappear when you switch to Windows mode but will return when you switch back to Linux mode. You will get a new Docker desktop! & $Env:ProgramFiles\\Docker\\Docker\\DockerCli.exe -SwitchDaemon To switch back to Linux mode later, refer to the image below: Create and Build the Dockerfile Next, create the Dockerfile. Open a command prompt and create a new directory: mkdir \"C:\\azp-agent-in-docker\\\" cd \"C:\\azp-agent-in-docker\\\" Save the following content to a file called C:\\azp-agent-in-docker\\azp-agent-windows.dockerfile : FROM mcr.microsoft.com/windows/servercore:ltsc2022 WORKDIR /azp/ COPY ./start.ps1 ./ CMD powershell .\\start.ps1 Save the following content to C:\\azp-agent-in-docker\\start.ps1 : Note, this script is taken as-is from this location . function Print-Header ($header) { Write-Host \"`n${header}`n\" -ForegroundColor Cyan } if (-not (Test-Path Env:AZP_URL)) { Write-Error \"error: missing AZP_URL environment variable\" exit 1 } if (-not (Test-Path Env:AZP_TOKEN_FILE)) { if (-not (Test-Path Env:AZP_TOKEN)) { Write-Error \"error: missing AZP_TOKEN environment variable\" exit 1 } $Env:AZP_TOKEN_FILE = \"\\azp\\.token\" $Env:AZP_TOKEN | Out-File -FilePath $Env:AZP_TOKEN_FILE } Remove-Item Env:AZP_TOKEN if ((Test-Path Env:AZP_WORK) -and -not (Test-Path $Env:AZP_WORK)) { New-Item $Env:AZP_WORK -ItemType directory | Out-Null } New-Item \"\\azp\\agent\" -ItemType directory | Out-Null # Let the agent ignore the token env variables $Env:VSO_AGENT_IGNORE = \"AZP_TOKEN,AZP_TOKEN_FILE\" Set-Location agent Print-Header \"1. Determining matching Azure Pipelines agent...\" $base64AuthInfo = [Convert]::ToBase64String([Text.Encoding]::ASCII.GetBytes(\":$(Get-Content ${Env:AZP_TOKEN_FILE})\")) $package = Invoke-RestMethod -Headers @{Authorization=(\"Basic $base64AuthInfo\")} \"$(${Env:AZP_URL})/_apis/distributedtask/packages/agent?platform=win-x64&`$top=1\" $packageUrl = $package[0].Value.downloadUrl Write-Host $packageUrl Print-Header \"2. Downloading and installing Azure Pipelines agent...\" $wc = New-Object System.Net.WebClient $wc.DownloadFile($packageUrl, \"$(Get-Location)\\agent.zip\") Expand-Archive -Path \"agent.zip\" -DestinationPath \"\\azp\\agent\" try { Print-Header \"3. Configuring Azure Pipelines agent...\" .\\config.cmd --unattended ` --agent \"$(if (Test-Path Env:AZP_AGENT_NAME) { ${Env:AZP_AGENT_NAME} } else { hostname })\" ` --url \"$(${Env:AZP_URL})\" ` --auth PAT ` --token \"$(Get-Content ${Env:AZP_TOKEN_FILE})\" ` --pool \"$(if (Test-Path Env:AZP_POOL) { ${Env:AZP_POOL} } else { 'Default' })\" ` --work \"$(if (Test-Path Env:AZP_WORK) { ${Env:AZP_WORK} } else { '_work' })\" ` --replace Print-Header \"4. Running Azure Pipelines agent...\" .\\run.cmd } finally { Print-Header \"Cleanup. Removing Azure Pipelines agent...\" .\\config.cmd remove --unattended ` --auth PAT ` --token \"$(Get-Content ${Env:AZP_TOKEN_FILE})\" } Build the Docker image: docker build --tag \"azp-agent:windows\" --file \"./azp-agent-windows.dockerfile\" . The final image will be tagged as azp-agent:windows . Start the Image Now that you have created an image, you can run a container. This will install the latest version of the agent, configure it, and run the agent. It targets the specified agent pool (the Default agent pool by default) of a specified Azure DevOps or Azure DevOps Server instance of your choice: docker run -e AZP_URL=\"<Azure DevOps instance>\" -e AZP_TOKEN=\"<Personal Access Token>\" -e AZP_POOL=\"<Agent Pool Name>\" -e AZP_AGENT_NAME=\"Docker Agent - Windows\" --name \"azp-agent-windows\" azp-agent:windows","title":"Self-Hosted-Agent-Win-Container"},{"location":"DevOps/2.2_Self-Hosted_Agent_Windows_Container/#create-a-self-hosted-agent-in-docker-windows-container","text":"You can run your Azure Pipeline agent in Windows , Windows Container , and Ubuntu Container . This guide will show you how to set up your Azure Pipeline agent in a Windows Container.","title":"Create a Self-Hosted Agent in Docker Windows Container"},{"location":"DevOps/2.2_Self-Hosted_Agent_Windows_Container/#setup","text":"We'll use a Windows laptop/desktop with Docker Desktop installed. First, we'll switch Docker Desktop to Windows Mode (it runs in Linux Mode by default) because the container will be a Windows OS. Then we'll create the image, and the rest will be easy.","title":"Setup"},{"location":"DevOps/2.2_Self-Hosted_Agent_Windows_Container/#install-docker-for-windows","text":"First, make sure Docker Desktop is installed on your Windows machine.","title":"Install Docker for Windows"},{"location":"DevOps/2.2_Self-Hosted_Agent_Windows_Container/#switch-docker-to-use-windows-containers","text":"Note: Docker can run both Linux and Windows containers. By default, Docker runs in Linux mode. For more details, refer to this guide . In PowerShell, run this command: Note: If you have existing Linux containers, they will disappear when you switch to Windows mode but will return when you switch back to Linux mode. You will get a new Docker desktop! & $Env:ProgramFiles\\Docker\\Docker\\DockerCli.exe -SwitchDaemon To switch back to Linux mode later, refer to the image below:","title":"Switch Docker to Use Windows Containers"},{"location":"DevOps/2.2_Self-Hosted_Agent_Windows_Container/#create-and-build-the-dockerfile","text":"Next, create the Dockerfile. Open a command prompt and create a new directory: mkdir \"C:\\azp-agent-in-docker\\\" cd \"C:\\azp-agent-in-docker\\\" Save the following content to a file called C:\\azp-agent-in-docker\\azp-agent-windows.dockerfile : FROM mcr.microsoft.com/windows/servercore:ltsc2022 WORKDIR /azp/ COPY ./start.ps1 ./ CMD powershell .\\start.ps1 Save the following content to C:\\azp-agent-in-docker\\start.ps1 : Note, this script is taken as-is from this location . function Print-Header ($header) { Write-Host \"`n${header}`n\" -ForegroundColor Cyan } if (-not (Test-Path Env:AZP_URL)) { Write-Error \"error: missing AZP_URL environment variable\" exit 1 } if (-not (Test-Path Env:AZP_TOKEN_FILE)) { if (-not (Test-Path Env:AZP_TOKEN)) { Write-Error \"error: missing AZP_TOKEN environment variable\" exit 1 } $Env:AZP_TOKEN_FILE = \"\\azp\\.token\" $Env:AZP_TOKEN | Out-File -FilePath $Env:AZP_TOKEN_FILE } Remove-Item Env:AZP_TOKEN if ((Test-Path Env:AZP_WORK) -and -not (Test-Path $Env:AZP_WORK)) { New-Item $Env:AZP_WORK -ItemType directory | Out-Null } New-Item \"\\azp\\agent\" -ItemType directory | Out-Null # Let the agent ignore the token env variables $Env:VSO_AGENT_IGNORE = \"AZP_TOKEN,AZP_TOKEN_FILE\" Set-Location agent Print-Header \"1. Determining matching Azure Pipelines agent...\" $base64AuthInfo = [Convert]::ToBase64String([Text.Encoding]::ASCII.GetBytes(\":$(Get-Content ${Env:AZP_TOKEN_FILE})\")) $package = Invoke-RestMethod -Headers @{Authorization=(\"Basic $base64AuthInfo\")} \"$(${Env:AZP_URL})/_apis/distributedtask/packages/agent?platform=win-x64&`$top=1\" $packageUrl = $package[0].Value.downloadUrl Write-Host $packageUrl Print-Header \"2. Downloading and installing Azure Pipelines agent...\" $wc = New-Object System.Net.WebClient $wc.DownloadFile($packageUrl, \"$(Get-Location)\\agent.zip\") Expand-Archive -Path \"agent.zip\" -DestinationPath \"\\azp\\agent\" try { Print-Header \"3. Configuring Azure Pipelines agent...\" .\\config.cmd --unattended ` --agent \"$(if (Test-Path Env:AZP_AGENT_NAME) { ${Env:AZP_AGENT_NAME} } else { hostname })\" ` --url \"$(${Env:AZP_URL})\" ` --auth PAT ` --token \"$(Get-Content ${Env:AZP_TOKEN_FILE})\" ` --pool \"$(if (Test-Path Env:AZP_POOL) { ${Env:AZP_POOL} } else { 'Default' })\" ` --work \"$(if (Test-Path Env:AZP_WORK) { ${Env:AZP_WORK} } else { '_work' })\" ` --replace Print-Header \"4. Running Azure Pipelines agent...\" .\\run.cmd } finally { Print-Header \"Cleanup. Removing Azure Pipelines agent...\" .\\config.cmd remove --unattended ` --auth PAT ` --token \"$(Get-Content ${Env:AZP_TOKEN_FILE})\" } Build the Docker image: docker build --tag \"azp-agent:windows\" --file \"./azp-agent-windows.dockerfile\" . The final image will be tagged as azp-agent:windows .","title":"Create and Build the Dockerfile"},{"location":"DevOps/2.2_Self-Hosted_Agent_Windows_Container/#start-the-image","text":"Now that you have created an image, you can run a container. This will install the latest version of the agent, configure it, and run the agent. It targets the specified agent pool (the Default agent pool by default) of a specified Azure DevOps or Azure DevOps Server instance of your choice: docker run -e AZP_URL=\"<Azure DevOps instance>\" -e AZP_TOKEN=\"<Personal Access Token>\" -e AZP_POOL=\"<Agent Pool Name>\" -e AZP_AGENT_NAME=\"Docker Agent - Windows\" --name \"azp-agent-windows\" azp-agent:windows","title":"Start the Image"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Setting Up a Self-Hosted Agent in Docker Windows Container In Azure DevOps, an agent is a software that runs on a machine and executes build and deployment tasks. These tasks are defined in your pipelines and can be executed on various environments, including Windows, Linux, and macOS. Agents are essential for continuous integration and continuous deployment (CI/CD) as they handle the actual operations required to build, test, and deploy your applications. There are two main types of agents in Azure DevOps: Microsoft-Hosted Agents : These agents are managed by Microsoft and automatically provisioned for you. They are ephemeral, meaning a new agent is created for each run and discarded afterward. Self-Hosted Agents : These agents are managed by you. You have full control over the machine where the agent runs. This type of agent is useful when you need more control over the environment, want to install specific software, or need to connect to resources within your private network. This guide focuses on setting up a self-hosted agent in a Ubuntu Container. Common Error with start.sh The main error you will get is that the start.sh file is not findable. This happens if you create a file for Linux from Windows. The only resolution that works confidently is to convert the file using the link: https://toolslick.com/conversion/text/dos-to-unix Just upload the start.sh file, convert, and download. Then use as is. Note: The Notepad++ method of changing the format won't work. Neither will PowerShell. These methods are not reliable. Steps to Create Docker Container Create and Navigate to Folder Create a folder azp-agent-in-docker and navigate to it. Save Dockerfile Save the following content in azp-agent-in-docker/azp-agent-linux.dockerfile : FROM ubuntu:22.04 ENV TARGETARCH=\"linux-x64\" # Also can be \"linux-arm\", \"linux-arm64\". RUN apt update RUN apt upgrade -y RUN apt install -y curl git jq libicu70 WORKDIR /azp/ COPY ./start.sh ./ RUN chmod +x ./start.sh # Create agent user and set up home directory RUN useradd -m -d /home/agent agent RUN chown -R agent:agent /azp /home/agent USER agent # Another option is to run the agent as root. # ENV AGENT_ALLOW_RUNASROOT=\"true\" ENTRYPOINT [ \"./start.sh\" ] Save start.sh Save this content to azp-agent-in-docker/start.sh : #!/bin/bash set -e if [ -z \"${AZP_URL}\" ]; then echo 1>&2 \"error: missing AZP_URL environment variable\" exit 1 fi if [ -z \"${AZP_TOKEN_FILE}\" ]; then if [ -z \"${AZP_TOKEN}\" ]; then echo 1>&2 \"error: missing AZP_TOKEN environment variable\" exit 1 fi AZP_TOKEN_FILE=\"/azp/.token\" echo -n \"${AZP_TOKEN}\" > \"${AZP_TOKEN_FILE}\" fi unset AZP_TOKEN if [ -n \"${AZP_WORK}\" ]; then mkdir -p \"${AZP_WORK}\" fi cleanup() { trap \"\" EXIT if [ -e ./config.sh ]; then print_header \"Cleanup. Removing Azure Pipelines agent...\" # If the agent has some running jobs, the configuration removal process will fail. # So, give it some time to finish the job. while true; do ./config.sh remove --unattended --auth \"PAT\" --token $(cat \"${AZP_TOKEN_FILE}\") && break echo \"Retrying in 30 seconds...\" sleep 30 done fi } print_header() { lightcyan=\"\\033[1;36m\" nocolor=\"\\033[0m\" echo -e \"\\n${lightcyan}$1${nocolor}\\n\" } # Let the agent ignore the token env variables export VSO_AGENT_IGNORE=\"AZP_TOKEN,AZP_TOKEN_FILE\" print_header \"1. Determining matching Azure Pipelines agent...\" AZP_AGENT_PACKAGES=$(curl -LsS \\ -u user:$(cat \"${AZP_TOKEN_FILE}\") \\ -H \"Accept:application/json;\" \\ \"${AZP_URL}/_apis/distributedtask/packages/agent?platform=${TARGETARCH}&top=1\") AZP_AGENT_PACKAGE_LATEST_URL=$(echo \"${AZP_AGENT_PACKAGES}\" | jq -r \".value[0].downloadUrl\") if [ -z \"${AZP_AGENT_PACKAGE_LATEST_URL}\" -o \"${AZP_AGENT_PACKAGE_LATEST_URL}\" == \"null\" ]; then echo 1>&2 \"error: could not determine a matching Azure Pipelines agent\" echo 1>&2 \"check that account \"${AZP_URL}\" is correct and the token is valid for that account\" exit 1 fi print_header \"2. Downloading and extracting Azure Pipelines agent...\" curl -LsS \"${AZP_AGENT_PACKAGE_LATEST_URL}\" | tar -xz & wait $! source ./env.sh trap \"cleanup; exit 0\" EXIT trap \"cleanup; exit 130\" INT trap \"cleanup; exit 143\" TERM print_header \"3. Configuring Azure Pipelines agent...\" ./config.sh --unattended \\ --agent \"${AZP_AGENT_NAME:-$(hostname)}\" \\ --url \"${AZP_URL}\" \\ --auth \"PAT\" \\ --token $(cat \"${AZP_TOKEN_FILE}\") \\ --pool \"${AZP_POOL:-Default}\" \\ --work \"${AZP_WORK:-_work}\" \\ --replace \\ --acceptTeeEula & wait $! print_header \"4. Running Azure Pipelines agent...\" chmod +x ./run.sh # To be aware of TERM and INT signals call ./run.sh # Running it with the --once flag at the end will shut down the agent after the build is executed ./run.sh \"$@\" & wait $! Build Docker Image Run the following command within that directory: docker build --tag \"azp-agent:linux\" --file \"./azp-agent-linux.dockerfile\" . The final image is tagged azp-agent:linux . Start the Image Now that you have created an image, you can run a container. This installs the latest version of the agent, configures it, and runs the agent. It targets the specified agent pool (the Default agent pool by default) of a specified Azure DevOps or Azure DevOps Server instance of your choice: docker run -e AZP_URL=\"<Azure DevOps instance>\" -e AZP_TOKEN=\"<Personal Access Token>\" -e AZP_POOL=\"<Agent Pool Name>\" -e AZP_AGENT_NAME=\"Docker Agent - Linux\" --name \"azp-agent-linux\" azp-agent:linux You might need to specify --interactive and --tty flags (or simply -it ) if you want to be able to stop the container and remove the agent with Ctrl + C. docker run --interactive --tty < . . . > If you want a fresh agent container for every pipeline job, pass the --once flag to the run command. docker run < . . . > --once What all types of Agents can we have? Microsoft-Hosted Agents Microsoft-hosted agents are managed by Azure DevOps. They are easy to use because they come pre-configured with common tools and are always up to date. You don\u2019t need to maintain them. However, they might not be the best choice if you need custom software or specific configurations. Self-Hosted Agents On-Premises Self-Hosted Agents These agents run on your own servers. They are great for accessing internal resources like databases and file shares. You have full control over the environment, which is good for running specialized software or meeting security requirements. Cloud Self-Hosted Agents These agents run on virtual machines in the cloud, like Azure VMs. They are perfect if you need to scale up or down based on demand. You get the benefits of the cloud without having to manage physical hardware. Why Use Different Containers Windows Containers : Best for running Windows applications like .NET Framework and ASP.NET. They ensure compatibility and make deployment easier. Ubuntu Containers : Ideal for open-source software, Python apps, Node.js, and other tools. They are lightweight and flexible. When to Use On-Premises Windows Containers Use these when you need to run Windows apps that need access to your internal resources. They give you the benefits of containers while allowing access to your internal systems and data.","title":"Self-Hosted-Agent-Ubuntu-Container"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container/#setting-up-a-self-hosted-agent-in-docker-windows-container","text":"In Azure DevOps, an agent is a software that runs on a machine and executes build and deployment tasks. These tasks are defined in your pipelines and can be executed on various environments, including Windows, Linux, and macOS. Agents are essential for continuous integration and continuous deployment (CI/CD) as they handle the actual operations required to build, test, and deploy your applications. There are two main types of agents in Azure DevOps: Microsoft-Hosted Agents : These agents are managed by Microsoft and automatically provisioned for you. They are ephemeral, meaning a new agent is created for each run and discarded afterward. Self-Hosted Agents : These agents are managed by you. You have full control over the machine where the agent runs. This type of agent is useful when you need more control over the environment, want to install specific software, or need to connect to resources within your private network. This guide focuses on setting up a self-hosted agent in a Ubuntu Container.","title":"Setting Up a Self-Hosted Agent in Docker Windows Container"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container/#common-error-with-startsh","text":"The main error you will get is that the start.sh file is not findable. This happens if you create a file for Linux from Windows. The only resolution that works confidently is to convert the file using the link: https://toolslick.com/conversion/text/dos-to-unix Just upload the start.sh file, convert, and download. Then use as is. Note: The Notepad++ method of changing the format won't work. Neither will PowerShell. These methods are not reliable.","title":"Common Error with start.sh"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container/#steps-to-create-docker-container","text":"","title":"Steps to Create Docker Container"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container/#create-and-navigate-to-folder","text":"Create a folder azp-agent-in-docker and navigate to it.","title":"Create and Navigate to Folder"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container/#save-dockerfile","text":"Save the following content in azp-agent-in-docker/azp-agent-linux.dockerfile : FROM ubuntu:22.04 ENV TARGETARCH=\"linux-x64\" # Also can be \"linux-arm\", \"linux-arm64\". RUN apt update RUN apt upgrade -y RUN apt install -y curl git jq libicu70 WORKDIR /azp/ COPY ./start.sh ./ RUN chmod +x ./start.sh # Create agent user and set up home directory RUN useradd -m -d /home/agent agent RUN chown -R agent:agent /azp /home/agent USER agent # Another option is to run the agent as root. # ENV AGENT_ALLOW_RUNASROOT=\"true\" ENTRYPOINT [ \"./start.sh\" ]","title":"Save Dockerfile"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container/#save-startsh","text":"Save this content to azp-agent-in-docker/start.sh : #!/bin/bash set -e if [ -z \"${AZP_URL}\" ]; then echo 1>&2 \"error: missing AZP_URL environment variable\" exit 1 fi if [ -z \"${AZP_TOKEN_FILE}\" ]; then if [ -z \"${AZP_TOKEN}\" ]; then echo 1>&2 \"error: missing AZP_TOKEN environment variable\" exit 1 fi AZP_TOKEN_FILE=\"/azp/.token\" echo -n \"${AZP_TOKEN}\" > \"${AZP_TOKEN_FILE}\" fi unset AZP_TOKEN if [ -n \"${AZP_WORK}\" ]; then mkdir -p \"${AZP_WORK}\" fi cleanup() { trap \"\" EXIT if [ -e ./config.sh ]; then print_header \"Cleanup. Removing Azure Pipelines agent...\" # If the agent has some running jobs, the configuration removal process will fail. # So, give it some time to finish the job. while true; do ./config.sh remove --unattended --auth \"PAT\" --token $(cat \"${AZP_TOKEN_FILE}\") && break echo \"Retrying in 30 seconds...\" sleep 30 done fi } print_header() { lightcyan=\"\\033[1;36m\" nocolor=\"\\033[0m\" echo -e \"\\n${lightcyan}$1${nocolor}\\n\" } # Let the agent ignore the token env variables export VSO_AGENT_IGNORE=\"AZP_TOKEN,AZP_TOKEN_FILE\" print_header \"1. Determining matching Azure Pipelines agent...\" AZP_AGENT_PACKAGES=$(curl -LsS \\ -u user:$(cat \"${AZP_TOKEN_FILE}\") \\ -H \"Accept:application/json;\" \\ \"${AZP_URL}/_apis/distributedtask/packages/agent?platform=${TARGETARCH}&top=1\") AZP_AGENT_PACKAGE_LATEST_URL=$(echo \"${AZP_AGENT_PACKAGES}\" | jq -r \".value[0].downloadUrl\") if [ -z \"${AZP_AGENT_PACKAGE_LATEST_URL}\" -o \"${AZP_AGENT_PACKAGE_LATEST_URL}\" == \"null\" ]; then echo 1>&2 \"error: could not determine a matching Azure Pipelines agent\" echo 1>&2 \"check that account \"${AZP_URL}\" is correct and the token is valid for that account\" exit 1 fi print_header \"2. Downloading and extracting Azure Pipelines agent...\" curl -LsS \"${AZP_AGENT_PACKAGE_LATEST_URL}\" | tar -xz & wait $! source ./env.sh trap \"cleanup; exit 0\" EXIT trap \"cleanup; exit 130\" INT trap \"cleanup; exit 143\" TERM print_header \"3. Configuring Azure Pipelines agent...\" ./config.sh --unattended \\ --agent \"${AZP_AGENT_NAME:-$(hostname)}\" \\ --url \"${AZP_URL}\" \\ --auth \"PAT\" \\ --token $(cat \"${AZP_TOKEN_FILE}\") \\ --pool \"${AZP_POOL:-Default}\" \\ --work \"${AZP_WORK:-_work}\" \\ --replace \\ --acceptTeeEula & wait $! print_header \"4. Running Azure Pipelines agent...\" chmod +x ./run.sh # To be aware of TERM and INT signals call ./run.sh # Running it with the --once flag at the end will shut down the agent after the build is executed ./run.sh \"$@\" & wait $!","title":"Save start.sh"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container/#build-docker-image","text":"Run the following command within that directory: docker build --tag \"azp-agent:linux\" --file \"./azp-agent-linux.dockerfile\" . The final image is tagged azp-agent:linux .","title":"Build Docker Image"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container/#start-the-image","text":"Now that you have created an image, you can run a container. This installs the latest version of the agent, configures it, and runs the agent. It targets the specified agent pool (the Default agent pool by default) of a specified Azure DevOps or Azure DevOps Server instance of your choice: docker run -e AZP_URL=\"<Azure DevOps instance>\" -e AZP_TOKEN=\"<Personal Access Token>\" -e AZP_POOL=\"<Agent Pool Name>\" -e AZP_AGENT_NAME=\"Docker Agent - Linux\" --name \"azp-agent-linux\" azp-agent:linux You might need to specify --interactive and --tty flags (or simply -it ) if you want to be able to stop the container and remove the agent with Ctrl + C. docker run --interactive --tty < . . . > If you want a fresh agent container for every pipeline job, pass the --once flag to the run command. docker run < . . . > --once","title":"Start the Image"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container/#what-all-types-of-agents-can-we-have","text":"","title":"What all types of Agents can we have?"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container/#microsoft-hosted-agents","text":"Microsoft-hosted agents are managed by Azure DevOps. They are easy to use because they come pre-configured with common tools and are always up to date. You don\u2019t need to maintain them. However, they might not be the best choice if you need custom software or specific configurations.","title":"Microsoft-Hosted Agents"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container/#self-hosted-agents","text":"","title":"Self-Hosted Agents"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container/#on-premises-self-hosted-agents","text":"These agents run on your own servers. They are great for accessing internal resources like databases and file shares. You have full control over the environment, which is good for running specialized software or meeting security requirements.","title":"On-Premises Self-Hosted Agents"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container/#cloud-self-hosted-agents","text":"These agents run on virtual machines in the cloud, like Azure VMs. They are perfect if you need to scale up or down based on demand. You get the benefits of the cloud without having to manage physical hardware.","title":"Cloud Self-Hosted Agents"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container/#why-use-different-containers","text":"Windows Containers : Best for running Windows applications like .NET Framework and ASP.NET. They ensure compatibility and make deployment easier. Ubuntu Containers : Ideal for open-source software, Python apps, Node.js, and other tools. They are lightweight and flexible.","title":"Why Use Different Containers"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container/#when-to-use-on-premises-windows-containers","text":"Use these when you need to run Windows apps that need access to your internal resources. They give you the benefits of containers while allowing access to your internal systems and data.","title":"When to Use On-Premises Windows Containers"},{"location":"DevOps/2_Azure-Pipelines/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Azure Pipelines In Azure DevOps, Azure Pipelines are used to automatically build, test, and deploy any project to Dev, UAT, or PROD environments. This is how CI/CD is implemented using Azure Pipelines. You can use Azure Pipelines for any type of project, be it C#, Python, iOS, Android, etc. Azure Pipeline is written in a YAML file , called azure-pipelines.yml , in your VS project folder. Create your first pipeline Azure Pipelines Agents When a pipeline runs, it creates one or more jobs. These jobs need 'a place' to run. This place is the agent. The agent is a machine, VM, or cloud environment with the agent software installed, where the jobs can execute. There are three types of agents: Microsoft-hosted agent Self-hosted agent Azure VM agents The agent is where the jobs for the pipelines run. Create a Pipeline in Azure DevOps What You Need GitHub account : Create a free repository on GitHub. Azure DevOps organization : Create one for free. If your team already has one, ensure you're an administrator of the Azure DevOps project you want to use. Ability to run pipelines on Microsoft-hosted agents : Your Azure DevOps organization must have access to Microsoft-hosted parallel jobs. You can either purchase a parallel job or request a free grant. Steps to Create a Pipeline Fork the Repository Open this link and click on Fork . Click on Create fork . Sign in to Azure DevOps Sign in to your Azure DevOps organization and go to your project. Create a New Pipeline Go to Pipelines , and then select New pipeline or Create pipeline if it's your first pipeline. Connect to GitHub Follow the steps in the wizard by first selecting GitHub as the location of your source code. You might be redirected to GitHub to install the Azure Pipelines app. If so, select Approve & install . Configure the Pipeline Azure Pipelines will analyze your repository and recommend the Python package pipeline template. When your new pipeline appears, check the YAML to see what it does. When you're ready, select Save and run . You'll be prompted to commit a new azure-pipelines.yml file to your repository. After you're happy with the message, select Save and run again. Watch Your Pipeline Run If you want to watch your pipeline in action, select the build job. You have now created and ran a pipeline that Azure automatically created for you, as your code was a good match for the Python package template. You now have a working YAML pipeline ( azure-pipelines.yml ) in your repository that's ready for you to customize! Editing Your Pipeline When you're ready to make changes to your pipeline, select it in the Pipelines page, and then edit the azure-pipelines.yml file. No Hosted Parallelism Available This means you haven't purchased Parallelism. You can do this: Request Free Parallelism : Request Parallelism . Check Usage : Go to your Azure DevOps settings and check if all your parallel jobs are in use. Buy More Parallelism : If needed, buy more parallelism from the billing section in Azure DevOps. Set Up Self-hosted Agents : If you have your own resources, set up self-hosted agents to run your pipelines . You can run the Azure Pipeline agent on your own computer and run the pipeline on a self-hosted Azure Pipeline Agent: Learn more .","title":"Azure-Pipelines"},{"location":"DevOps/2_Azure-Pipelines/#azure-pipelines","text":"In Azure DevOps, Azure Pipelines are used to automatically build, test, and deploy any project to Dev, UAT, or PROD environments. This is how CI/CD is implemented using Azure Pipelines. You can use Azure Pipelines for any type of project, be it C#, Python, iOS, Android, etc. Azure Pipeline is written in a YAML file , called azure-pipelines.yml , in your VS project folder. Create your first pipeline","title":"Azure Pipelines"},{"location":"DevOps/2_Azure-Pipelines/#azure-pipelines-agents","text":"When a pipeline runs, it creates one or more jobs. These jobs need 'a place' to run. This place is the agent. The agent is a machine, VM, or cloud environment with the agent software installed, where the jobs can execute. There are three types of agents: Microsoft-hosted agent Self-hosted agent Azure VM agents The agent is where the jobs for the pipelines run.","title":"Azure Pipelines Agents"},{"location":"DevOps/2_Azure-Pipelines/#create-a-pipeline-in-azure-devops","text":"","title":"Create a Pipeline in Azure DevOps"},{"location":"DevOps/2_Azure-Pipelines/#what-you-need","text":"GitHub account : Create a free repository on GitHub. Azure DevOps organization : Create one for free. If your team already has one, ensure you're an administrator of the Azure DevOps project you want to use. Ability to run pipelines on Microsoft-hosted agents : Your Azure DevOps organization must have access to Microsoft-hosted parallel jobs. You can either purchase a parallel job or request a free grant.","title":"What You Need"},{"location":"DevOps/2_Azure-Pipelines/#steps-to-create-a-pipeline","text":"Fork the Repository Open this link and click on Fork . Click on Create fork . Sign in to Azure DevOps Sign in to your Azure DevOps organization and go to your project. Create a New Pipeline Go to Pipelines , and then select New pipeline or Create pipeline if it's your first pipeline. Connect to GitHub Follow the steps in the wizard by first selecting GitHub as the location of your source code. You might be redirected to GitHub to install the Azure Pipelines app. If so, select Approve & install . Configure the Pipeline Azure Pipelines will analyze your repository and recommend the Python package pipeline template. When your new pipeline appears, check the YAML to see what it does. When you're ready, select Save and run . You'll be prompted to commit a new azure-pipelines.yml file to your repository. After you're happy with the message, select Save and run again. Watch Your Pipeline Run If you want to watch your pipeline in action, select the build job. You have now created and ran a pipeline that Azure automatically created for you, as your code was a good match for the Python package template. You now have a working YAML pipeline ( azure-pipelines.yml ) in your repository that's ready for you to customize!","title":"Steps to Create a Pipeline"},{"location":"DevOps/2_Azure-Pipelines/#editing-your-pipeline","text":"When you're ready to make changes to your pipeline, select it in the Pipelines page, and then edit the azure-pipelines.yml file.","title":"Editing Your Pipeline"},{"location":"DevOps/2_Azure-Pipelines/#no-hosted-parallelism-available","text":"This means you haven't purchased Parallelism. You can do this: Request Free Parallelism : Request Parallelism . Check Usage : Go to your Azure DevOps settings and check if all your parallel jobs are in use. Buy More Parallelism : If needed, buy more parallelism from the billing section in Azure DevOps. Set Up Self-hosted Agents : If you have your own resources, set up self-hosted agents to run your pipelines . You can run the Azure Pipeline agent on your own computer and run the pipeline on a self-hosted Azure Pipeline Agent: Learn more .","title":"No Hosted Parallelism Available"},{"location":"DevOps/ADF_CICD/","text":"Table of contents {: .text-delta } 1. TOC {:toc} ADF - Dev to PROD using GitHub workflows and Azure Pipelines Moving your Azure Data Factory from Dev to PROD can be CI/CDed using GitHub Actions Workflows or using Azure DevOps Pipelines. Main Article ADF uses ARM templates to store the configuration of pipelines, datasets, data flows, etc. There are two methods to deploy a data factory to another environment: Automated deployment using Data Factory's integration with Azure Pipelines or GitHub workflows. Manual deployment by uploading a Resource Manager template using Data Factory UX integration with Azure Resource Manager. Using GitHub Set Up Version Control - Connect ADF with GitHub : Open your ADF in the development environment. Go to the \"Manage\" tab and connect your ADF to a GitHub repository. - Continuous Integration (CI) : Develop and test your ADF pipelines, datasets, and linked services. Commit and push your changes to the GitHub repository. Create a GitHub Actions Workflow Use this YAML code for your workflow: name: ADF CI on: push: branches: - main # or your development branch jobs: build: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Set up .NET uses: actions/setup-dotnet@v1 with: dotnet-version: '3.x' - name: Restore dependencies run: dotnet restore - name: Build run: dotnet build --no-restore - name: Publish artifacts uses: actions/upload-artifact@v2 with: name: drop path: | ARMTemplateForFactory.json ARMTemplateParametersForFactory.json Continuous Deployment (CD) Create a deployment workflow in .github/workflows : name: ADF CD on: push: branches: - main jobs: deploy: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Azure Login uses: azure/login@v1 with: creds: ${{ secrets.AZURE_CREDENTIALS }} - name: Deploy ARM Template uses: azure/arm-deploy@v1 with: resource-group: 'YOUR_RESOURCE_GROUP_NAME' template: './ARMTemplateForFactory.json' parameters: './ARMTemplateParametersForFactory.json' deployment-name: 'adf-deployment' Manage Parameters Set different values for production in your parameters file, like connections and dataset paths. Run the Workflow Push changes to the GitHub repository to trigger the CI workflow. After a successful build, the CD workflow will automatically deploy changes to the production ADF. Using Azure DevOps Set Up Version Control - Connect ADF with Azure Repos : Open your ADF in the development environment. Go to the \"Manage\" tab and connect your ADF to an Azure Repos Git repository. - Continuous Integration (CI) : Develop and test your ADF pipelines, datasets, and linked services. Commit and push your changes to the Azure Repos Git repository. Create a Build Pipeline Go to Azure DevOps > Pipelines > Builds > New Pipeline. Select your Azure Repos Git repository. Use this YAML code: trigger: branches: include: - main # or your development branch pool: vmImage: 'ubuntu-latest' steps: - task: UseDotNet@2 inputs: packageType: 'sdk' version: '3.x' installationPath: $(Agent.ToolsDirectory)/dotnet - task: DotNetCoreCLI@2 inputs: command: 'restore' projects: '**/*.csproj' - task: DotNetCoreCLI@2 inputs: command: 'build' projects: '**/*.csproj' - task: PublishPipelineArtifact@1 inputs: targetPath: '$(Build.ArtifactStagingDirectory)' artifact: 'drop' Continuous Deployment (CD) Create a release pipeline in Azure DevOps: Create a Release Pipeline 1. Go to Azure DevOps > Pipelines > Releases > New pipeline. 2. Select an empty job, add an artifact, and choose the build pipeline you created. 3. Add a new stage for deployment. Add a task to deploy the ARM template of your ADF in the deployment stage: - task: AzureResourceManagerTemplateDeployment@3 inputs: deploymentScope: 'Resource Group' azureResourceManagerConnection: 'AzureRMConnection' subscriptionId: 'YOUR_SUBSCRIPTION_ID' action: 'Create Or Update Resource Group' resourceGroupName: 'YOUR_RESOURCE_GROUP_NAME' location: 'YOUR_RESOURCE_LOCATION' templateLocation: 'Linked artifact' csmFile: '$(System.DefaultWorkingDirectory)/_your-build-pipeline/drop/ARMTemplateForFactory.json' csmParametersFile: '$(System.DefaultWorkingDirectory)/_your-build-pipeline/drop/ARMTemplateParametersForFactory.json' Manage Parameters Set different values for production in your parameters file, like connections and dataset paths. Run the Pipeline Push changes to the Azure Repos Git repository to trigger the CI pipeline. After a successful build, the CD pipeline will automatically deploy changes to the production ADF.","title":"ADF-CI-CD"},{"location":"DevOps/ADF_CICD/#adf-dev-to-prod-using-github-workflows-and-azure-pipelines","text":"Moving your Azure Data Factory from Dev to PROD can be CI/CDed using GitHub Actions Workflows or using Azure DevOps Pipelines. Main Article ADF uses ARM templates to store the configuration of pipelines, datasets, data flows, etc. There are two methods to deploy a data factory to another environment: Automated deployment using Data Factory's integration with Azure Pipelines or GitHub workflows. Manual deployment by uploading a Resource Manager template using Data Factory UX integration with Azure Resource Manager.","title":"ADF - Dev to PROD using GitHub workflows and Azure Pipelines"},{"location":"DevOps/ADF_CICD/#using-github","text":"Set Up Version Control - Connect ADF with GitHub : Open your ADF in the development environment. Go to the \"Manage\" tab and connect your ADF to a GitHub repository. - Continuous Integration (CI) : Develop and test your ADF pipelines, datasets, and linked services. Commit and push your changes to the GitHub repository. Create a GitHub Actions Workflow Use this YAML code for your workflow: name: ADF CI on: push: branches: - main # or your development branch jobs: build: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Set up .NET uses: actions/setup-dotnet@v1 with: dotnet-version: '3.x' - name: Restore dependencies run: dotnet restore - name: Build run: dotnet build --no-restore - name: Publish artifacts uses: actions/upload-artifact@v2 with: name: drop path: | ARMTemplateForFactory.json ARMTemplateParametersForFactory.json Continuous Deployment (CD) Create a deployment workflow in .github/workflows : name: ADF CD on: push: branches: - main jobs: deploy: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Azure Login uses: azure/login@v1 with: creds: ${{ secrets.AZURE_CREDENTIALS }} - name: Deploy ARM Template uses: azure/arm-deploy@v1 with: resource-group: 'YOUR_RESOURCE_GROUP_NAME' template: './ARMTemplateForFactory.json' parameters: './ARMTemplateParametersForFactory.json' deployment-name: 'adf-deployment' Manage Parameters Set different values for production in your parameters file, like connections and dataset paths. Run the Workflow Push changes to the GitHub repository to trigger the CI workflow. After a successful build, the CD workflow will automatically deploy changes to the production ADF.","title":"Using GitHub"},{"location":"DevOps/ADF_CICD/#using-azure-devops","text":"Set Up Version Control - Connect ADF with Azure Repos : Open your ADF in the development environment. Go to the \"Manage\" tab and connect your ADF to an Azure Repos Git repository. - Continuous Integration (CI) : Develop and test your ADF pipelines, datasets, and linked services. Commit and push your changes to the Azure Repos Git repository. Create a Build Pipeline Go to Azure DevOps > Pipelines > Builds > New Pipeline. Select your Azure Repos Git repository. Use this YAML code: trigger: branches: include: - main # or your development branch pool: vmImage: 'ubuntu-latest' steps: - task: UseDotNet@2 inputs: packageType: 'sdk' version: '3.x' installationPath: $(Agent.ToolsDirectory)/dotnet - task: DotNetCoreCLI@2 inputs: command: 'restore' projects: '**/*.csproj' - task: DotNetCoreCLI@2 inputs: command: 'build' projects: '**/*.csproj' - task: PublishPipelineArtifact@1 inputs: targetPath: '$(Build.ArtifactStagingDirectory)' artifact: 'drop' Continuous Deployment (CD) Create a release pipeline in Azure DevOps: Create a Release Pipeline 1. Go to Azure DevOps > Pipelines > Releases > New pipeline. 2. Select an empty job, add an artifact, and choose the build pipeline you created. 3. Add a new stage for deployment. Add a task to deploy the ARM template of your ADF in the deployment stage: - task: AzureResourceManagerTemplateDeployment@3 inputs: deploymentScope: 'Resource Group' azureResourceManagerConnection: 'AzureRMConnection' subscriptionId: 'YOUR_SUBSCRIPTION_ID' action: 'Create Or Update Resource Group' resourceGroupName: 'YOUR_RESOURCE_GROUP_NAME' location: 'YOUR_RESOURCE_LOCATION' templateLocation: 'Linked artifact' csmFile: '$(System.DefaultWorkingDirectory)/_your-build-pipeline/drop/ARMTemplateForFactory.json' csmParametersFile: '$(System.DefaultWorkingDirectory)/_your-build-pipeline/drop/ARMTemplateParametersForFactory.json' Manage Parameters Set different values for production in your parameters file, like connections and dataset paths. Run the Pipeline Push changes to the Azure Repos Git repository to trigger the CI pipeline. After a successful build, the CD pipeline will automatically deploy changes to the production ADF.","title":"Using Azure DevOps"},{"location":"DevOps/Biceps/","text":"What is Bicep? Bicep is a new Language for developing ARM templates. Bicep eventually compiles to JSON files but, its less complicated than JSON. There are two types of ARM template files: JSON and Bicep You can deploy azure resources using Bicep using Azure pipelines or Github Actions. We will see Bicep and Github actions here. Deploy Azure resources by using Bicep and GitHub Actions To deploy on Azure here are the options we have: Azure portal Azure CLI - This is an IAC option Azure PowerShell - This is an IAC option Azure Resource Manager templates (JSON and Bicep) This is an Infrasructre as code: az group create --name storage-resource-group --location eastus If you use it, it will make things faster also more accurate. Manually creating might be typo error by user. Benefits of Infrasturctre as code than going-to-portal-and-doing-it-manually: Create new enviornments quickly Deploy a feature to dev and than using parameters deploy it to prod. This was it will be identical and less error. DR. If you have everything as IAC. you can quicly create the DR enviornment. Imperative and declarative code You can write an instruction manual for new toy assembly in different ways. When you automate the deployment of services and infrastructure, you can take two approaches: imperative and declarative. With imperative code, you execute a sequence of commands, in a specific order, to reach an end configuration. This process defines what the code should accomplish, and it defines how to accomplish the task. The imperative approach is like a step-by-step instruction manual. With declarative code, you specify only the end configuration. The code doesn't define how to accomplish the task. The declarative approach is like the exploded view instruction manual. When you choose between using an imperative approach and a declarative approach to resource provisioning, consider the tools that might already be in use in your organization. Also consider which approach might match your own skills. Imperative code In Azure, an imperative code approach is accomplished programmatically by using a scripting language like Bash or Azure PowerShell. The scripts execute a series of steps to create, modify, and even remove your resources. This example shows two Azure CLI commands that create a resource group and a storage account. Azure CLI Copy !/usr/bin/env bash az group create \\ --name storage-resource-group \\ --location eastus az storage account create \\ --name mystorageaccount \\ --resource-group storage-resource-group \\ --location eastus \\ --sku Standard_LRS \\ --kind StorageV2 \\ --access-tier Hot \\ --https-only true Declarative code In Azure, a declarative code approach is accomplished by using templates. Many types of templates are available to use, including: JSON Bicep Ansible, by RedHat Terraform, by HashiCorp Note This module focuses on using Bicep templates. Take a look at the following example of a Bicep template that configures a storage account. The configuration of the storage account matches the Azure CLI example. Copy resource storageAccount 'Microsoft.Storage/storageAccounts@2022-05-01' = { name: 'mystorageaccount' location: 'eastus' sku: { name: 'Standard_LRS' } kind: 'StorageV2' properties: { accessTier: 'Hot' supportsHttpsTrafficOnly: true } } The resources section defines the storage account configuration. This section contains the name, location, and properties of the storage account, including its SKU and the kind of account. You might notice that the Bicep template doesn't specify how to deploy the storage account. It specifies only what the storage account needs to look like. The actual steps that are executed behind the scenes to create this storage account or to update it to match the specification are left for Azure to decide. Learn Training Browse Fundamentals of Bicep Introduction to infrastructure as code using Bicep What is Azure Resource Manager? Completed 100 XP 7 minutes You've spent some time with your team learning the benefits of infrastructure as code and the different approaches that are available. Your company is growing at a rapid pace and your team knows it will be deploying a significant number of resources to Azure. As a team, you've decided that declarative infrastructure as code is the right approach to resource provisioning. The team doesn't want to maintain scripts that list every deployment step. Before beginning the process of building your first template, you need to understand how Azure Resource Manager works. Investigating the types of templates that are available to use with Azure will help you determine the next steps in your infrastructure-as-code strategy. In this unit, you'll learn about Resource Manager and the two types of Resource Manager templates. Azure Resource Manager concepts Azure Resource Manager is the service that's used to deploy and manage resources in Azure. You can use Resource Manager to create, update, and delete resources in your Azure subscription. You can interact with Resource Manager by using many tools, including the Azure portal. Resource Manager also provides a series of other features, like access control, auditing, and tagging, to help manage your resources after deployment. Terminology As you begin your cloud journey with Resource Manager, it's important to understand some terms and concepts: Resource: A manageable item that's available on the Azure platform. Virtual networks, virtual machines, storage accounts, web apps, and databases are examples of resources. Resource group: A logical container that holds related resources for an Azure solution. The resource group includes resources you want to manage as a group. Most Azure resources are contained in a resource group. You decide which resources belong in a resource group based on what makes the most sense for your solution. Note A small number of resources aren't contained in resource groups. These resource types are for specific purposes like managing access control and enforcing policies. You'll learn more about these resources in a later module. Subscription: A logical container and billing boundary for your resources and resource groups. Each Azure resource and resource group is associated with only one subscription. Management group: A logical container that you use to manage more than one subscription. You can define a hierarchy of management groups, subscriptions, resource groups, and resources to efficiently manage access, policies, and compliance through inheritance. Azure Resource Manager template (ARM template): A template file that defines one or more resources to deploy to a resource group, subscription, management group, or tenant. You can use the template to deploy the resources in a consistent and repeatable way. There are two types of ARM template files: JSON and Bicep. This module focuses on Bicep. Benefits Resource Manager provides many benefits and capabilities related to infrastructure-as-code resource provisioning: You can deploy, manage, and monitor the resources in your solution as a group instead of individually. You can redeploy your solution throughout the development lifecycle and have confidence that your resources are deployed in a consistent state. You can manage your infrastructure through declarative templates instead of by using scripts. You can specify resource dependencies to ensure that resources are deployed in the correct order. Operations: Control plane and data plane You can execute two types of operations in Azure: control plane operations and data plane operations. Use the control plane to manage the resources in your subscription. Use the data plane to access features that are exposed by a resource. For example, you use a control plane operation to create a virtual machine, but you use a data plane operation to connect to the virtual machine by using Remote Desktop Protocol (RDP). Control plane When you send a request from any of the Azure tools, APIs, or SDKs, Resource Manager receives, authenticates, and authorizes the request. Then, it sends the request to the Azure resource provider, which takes the requested action. Because all requests are handled through the same API, you see consistent results and capabilities in all the different tools that are available in Azure. The following image shows the role that Resource Manager plays in handling Azure requests: Diagram that shows how Azure Resource Manager accepts requests from all Azure clients and libraries. All control plane operation requests are sent to a Resource Manager URL. For example, the create or update operation for virtual machines is a control plane operation. Here's the request URL for this operation: HTTP Copy PUT https://management.azure.com/subscriptions/ /resourceGroups/ /providers/Microsoft.Compute/virtualMachines/{virtualMachineName}?api-version=2022-08-01 The control plane understands which resources need to be created and which resources already exist. Resource Manager understands the difference between these requests and won't create identical resources or delete existing resources, although there are ways to override this behavior. Data plane When a data plane operation starts, the requests are sent to a specific endpoint in your Azure subscription. For example, the Detect Language operation in Azure AI services is a data plane operation because the request URL is: HTTP Copy POST https://eastus.api.cognitive.microsoft.com/text/analytics/v2.0/languages Resource Manager features like access control and locks don't always apply to data plane operations. For example, a user might not have permissions to manage a virtual machine by using the control plane, but the user can sign in to the operating system. What are ARM templates? Azure Resource Manager templates are files that define the infrastructure and configuration for your deployment. When you write an ARM template, you take a declarative approach to your resource provisioning. These templates describe each resource in the deployment, but they don't describe how to deploy the resources. When you submit a template to Resource Manager for deployment, the control plane can deploy the defined resources in an organized and consistent manner. In the preceding unit, you learned about the differences between imperative code and declarative code. Why use ARM templates? There are many benefits to using ARM templates, either JSON or Bicep, for your resource provisioning. Repeatable results: ARM templates are idempotent, which means that you can repeatedly deploy the same template and get the same result. The template doesn't duplicate resources. Orchestration: When a template deployment is submitted to Resource Manager, the resources in the template are deployed in parallel. This process allows deployments to finish faster. Resource Manager orchestrates these deployments in the correct order if one resource depends on another. Preview: The what-if tool, available in Azure PowerShell and Azure CLI, allows you to preview changes to your environment before template deployment. This tool details any creations, modification, and deletions that will be made by your template. Testing and Validation: You can use tools like the Bicep linter to check the quality of your templates before deployment. ARM templates submitted to Resource Manager are validated before the deployment process. This validation alerts you to any errors in your template before resource provisioning. Modularity: You can break up your templates into smaller components and link them together at deployment. CI/CD integration: Your ARM templates can be integrated into multiple CI/CD tools, like Azure DevOps and GitHub Actions. You can use these tools to version templates through source control and build release pipelines. Extensibility: With deployment scripts, you can run Bash or PowerShell scripts from within your ARM templates. These scripts perform tasks, like data plane operations, at deployment. Through extensibility, you can use a single ARM template to deploy a complete solution. JSON and Bicep templates Two types of ARM templates are available for use today: JSON templates and Bicep templates. JavaScript Object Notation (JSON) is an open-standard file format that multiple languages can use. Bicep is a new domain-specific language that was recently developed for authoring ARM templates by using an easier syntax. You can use either template format for your ARM templates and resource deployments.","title":"What is Bicep?"},{"location":"DevOps/Biceps/#what-is-bicep","text":"Bicep is a new Language for developing ARM templates. Bicep eventually compiles to JSON files but, its less complicated than JSON. There are two types of ARM template files: JSON and Bicep You can deploy azure resources using Bicep using Azure pipelines or Github Actions. We will see Bicep and Github actions here. Deploy Azure resources by using Bicep and GitHub Actions To deploy on Azure here are the options we have: Azure portal Azure CLI - This is an IAC option Azure PowerShell - This is an IAC option Azure Resource Manager templates (JSON and Bicep) This is an Infrasructre as code: az group create --name storage-resource-group --location eastus If you use it, it will make things faster also more accurate. Manually creating might be typo error by user. Benefits of Infrasturctre as code than going-to-portal-and-doing-it-manually: Create new enviornments quickly Deploy a feature to dev and than using parameters deploy it to prod. This was it will be identical and less error. DR. If you have everything as IAC. you can quicly create the DR enviornment. Imperative and declarative code You can write an instruction manual for new toy assembly in different ways. When you automate the deployment of services and infrastructure, you can take two approaches: imperative and declarative. With imperative code, you execute a sequence of commands, in a specific order, to reach an end configuration. This process defines what the code should accomplish, and it defines how to accomplish the task. The imperative approach is like a step-by-step instruction manual. With declarative code, you specify only the end configuration. The code doesn't define how to accomplish the task. The declarative approach is like the exploded view instruction manual. When you choose between using an imperative approach and a declarative approach to resource provisioning, consider the tools that might already be in use in your organization. Also consider which approach might match your own skills. Imperative code In Azure, an imperative code approach is accomplished programmatically by using a scripting language like Bash or Azure PowerShell. The scripts execute a series of steps to create, modify, and even remove your resources. This example shows two Azure CLI commands that create a resource group and a storage account. Azure CLI Copy","title":"What is Bicep?"},{"location":"DevOps/Biceps/#usrbinenv-bash","text":"az group create \\ --name storage-resource-group \\ --location eastus az storage account create \\ --name mystorageaccount \\ --resource-group storage-resource-group \\ --location eastus \\ --sku Standard_LRS \\ --kind StorageV2 \\ --access-tier Hot \\ --https-only true Declarative code In Azure, a declarative code approach is accomplished by using templates. Many types of templates are available to use, including: JSON Bicep Ansible, by RedHat Terraform, by HashiCorp Note This module focuses on using Bicep templates. Take a look at the following example of a Bicep template that configures a storage account. The configuration of the storage account matches the Azure CLI example. Copy resource storageAccount 'Microsoft.Storage/storageAccounts@2022-05-01' = { name: 'mystorageaccount' location: 'eastus' sku: { name: 'Standard_LRS' } kind: 'StorageV2' properties: { accessTier: 'Hot' supportsHttpsTrafficOnly: true } } The resources section defines the storage account configuration. This section contains the name, location, and properties of the storage account, including its SKU and the kind of account. You might notice that the Bicep template doesn't specify how to deploy the storage account. It specifies only what the storage account needs to look like. The actual steps that are executed behind the scenes to create this storage account or to update it to match the specification are left for Azure to decide. Learn Training Browse Fundamentals of Bicep Introduction to infrastructure as code using Bicep What is Azure Resource Manager? Completed 100 XP 7 minutes You've spent some time with your team learning the benefits of infrastructure as code and the different approaches that are available. Your company is growing at a rapid pace and your team knows it will be deploying a significant number of resources to Azure. As a team, you've decided that declarative infrastructure as code is the right approach to resource provisioning. The team doesn't want to maintain scripts that list every deployment step. Before beginning the process of building your first template, you need to understand how Azure Resource Manager works. Investigating the types of templates that are available to use with Azure will help you determine the next steps in your infrastructure-as-code strategy. In this unit, you'll learn about Resource Manager and the two types of Resource Manager templates. Azure Resource Manager concepts Azure Resource Manager is the service that's used to deploy and manage resources in Azure. You can use Resource Manager to create, update, and delete resources in your Azure subscription. You can interact with Resource Manager by using many tools, including the Azure portal. Resource Manager also provides a series of other features, like access control, auditing, and tagging, to help manage your resources after deployment. Terminology As you begin your cloud journey with Resource Manager, it's important to understand some terms and concepts: Resource: A manageable item that's available on the Azure platform. Virtual networks, virtual machines, storage accounts, web apps, and databases are examples of resources. Resource group: A logical container that holds related resources for an Azure solution. The resource group includes resources you want to manage as a group. Most Azure resources are contained in a resource group. You decide which resources belong in a resource group based on what makes the most sense for your solution. Note A small number of resources aren't contained in resource groups. These resource types are for specific purposes like managing access control and enforcing policies. You'll learn more about these resources in a later module. Subscription: A logical container and billing boundary for your resources and resource groups. Each Azure resource and resource group is associated with only one subscription. Management group: A logical container that you use to manage more than one subscription. You can define a hierarchy of management groups, subscriptions, resource groups, and resources to efficiently manage access, policies, and compliance through inheritance. Azure Resource Manager template (ARM template): A template file that defines one or more resources to deploy to a resource group, subscription, management group, or tenant. You can use the template to deploy the resources in a consistent and repeatable way. There are two types of ARM template files: JSON and Bicep. This module focuses on Bicep. Benefits Resource Manager provides many benefits and capabilities related to infrastructure-as-code resource provisioning: You can deploy, manage, and monitor the resources in your solution as a group instead of individually. You can redeploy your solution throughout the development lifecycle and have confidence that your resources are deployed in a consistent state. You can manage your infrastructure through declarative templates instead of by using scripts. You can specify resource dependencies to ensure that resources are deployed in the correct order. Operations: Control plane and data plane You can execute two types of operations in Azure: control plane operations and data plane operations. Use the control plane to manage the resources in your subscription. Use the data plane to access features that are exposed by a resource. For example, you use a control plane operation to create a virtual machine, but you use a data plane operation to connect to the virtual machine by using Remote Desktop Protocol (RDP). Control plane When you send a request from any of the Azure tools, APIs, or SDKs, Resource Manager receives, authenticates, and authorizes the request. Then, it sends the request to the Azure resource provider, which takes the requested action. Because all requests are handled through the same API, you see consistent results and capabilities in all the different tools that are available in Azure. The following image shows the role that Resource Manager plays in handling Azure requests: Diagram that shows how Azure Resource Manager accepts requests from all Azure clients and libraries. All control plane operation requests are sent to a Resource Manager URL. For example, the create or update operation for virtual machines is a control plane operation. Here's the request URL for this operation: HTTP Copy PUT https://management.azure.com/subscriptions/ /resourceGroups/ /providers/Microsoft.Compute/virtualMachines/{virtualMachineName}?api-version=2022-08-01 The control plane understands which resources need to be created and which resources already exist. Resource Manager understands the difference between these requests and won't create identical resources or delete existing resources, although there are ways to override this behavior. Data plane When a data plane operation starts, the requests are sent to a specific endpoint in your Azure subscription. For example, the Detect Language operation in Azure AI services is a data plane operation because the request URL is: HTTP Copy POST https://eastus.api.cognitive.microsoft.com/text/analytics/v2.0/languages Resource Manager features like access control and locks don't always apply to data plane operations. For example, a user might not have permissions to manage a virtual machine by using the control plane, but the user can sign in to the operating system. What are ARM templates? Azure Resource Manager templates are files that define the infrastructure and configuration for your deployment. When you write an ARM template, you take a declarative approach to your resource provisioning. These templates describe each resource in the deployment, but they don't describe how to deploy the resources. When you submit a template to Resource Manager for deployment, the control plane can deploy the defined resources in an organized and consistent manner. In the preceding unit, you learned about the differences between imperative code and declarative code. Why use ARM templates? There are many benefits to using ARM templates, either JSON or Bicep, for your resource provisioning. Repeatable results: ARM templates are idempotent, which means that you can repeatedly deploy the same template and get the same result. The template doesn't duplicate resources. Orchestration: When a template deployment is submitted to Resource Manager, the resources in the template are deployed in parallel. This process allows deployments to finish faster. Resource Manager orchestrates these deployments in the correct order if one resource depends on another. Preview: The what-if tool, available in Azure PowerShell and Azure CLI, allows you to preview changes to your environment before template deployment. This tool details any creations, modification, and deletions that will be made by your template. Testing and Validation: You can use tools like the Bicep linter to check the quality of your templates before deployment. ARM templates submitted to Resource Manager are validated before the deployment process. This validation alerts you to any errors in your template before resource provisioning. Modularity: You can break up your templates into smaller components and link them together at deployment. CI/CD integration: Your ARM templates can be integrated into multiple CI/CD tools, like Azure DevOps and GitHub Actions. You can use these tools to version templates through source control and build release pipelines. Extensibility: With deployment scripts, you can run Bash or PowerShell scripts from within your ARM templates. These scripts perform tasks, like data plane operations, at deployment. Through extensibility, you can use a single ARM template to deploy a complete solution. JSON and Bicep templates Two types of ARM templates are available for use today: JSON templates and Bicep templates. JavaScript Object Notation (JSON) is an open-standard file format that multiple languages can use. Bicep is a new domain-specific language that was recently developed for authoring ARM templates by using an easier syntax. You can use either template format for your ARM templates and resource deployments.","title":"!/usr/bin/env bash"},{"location":"DevOps/CI-CD_in%20ADF/","text":"","title":"CI CD in ADF"},{"location":"DevOps/GitScenarios/","text":"layout: default title: GitHub Concepts parent: Devops nav_order: 1 has_children: true","title":"GitScenarios"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Jenkins Vs GitHub Vs Azure DevOps CI/CD makes your code building and deployment automatic so that you can build and deploy yoru code to PROD quickly . CI/CD is implemented using workflows. In GitHub they are called GitHub Actions workflows, In Azure DevOps they are called Azure Pipelines. In this article, I will give you a quick summary of the popular CI/CD tools. For busy people, GitHub is the best. It's very well integrated with VS Code and has everything in one place \u2013 repositories, version control, CI/CD, many VS Code extensions, GitHub Desktop, and much more. Learning GitHub is the easiest among all the options. Jenkins What is Jenkins? Jenkins is an open-source CI/CD software mainly based on Java. It helps with CI/CD, but for version control, you need to use something else like GitHub. Hosting and Pricing Jenkins is self-hosted, meaning you need to install and manage it on your infrastructure, whether on-premise or on cloud services like Azure, AWS, or Google Cloud. Jenkins is free software, but you will have to pay for the infrastructure, servers, maintenance, and everything else. Best For If your company prefers open-source tools and needs a lot of customization, Jenkins is a good choice. You can control everything and customize it heavily. But remember, you have to handle all the infrastructure, installation, maintenance, and troubleshooting yourself. Companies like Netflix , Etsy , and Yahoo use Jenkins. Note, Jenkins doesn't come with version control like Git, which you'll need for CI/CD projects. GitHub GitHub is the most popular, the easiest, and the most all-in-one choice for CI/CD. It provides version control and CI/CD all as a service. You can have your own runners too (servers) for hybrid setups and to reduce costs. Personally, this is my first choice. What is GitHub Actions? The CI/CD part of GitHub is done using GitHub Actions. These are workflows for building, deploying, etc., written in a .yml file. Hosting and Pricing GitHub Actions is cloud-based. It uses GitHub\u2019s infrastructure, so you don\u2019t need to manage any servers. But you can have your own runners (hybrid setup) if you want more customization and don't want to pay a lot for services. GitHub is free for public repositories. For private repositories, a number of hosted runner minutes are included with each pricing tier. Additional minutes cost $0.008 per minute for Linux. Best For The easiest, most all-in-one solution with version control (GitHub repositories) and CI/CD (GitHub Actions) together. Examples: Facebook , Airbnb , many open-source projects. Azure DevOps What is Azure DevOps? Azure DevOps is a collection of development tools by Microsoft that supports the entire software development lifecycle, including Git repositories, CI/CD pipelines, and more. It was earlier known as TFS. So, like GitHub, it also has its own repositories and version control. It can connect easily with Git repositories as well. Hosting and Pricing Azure DevOps offers both cloud and self-hosted options. You can use Microsoft\u2019s cloud infrastructure or set up your own servers. Azure DevOps is: Free for open-source projects with up to 10 parallel jobs. Basic plan: Free for the first 5 users, then $6 per user/month with one free pipeline. Additional pipelines start at $40/month for cloud-hosted or $15/month for self-hosted. Best For Teams heavily invested in the Microsoft ecosystem. Organizations needing integration with Azure services. Examples: Microsoft , Adobe , Accenture . Let's put the comparison in a table CI Tool Open Source Hosting Free Version Build Agent Pricing Supported Platforms GitHub Actions No Cloud Yes Additional minutes start at $0.008 per minute Linux, Windows, macOS Jenkins Yes Self-hosted Yes Free Linux, Windows, macOS GitLab CI No Cloud/Self-hosted Yes Additional units start at $10 for 1,000 minutes Linux, Windows, macOS, Docker Azure DevOps No Cloud/Self-hosted Yes Additional pipelines start at $15/month (self-hosted) Linux, Windows, macOS","title":"Jenkins Vs GitHub Vs Devops"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps/#jenkins-vs-github-vs-azure-devops","text":"CI/CD makes your code building and deployment automatic so that you can build and deploy yoru code to PROD quickly . CI/CD is implemented using workflows. In GitHub they are called GitHub Actions workflows, In Azure DevOps they are called Azure Pipelines. In this article, I will give you a quick summary of the popular CI/CD tools. For busy people, GitHub is the best. It's very well integrated with VS Code and has everything in one place \u2013 repositories, version control, CI/CD, many VS Code extensions, GitHub Desktop, and much more. Learning GitHub is the easiest among all the options.","title":"Jenkins Vs GitHub Vs Azure DevOps"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps/#jenkins","text":"","title":"Jenkins"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps/#what-is-jenkins","text":"Jenkins is an open-source CI/CD software mainly based on Java. It helps with CI/CD, but for version control, you need to use something else like GitHub.","title":"What is Jenkins?"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps/#hosting-and-pricing","text":"Jenkins is self-hosted, meaning you need to install and manage it on your infrastructure, whether on-premise or on cloud services like Azure, AWS, or Google Cloud. Jenkins is free software, but you will have to pay for the infrastructure, servers, maintenance, and everything else.","title":"Hosting and Pricing"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps/#best-for","text":"If your company prefers open-source tools and needs a lot of customization, Jenkins is a good choice. You can control everything and customize it heavily. But remember, you have to handle all the infrastructure, installation, maintenance, and troubleshooting yourself. Companies like Netflix , Etsy , and Yahoo use Jenkins. Note, Jenkins doesn't come with version control like Git, which you'll need for CI/CD projects.","title":"Best For"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps/#github","text":"GitHub is the most popular, the easiest, and the most all-in-one choice for CI/CD. It provides version control and CI/CD all as a service. You can have your own runners too (servers) for hybrid setups and to reduce costs. Personally, this is my first choice.","title":"GitHub"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps/#what-is-github-actions","text":"The CI/CD part of GitHub is done using GitHub Actions. These are workflows for building, deploying, etc., written in a .yml file.","title":"What is GitHub Actions?"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps/#hosting-and-pricing_1","text":"GitHub Actions is cloud-based. It uses GitHub\u2019s infrastructure, so you don\u2019t need to manage any servers. But you can have your own runners (hybrid setup) if you want more customization and don't want to pay a lot for services. GitHub is free for public repositories. For private repositories, a number of hosted runner minutes are included with each pricing tier. Additional minutes cost $0.008 per minute for Linux.","title":"Hosting and Pricing"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps/#best-for_1","text":"The easiest, most all-in-one solution with version control (GitHub repositories) and CI/CD (GitHub Actions) together. Examples: Facebook , Airbnb , many open-source projects.","title":"Best For"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps/#azure-devops","text":"","title":"Azure DevOps"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps/#what-is-azure-devops","text":"Azure DevOps is a collection of development tools by Microsoft that supports the entire software development lifecycle, including Git repositories, CI/CD pipelines, and more. It was earlier known as TFS. So, like GitHub, it also has its own repositories and version control. It can connect easily with Git repositories as well.","title":"What is Azure DevOps?"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps/#hosting-and-pricing_2","text":"Azure DevOps offers both cloud and self-hosted options. You can use Microsoft\u2019s cloud infrastructure or set up your own servers. Azure DevOps is: Free for open-source projects with up to 10 parallel jobs. Basic plan: Free for the first 5 users, then $6 per user/month with one free pipeline. Additional pipelines start at $40/month for cloud-hosted or $15/month for self-hosted.","title":"Hosting and Pricing"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps/#best-for_2","text":"Teams heavily invested in the Microsoft ecosystem. Organizations needing integration with Azure services. Examples: Microsoft , Adobe , Accenture .","title":"Best For"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps/#lets-put-the-comparison-in-a-table","text":"CI Tool Open Source Hosting Free Version Build Agent Pricing Supported Platforms GitHub Actions No Cloud Yes Additional minutes start at $0.008 per minute Linux, Windows, macOS Jenkins Yes Self-hosted Yes Free Linux, Windows, macOS GitLab CI No Cloud/Self-hosted Yes Additional units start at $10 for 1,000 minutes Linux, Windows, macOS, Docker Azure DevOps No Cloud/Self-hosted Yes Additional pipelines start at $15/month (self-hosted) Linux, Windows, macOS","title":"Let's put the comparison in a table"},{"location":"DockerAndKubernetes/AirflowDocker/1_AirflowDocker/","text":"Apache Airflow on Windows Docker Here, I will show you how to install Airflow on a Docker container using two methods: Method 1: Simple Development Setup Single Container : This method runs everything inside one container . It includes all Airflow services like the scheduler, webserver, worker, etc., bundled together. Database : Uses SQLite as the database Docker Volume : A Docker volume is used to store data. Method 2: Full Production Setup Number of Containers : This method sets up seven containers . These include: Scheduler : Manages task scheduling. Webserver : Provides the Airflow user interface. Worker : Executes the tasks. PostgreSQL : Used as the database to store metadata. Redis : Acts as a message broker between the components. Triggerer : Manages task triggering. Flower : For monitoring the Celery workers. Local Folders : Data is stored in local(laptop) folders. Method 1 - Simple Development Setup Download Airflow Docker Image Run the following command in your command prompt or power shell to pull the latest Airflow Docker image: docker pull apache/airflow:latest Create a Docker Volume Execute this command to create a Docker volume named airflow-volume for data persistence: docker volume create airflow-volume Initialize Airflow Database Initialize the Airflow database using the following two commands: docker run --rm --network dasnet -v airflow-volume:/opt/airflow apache/airflow:latest db init docker run --rm --network dasnet -v airflow-volume:/opt/airflow apache/airflow:latest users create --username airflow --firstname FIRST_NAME --lastname LAST_NAME --role Admin --email admin@example.com --password airflow Note: I use a network dasnet. Hence --network part. You can totally remove the --network. Start the Airflow Webserver To start the Airflow webserver, use this command: docker run -d --name airflow --network dasnet -p 8080:8080 -e AIRFLOW_UID=50000 -v airflow-volume:/opt/airflow apache/airflow:latest webserver Note: I use a network dasnet. Hence --network part. You can totally remove the --network. Login into Airflow UI To login open http://localhost:8080 and enter credential: airflow/airflow Method 2 - Full Production Setup Create Required Folders Create a base directory, anywhere, for Airflow, e.g., C:\\Airflow_Das . Within this directory, create three subdirectories: dags , plugins , and logs . Download the Docker Compose File Save the docker-compose.yaml from link to Airflow_Das folder. Note on docker-compose.yaml When this article was written, the Airflow image used was apache/airflow:2.7.2 . You can find the relevant docker-compose.yaml file here . If the link doesn\u2019t work, visit the Apache Airflow site and search for the latest docker-compose.yaml . Initialize and Run Airflow Open PowerShell(with admin priv) and cd to Airflow_Das Run command: docker-compose up airflow-init Then run command: docker-compose up You can see the logs cascading down your PowerShell window. Wait a few seconds and then you can safely close the window. Verify the Installation On Docker Desktop, look for a container named Airflow_Das , containing seven subcontainers. Note: airflow-init-1, init container will exit after initialization. This is the expected, normal, beheviour. Don't panic. Open localhost:8080 in a web browser. Log in with the username and password: airflow . Components Installed The table shows some important components of our Airflow setup. Component What It Does Environment Variables Folders Ports Command Locations Inside Container Webserver The main part of Airflow where you can see and manage your workflows, logs, etc. AIRFLOW__CORE__EXECUTOR (sets the type of executor), AIRFLOW__WEBSERVER__WORKERS (number of workers) ./dags:/opt/airflow/dags , ./logs:/opt/airflow/logs , ./plugins:/opt/airflow/plugins 8080:8080 airflow webserver /opt/airflow (inside container) Scheduler Handles the scheduling of workflows, making sure tasks run on time. AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL (how often to check the DAG folder) ./dags:/opt/airflow/dags , ./logs:/opt/airflow/logs , ./plugins:/opt/airflow/plugins N/A airflow scheduler /opt/airflow Worker Runs the tasks in the workflows. Needed when using CeleryExecutor. AIRFLOW__CORE__EXECUTOR (CeleryExecutor), AIRFLOW__CELERY__BROKER_URL (URL for Celery broker) ./dags:/opt/airflow/dags , ./logs:/opt/airflow/logs , ./plugins:/opt/airflow/plugins N/A airflow celery worker /opt/airflow Postgres The database that stores all the Airflow information like DAGs, task statuses, etc. POSTGRES_USER=airflow , POSTGRES_PASSWORD=airflow , POSTGRES_DB=airflow postgres_data:/var/lib/postgresql/data N/A postgres /var/lib/postgresql/data Redis A messaging service that helps workers communicate with each other when using CeleryExecutor. REDIS_PASSWORD=redis_password (if you want to secure it) redis_data:/data N/A redis-server /data Flower A tool to monitor and manage Celery workers and tasks. FLOWER_BASIC_AUTH=admin:password (to secure it) N/A 5555:5555 flower N/A Reference Airflow official Docker Link \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Airflow"},{"location":"DockerAndKubernetes/AirflowDocker/1_AirflowDocker/#apache-airflow-on-windows-docker","text":"Here, I will show you how to install Airflow on a Docker container using two methods:","title":"Apache Airflow on Windows Docker"},{"location":"DockerAndKubernetes/AirflowDocker/1_AirflowDocker/#method-1-simple-development-setup","text":"Single Container : This method runs everything inside one container . It includes all Airflow services like the scheduler, webserver, worker, etc., bundled together. Database : Uses SQLite as the database Docker Volume : A Docker volume is used to store data.","title":"Method 1: Simple Development Setup"},{"location":"DockerAndKubernetes/AirflowDocker/1_AirflowDocker/#method-2-full-production-setup","text":"Number of Containers : This method sets up seven containers . These include: Scheduler : Manages task scheduling. Webserver : Provides the Airflow user interface. Worker : Executes the tasks. PostgreSQL : Used as the database to store metadata. Redis : Acts as a message broker between the components. Triggerer : Manages task triggering. Flower : For monitoring the Celery workers. Local Folders : Data is stored in local(laptop) folders.","title":"Method 2: Full Production Setup"},{"location":"DockerAndKubernetes/AirflowDocker/1_AirflowDocker/#method-1-simple-development-setup_1","text":"","title":"Method 1 - Simple Development Setup"},{"location":"DockerAndKubernetes/AirflowDocker/1_AirflowDocker/#download-airflow-docker-image","text":"Run the following command in your command prompt or power shell to pull the latest Airflow Docker image: docker pull apache/airflow:latest","title":"Download Airflow Docker Image"},{"location":"DockerAndKubernetes/AirflowDocker/1_AirflowDocker/#create-a-docker-volume","text":"Execute this command to create a Docker volume named airflow-volume for data persistence: docker volume create airflow-volume","title":"Create a Docker Volume"},{"location":"DockerAndKubernetes/AirflowDocker/1_AirflowDocker/#initialize-airflow-database","text":"Initialize the Airflow database using the following two commands: docker run --rm --network dasnet -v airflow-volume:/opt/airflow apache/airflow:latest db init docker run --rm --network dasnet -v airflow-volume:/opt/airflow apache/airflow:latest users create --username airflow --firstname FIRST_NAME --lastname LAST_NAME --role Admin --email admin@example.com --password airflow Note: I use a network dasnet. Hence --network part. You can totally remove the --network.","title":"Initialize Airflow Database"},{"location":"DockerAndKubernetes/AirflowDocker/1_AirflowDocker/#start-the-airflow-webserver","text":"To start the Airflow webserver, use this command: docker run -d --name airflow --network dasnet -p 8080:8080 -e AIRFLOW_UID=50000 -v airflow-volume:/opt/airflow apache/airflow:latest webserver Note: I use a network dasnet. Hence --network part. You can totally remove the --network.","title":"Start the Airflow Webserver"},{"location":"DockerAndKubernetes/AirflowDocker/1_AirflowDocker/#login-into-airflow-ui","text":"To login open http://localhost:8080 and enter credential: airflow/airflow","title":"Login into Airflow UI"},{"location":"DockerAndKubernetes/AirflowDocker/1_AirflowDocker/#method-2-full-production-setup_1","text":"","title":"Method 2 - Full Production Setup"},{"location":"DockerAndKubernetes/AirflowDocker/1_AirflowDocker/#create-required-folders","text":"Create a base directory, anywhere, for Airflow, e.g., C:\\Airflow_Das . Within this directory, create three subdirectories: dags , plugins , and logs .","title":"Create Required Folders"},{"location":"DockerAndKubernetes/AirflowDocker/1_AirflowDocker/#download-the-docker-compose-file","text":"Save the docker-compose.yaml from link to Airflow_Das folder.","title":"Download the Docker Compose File"},{"location":"DockerAndKubernetes/AirflowDocker/1_AirflowDocker/#note-on-docker-composeyaml","text":"When this article was written, the Airflow image used was apache/airflow:2.7.2 . You can find the relevant docker-compose.yaml file here . If the link doesn\u2019t work, visit the Apache Airflow site and search for the latest docker-compose.yaml .","title":"Note on docker-compose.yaml"},{"location":"DockerAndKubernetes/AirflowDocker/1_AirflowDocker/#initialize-and-run-airflow","text":"Open PowerShell(with admin priv) and cd to Airflow_Das Run command: docker-compose up airflow-init Then run command: docker-compose up You can see the logs cascading down your PowerShell window. Wait a few seconds and then you can safely close the window.","title":"Initialize and Run Airflow"},{"location":"DockerAndKubernetes/AirflowDocker/1_AirflowDocker/#verify-the-installation","text":"On Docker Desktop, look for a container named Airflow_Das , containing seven subcontainers. Note: airflow-init-1, init container will exit after initialization. This is the expected, normal, beheviour. Don't panic. Open localhost:8080 in a web browser. Log in with the username and password: airflow .","title":"Verify the Installation"},{"location":"DockerAndKubernetes/AirflowDocker/1_AirflowDocker/#components-installed","text":"The table shows some important components of our Airflow setup. Component What It Does Environment Variables Folders Ports Command Locations Inside Container Webserver The main part of Airflow where you can see and manage your workflows, logs, etc. AIRFLOW__CORE__EXECUTOR (sets the type of executor), AIRFLOW__WEBSERVER__WORKERS (number of workers) ./dags:/opt/airflow/dags , ./logs:/opt/airflow/logs , ./plugins:/opt/airflow/plugins 8080:8080 airflow webserver /opt/airflow (inside container) Scheduler Handles the scheduling of workflows, making sure tasks run on time. AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL (how often to check the DAG folder) ./dags:/opt/airflow/dags , ./logs:/opt/airflow/logs , ./plugins:/opt/airflow/plugins N/A airflow scheduler /opt/airflow Worker Runs the tasks in the workflows. Needed when using CeleryExecutor. AIRFLOW__CORE__EXECUTOR (CeleryExecutor), AIRFLOW__CELERY__BROKER_URL (URL for Celery broker) ./dags:/opt/airflow/dags , ./logs:/opt/airflow/logs , ./plugins:/opt/airflow/plugins N/A airflow celery worker /opt/airflow Postgres The database that stores all the Airflow information like DAGs, task statuses, etc. POSTGRES_USER=airflow , POSTGRES_PASSWORD=airflow , POSTGRES_DB=airflow postgres_data:/var/lib/postgresql/data N/A postgres /var/lib/postgresql/data Redis A messaging service that helps workers communicate with each other when using CeleryExecutor. REDIS_PASSWORD=redis_password (if you want to secure it) redis_data:/data N/A redis-server /data Flower A tool to monitor and manage Celery workers and tasks. FLOWER_BASIC_AUTH=admin:password (to secure it) N/A 5555:5555 flower N/A","title":"Components Installed"},{"location":"DockerAndKubernetes/AirflowDocker/1_AirflowDocker/#reference","text":"Airflow official Docker Link \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Reference"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/","text":"Table of Contents Confluent Kafka with KRaft on Docker Step 1: Download the Docker Compose file Step 2: Run the container Appendix Connecting to Kafka Containers Troubleshooting Broker Connection Issues Error: no matching manifest for linux/arm64/v8 Resolving Port Conflicts for Kafka Rest Proxy in Docker About the docker-compose.yml Fully commented docker-compose.yml Conclusion Further reading Kafka 6 with Zookeeper on Windows Docker Overview Prerequisites Steps to create the setup 1. Create the docker-compose.yaml 2. Start the Services 3. Check the setup Component Details Confluent Kafka with KRaft on Docker Here, I will show you how to setup a Kafka setup on Docker. We will use KRaft rather than Zookeper. We will use a docker-compose.yaml file and setup Kafka broker, Schema Registry, Kafka Connect, Control Center, ksqlDB, and a REST Proxy. I have tested the installation in both Windows and Mac machines with M1 chip. Step 1: Download the Docker Compose file The first step is to download the docker-compose.yaml file, KRaft version from confluent's github site Note: When choosing between KRaft and ZooKeeper as the metadata service for your Apache Kafka cluster, KRaft is the recommended option. Step 2: Run the container Open command prompt/terminal and CD to the folder containing the docker-compose.yml Run the following command to start all services: docker-compose up -d The Docker Compose will start all the necessary services in the background. Once finished, go to the docker desktop window and see the services You can access the Control Center at http://localhost:9021 once the container is operational. To create topics and proceed further you can refer to the this quickstart guide . Appendix Connecting to Kafka Containers The container group created using the docker-compose become part of confluent-kafka_default network group, restricting access from external containers or local machines not in this network. This means, you won't be able to connect to the broker from outside containers or local machine. To connect to Kafka from an external container, add it to the confluent-kafka_default network: docker network connect confluent-kafka_default [external-container-name-or-id] After adding, connect to the Kafka broker at broker:29092 . Example for Spark: from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"Streaming from Kafka\") \\ .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\ .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') \\ .getOrCreate() streaming_df = spark.readStream \\ .format(\"kafka\") \\ .option(\"kafka.bootstrap.servers\", \"broker:29092) \\ .option(\"subscribe\", \"sometopic\") \\ .option(\"startingOffsets\", \"earliest\") \\ .load() Troubleshooting Broker Connection Issues Inspect the Kafka broker container using docker inspect [broker-container-id] . For detailed network information: docker network inspect [network-name] To find the broker's IP address: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' [container-name-or-id] Test connectivity to the broker: nc -vz -w 5 [broker-ip] [listener-port] Error: no matching manifest for linux/arm64/v8 We might run into an error like no matching manifest for linux/arm65/v8 this error error indicates that the Docker images specified in the docker-compose.yml file do not have a version compatible with the architecture of our Mac's processor. This error is less likely if you use the KRaft version. I have tested the KRaft versin on both Windows and Mac M1, they both showed no error in architecture or compatibility. Resolving Port Conflicts for Kafka Rest Proxy in Docker When deploying Kafka Rest Proxy using Docker Compose, port conflicts are a common issue that can prevent the service from starting successfully. Here is a typical example of this error: Cannot start Docker Compose application. Reason: compose [start] exit status 1. Container broker Starting Container broker Started Container schema-registry Starting Container schema-registry Started Container rest-proxy Starting Container connect Starting Container connect Started Error response from daemon: Ports are not available: exposing port TCP 0.0.0.0:8082 -> 0.0.0.0:0: listen tcp 0.0.0.0:8082: bind: An attempt was made to access a socket in a way forbidden by its access permissions. Long story short: Use this modified docker-compose.yml which uses port 8086 instead of 8082 Detailed investigation : Identify Port Usage : Use netstat or lsof to check if the intended port is already in use. For Windows: netstat -ano | findstr :<PORT> For Linux/Mac: sudo lsof -i :<PORT> or sudo netstat -tulnp | grep :<PORT> Modify Docker Compose File : If the port is in use, select an unused port and update the docker-compose.yml file. Change the host port mapping for the rest-proxy service: yaml ports: - <UNUSED_PORT>:8082 # Change <UNUSED_PORT> to an available port on your host e.g: ![Alt text](images/image-5.png) Restart Docker Compose : Apply the changes by running: bash docker-compose down docker-compose up -d Update Applications : Ensure all applications that interact with Kafka Rest Proxy are updated to use the new port. About the docker-compose.yml Here is an explanation of the differnet services in the docker-compose.yml broker : A Kafka broker service with custom environment configurations for topics, listeners, ports, IDs, etc. schema-registry : This service provides a serving layer for your metadata. It is configured to communicate with the Kafka broker and expose port 8081. connect : Kafka Connect with dependencies on the broker and schema-registry, configured to run connectors. control-center : Confluent's web-based tool for managing and monitoring the Kafka ecosystem. ksqldb-server : The server for ksqlDB, streaming SQL engine that enables real-time data processing against Apache Kafka. ksqldb-cli : A command-line interface for interacting with ksqlDB Server. ksql-datagen : A tool for generating sample data for Kafka topics and provides a source of continuously flowing data. rest-proxy : Provides a RESTful interface to Kafka clusters. Fully commented docker-compose.yml If you want to know each step of the docker compose file. I have placed a fully commented docker-compose.yml Conclusion You now have a fully functional local Kafka development environment that includes a broker, Schema Registry, Kafka Connect, Control Center, ksqlDB, and a REST Proxy. The KRaft version of the docker compose has been tested in both WIndows and Mac M1 machines. Further reading Confluent Documentation. Quick Start. Docker Container Confluent Documentation. Quick Start using CLI Kafka 6 with Zookeeper on Windows Docker In this article, I will show you how to set up Confluent Kafka on Docker in Windows. We will be using a total of 9 containers, and this setup is very stable. You will only need one docker-compose.yml file, and there is no need for a Dockerfile. The setup will install the following Confluent Kafka components: confluentinc/cp-zookeeper:6.0.1 confluentinc/cp-server:6.0.1 confluentinc/cp-schema-registry:6.0.1 confluentinc/cp-kafka-connect-base:6.0.1 confluentinc/cp-enterprise-control-center:6.0.1 confluentinc/cp-ksqldb-server:6.0.1 confluentinc/cp-ksqldb-cli:6.0.1 confluentinc/ksqldb-examples:6.0.1 confluentinc/cp-kafka-rest:6.0.1 For the Busy Professionals Download and unzip the Docker Compose file to a folder on your machine. Open Command Prompt and navigate to the folder by running cd [folder_name] . Execute docker-compose up -d to start the services. That's it! You will have 9 containers up and running, ready to serve. See the example image below for reference. Overview Many people struggle with setting up Confluent Kafka on their local machines. While it's easier to use 'templates' on AWS or Azure, setting it up locally can be quite complicated. That\u2019s why I\u2019m sharing this method\u2014it helps you create a stable, fully-working, production-like environment on your local system. Prerequisites Before you start, ensure that you have the following installed: - Docker Desktop : Install it from the Docker website . Docker Compose : It usually comes with Docker Desktop, so you don\u2019t need to install it separately. - Run the command docker network create dasnet in CMD . Note: I use a network so that all my containers are part of the same network. This is just an extra step. It\u2019s necessary because the Docker Compose file has the dasnet network mentioned everywhere. If you don\u2019t want to use this network, then just remove all occurrences of it from the Docker Compose file. Steps to create the setup The steps are simple. Just follow these two steps and your setup will be up and running. 1. Create the docker-compose.yaml Create a docker-compose.yaml file with the content below inside any folder. Alternatively download the file from the link given at the start of the article. I have verbosely commented the file. Hope you will find it useful. # Docker Compose file for setting up a Confluent Platform environment with various services. # Author: DDas version: '2' # Specifies the Docker Compose version. Version 2 is stable and compatible with most setups. services: # This section defines all the services (containers) that will be created as part of this setup. zookeeper: # Zookeeper service - essential for managing and coordinating Kafka brokers. image: confluentinc/cp-zookeeper:6.0.1 # Zookeeper version 6.0.1 from Confluent. hostname: zookeeper # Internal hostname used within the Docker network. container_name: zookeeper # Name of the container as it will appear in Docker. ports: - \"2181:2181\" # laptop:container environment: # Environment variables that configure Zookeeper. ZOOKEEPER_CLIENT_PORT: 2181 # Port where Zookeeper listens for client connections. ZOOKEEPER_TICK_TIME: 2000 # Basic time unit in milliseconds used by Zookeeper. ZOOKEEPER_SERVER_ID: 1 ZOOKEEPER_INIT_LIMIT: 5 ZOOKEEPER_SYNC_LIMIT: 2 ZOOKEEPER_CLIENT_CNXNS: 60 networks: - dasnet # Connecting Zookeeper to the custom network 'dasnet'. broker: # Kafka broker service - handles message publishing and subscribing. image: confluentinc/cp-server:6.0.1 # Kafka broker version 6.0.1 from Confluent. hostname: broker # Internal hostname used within the Docker network. container_name: broker # Name of the container as it will appear in Docker. depends_on: # Ensures Zookeeper is started before the broker. - zookeeper ports: - \"9192:9092\" # laptop:container - \"9111:9101\" # laptop:container environment: # Environment variables that configure the Kafka broker. KAFKA_BROKER_ID: 1 # Unique identifier for the broker. KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181' # Address of the Zookeeper service. KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT # Protocol mapping. KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092 # Advertised listeners. KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter # Metrics reporter. KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 # Replication factor for the offsets topic. KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0 # Delay in rebalancing consumer groups. KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1 # Replication factor for the license topic. KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1 # Replication factor for the balancer topic. KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1 # Minimum in-sync replicas for transaction state log. KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1 # Replication factor for the transaction state log. KAFKA_JMX_PORT: 9101 # Port for JMX monitoring. KAFKA_JMX_HOSTNAME: localhost # Hostname for JMX monitoring. KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:8081 # Schema Registry URL. CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:29092 # Bootstrap servers for metrics reporter. CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1 # Replication factor for metrics topics. CONFLUENT_METRICS_ENABLE: 'true' # Enable Confluent metrics. CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous' # Customer ID for support. KAFKA_SASL_JAAS_CONFIG: \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"admin\\\" password=\\\"Passw0rd\\\";\" networks: - dasnet # Connecting the Kafka broker to the custom network 'dasnet'. schema-registry: # Schema Registry service - manages Avro schemas for Kafka topics. image: confluentinc/cp-schema-registry:6.0.1 # Schema Registry version 6.0.1 from Confluent. hostname: schema-registry # Internal hostname used within the Docker network. container_name: schema-registry # Name of the container as it will appear in Docker. depends_on: # Ensures the broker is started before the Schema Registry. - broker ports: - \"8181:8081\" # laptop:container environment: # Environment variables that configure the Schema Registry. SCHEMA_REGISTRY_HOST_NAME: schema-registry # Internal hostname for Schema Registry. SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker:29092' # Kafka broker address for Schema Registry. SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081 # Listeners for Schema Registry. SCHEMA_REGISTRY_AUTH: \"admin:Passw0rd\" networks: - dasnet # Connecting the Schema Registry to the custom network 'dasnet'. connect: # Kafka Connect service - facilitates integration with external systems. image: confluentinc/cp-kafka-connect-base:6.0.1 # Kafka Connect version 6.0.1 from Confluent. hostname: connect # Internal hostname used within the Docker network. container_name: kafka-connect # Name of the container as it will appear in Docker. depends_on: # Ensures the broker and Schema Registry are started before Kafka Connect. - broker - schema-registry ports: - \"8183:8083\" # laptop:container environment: # Environment variables that configure Kafka Connect. CONNECT_BOOTSTRAP_SERVERS: 'broker:29092' # Kafka broker address for Kafka Connect. CONNECT_REST_ADVERTISED_HOST_NAME: connect # Hostname for the Kafka Connect REST API. CONNECT_REST_PORT: 8083 # Port for the Kafka Connect REST API. CONNECT_GROUP_ID: kafka-connect # Group ID for Kafka Connect. CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs # Topic for storing connector configs. CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1 # Replication factor for the config storage topic. CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000 # Interval for flushing offsets. CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets # Topic for storing connector offsets. CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1 # Replication factor for the offset storage topic. CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status # Topic for storing connector statuses. CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1 # Replication factor for the status storage topic. CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter # Converter for Kafka Connect keys. CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter # Converter for Kafka Connect values. CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081 # Schema Registry URL. CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-6.0.1.jar # Additional classpath for interceptors. CONNECT_PRODUCER_INTERCEPTOR_CLASSES: \"io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor\" # Producer interceptor. CONNECT_CONSUMER_INTERCEPTOR_CLASSES: \"io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor\" # Consumer interceptor. CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR # Logging configuration. CONNECT_PLUGIN_PATH: \"/usr/share/java,/usr/share/confluent-hub-components,/data/connect-jars\" # Plugin paths. CONNECT_SASL_JAAS_CONFIG: \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"admin\\\" password=\\\"Passw0rd\\\";\" volumes: # Mounting a volume for persistent storage. - ${PWD}/data:/data networks: - dasnet # Connecting Kafka Connect to the custom network 'dasnet'. control-center: # Confluent Control Center service - web interface for managing and monitoring Kafka. image: confluentinc/cp-enterprise-control-center:6.0.1 # Control Center version 6.0.1 from Confluent. hostname: control-center # Internal hostname used within the Docker network. container_name: control-center # Name of the container as it will appear in Docker. depends_on: # Ensures the broker, Schema Registry, and Kafka Connect are started before Control Center. - broker - schema-registry - connect - ksqldb-server ports: - \"9121:9021\" # laptop:container environment: # Environment variables that configure the Control Center. CONTROL_CENTER_BOOTSTRAP_SERVERS: 'broker:29092' # Kafka broker address for Control Center. CONTROL_CENTER_CONNECT_CLUSTER: 'connect:8083' # Kafka Connect cluster address for Control Center. CONTROL_CENTER_KSQL_KSQLDB1_URL: \"http://ksqldb-server:8088\" # KSQL server URL for Control Center. CONTROL_CENTER_KSQL_KSQLDB1_ADVERTISED_URL: \"http://localhost:8088\" # Advertised KSQL server URL. CONTROL_CENTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\" # Schema Registry URL for Control Center. CONTROL_CENTER_REPLICATION_FACTOR: 1 # Replication factor for Control Center's internal topics. CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1 # Number of partitions for Control Center's internal topics. CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1 # Partitions for monitoring interceptor topic. CONFLUENT_METRICS_TOPIC_REPLICATION: 1 # Replication factor for metrics topics. PORT: 9021 # Internal port for Control Center. networks: - dasnet # Connecting Control Center to the custom network 'dasnet'. ksqldb-server: # KSQL server service - SQL engine for processing real-time streams. image: confluentinc/cp-ksqldb-server:6.0.1 # KSQL server version 6.0.1 from Confluent. hostname: ksqldb-server # Internal hostname used within the Docker network. container_name: ksqldb-server # Name of the container as it will appear in Docker. depends_on: # Ensures the broker and Kafka Connect are started before KSQL server. - broker - connect ports: - \"8188:8088\" # laptop:container environment: # Environment variables that configure KSQL server. KSQL_CONFIG_DIR: \"/etc/ksql\" # Directory for KSQL configuration files. KSQL_BOOTSTRAP_SERVERS: \"broker:29092\" # Kafka broker address for KSQL server. KSQL_HOST_NAME: ksqldb-server # Internal hostname for KSQL server. KSQL_LISTENERS: \"http://0.0.0.0:8088\" # Listeners for KSQL server. KSQL_CACHE_MAX_BYTES_BUFFERING: 0 # Disables caching in KSQL. KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\" # Schema Registry URL for KSQL server. KSQL_PRODUCER_INTERCEPTOR_CLASSES: \"io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor\" # Producer interceptor. KSQL_CONSUMER_INTERCEPTOR_CLASSES: \"io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor\" # Consumer interceptor. KSQL_KSQL_CONNECT_URL: \"http://connect:8083\" # Kafka Connect URL for KSQL server. KSQL_KSQL_LOGGING_PROCESSING_TOPIC_REPLICATION_FACTOR: 1 # Replication factor for logging processing topics. KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: 'true' # Auto-create logging processing topics. KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: 'true' # Auto-create logging processing streams. networks: - dasnet # Connecting KSQL server to the custom network 'dasnet'. ksqldb-cli: # KSQL CLI service - command-line interface for interacting with KSQL server. image: confluentinc/cp-ksqldb-cli:6.0.1 # KSQL CLI version 6.0.1 from Confluent. container_name: ksqldb-cli # Name of the container as it will appear in Docker. depends_on: # Ensures the broker, Kafka Connect, and KSQL server are started before KSQL CLI. - broker - connect - ksqldb-server entrypoint: /bin/sh # Entry point for the container to start a shell. tty: true # Allocate a pseudo-TTY for interactive use. networks: - dasnet # Connecting KSQL CLI to the custom network 'dasnet'. ksql-datagen: # KSQL DataGen service - generates sample data for KSQL. image: confluentinc/ksqldb-examples:6.0.1 # KSQL DataGen version 6.0.1 from Confluent. hostname: ksql-datagen # Internal hostname used within the Docker network. container_name: ksql-datagen # Name of the container as it will appear in Docker. depends_on: # Ensures KSQL server, broker, Schema Registry, and Kafka Connect are started before KSQL DataGen. - ksqldb-server - broker - schema-registry - connect command: \"bash -c 'echo Waiting for Kafka to be ready... && \\ cub kafka-ready -b broker:29092 1 40 && \\ echo Waiting for Confluent Schema Registry to be ready... && \\ cub sr-ready schema-registry 8081 40 && \\ echo Waiting a few seconds for topic creation to finish... && \\ sleep 11 && \\ tail -f /dev/null'\" # Command to ensure services are ready before starting data generation. environment: # Environment variables that configure KSQL DataGen. KSQL_CONFIG_DIR: \"/etc/ksql\" # Directory for KSQL configuration files. STREAMS_BOOTSTRAP_SERVERS: broker:29092 # Kafka broker address for KSQL DataGen. STREAMS_SCHEMA_REGISTRY_HOST: schema-registry # Schema Registry host for KSQL DataGen. STREAMS_SCHEMA_REGISTRY_PORT: 8081 # Schema Registry port for KSQL DataGen. networks: - dasnet # Connecting KSQL DataGen to the custom network 'dasnet'. rest-proxy: # Kafka REST Proxy service - provides a RESTful interface to Kafka. image: confluentinc/cp-kafka-rest:6.0.1 # Kafka REST Proxy version 6.0.1 from Confluent. hostname: rest-proxy # Internal hostname used within the Docker network. container_name: rest-proxy # Name of the container as it will appear in Docker. depends_on: # Ensures the broker and Schema Registry are started before REST Proxy. - broker - schema-registry ports: - \"8182:8082\" # laptop:container environment: # Environment variables that configure the REST Proxy. KAFKA_REST_HOST_NAME: rest-proxy # Internal hostname for REST Proxy. KAFKA_REST_BOOTSTRAP_SERVERS: 'broker:29092' # Kafka broker address for REST Proxy. KAFKA_REST_LISTENERS: \"http://0.0.0.0:8082\" # Listeners for REST Proxy. KAFKA_REST_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081' # Schema Registry URL for REST Proxy. networks: - dasnet # Connecting REST Proxy to the custom network 'dasnet'. networks: # Network configuration for Docker containers. dasnet: # Defines the custom network 'dasnet'. external: true # Indicates that 'dasnet' is a pre-existing external network. 2. Start the Services CD to docker-compose.yml folder. Run the command docker-compose up -d 3. Check the setup Open Docker Desktop. You should see all the containres up and running Open the control centre to see the actual working. Go through the tabs to see all the functionalities Component Details Refer to the table for quick info on the the setup: Component Purpose Host Name Ports (Laptop:Container) Key Environment Variables Network Zookeeper Coordinates and manages Kafka brokers. zookeeper 2181:2181 ZOOKEEPER_CLIENT_PORT=2181 ZOOKEEPER_TICK_TIME=2000 dasnet Kafka Broker Handles message publishing and subscribing. broker 9192:9092 9111:9101 KAFKA_BROKER_ID=1 KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://broker:29092 dasnet Schema Registry Manages Avro schemas for Kafka topics. schema-registry 8181:8081 SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS=broker:29092 SCHEMA_REGISTRY_LISTENERS=http://0.0.0.0:8081 dasnet Kafka Connect Integrates Kafka with external systems (e.g., databases). connect 8183:8083 CONNECT_BOOTSTRAP_SERVERS=broker:29092 CONNECT_REST_PORT=8083 CONNECT_GROUP_ID=kafka-connect dasnet Control Center Web interface for managing and monitoring Kafka. control-center 9121:9021 CONTROL_CENTER_BOOTSTRAP_SERVERS=broker:29092 CONTROL_CENTER_CONNECT_CLUSTER=connect:8083 dasnet KSQL Server SQL engine for processing real-time data streams. ksqldb-server 8188:8088 KSQL_BOOTSTRAP_SERVERS=broker:29092 KSQL_LISTENERS=http://0.0.0.0:8088 KSQL_KSQL_SCHEMA_REGISTRY_URL=http://schema-registry:8081 dasnet REST Proxy Provides a RESTful interface to Kafka. rest-proxy 8182:8082 KAFKA_REST_BOOTSTRAP_SERVERS=broker:29092 KAFKA_REST_LISTENERS=http://0.0.0.0:8082 dasnet KSQL DataGen Generates sample data for KSQL testing (optional). ksql-datagen N/A STREAMS_BOOTSTRAP_SERVERS=broker:29092 STREAMS_SCHEMA_REGISTRY_HOST=schema-registry dasnet All components are connected via the dasnet custom Docker network.","title":"Confluent Kafka"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#table-of-contents","text":"Confluent Kafka with KRaft on Docker Step 1: Download the Docker Compose file Step 2: Run the container Appendix Connecting to Kafka Containers Troubleshooting Broker Connection Issues Error: no matching manifest for linux/arm64/v8 Resolving Port Conflicts for Kafka Rest Proxy in Docker About the docker-compose.yml Fully commented docker-compose.yml Conclusion Further reading Kafka 6 with Zookeeper on Windows Docker Overview Prerequisites Steps to create the setup 1. Create the docker-compose.yaml 2. Start the Services 3. Check the setup Component Details","title":"Table of Contents"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#confluent-kafka-with-kraft-on-docker","text":"Here, I will show you how to setup a Kafka setup on Docker. We will use KRaft rather than Zookeper. We will use a docker-compose.yaml file and setup Kafka broker, Schema Registry, Kafka Connect, Control Center, ksqlDB, and a REST Proxy. I have tested the installation in both Windows and Mac machines with M1 chip.","title":"Confluent Kafka with KRaft on Docker"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#step-1-download-the-docker-compose-file","text":"The first step is to download the docker-compose.yaml file, KRaft version from confluent's github site Note: When choosing between KRaft and ZooKeeper as the metadata service for your Apache Kafka cluster, KRaft is the recommended option.","title":"Step 1: Download the Docker Compose file"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#step-2-run-the-container","text":"Open command prompt/terminal and CD to the folder containing the docker-compose.yml Run the following command to start all services: docker-compose up -d The Docker Compose will start all the necessary services in the background. Once finished, go to the docker desktop window and see the services You can access the Control Center at http://localhost:9021 once the container is operational. To create topics and proceed further you can refer to the this quickstart guide .","title":"Step 2: Run the container"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#appendix","text":"","title":"Appendix"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#connecting-to-kafka-containers","text":"The container group created using the docker-compose become part of confluent-kafka_default network group, restricting access from external containers or local machines not in this network. This means, you won't be able to connect to the broker from outside containers or local machine. To connect to Kafka from an external container, add it to the confluent-kafka_default network: docker network connect confluent-kafka_default [external-container-name-or-id] After adding, connect to the Kafka broker at broker:29092 . Example for Spark: from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"Streaming from Kafka\") \\ .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\ .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') \\ .getOrCreate() streaming_df = spark.readStream \\ .format(\"kafka\") \\ .option(\"kafka.bootstrap.servers\", \"broker:29092) \\ .option(\"subscribe\", \"sometopic\") \\ .option(\"startingOffsets\", \"earliest\") \\ .load()","title":"Connecting to Kafka Containers"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#troubleshooting-broker-connection-issues","text":"Inspect the Kafka broker container using docker inspect [broker-container-id] . For detailed network information: docker network inspect [network-name] To find the broker's IP address: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' [container-name-or-id] Test connectivity to the broker: nc -vz -w 5 [broker-ip] [listener-port]","title":"Troubleshooting Broker Connection Issues"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#error-no-matching-manifest-for-linuxarm64v8","text":"We might run into an error like no matching manifest for linux/arm65/v8 this error error indicates that the Docker images specified in the docker-compose.yml file do not have a version compatible with the architecture of our Mac's processor. This error is less likely if you use the KRaft version. I have tested the KRaft versin on both Windows and Mac M1, they both showed no error in architecture or compatibility.","title":"Error: no matching manifest for linux/arm64/v8"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#resolving-port-conflicts-for-kafka-rest-proxy-in-docker","text":"When deploying Kafka Rest Proxy using Docker Compose, port conflicts are a common issue that can prevent the service from starting successfully. Here is a typical example of this error: Cannot start Docker Compose application. Reason: compose [start] exit status 1. Container broker Starting Container broker Started Container schema-registry Starting Container schema-registry Started Container rest-proxy Starting Container connect Starting Container connect Started Error response from daemon: Ports are not available: exposing port TCP 0.0.0.0:8082 -> 0.0.0.0:0: listen tcp 0.0.0.0:8082: bind: An attempt was made to access a socket in a way forbidden by its access permissions. Long story short: Use this modified docker-compose.yml which uses port 8086 instead of 8082 Detailed investigation : Identify Port Usage : Use netstat or lsof to check if the intended port is already in use. For Windows: netstat -ano | findstr :<PORT> For Linux/Mac: sudo lsof -i :<PORT> or sudo netstat -tulnp | grep :<PORT> Modify Docker Compose File : If the port is in use, select an unused port and update the docker-compose.yml file. Change the host port mapping for the rest-proxy service: yaml ports: - <UNUSED_PORT>:8082 # Change <UNUSED_PORT> to an available port on your host e.g: ![Alt text](images/image-5.png) Restart Docker Compose : Apply the changes by running: bash docker-compose down docker-compose up -d Update Applications : Ensure all applications that interact with Kafka Rest Proxy are updated to use the new port.","title":"Resolving Port Conflicts for Kafka Rest Proxy in Docker"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#about-the-docker-composeyml","text":"Here is an explanation of the differnet services in the docker-compose.yml broker : A Kafka broker service with custom environment configurations for topics, listeners, ports, IDs, etc. schema-registry : This service provides a serving layer for your metadata. It is configured to communicate with the Kafka broker and expose port 8081. connect : Kafka Connect with dependencies on the broker and schema-registry, configured to run connectors. control-center : Confluent's web-based tool for managing and monitoring the Kafka ecosystem. ksqldb-server : The server for ksqlDB, streaming SQL engine that enables real-time data processing against Apache Kafka. ksqldb-cli : A command-line interface for interacting with ksqlDB Server. ksql-datagen : A tool for generating sample data for Kafka topics and provides a source of continuously flowing data. rest-proxy : Provides a RESTful interface to Kafka clusters.","title":"About the docker-compose.yml"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#fully-commented-docker-composeyml","text":"If you want to know each step of the docker compose file. I have placed a fully commented docker-compose.yml","title":"Fully commented docker-compose.yml"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#conclusion","text":"You now have a fully functional local Kafka development environment that includes a broker, Schema Registry, Kafka Connect, Control Center, ksqlDB, and a REST Proxy. The KRaft version of the docker compose has been tested in both WIndows and Mac M1 machines.","title":"Conclusion"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#further-reading","text":"Confluent Documentation. Quick Start. Docker Container Confluent Documentation. Quick Start using CLI","title":"Further reading"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#kafka-6-with-zookeeper-on-windows-docker","text":"In this article, I will show you how to set up Confluent Kafka on Docker in Windows. We will be using a total of 9 containers, and this setup is very stable. You will only need one docker-compose.yml file, and there is no need for a Dockerfile. The setup will install the following Confluent Kafka components: confluentinc/cp-zookeeper:6.0.1 confluentinc/cp-server:6.0.1 confluentinc/cp-schema-registry:6.0.1 confluentinc/cp-kafka-connect-base:6.0.1 confluentinc/cp-enterprise-control-center:6.0.1 confluentinc/cp-ksqldb-server:6.0.1 confluentinc/cp-ksqldb-cli:6.0.1 confluentinc/ksqldb-examples:6.0.1 confluentinc/cp-kafka-rest:6.0.1 For the Busy Professionals Download and unzip the Docker Compose file to a folder on your machine. Open Command Prompt and navigate to the folder by running cd [folder_name] . Execute docker-compose up -d to start the services. That's it! You will have 9 containers up and running, ready to serve. See the example image below for reference.","title":"Kafka 6 with Zookeeper on Windows Docker"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#overview","text":"Many people struggle with setting up Confluent Kafka on their local machines. While it's easier to use 'templates' on AWS or Azure, setting it up locally can be quite complicated. That\u2019s why I\u2019m sharing this method\u2014it helps you create a stable, fully-working, production-like environment on your local system.","title":"Overview"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#prerequisites","text":"Before you start, ensure that you have the following installed: - Docker Desktop : Install it from the Docker website . Docker Compose : It usually comes with Docker Desktop, so you don\u2019t need to install it separately. - Run the command docker network create dasnet in CMD . Note: I use a network so that all my containers are part of the same network. This is just an extra step. It\u2019s necessary because the Docker Compose file has the dasnet network mentioned everywhere. If you don\u2019t want to use this network, then just remove all occurrences of it from the Docker Compose file.","title":"Prerequisites"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#steps-to-create-the-setup","text":"The steps are simple. Just follow these two steps and your setup will be up and running.","title":"Steps to create the setup"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#1-create-the-docker-composeyaml","text":"Create a docker-compose.yaml file with the content below inside any folder. Alternatively download the file from the link given at the start of the article. I have verbosely commented the file. Hope you will find it useful. # Docker Compose file for setting up a Confluent Platform environment with various services. # Author: DDas version: '2' # Specifies the Docker Compose version. Version 2 is stable and compatible with most setups. services: # This section defines all the services (containers) that will be created as part of this setup. zookeeper: # Zookeeper service - essential for managing and coordinating Kafka brokers. image: confluentinc/cp-zookeeper:6.0.1 # Zookeeper version 6.0.1 from Confluent. hostname: zookeeper # Internal hostname used within the Docker network. container_name: zookeeper # Name of the container as it will appear in Docker. ports: - \"2181:2181\" # laptop:container environment: # Environment variables that configure Zookeeper. ZOOKEEPER_CLIENT_PORT: 2181 # Port where Zookeeper listens for client connections. ZOOKEEPER_TICK_TIME: 2000 # Basic time unit in milliseconds used by Zookeeper. ZOOKEEPER_SERVER_ID: 1 ZOOKEEPER_INIT_LIMIT: 5 ZOOKEEPER_SYNC_LIMIT: 2 ZOOKEEPER_CLIENT_CNXNS: 60 networks: - dasnet # Connecting Zookeeper to the custom network 'dasnet'. broker: # Kafka broker service - handles message publishing and subscribing. image: confluentinc/cp-server:6.0.1 # Kafka broker version 6.0.1 from Confluent. hostname: broker # Internal hostname used within the Docker network. container_name: broker # Name of the container as it will appear in Docker. depends_on: # Ensures Zookeeper is started before the broker. - zookeeper ports: - \"9192:9092\" # laptop:container - \"9111:9101\" # laptop:container environment: # Environment variables that configure the Kafka broker. KAFKA_BROKER_ID: 1 # Unique identifier for the broker. KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181' # Address of the Zookeeper service. KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT # Protocol mapping. KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092 # Advertised listeners. KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter # Metrics reporter. KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 # Replication factor for the offsets topic. KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0 # Delay in rebalancing consumer groups. KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1 # Replication factor for the license topic. KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1 # Replication factor for the balancer topic. KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1 # Minimum in-sync replicas for transaction state log. KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1 # Replication factor for the transaction state log. KAFKA_JMX_PORT: 9101 # Port for JMX monitoring. KAFKA_JMX_HOSTNAME: localhost # Hostname for JMX monitoring. KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:8081 # Schema Registry URL. CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:29092 # Bootstrap servers for metrics reporter. CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1 # Replication factor for metrics topics. CONFLUENT_METRICS_ENABLE: 'true' # Enable Confluent metrics. CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous' # Customer ID for support. KAFKA_SASL_JAAS_CONFIG: \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"admin\\\" password=\\\"Passw0rd\\\";\" networks: - dasnet # Connecting the Kafka broker to the custom network 'dasnet'. schema-registry: # Schema Registry service - manages Avro schemas for Kafka topics. image: confluentinc/cp-schema-registry:6.0.1 # Schema Registry version 6.0.1 from Confluent. hostname: schema-registry # Internal hostname used within the Docker network. container_name: schema-registry # Name of the container as it will appear in Docker. depends_on: # Ensures the broker is started before the Schema Registry. - broker ports: - \"8181:8081\" # laptop:container environment: # Environment variables that configure the Schema Registry. SCHEMA_REGISTRY_HOST_NAME: schema-registry # Internal hostname for Schema Registry. SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker:29092' # Kafka broker address for Schema Registry. SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081 # Listeners for Schema Registry. SCHEMA_REGISTRY_AUTH: \"admin:Passw0rd\" networks: - dasnet # Connecting the Schema Registry to the custom network 'dasnet'. connect: # Kafka Connect service - facilitates integration with external systems. image: confluentinc/cp-kafka-connect-base:6.0.1 # Kafka Connect version 6.0.1 from Confluent. hostname: connect # Internal hostname used within the Docker network. container_name: kafka-connect # Name of the container as it will appear in Docker. depends_on: # Ensures the broker and Schema Registry are started before Kafka Connect. - broker - schema-registry ports: - \"8183:8083\" # laptop:container environment: # Environment variables that configure Kafka Connect. CONNECT_BOOTSTRAP_SERVERS: 'broker:29092' # Kafka broker address for Kafka Connect. CONNECT_REST_ADVERTISED_HOST_NAME: connect # Hostname for the Kafka Connect REST API. CONNECT_REST_PORT: 8083 # Port for the Kafka Connect REST API. CONNECT_GROUP_ID: kafka-connect # Group ID for Kafka Connect. CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs # Topic for storing connector configs. CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1 # Replication factor for the config storage topic. CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000 # Interval for flushing offsets. CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets # Topic for storing connector offsets. CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1 # Replication factor for the offset storage topic. CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status # Topic for storing connector statuses. CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1 # Replication factor for the status storage topic. CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter # Converter for Kafka Connect keys. CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter # Converter for Kafka Connect values. CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081 # Schema Registry URL. CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-6.0.1.jar # Additional classpath for interceptors. CONNECT_PRODUCER_INTERCEPTOR_CLASSES: \"io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor\" # Producer interceptor. CONNECT_CONSUMER_INTERCEPTOR_CLASSES: \"io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor\" # Consumer interceptor. CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR # Logging configuration. CONNECT_PLUGIN_PATH: \"/usr/share/java,/usr/share/confluent-hub-components,/data/connect-jars\" # Plugin paths. CONNECT_SASL_JAAS_CONFIG: \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"admin\\\" password=\\\"Passw0rd\\\";\" volumes: # Mounting a volume for persistent storage. - ${PWD}/data:/data networks: - dasnet # Connecting Kafka Connect to the custom network 'dasnet'. control-center: # Confluent Control Center service - web interface for managing and monitoring Kafka. image: confluentinc/cp-enterprise-control-center:6.0.1 # Control Center version 6.0.1 from Confluent. hostname: control-center # Internal hostname used within the Docker network. container_name: control-center # Name of the container as it will appear in Docker. depends_on: # Ensures the broker, Schema Registry, and Kafka Connect are started before Control Center. - broker - schema-registry - connect - ksqldb-server ports: - \"9121:9021\" # laptop:container environment: # Environment variables that configure the Control Center. CONTROL_CENTER_BOOTSTRAP_SERVERS: 'broker:29092' # Kafka broker address for Control Center. CONTROL_CENTER_CONNECT_CLUSTER: 'connect:8083' # Kafka Connect cluster address for Control Center. CONTROL_CENTER_KSQL_KSQLDB1_URL: \"http://ksqldb-server:8088\" # KSQL server URL for Control Center. CONTROL_CENTER_KSQL_KSQLDB1_ADVERTISED_URL: \"http://localhost:8088\" # Advertised KSQL server URL. CONTROL_CENTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\" # Schema Registry URL for Control Center. CONTROL_CENTER_REPLICATION_FACTOR: 1 # Replication factor for Control Center's internal topics. CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1 # Number of partitions for Control Center's internal topics. CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1 # Partitions for monitoring interceptor topic. CONFLUENT_METRICS_TOPIC_REPLICATION: 1 # Replication factor for metrics topics. PORT: 9021 # Internal port for Control Center. networks: - dasnet # Connecting Control Center to the custom network 'dasnet'. ksqldb-server: # KSQL server service - SQL engine for processing real-time streams. image: confluentinc/cp-ksqldb-server:6.0.1 # KSQL server version 6.0.1 from Confluent. hostname: ksqldb-server # Internal hostname used within the Docker network. container_name: ksqldb-server # Name of the container as it will appear in Docker. depends_on: # Ensures the broker and Kafka Connect are started before KSQL server. - broker - connect ports: - \"8188:8088\" # laptop:container environment: # Environment variables that configure KSQL server. KSQL_CONFIG_DIR: \"/etc/ksql\" # Directory for KSQL configuration files. KSQL_BOOTSTRAP_SERVERS: \"broker:29092\" # Kafka broker address for KSQL server. KSQL_HOST_NAME: ksqldb-server # Internal hostname for KSQL server. KSQL_LISTENERS: \"http://0.0.0.0:8088\" # Listeners for KSQL server. KSQL_CACHE_MAX_BYTES_BUFFERING: 0 # Disables caching in KSQL. KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\" # Schema Registry URL for KSQL server. KSQL_PRODUCER_INTERCEPTOR_CLASSES: \"io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor\" # Producer interceptor. KSQL_CONSUMER_INTERCEPTOR_CLASSES: \"io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor\" # Consumer interceptor. KSQL_KSQL_CONNECT_URL: \"http://connect:8083\" # Kafka Connect URL for KSQL server. KSQL_KSQL_LOGGING_PROCESSING_TOPIC_REPLICATION_FACTOR: 1 # Replication factor for logging processing topics. KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: 'true' # Auto-create logging processing topics. KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: 'true' # Auto-create logging processing streams. networks: - dasnet # Connecting KSQL server to the custom network 'dasnet'. ksqldb-cli: # KSQL CLI service - command-line interface for interacting with KSQL server. image: confluentinc/cp-ksqldb-cli:6.0.1 # KSQL CLI version 6.0.1 from Confluent. container_name: ksqldb-cli # Name of the container as it will appear in Docker. depends_on: # Ensures the broker, Kafka Connect, and KSQL server are started before KSQL CLI. - broker - connect - ksqldb-server entrypoint: /bin/sh # Entry point for the container to start a shell. tty: true # Allocate a pseudo-TTY for interactive use. networks: - dasnet # Connecting KSQL CLI to the custom network 'dasnet'. ksql-datagen: # KSQL DataGen service - generates sample data for KSQL. image: confluentinc/ksqldb-examples:6.0.1 # KSQL DataGen version 6.0.1 from Confluent. hostname: ksql-datagen # Internal hostname used within the Docker network. container_name: ksql-datagen # Name of the container as it will appear in Docker. depends_on: # Ensures KSQL server, broker, Schema Registry, and Kafka Connect are started before KSQL DataGen. - ksqldb-server - broker - schema-registry - connect command: \"bash -c 'echo Waiting for Kafka to be ready... && \\ cub kafka-ready -b broker:29092 1 40 && \\ echo Waiting for Confluent Schema Registry to be ready... && \\ cub sr-ready schema-registry 8081 40 && \\ echo Waiting a few seconds for topic creation to finish... && \\ sleep 11 && \\ tail -f /dev/null'\" # Command to ensure services are ready before starting data generation. environment: # Environment variables that configure KSQL DataGen. KSQL_CONFIG_DIR: \"/etc/ksql\" # Directory for KSQL configuration files. STREAMS_BOOTSTRAP_SERVERS: broker:29092 # Kafka broker address for KSQL DataGen. STREAMS_SCHEMA_REGISTRY_HOST: schema-registry # Schema Registry host for KSQL DataGen. STREAMS_SCHEMA_REGISTRY_PORT: 8081 # Schema Registry port for KSQL DataGen. networks: - dasnet # Connecting KSQL DataGen to the custom network 'dasnet'. rest-proxy: # Kafka REST Proxy service - provides a RESTful interface to Kafka. image: confluentinc/cp-kafka-rest:6.0.1 # Kafka REST Proxy version 6.0.1 from Confluent. hostname: rest-proxy # Internal hostname used within the Docker network. container_name: rest-proxy # Name of the container as it will appear in Docker. depends_on: # Ensures the broker and Schema Registry are started before REST Proxy. - broker - schema-registry ports: - \"8182:8082\" # laptop:container environment: # Environment variables that configure the REST Proxy. KAFKA_REST_HOST_NAME: rest-proxy # Internal hostname for REST Proxy. KAFKA_REST_BOOTSTRAP_SERVERS: 'broker:29092' # Kafka broker address for REST Proxy. KAFKA_REST_LISTENERS: \"http://0.0.0.0:8082\" # Listeners for REST Proxy. KAFKA_REST_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081' # Schema Registry URL for REST Proxy. networks: - dasnet # Connecting REST Proxy to the custom network 'dasnet'. networks: # Network configuration for Docker containers. dasnet: # Defines the custom network 'dasnet'. external: true # Indicates that 'dasnet' is a pre-existing external network.","title":"1. Create the docker-compose.yaml"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#2-start-the-services","text":"CD to docker-compose.yml folder. Run the command docker-compose up -d","title":"2. Start the Services"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#3-check-the-setup","text":"Open Docker Desktop. You should see all the containres up and running Open the control centre to see the actual working. Go through the tabs to see all the functionalities","title":"3. Check the setup"},{"location":"DockerAndKubernetes/Kafka/2_Confluent%20Kafka/#component-details","text":"Refer to the table for quick info on the the setup: Component Purpose Host Name Ports (Laptop:Container) Key Environment Variables Network Zookeeper Coordinates and manages Kafka brokers. zookeeper 2181:2181 ZOOKEEPER_CLIENT_PORT=2181 ZOOKEEPER_TICK_TIME=2000 dasnet Kafka Broker Handles message publishing and subscribing. broker 9192:9092 9111:9101 KAFKA_BROKER_ID=1 KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://broker:29092 dasnet Schema Registry Manages Avro schemas for Kafka topics. schema-registry 8181:8081 SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS=broker:29092 SCHEMA_REGISTRY_LISTENERS=http://0.0.0.0:8081 dasnet Kafka Connect Integrates Kafka with external systems (e.g., databases). connect 8183:8083 CONNECT_BOOTSTRAP_SERVERS=broker:29092 CONNECT_REST_PORT=8083 CONNECT_GROUP_ID=kafka-connect dasnet Control Center Web interface for managing and monitoring Kafka. control-center 9121:9021 CONTROL_CENTER_BOOTSTRAP_SERVERS=broker:29092 CONTROL_CENTER_CONNECT_CLUSTER=connect:8083 dasnet KSQL Server SQL engine for processing real-time data streams. ksqldb-server 8188:8088 KSQL_BOOTSTRAP_SERVERS=broker:29092 KSQL_LISTENERS=http://0.0.0.0:8088 KSQL_KSQL_SCHEMA_REGISTRY_URL=http://schema-registry:8081 dasnet REST Proxy Provides a RESTful interface to Kafka. rest-proxy 8182:8082 KAFKA_REST_BOOTSTRAP_SERVERS=broker:29092 KAFKA_REST_LISTENERS=http://0.0.0.0:8082 dasnet KSQL DataGen Generates sample data for KSQL testing (optional). ksql-datagen N/A STREAMS_BOOTSTRAP_SERVERS=broker:29092 STREAMS_SCHEMA_REGISTRY_HOST=schema-registry dasnet All components are connected via the dasnet custom Docker network.","title":"Component Details"},{"location":"DockerAndKubernetes/Mongodb/3_DockerMongodb/","text":"Create a MongoDB Container on Docker I will show you how to create a MongoDB container easily. We won't use Dockerfile, Docker-compose, or any files. Instead, we'll set up everything using just the command line. We'll use Docker volumes to store the data so it stays even after the container is closed. Create the network and docker volume docker network create spark-network docker volume create mongo_data Now run the main command docker run -d --name mongoDB --network spark-network -p 27017:27017 -v mongo-data:/data/db mongo -d : Run the container in detached mode. --name mongo : Assign the name \"mongo\" to the container. --network spark-network : Connect the container to the netowrk you created, can be any network \"spark-network\". -p 27017:27017 : Map port 27017 on the host to port 27017 in the container. -v mongo-data:/data/db : Use a Docker volume named \"mongo-data\" to persist MongoDB data. Connect to MongoDB You can now connect to MongoDB using a MongoDB client or management tool like MongoDB Compass, Robo 3T, or Studio 3T. Here is an example: mongodb://localhost:27017 Summary of the setup and steps Step by Step Details Pull MongoDB Image First, pull the latest MongoDB image from Docker Hub using this command: docker pull mongo Run MongoDB Container Run a MongoDB container with this command. It names the container, exposes the port, and sets up a volume for data: docker run -d --name mongoDB --network cms-network -p 27017:27017 -v mongo-data:/data/db mongo Access MongoDB Shell After starting the MongoDB container, access the MongoDB shell with this command: docker exec -it my-mongo mongo Connect to MongoDB from an App Connect to your MongoDB container from any app using localhost as the hostname and 27017 as the port. Add a Docker Volume to MongoDB To persist data, create a Docker volume and attach it to your MongoDB container with these commands: docker volume create mongo-data docker run --name my-mongo -d -p 27017:27017 -v mongo-data:/data/db mongo This creates a volume named mongo-data and starts a MongoDB container using this volume for data storage. Mount Host Directory in MongoDB Container You can also mount a directory from your host system into the container to store data. This is useful for backups, restores, or direct access to MongoDB files: docker run --name my-mongo -d -p 27017:27017 -v /path/on/host:/data/db mongo Replace /path/on/host with the directory path on your host machine where you want to store MongoDB data. This links the /data/db directory in the container to the specified path on your host. Custom Configuration For custom MongoDB settings, you can mount a configuration file from the host into the container when it runs. Secure MongoDB For production, secure MongoDB with authentication. Start by running the container with the --auth flag to enable security: docker run --name my-mongo -d -p 27017:27017 -v ~/mongo-data:/data/db mongo --auth Then, create an admin user inside the MongoDB shell.","title":"MongoDB"},{"location":"DockerAndKubernetes/Mongodb/3_DockerMongodb/#create-a-mongodb-container-on-docker","text":"I will show you how to create a MongoDB container easily. We won't use Dockerfile, Docker-compose, or any files. Instead, we'll set up everything using just the command line. We'll use Docker volumes to store the data so it stays even after the container is closed.","title":"Create a MongoDB Container on Docker"},{"location":"DockerAndKubernetes/Mongodb/3_DockerMongodb/#create-the-network-and-docker-volume","text":"docker network create spark-network docker volume create mongo_data","title":"Create the network and docker volume"},{"location":"DockerAndKubernetes/Mongodb/3_DockerMongodb/#now-run-the-main-command","text":"docker run -d --name mongoDB --network spark-network -p 27017:27017 -v mongo-data:/data/db mongo -d : Run the container in detached mode. --name mongo : Assign the name \"mongo\" to the container. --network spark-network : Connect the container to the netowrk you created, can be any network \"spark-network\". -p 27017:27017 : Map port 27017 on the host to port 27017 in the container. -v mongo-data:/data/db : Use a Docker volume named \"mongo-data\" to persist MongoDB data.","title":"Now run the main command"},{"location":"DockerAndKubernetes/Mongodb/3_DockerMongodb/#connect-to-mongodb","text":"You can now connect to MongoDB using a MongoDB client or management tool like MongoDB Compass, Robo 3T, or Studio 3T. Here is an example: mongodb://localhost:27017","title":"Connect to MongoDB"},{"location":"DockerAndKubernetes/Mongodb/3_DockerMongodb/#summary-of-the-setup-and-steps","text":"Step by Step Details Pull MongoDB Image First, pull the latest MongoDB image from Docker Hub using this command: docker pull mongo Run MongoDB Container Run a MongoDB container with this command. It names the container, exposes the port, and sets up a volume for data: docker run -d --name mongoDB --network cms-network -p 27017:27017 -v mongo-data:/data/db mongo Access MongoDB Shell After starting the MongoDB container, access the MongoDB shell with this command: docker exec -it my-mongo mongo Connect to MongoDB from an App Connect to your MongoDB container from any app using localhost as the hostname and 27017 as the port. Add a Docker Volume to MongoDB To persist data, create a Docker volume and attach it to your MongoDB container with these commands: docker volume create mongo-data docker run --name my-mongo -d -p 27017:27017 -v mongo-data:/data/db mongo This creates a volume named mongo-data and starts a MongoDB container using this volume for data storage. Mount Host Directory in MongoDB Container You can also mount a directory from your host system into the container to store data. This is useful for backups, restores, or direct access to MongoDB files: docker run --name my-mongo -d -p 27017:27017 -v /path/on/host:/data/db mongo Replace /path/on/host with the directory path on your host machine where you want to store MongoDB data. This links the /data/db directory in the container to the specified path on your host. Custom Configuration For custom MongoDB settings, you can mount a configuration file from the host into the container when it runs. Secure MongoDB For production, secure MongoDB with authentication. Start by running the container with the --auth flag to enable security: docker run --name my-mongo -d -p 27017:27017 -v ~/mongo-data:/data/db mongo --auth Then, create an admin user inside the MongoDB shell.","title":"Summary of the setup and steps"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Ubuntu, Python, OpenJDK & PySpark In this article I will show you how to create Docker containers with Pyspark and Spark components. For Busy People Save the Dockerfile content as Dockerfile (no extension). cd to the folder containtng the Dockerfile Run Commands : sh docker build -t ubuntu-pyspark . docker run -it --name Ubuntu-PySpark --network dasnet ubuntu-pyspark That\u2019s it! Steps to Create the Image and Container In this article I will show you how to create a single container with Ubuntu OS, Python and PySpark. We will use just a dockerfile to create it. Follow the steps below to create the container. Create the Dockerfile In a folder create a file Dockerfile (No extension) with the content below. Dockerfile # Use Ubuntu 20.04 as the base image to avoid \"externally-managed-environment\" restrictions FROM ubuntu:20.04 # Set environment variable to avoid interactive prompts during package installation ENV DEBIAN_FRONTEND=noninteractive # Update the package list to ensure we have the latest information about available packages RUN apt-get update # Install necessary packages including curl, sudo, and nano RUN apt-get install -y curl sudo nano software-properties-common # Add the 'deadsnakes' PPA (Personal Package Archive) to access newer Python versions RUN add-apt-repository ppa:deadsnakes/ppa # Add the OpenJDK PPA to get the latest JDK versions RUN add-apt-repository ppa:openjdk-r/ppa # Update the package list again to include the new PPAs RUN apt-get update # Install Python 3.12, pip, and OpenJDK 17 RUN apt-get install -y python3.12 python3-pip openjdk-17-jdk-headless # Install the PySpark library using pip RUN pip3 install pyspark # Clean up the package lists to reduce the image size RUN apt-get clean && rm -rf /var/lib/apt/lists/* # Create a root user and set its password RUN echo 'root:Passw0rd' | chpasswd # Create a new user 'dwdas', set a password, and add this user to the sudo group RUN useradd -ms /bin/bash dwdas && echo 'dwdas:Passw0rd' | chpasswd && adduser dwdas sudo # Allow the 'dwdas' user to run sudo commands without a password RUN echo 'dwdas ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers # Set the working directory to the home directory of the new user WORKDIR /home/dwdas # Switch to the new user 'dwdas' USER dwdas # Expose port 8888, commonly used for Jupyter Notebook, if needed EXPOSE 8888 # Set the default command to start a bash shell CMD [\"bash\"] Build the Image Open CMD, navigate to the folder with the Dockerfile, and run: docker build -t ubuntu-pyspark-img . After successfully running the command, you will see an image in your Docker Desktop app: Run the Docker Container In command prompt, run: docker run -it --name Debian-PySpark --network dasnet debian-pyspark This will create a container with the image we created earlier and start it. You can see it from the Container section of your Docker window. Details of the container Here are the details of the installed components. The table will be a handy reference to know which components are installed and important locations, variables etc. Component Details Base Image ubuntu:20.04 Python Version Python 3.12, installed via the deadsnakes PPA Java Version OpenJDK 17 (Headless), installed via the openjdk-r PPA PySpark Version Latest version of PySpark installed via pip Home Directory for User /home/dwdas Spark Home /opt/bitnami/spark Java Home /opt/bitnami/java Python Path /opt/bitnami/spark/python/ (for PySpark integration) Spark Configuration Directory /opt/bitnami/spark/conf Spark Worker Directory /opt/bitnami/spark/work Environment Variables DEBIAN_FRONTEND=noninteractive to avoid interactive prompts during installation User Created dwdas with sudo privileges and passwordless sudo access Exposed Port Port 8888 , commonly used for Jupyter Notebooks Default Command bash shell set as the default command Network Configuration Connected to the dasnet network Spark Ports Spark Master: 7077 (mapped to host port 17077 ), Spark Master UI: 8080 (mapped to host port 16080 ), Spark Worker UI: 8081 (mapped to host port 16002 ), 8082 (mapped to host port 16004 ) Error: Package Not Found (404 Not Found) When building the Docker image, I got a 404 Not Found error because some packages like python3.12 and openjdk-17-jdk-headless couldn't be found. This usually happens if the package lists are outdated or there's an issue with the repositories. Here's how to fix it: Update Package Lists : Run apt-get update first to make sure your package lists are current. Add Correct PPAs : Update the Dockerfile to include these PPAs: deadsnakes for Python. openjdk-r for OpenJDK. Use --fix-missing Option : If the problem continues, try apt-get install --fix-missing to fix missing packages. Install Specific Versions : If the latest version isn't available, try installing a slightly older but stable version. Debian, Downloaded Python, Pyspark - no venv. This Section shows you how to create a Docker container with the latest Debian, Python 3.11, OpenJDK 17, and PySpark. We\u2019ll set up a root user and a named user with essential environment variables. Note: If you install python using apt-get install in new Debain it will ask you to install in venv mode. We want to avoid this. Hence we download it(weget) then intstall it manually. We\u2019ll use a Dockerfile and docker-compose.yml for the setup. Steps to Create the Image and Container Create a Dockerfile: Create a Dockerfile.txt with the contents below and remove the .txt extension Dockerfile # Use Debian as the base image FROM debian:latest # Set environment variable to avoid interactive prompts during package installation ENV DEBIAN_FRONTEND=noninteractive # Update the package lists and install essential packages RUN apt-get update && apt-get install -y --no-install-recommends \\ curl \\ wget \\ tar \\ bash \\ ca-certificates \\ sudo \\ build-essential \\ libssl-dev \\ zlib1g-dev \\ libbz2-dev \\ libreadline-dev \\ libsqlite3-dev \\ libffi-dev # Copy the Python source tarball into the image COPY Python-3.11.9.tgz /tmp/ # Extract, build, and install Python 3.11.9 RUN cd /tmp && \\ tar -xvf Python-3.11.9.tgz && \\ cd Python-3.11.9 && \\ ./configure --enable-optimizations && \\ make -j 8 && \\ make altinstall && \\ cd .. && \\ rm -rf Python-3.11.9 Python-3.11.9.tgz # Create symbolic links for python, python3, pip, and pip3 RUN ln -s /usr/local/bin/python3.11 /usr/bin/python && \\ ln -s /usr/local/bin/python3.11 /usr/bin/python3 && \\ ln -s /usr/local/bin/pip3.11 /usr/bin/pip && \\ ln -s /usr/local/bin/pip3.11 /usr/bin/pip3 # Install OpenJDK 17 RUN apt-get install -y openjdk-17-jdk-headless # Install the PySpark library using pip RUN python3.11 -m pip install pyspark # Set environment variables ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64 ENV PYTHONPATH=/usr/local/lib/python3.11/dist-packages ENV PYSPARK_PYTHON=/usr/local/bin/python3.11 ENV PATH=$PATH:$JAVA_HOME/bin # Clean up the package lists to reduce the image size RUN apt-get clean && rm -rf /var/lib/apt/lists/* # Create a root user and set its password RUN echo 'root:Passw0rd' | chpasswd # Create a new user 'dwdas', set a password, and add this user to the sudo group RUN useradd -ms /bin/bash dwdas && echo 'dwdas:Passw0rd' | chpasswd && adduser dwdas sudo # Allow the 'dwdas' user to run sudo commands without a password RUN echo 'dwdas ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers # Set the working directory to the home directory of the new user WORKDIR /home/dwdas # Switch to the new user 'dwdas' USER dwdas # Expose port 8888, commonly used for Jupyter Notebook, if needed EXPOSE 8888 # Set the default command to start a bash shell CMD [\"bash\"] ``` </details> ### <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #485BDA;\">Download Python and place in the same folder</span> Download Python 3.11.9 from [this site](https://www.python.org/ftp/python/3.11.9/Python-3.11.9.tgz) and place it in the same directory. ### <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #485BDA;\">Build the Docker Image:</span> - Open a terminal and navigate to the directory containing the Dockerfile. - Run the following command to build the Docker image: ```bash docker build -t my-debian-pyspark . ``` ### <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #485BDA;\">Run the Docker Container:</span> - Once the image is built, run the container using the command: ```bash docker run -it --name my-debian-pyspark-container my-debian-pyspark ``` ## <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #485BDA;\">Details of the Container</span> | **Category** | **Details** | |--------------------------|-------------------------------------------------------------------------------------------------| | **Base Image** | Debian (latest) | | **Python Version** | Python 3.11.9 | | **Java Version** | OpenJDK 17 | | **PySpark Version** | Latest via pip | | **Environment Variables** | `JAVA_HOME`: `/usr/lib/jvm/java-17-openjdk-amd64`, `PYTHONPATH`: `/usr/local/lib/python3.11/dist-packages`, `PYSPARK_PYTHON`: `/usr/local/bin/python3.11`, `PATH`: `$PATH:$JAVA_HOME/bin` | | **Installed Packages** | Build tools (curl, wget, tar, etc.), Python 3.11.9 (source), OpenJDK 17, PySpark (pip) | | **User Configuration** | Root user & `dwdas` (password: `Passw0rd`, sudo access) | | **Exposed Port** | 8888 (for Jupyter) | | **Default Command** | Bash shell start | # <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #006600;\">**Debian, Pip Python, Pip Pyspark - venv.**</span> This section shows you how to create a Docker container with the latest Debian, Python 3.11, OpenJDK 17, and PySpark using the recommended venv approach. We\u2019ll set up a root user and a named user with essential environment variables. Note: Newer Debian versions enforce using venv for pip install. We will install Python using apt-get and set up venv from the command line. We\u2019ll use a Dockerfile and docker-compose.yml for the setup. ## <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #006600;\">Steps to Create the Container</span> ### <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #006600;\">Create a Dockerfile:</span> - Create a Dockerfile with the following content: <details open markdown=\"block\"> <summary> Dockerfile </summary> ```dockerfile # Use the latest version of Debian as the base image FROM debian:latest # Set environment variable to avoid interactive prompts during package installation ENV DEBIAN_FRONTEND=noninteractive # Update the package lists and install essential packages RUN apt-get update && \\ apt-get install -y curl wget tar bash ca-certificates sudo gnupg # Install Python 3.11, venv, pip, and OpenJDK 17 RUN apt-get install -y python3.11 python3.11-venv python3.11-dev python3-pip openjdk-17-jdk-headless # Create a virtual environment RUN python3.11 -m venv /opt/venv # Activate the virtual environment and install PySpark RUN /opt/venv/bin/python -m pip install pyspark # Set environment variables ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64 ENV PYTHONPATH=/opt/venv/lib/python3.11/site-packages ENV PYSPARK_PYTHON=/opt/venv/bin/python ENV PATH=$PATH:$JAVA_HOME/bin:/opt/venv/bin # Clean up the package lists to reduce the image size RUN apt-get clean && rm -rf /var/lib/apt/lists/* # Create a root user and set its password RUN echo 'root:Passw0rd' | chpasswd # Create a new user 'dwdas', set a password, and add this user to the sudo group RUN useradd -ms /bin/bash dwdas && echo 'dwdas:Passw0rd' | chpasswd && adduser dwdas sudo # Allow the 'dwdas' user to run sudo commands without a password RUN echo 'dwdas ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers # Set the working directory to the home directory of the new user WORKDIR /home/dwdas # Switch to the new user 'dwdas' USER dwdas # Expose port 8888, commonly used for Jupyter Notebook, if needed EXPOSE 8888 # Set the default command to start a bash shell CMD [\"bash\"] ``` </details> ### <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #006600;\">Build the Docker Image:</span> - Open a terminal and navigate to the directory containing the Dockerfile. - Run the following command to build the Docker image: ```bash docker build -t my-debian-pyspark-venv . ``` ### <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #006600;\">Run the Docker Container:</span> - Once the image is built, run the container using the command: ```bash docker run -it --name my-debian-pyspark-venv-container my-debian-pyspark-venv ``` ## <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #006600;\">Details of the Container</span> | **Category** | **Details** | |--------------------------|--------------------------------------------------------------------------------------------------------------------------| | **Base Image** | Debian (latest) | | **Python Version** | Python 3.11.9 | | **Java Version** | OpenJDK 17 | | **PySpark Version** | Installed via pip in a virtual environment | | **Virtual Environment** | Created with `python3.11 -m venv /opt/venv` | | **Environment Variables** | `JAVA_HOME`: `/usr/lib/jvm/java-17-openjdk-amd64`, `PYTHONPATH`: `/opt/venv/lib/python3.11/site-packages`, `PYSPARK_PYTHON`: `/opt/venv/bin/python`, `PATH`: `$PATH:$JAVA_HOME/bin:/opt/venv/bin` | | **Installed Packages** | Essential tools (curl, wget, tar, bash, etc.), Python 3.11, venv, pip, OpenJDK 17, PySpark (in venv) | | **User Configuration** | Root user & `dwdas` (password: `Passw0rd`, sudo access) | | **Exposed Port** | 8888 (for Jupyter) | | **Default Command** | Bash shell start | To modify the setup so that the `conf` directory in the Spark container is mapped to a local folder on your machine (and the folder is auto-created), we need to update the `docker run` command to include a volume mapping. # <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #C22E2E;\">**Single-Node Bitnami Spark With Master and Worker**</span> Here, we will use the official Bitnami Spark Docker image to set up a single-node Spark environment. This setup will include both the Master and Worker. ## <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #C22E2E;\">Steps to create the container</span> You can either download, unzip, and run the `.bat` file from [this link](Dockerfiles/Bitnami_Spark_SingleNode_GOLD.zip) to create the entire container. Or, you can follow the steps manually. Both methods will give the same result. ### <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #C22E2E;\">Create a Custom Dockerfile</span> In your folder create a file Dockerfile(no extension) with the following content: <details open markdown=\"block\"> <summary> Dockerfile </summary> ```dockerfile # Use the official Bitnami Spark image as the base. I always pull a constant image and not :latest. FROM bitnami/spark:3.5.2-debian-12-r2 # Step 1: Switch to root user to install software # We need to be root to install utilities and set up sudo permissions. USER root # Step 2: Update the package list and install utilities. py4j and ipykernel is for VS studio connection. # Install common utilities like sudo, ping, and nano. # Update the base system RUN apt-get update && \\ apt-get install -y sudo nano iputils-ping grep curl wget vim net-tools procps lsof telnet && \\ apt-get clean # Install pip (if not already installed) RUN apt-get install -y python3-pip # Install py4j and ipykernel using pip. Required VS Code connection. RUN pip3 install py4j ipykernel # Step 3: Set the root user password to 'Passw0rd' # This sets the root password to 'Passw0rd' for future access. RUN echo \"root:Passw0rd\" | chpasswd # Step 4: Give sudo privileges to the 'spark' user # Here, we are allowing the 'spark' user to run commands as sudo without a password. RUN echo \"spark ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers # After finishing the setup, we dont switch back to any user. The bitnami original Dockerfile switches to user 1001 and the directory is /opt/bitnami/spark # Step 6: Expose necessary ports for Spark Web UI and communication # 4040: Spark Worker Web UI # 7077: Spark Master communication # 8080: Spark Master Web UI EXPOSE 4040 7077 8080 # End of the Dockerfile Create a Local conf Directory Before running the container, create a folder named local-spark-conf in the same folder where your Dockerfiles are. This folder will store the configuration files and will be mapped to the conf directory inside the container. mkdir local-spark-conf Build the Docker Image Once the Dockerfile is ready, you can build the Docker image with the following command: docker build -t bitnami-spark-single-node . This command will create a Docker image called bitnami-spark-single-node using the Dockerfile you just created. Run the Container and Map the conf Directory Now, we run the Spark container with the conf directory mapped to the local folder you created earlier. If the folder doesn\u2019t exist, Docker will create it. docker run -d --network dasnet --name bitnami-spark-single-node -p 4040:4040 -p 8080:8080 -p 7077:7077 -v ./local-spark-conf:/opt/spark/conf bitnami-spark-single-node Configuration Reference Here is a list of some components in this environment. The list is compiled from the official bitnami spark github page . Environment Details Component Value OS debian 12: bitnami/minideb:bookworm Python python-3.12.5-1: /opt/bitnami/python/bin/python PYTHONPATH /opt/bitnami/spark/python/ Java java-17.0.12-10-1: JAVA_HOME = /opt/bitnami/java JAVA_HOME /opt/bitnami/java SPARK_HOME /opt/bitnami/spark SPARK_USER spark SPARK JARS Location for Installing External Jars /opt/bitnami/spark/jars Workdir /opt/bitnami/spark User 1001 Entrypoint /opt/bitnami/scripts/spark/entrypoint.sh Command /opt/bitnami/scripts/spark/run.sh Certificates /opt/bitnami/spark/conf/certs SPARK_SSL_KEYSTORE_FILE /opt/bitnami/spark/conf/certs/spark-keystore.jks SPARK_MODE master SPARK_MASTER_URL spark://spark-master:7077 SPARK_SSL_ENABLED no Docker Logs Command docker logs bitnami-spark-single-node Note: The Dockerfile also install py4j and ipykernel. These are reuqired for VS code to container using remote containers extension. Read-only Environment Variables ( Official Bitnami github ) Name Description Value SPARK_BASE_DIR Spark installation directory. ${BITNAMI_ROOT_DIR}/spark SPARK_CONF_DIR Spark configuration directory. ${SPARK_BASE_DIR}/conf SPARK_DEFAULT_CONF_DIR Spark default configuration directory. ${SPARK_BASE_DIR}/conf.default SPARK_WORK_DIR Spark workspace directory. ${SPARK_BASE_DIR}/work SPARK_CONF_FILE Spark configuration file path. ${SPARK_CONF_DIR}/spark-defaults.conf SPARK_LOG_DIR Spark logs directory. ${SPARK_BASE_DIR}/logs SPARK_TMP_DIR Spark tmp directory. ${SPARK_BASE_DIR}/tmp SPARK_JARS_DIR Spark jar directory. ${SPARK_BASE_DIR}/jars SPARK_INITSCRIPTS_DIR Spark init scripts directory. /docker-entrypoint-initdb.d SPARK_USER Spark user. spark SPARK_DAEMON_USER Spark system user. spark SPARK_DAEMON_GROUP Spark system group. spark","title":"PySpark"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/#ubuntu-python-openjdk-pyspark","text":"In this article I will show you how to create Docker containers with Pyspark and Spark components.","title":"Ubuntu, Python, OpenJDK &amp; PySpark"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/#for-busy-people","text":"Save the Dockerfile content as Dockerfile (no extension). cd to the folder containtng the Dockerfile Run Commands : sh docker build -t ubuntu-pyspark . docker run -it --name Ubuntu-PySpark --network dasnet ubuntu-pyspark That\u2019s it!","title":"For Busy People"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/#steps-to-create-the-image-and-container","text":"In this article I will show you how to create a single container with Ubuntu OS, Python and PySpark. We will use just a dockerfile to create it. Follow the steps below to create the container.","title":"Steps to Create the Image and Container"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/#create-the-dockerfile","text":"In a folder create a file Dockerfile (No extension) with the content below. Dockerfile # Use Ubuntu 20.04 as the base image to avoid \"externally-managed-environment\" restrictions FROM ubuntu:20.04 # Set environment variable to avoid interactive prompts during package installation ENV DEBIAN_FRONTEND=noninteractive # Update the package list to ensure we have the latest information about available packages RUN apt-get update # Install necessary packages including curl, sudo, and nano RUN apt-get install -y curl sudo nano software-properties-common # Add the 'deadsnakes' PPA (Personal Package Archive) to access newer Python versions RUN add-apt-repository ppa:deadsnakes/ppa # Add the OpenJDK PPA to get the latest JDK versions RUN add-apt-repository ppa:openjdk-r/ppa # Update the package list again to include the new PPAs RUN apt-get update # Install Python 3.12, pip, and OpenJDK 17 RUN apt-get install -y python3.12 python3-pip openjdk-17-jdk-headless # Install the PySpark library using pip RUN pip3 install pyspark # Clean up the package lists to reduce the image size RUN apt-get clean && rm -rf /var/lib/apt/lists/* # Create a root user and set its password RUN echo 'root:Passw0rd' | chpasswd # Create a new user 'dwdas', set a password, and add this user to the sudo group RUN useradd -ms /bin/bash dwdas && echo 'dwdas:Passw0rd' | chpasswd && adduser dwdas sudo # Allow the 'dwdas' user to run sudo commands without a password RUN echo 'dwdas ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers # Set the working directory to the home directory of the new user WORKDIR /home/dwdas # Switch to the new user 'dwdas' USER dwdas # Expose port 8888, commonly used for Jupyter Notebook, if needed EXPOSE 8888 # Set the default command to start a bash shell CMD [\"bash\"]","title":"Create the Dockerfile"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/#build-the-image","text":"Open CMD, navigate to the folder with the Dockerfile, and run: docker build -t ubuntu-pyspark-img . After successfully running the command, you will see an image in your Docker Desktop app:","title":"Build the Image"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/#run-the-docker-container","text":"In command prompt, run: docker run -it --name Debian-PySpark --network dasnet debian-pyspark This will create a container with the image we created earlier and start it. You can see it from the Container section of your Docker window.","title":"Run the Docker Container"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/#details-of-the-container","text":"Here are the details of the installed components. The table will be a handy reference to know which components are installed and important locations, variables etc. Component Details Base Image ubuntu:20.04 Python Version Python 3.12, installed via the deadsnakes PPA Java Version OpenJDK 17 (Headless), installed via the openjdk-r PPA PySpark Version Latest version of PySpark installed via pip Home Directory for User /home/dwdas Spark Home /opt/bitnami/spark Java Home /opt/bitnami/java Python Path /opt/bitnami/spark/python/ (for PySpark integration) Spark Configuration Directory /opt/bitnami/spark/conf Spark Worker Directory /opt/bitnami/spark/work Environment Variables DEBIAN_FRONTEND=noninteractive to avoid interactive prompts during installation User Created dwdas with sudo privileges and passwordless sudo access Exposed Port Port 8888 , commonly used for Jupyter Notebooks Default Command bash shell set as the default command Network Configuration Connected to the dasnet network Spark Ports Spark Master: 7077 (mapped to host port 17077 ), Spark Master UI: 8080 (mapped to host port 16080 ), Spark Worker UI: 8081 (mapped to host port 16002 ), 8082 (mapped to host port 16004 )","title":"Details of the container"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/#error-package-not-found-404-not-found","text":"When building the Docker image, I got a 404 Not Found error because some packages like python3.12 and openjdk-17-jdk-headless couldn't be found. This usually happens if the package lists are outdated or there's an issue with the repositories. Here's how to fix it: Update Package Lists : Run apt-get update first to make sure your package lists are current. Add Correct PPAs : Update the Dockerfile to include these PPAs: deadsnakes for Python. openjdk-r for OpenJDK. Use --fix-missing Option : If the problem continues, try apt-get install --fix-missing to fix missing packages. Install Specific Versions : If the latest version isn't available, try installing a slightly older but stable version.","title":"Error: Package Not Found (404 Not Found)"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/#debian-downloaded-python-pyspark-no-venv","text":"This Section shows you how to create a Docker container with the latest Debian, Python 3.11, OpenJDK 17, and PySpark. We\u2019ll set up a root user and a named user with essential environment variables. Note: If you install python using apt-get install in new Debain it will ask you to install in venv mode. We want to avoid this. Hence we download it(weget) then intstall it manually. We\u2019ll use a Dockerfile and docker-compose.yml for the setup.","title":"Debian, Downloaded Python, Pyspark - no venv."},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/#steps-to-create-the-image-and-container_1","text":"","title":"Steps to Create the Image and Container"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/#create-a-dockerfile","text":"Create a Dockerfile.txt with the contents below and remove the .txt extension Dockerfile # Use Debian as the base image FROM debian:latest # Set environment variable to avoid interactive prompts during package installation ENV DEBIAN_FRONTEND=noninteractive # Update the package lists and install essential packages RUN apt-get update && apt-get install -y --no-install-recommends \\ curl \\ wget \\ tar \\ bash \\ ca-certificates \\ sudo \\ build-essential \\ libssl-dev \\ zlib1g-dev \\ libbz2-dev \\ libreadline-dev \\ libsqlite3-dev \\ libffi-dev # Copy the Python source tarball into the image COPY Python-3.11.9.tgz /tmp/ # Extract, build, and install Python 3.11.9 RUN cd /tmp && \\ tar -xvf Python-3.11.9.tgz && \\ cd Python-3.11.9 && \\ ./configure --enable-optimizations && \\ make -j 8 && \\ make altinstall && \\ cd .. && \\ rm -rf Python-3.11.9 Python-3.11.9.tgz # Create symbolic links for python, python3, pip, and pip3 RUN ln -s /usr/local/bin/python3.11 /usr/bin/python && \\ ln -s /usr/local/bin/python3.11 /usr/bin/python3 && \\ ln -s /usr/local/bin/pip3.11 /usr/bin/pip && \\ ln -s /usr/local/bin/pip3.11 /usr/bin/pip3 # Install OpenJDK 17 RUN apt-get install -y openjdk-17-jdk-headless # Install the PySpark library using pip RUN python3.11 -m pip install pyspark # Set environment variables ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64 ENV PYTHONPATH=/usr/local/lib/python3.11/dist-packages ENV PYSPARK_PYTHON=/usr/local/bin/python3.11 ENV PATH=$PATH:$JAVA_HOME/bin # Clean up the package lists to reduce the image size RUN apt-get clean && rm -rf /var/lib/apt/lists/* # Create a root user and set its password RUN echo 'root:Passw0rd' | chpasswd # Create a new user 'dwdas', set a password, and add this user to the sudo group RUN useradd -ms /bin/bash dwdas && echo 'dwdas:Passw0rd' | chpasswd && adduser dwdas sudo # Allow the 'dwdas' user to run sudo commands without a password RUN echo 'dwdas ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers # Set the working directory to the home directory of the new user WORKDIR /home/dwdas # Switch to the new user 'dwdas' USER dwdas # Expose port 8888, commonly used for Jupyter Notebook, if needed EXPOSE 8888 # Set the default command to start a bash shell CMD [\"bash\"] ``` </details> ### <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #485BDA;\">Download Python and place in the same folder</span> Download Python 3.11.9 from [this site](https://www.python.org/ftp/python/3.11.9/Python-3.11.9.tgz) and place it in the same directory. ### <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #485BDA;\">Build the Docker Image:</span> - Open a terminal and navigate to the directory containing the Dockerfile. - Run the following command to build the Docker image: ```bash docker build -t my-debian-pyspark . ``` ### <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #485BDA;\">Run the Docker Container:</span> - Once the image is built, run the container using the command: ```bash docker run -it --name my-debian-pyspark-container my-debian-pyspark ``` ## <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #485BDA;\">Details of the Container</span> | **Category** | **Details** | |--------------------------|-------------------------------------------------------------------------------------------------| | **Base Image** | Debian (latest) | | **Python Version** | Python 3.11.9 | | **Java Version** | OpenJDK 17 | | **PySpark Version** | Latest via pip | | **Environment Variables** | `JAVA_HOME`: `/usr/lib/jvm/java-17-openjdk-amd64`, `PYTHONPATH`: `/usr/local/lib/python3.11/dist-packages`, `PYSPARK_PYTHON`: `/usr/local/bin/python3.11`, `PATH`: `$PATH:$JAVA_HOME/bin` | | **Installed Packages** | Build tools (curl, wget, tar, etc.), Python 3.11.9 (source), OpenJDK 17, PySpark (pip) | | **User Configuration** | Root user & `dwdas` (password: `Passw0rd`, sudo access) | | **Exposed Port** | 8888 (for Jupyter) | | **Default Command** | Bash shell start | # <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #006600;\">**Debian, Pip Python, Pip Pyspark - venv.**</span> This section shows you how to create a Docker container with the latest Debian, Python 3.11, OpenJDK 17, and PySpark using the recommended venv approach. We\u2019ll set up a root user and a named user with essential environment variables. Note: Newer Debian versions enforce using venv for pip install. We will install Python using apt-get and set up venv from the command line. We\u2019ll use a Dockerfile and docker-compose.yml for the setup. ## <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #006600;\">Steps to Create the Container</span> ### <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #006600;\">Create a Dockerfile:</span> - Create a Dockerfile with the following content: <details open markdown=\"block\"> <summary> Dockerfile </summary> ```dockerfile # Use the latest version of Debian as the base image FROM debian:latest # Set environment variable to avoid interactive prompts during package installation ENV DEBIAN_FRONTEND=noninteractive # Update the package lists and install essential packages RUN apt-get update && \\ apt-get install -y curl wget tar bash ca-certificates sudo gnupg # Install Python 3.11, venv, pip, and OpenJDK 17 RUN apt-get install -y python3.11 python3.11-venv python3.11-dev python3-pip openjdk-17-jdk-headless # Create a virtual environment RUN python3.11 -m venv /opt/venv # Activate the virtual environment and install PySpark RUN /opt/venv/bin/python -m pip install pyspark # Set environment variables ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64 ENV PYTHONPATH=/opt/venv/lib/python3.11/site-packages ENV PYSPARK_PYTHON=/opt/venv/bin/python ENV PATH=$PATH:$JAVA_HOME/bin:/opt/venv/bin # Clean up the package lists to reduce the image size RUN apt-get clean && rm -rf /var/lib/apt/lists/* # Create a root user and set its password RUN echo 'root:Passw0rd' | chpasswd # Create a new user 'dwdas', set a password, and add this user to the sudo group RUN useradd -ms /bin/bash dwdas && echo 'dwdas:Passw0rd' | chpasswd && adduser dwdas sudo # Allow the 'dwdas' user to run sudo commands without a password RUN echo 'dwdas ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers # Set the working directory to the home directory of the new user WORKDIR /home/dwdas # Switch to the new user 'dwdas' USER dwdas # Expose port 8888, commonly used for Jupyter Notebook, if needed EXPOSE 8888 # Set the default command to start a bash shell CMD [\"bash\"] ``` </details> ### <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #006600;\">Build the Docker Image:</span> - Open a terminal and navigate to the directory containing the Dockerfile. - Run the following command to build the Docker image: ```bash docker build -t my-debian-pyspark-venv . ``` ### <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #006600;\">Run the Docker Container:</span> - Once the image is built, run the container using the command: ```bash docker run -it --name my-debian-pyspark-venv-container my-debian-pyspark-venv ``` ## <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #006600;\">Details of the Container</span> | **Category** | **Details** | |--------------------------|--------------------------------------------------------------------------------------------------------------------------| | **Base Image** | Debian (latest) | | **Python Version** | Python 3.11.9 | | **Java Version** | OpenJDK 17 | | **PySpark Version** | Installed via pip in a virtual environment | | **Virtual Environment** | Created with `python3.11 -m venv /opt/venv` | | **Environment Variables** | `JAVA_HOME`: `/usr/lib/jvm/java-17-openjdk-amd64`, `PYTHONPATH`: `/opt/venv/lib/python3.11/site-packages`, `PYSPARK_PYTHON`: `/opt/venv/bin/python`, `PATH`: `$PATH:$JAVA_HOME/bin:/opt/venv/bin` | | **Installed Packages** | Essential tools (curl, wget, tar, bash, etc.), Python 3.11, venv, pip, OpenJDK 17, PySpark (in venv) | | **User Configuration** | Root user & `dwdas` (password: `Passw0rd`, sudo access) | | **Exposed Port** | 8888 (for Jupyter) | | **Default Command** | Bash shell start | To modify the setup so that the `conf` directory in the Spark container is mapped to a local folder on your machine (and the folder is auto-created), we need to update the `docker run` command to include a volume mapping. # <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #C22E2E;\">**Single-Node Bitnami Spark With Master and Worker**</span> Here, we will use the official Bitnami Spark Docker image to set up a single-node Spark environment. This setup will include both the Master and Worker. ## <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #C22E2E;\">Steps to create the container</span> You can either download, unzip, and run the `.bat` file from [this link](Dockerfiles/Bitnami_Spark_SingleNode_GOLD.zip) to create the entire container. Or, you can follow the steps manually. Both methods will give the same result. ### <span style=\"font-family: 'Segoe UI', sans-serif; letter-spacing: 1px;color: #C22E2E;\">Create a Custom Dockerfile</span> In your folder create a file Dockerfile(no extension) with the following content: <details open markdown=\"block\"> <summary> Dockerfile </summary> ```dockerfile # Use the official Bitnami Spark image as the base. I always pull a constant image and not :latest. FROM bitnami/spark:3.5.2-debian-12-r2 # Step 1: Switch to root user to install software # We need to be root to install utilities and set up sudo permissions. USER root # Step 2: Update the package list and install utilities. py4j and ipykernel is for VS studio connection. # Install common utilities like sudo, ping, and nano. # Update the base system RUN apt-get update && \\ apt-get install -y sudo nano iputils-ping grep curl wget vim net-tools procps lsof telnet && \\ apt-get clean # Install pip (if not already installed) RUN apt-get install -y python3-pip # Install py4j and ipykernel using pip. Required VS Code connection. RUN pip3 install py4j ipykernel # Step 3: Set the root user password to 'Passw0rd' # This sets the root password to 'Passw0rd' for future access. RUN echo \"root:Passw0rd\" | chpasswd # Step 4: Give sudo privileges to the 'spark' user # Here, we are allowing the 'spark' user to run commands as sudo without a password. RUN echo \"spark ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers # After finishing the setup, we dont switch back to any user. The bitnami original Dockerfile switches to user 1001 and the directory is /opt/bitnami/spark # Step 6: Expose necessary ports for Spark Web UI and communication # 4040: Spark Worker Web UI # 7077: Spark Master communication # 8080: Spark Master Web UI EXPOSE 4040 7077 8080 # End of the Dockerfile","title":"Create a Dockerfile:"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/#create-a-local-conf-directory","text":"Before running the container, create a folder named local-spark-conf in the same folder where your Dockerfiles are. This folder will store the configuration files and will be mapped to the conf directory inside the container. mkdir local-spark-conf","title":"Create a Local conf Directory"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/#build-the-docker-image","text":"Once the Dockerfile is ready, you can build the Docker image with the following command: docker build -t bitnami-spark-single-node . This command will create a Docker image called bitnami-spark-single-node using the Dockerfile you just created.","title":"Build the Docker Image"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/#run-the-container-and-map-the-conf-directory","text":"Now, we run the Spark container with the conf directory mapped to the local folder you created earlier. If the folder doesn\u2019t exist, Docker will create it. docker run -d --network dasnet --name bitnami-spark-single-node -p 4040:4040 -p 8080:8080 -p 7077:7077 -v ./local-spark-conf:/opt/spark/conf bitnami-spark-single-node","title":"Run the Container and Map the conf Directory"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/#configuration-reference","text":"Here is a list of some components in this environment. The list is compiled from the official bitnami spark github page .","title":"Configuration Reference"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/#environment-details","text":"Component Value OS debian 12: bitnami/minideb:bookworm Python python-3.12.5-1: /opt/bitnami/python/bin/python PYTHONPATH /opt/bitnami/spark/python/ Java java-17.0.12-10-1: JAVA_HOME = /opt/bitnami/java JAVA_HOME /opt/bitnami/java SPARK_HOME /opt/bitnami/spark SPARK_USER spark SPARK JARS Location for Installing External Jars /opt/bitnami/spark/jars Workdir /opt/bitnami/spark User 1001 Entrypoint /opt/bitnami/scripts/spark/entrypoint.sh Command /opt/bitnami/scripts/spark/run.sh Certificates /opt/bitnami/spark/conf/certs SPARK_SSL_KEYSTORE_FILE /opt/bitnami/spark/conf/certs/spark-keystore.jks SPARK_MODE master SPARK_MASTER_URL spark://spark-master:7077 SPARK_SSL_ENABLED no Docker Logs Command docker logs bitnami-spark-single-node Note: The Dockerfile also install py4j and ipykernel. These are reuqired for VS code to container using remote containers extension.","title":"Environment Details"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/#read-only-environment-variables-official-bitnami-github","text":"Name Description Value SPARK_BASE_DIR Spark installation directory. ${BITNAMI_ROOT_DIR}/spark SPARK_CONF_DIR Spark configuration directory. ${SPARK_BASE_DIR}/conf SPARK_DEFAULT_CONF_DIR Spark default configuration directory. ${SPARK_BASE_DIR}/conf.default SPARK_WORK_DIR Spark workspace directory. ${SPARK_BASE_DIR}/work SPARK_CONF_FILE Spark configuration file path. ${SPARK_CONF_DIR}/spark-defaults.conf SPARK_LOG_DIR Spark logs directory. ${SPARK_BASE_DIR}/logs SPARK_TMP_DIR Spark tmp directory. ${SPARK_BASE_DIR}/tmp SPARK_JARS_DIR Spark jar directory. ${SPARK_BASE_DIR}/jars SPARK_INITSCRIPTS_DIR Spark init scripts directory. /docker-entrypoint-initdb.d SPARK_USER Spark user. spark SPARK_DAEMON_USER Spark system user. spark SPARK_DAEMON_GROUP Spark system group. spark","title":"Read-only Environment Variables (Official Bitnami github)"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/","text":"Create a Spark cluster using Bitnami Spark Image Follow just three steps below to create the setup Dockerfile and docker-compose Dockerfile docker-compose.yml How to add an extra node to the cluster Configuration Reference Bitnami Spark Reference Bitnami Spark Cluster with Shared Volume Steps to create the environment Create the Dockerfile Create the docker-compose.yml file Build Your Docker Images Run the containers using the images Check Permissions for /user/hive/warehouse Connect to SparkSQL using spark-sql CLI Connect to a container and create a spark session Common Errors Create a Spark cluster using Bitnami Spark Image Here I will show you how you can setup a Spark Cluster, 1 master, 2 workers node. We will use the Bitnami Docker image for this. This cluster works wonderfully and the easiest of all setups. Make sure Docker is installed and running on your machine. You can Download Docker Desktop for windows from here. Follow just three steps below to create the setup Create a file docker-compose.yml with the content from the docker-compose section. Create a file Dockerfile , with the content from Dockerfile . Note Dockerfile has NO extension . Now open Command prompt and run the following commands bash docker network create dasnet docker-compose -p bitnami-spark-cluster build docker-compose -p bitnami-spark-cluster up -d Open the Docker app and navigate to the container section. The containers should be up and running. Dockerfile and docker-compose Dockerfile Save the content below in file, Dockerfile(no extension) # Use the official Bitnami Spark image as the base image # To use the latest version just replace the line below with FROM bitnami/spark:latest FROM bitnami/spark:3.5 # Switch to root user to install necessary packages and set permissions USER root # Install sudo package RUN apt-get update && apt-get install -y sudo # Add a non-root user named dwdas with a home directory and bash shell RUN useradd -ms /bin/bash dwdas # Set the password for dwdas as Passw0rd RUN echo \"dwdas:Passw0rd\" | chpasswd # Add the user to the sudo group and configure sudoers file to allow passwordless sudo RUN adduser dwdas sudo RUN echo \"dwdas ALL=(ALL) NOPASSWD:ALL\" >> /etc/sudoers # Ensure dwdas has write permissions to necessary directories and files RUN mkdir -p /opt/bitnami/spark/tmp && chown -R dwdas:dwdas /opt/bitnami/spark/tmp RUN chown -R dwdas:dwdas /opt/bitnami/spark/conf RUN chown -R dwdas:dwdas /opt/bitnami/spark/work RUN chown -R dwdas:dwdas /opt/bitnami/spark/logs # Switch back to the non-root user USER dwdas # Set the working directory WORKDIR /home/dwdas docker-compose.yml Here is the content for the docker-compose.yml file. Simply copy the contents into a file named docker-compose.yml in your folder. version: '3.9' # Specify the version of Docker Compose syntax. Latest is 3.9 on Aug 2024 services: # Define the services (containers) that make up your application master: build: context: . # Build context is the current directory (where the Dockerfile is located) dockerfile: Dockerfile # Dockerfile to use for building the image image: bitnami-spark-master # Name the image for the master node container_name: master # Set a custom name for the master container environment: - SPARK_MODE=master # Environment variable to set the Spark mode to master - SPARK_RPC_AUTHENTICATION_ENABLED=no - SPARK_RPC_ENCRYPTION_ENABLED=no - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no - SPARK_SSL_ENABLED=no - SPARK_USER=spark ports: # Mapping ports: <Host_Port>:<Container_Port> # Use rare ports on the host to avoid conflicts, while using standard ports inside the container - \"16080:8080\" # Map a rare port on the host (16080) to the standard port 8080 on the container for Spark Master web UI - \"17077:7077\" # Map a rare port on the host (17077) to the standard port 7077 on the container for Spark Master communication volumes: - spark-warehouse:/user/hive/warehouse # Mount the shared volume for Hive warehouse networks: - dasnet # Connect the master container to the defined network worker1: build: context: . # Build context is the current directory (where the Dockerfile is located) dockerfile: Dockerfile # Dockerfile to use for building the image image: bitnami-spark-worker # Name the image for the worker node container_name: worker1 # Set a custom name for the first worker container environment: - SPARK_MODE=worker # Environment variable to set the Spark mode to worker - SPARK_MASTER_URL=spark://master:7077 # URL for the worker to connect to the master, using the standard port 7077 - SPARK_WORKER_MEMORY=2G # Set the memory allocated for the worker - SPARK_WORKER_CORES=2 # Set the number of CPU cores allocated for the worker - SPARK_RPC_AUTHENTICATION_ENABLED=no - SPARK_RPC_ENCRYPTION_ENABLED=no - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no - SPARK_SSL_ENABLED=no - SPARK_USER=spark depends_on: - master # Ensure that the master service is started before this worker ports: # Mapping ports: <Host_Port>:<Container_Port> - \"16002:8081\" # Map a rare port on the host (16002) to the standard port 8081 on the container for Spark Worker 1 web UI volumes: - spark-warehouse:/user/hive/warehouse # Mount the shared volume for Hive warehouse networks: - dasnet # Connect the worker container to the defined network worker2: build: context: . # Build context is the current directory (where the Dockerfile is located) dockerfile: Dockerfile # Dockerfile to use for building the image image: bitnami-spark-worker # Name the image for the worker node container_name: worker2 # Set a custom name for the second worker container environment: - SPARK_MODE=worker # Environment variable to set the Spark mode to worker - SPARK_MASTER_URL=spark://master:7077 # URL for the worker to connect to the master, using the standard port 7077 - SPARK_WORKER_MEMORY=2G # Set the memory allocated for the worker - SPARK_WORKER_CORES=2 # Set the number of CPU cores allocated for the worker - SPARK_RPC_AUTHENTICATION_ENABLED=no - SPARK_RPC_ENCRYPTION_ENABLED=no - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no - SPARK_SSL_ENABLED=no - SPARK_USER=spark depends_on: - master # Ensure that the master service is started before this worker ports: # Mapping ports: <Host_Port>:<Container_Port> - \"16004:8082\" # Map a rare port on the host (16004) to a different standard port 8082 on the container for Spark Worker 2 web UI volumes: - spark-warehouse:/user/hive/warehouse # Mount the shared volume for Hive warehouse networks: - dasnet # Connect the worker container to the defined network volumes: spark-warehouse: driver: local # Use the local driver to create a shared volume for the warehouse networks: dasnet: external: true # Use the existing 'dasnet' network, created externally via 'docker network create dasnet' How to add an extra node to the cluster To add an extra node just copy paste the contents worker2 and replace the values like `container_name: worker3` `ports: 16005:8083` etc. Configuration Reference Configuration Item Value Base Image bitnami/spark:3.5 Spark Version 3.5.2 Python Version 3.12 Java Version OpenJDK 17.0.12 Environment Variables SPARK_MODE , SPARK_USER , SPARK_MASTER_URL , SPARK_WORKER_MEMORY , SPARK_WORKER_CORES , SPARK_RPC_AUTHENTICATION_ENABLED=no , SPARK_RPC_ENCRYPTION_ENABLED=no , SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no , SPARK_SSL_ENABLED=no Java Home /opt/bitnami/java Spark Home /opt/bitnami/spark Python Path /opt/bitnami/spark/python/ Pyspark Installation Location /opt/bitnami/spark/python/pyspark Users dwdas with sudo privileges, spark (default user) User Directory /home/dwdas Ports Master: 16080:8080 (Web UI), 17077:7077 (RPC); Workers: 16002:8081 (Web UI worker1), 16004:8082 (Web UI worker2) Master http://localhost:16080/ , Worker1 http://localhost:16002/ , Worker2 http://localhost:16004/ Volumes /opt/bitnami/spark/tmp , /opt/bitnami/spark/conf , /opt/bitnami/spark/work , /opt/bitnami/spark/logs , spark-warehouse:/user/hive/warehouse Network Configuration Custom Docker network dasnet Entry Point /opt/bitnami/scripts/spark/entrypoint.sh CMD /opt/bitnami/scripts/spark/run.sh Spark Web UI Ports Master: 8080 , Worker 1: 8081 , Worker 2: 8082 Spark RPC Port 7077 (mapped to 17077 on host) Spark SSL Configuration SPARK_SSL_ENABLED=no Spark Authentication SPARK_RPC_AUTHENTICATION_ENABLED=no Spark Configuration Files /opt/bitnami/spark/conf spark-sql CLI Just use spark-sql . Location: /opt/bitnami/spark/bin/spark-sql spark-shell CLI Just use spark-shell . Location: /opt/bitnami/spark/bin/spark-shell Bitnami Spark Reference You can check out the home directory for the Bitnami Spark Container here. It has all the documentation, configuration files, and version details you might need. If you want to see how the base image is built, you can look at their Dockerfile . Bitnami makes sure to keep up with any changes from the source and quickly rolls out new versions using their automated systems. This means you get the latest fixes and features without any delay. Whether you're using containers, virtual machines, or cloud images, Bitnami keeps everything consistent, making it easy to switch formats based on what your project needs. All Bitnami images are based on minideb , which is a lightweight Debian-based container image, or scratch , which is an empty image\u2014so you get a small base that\u2019s easy to work with. These containers are set up as non-root by default, but I\u2019ve added myself as a root user. Bitnami Spark Cluster with Shared Volume Here, I will show you how to set up a Spark Cluster with the following Details: Configuration Detail Value Image bitnami/spark:latest (Spark 3.5.1), OS_FLAVOUR=debian-12 Spark Mode 1 Master + 2 Workers Spark Submission URL spark://spark-master:7077 Spark URLs Local Master http://localhost:8080/ , Worker 1 http://localhost:8081/ , Worker 2 http://localhost:8082/ Spark Worker Specs Memory 2G, Cores 2 Common Mounted Volumes spark-warehouse:/user/hive/warehouse to all nodes Mounted Folder Details Temp Dir: /opt/bitnami/spark/tmp , Spark Conf: /opt/bitnami/spark/conf , /opt/bitnami/spark/work , Logs Dir: /opt/bitnami/spark/logs Networks spark-network (bridge network) User dwdas (Password: Passw0rd, Sudo Privileges: NOPASSWD:ALL ), Working dir: /home/dwdas Java Version OpenJDK 17.0.11 (LTS) Python Version Python 3.11.9 JAVA_HOME /opt/bitnami/java SPARK_HOME /opt/bitnami/spark PYTHONPATH /opt/bitnami/spark/python/ spark-sql CLI Just use spark-sql . Location: /opt/bitnami/spark/bin/spark-sql spark-shell CLI Just use spark-shell . Location: /opt/bitnami/spark/bin/spark-shell Steps to create the environment Create the Dockerfile Create a Dockerfile.txt with the contents below(remove .txt later). # Use the official Bitnami Spark image as the base image FROM bitnami/spark:latest # Switch to root user to install necessary packages and set permissions USER root # Install sudo package RUN apt-get update && apt-get install -y sudo # Add a non-root user named dwdas with a home directory and bash shell RUN useradd -ms /bin/bash dwdas # Set the password for dwdas as Passw0rd RUN echo \"dwdas:Passw0rd\" | chpasswd # Add the user to the sudo group and configure sudoers file to allow passwordless sudo RUN adduser dwdas sudo RUN echo \"dwdas ALL=(ALL) NOPASSWD:ALL\" >> /etc/sudoers # Ensure dwdas has write permissions to necessary directories and files RUN mkdir -p /opt/bitnami/spark/tmp && chown -R dwdas:dwdas /opt/bitnami/spark/tmp RUN chown -R dwdas:dwdas /opt/bitnami/spark/conf RUN chown -R dwdas:dwdas /opt/bitnami/spark/work RUN chown -R dwdas:dwdas /opt/bitnami/spark/logs # Switch back to the non-root user USER dwdas # Set the working directory WORKDIR /home/dwdas Create the docker-compose.yml file In the same folder, create a docker-compose.yml with the content below. version: '3' # Specify the version of Docker Compose syntax services: # Define the services (containers) that make up your application spark-master: build: context: . # Build context is the current directory (where the Dockerfile is located) dockerfile: Dockerfile # Dockerfile to use for building the image image: bitnami-spark-master # Name the image for the master node container_name: spark-master # Set a custom name for the master container environment: - SPARK_MODE=master # Environment variable to set the Spark mode to master ports: - \"8080:8080\" # Map port 8080 on the host to port 8080 on the container for Spark Master web UI - \"7077:7077\" # Map port 7077 on the host to port 7077 on the container for Spark Master volumes: - spark-warehouse:/user/hive/warehouse # Mount the shared volume for Hive warehouse networks: - spark-network # Connect the master container to the defined network spark-worker-1: build: context: . # Build context is the current directory (where the Dockerfile is located) dockerfile: Dockerfile # Dockerfile to use for building the image image: bitnami-spark-worker # Name the image for the worker node container_name: spark-worker-1 # Set a custom name for the first worker container environment: - SPARK_MODE=worker # Environment variable to set the Spark mode to worker - SPARK_MASTER_URL=spark://spark-master:7077 # URL for the worker to connect to the master - SPARK_WORKER_MEMORY=2G # Set the memory allocated for the worker - SPARK_WORKER_CORES=2 # Set the number of CPU cores allocated for the worker depends_on: - spark-master # Ensure that the master service is started before this worker ports: - \"8081:8081\" # Map port 8081 on the host to port 8081 on the container for Spark Worker 1 web UI volumes: - spark-warehouse:/user/hive/warehouse # Mount the shared volume for Hive warehouse networks: - spark-network # Connect the worker container to the defined network spark-worker-2: build: context: . # Build context is the current directory (where the Dockerfile is located) dockerfile: Dockerfile # Dockerfile to use for building the image image: bitnami-spark-worker # Name the image for the worker node container_name: spark-worker-2 # Set a custom name for the second worker container environment: - SPARK_MODE=worker # Environment variable to set the Spark mode to worker - SPARK_MASTER_URL=spark://spark-master:7077 # URL for the worker to connect to the master - SPARK_WORKER_MEMORY=2G # Set the memory allocated for the worker - SPARK_WORKER_CORES=2 # Set the number of CPU cores allocated for the worker depends_on: - spark-master # Ensure that the master service is started before this worker ports: - \"8082:8081\" # Map port 8082 on the host to port 8081 on the container for Spark Worker 2 web UI volumes: - spark-warehouse:/user/hive/warehouse # Mount the shared volume for Hive warehouse networks: - spark-network # Connect the worker container to the defined network volumes: spark-warehouse: driver: local # Use the local driver to create a shared volume for the warehouse networks: spark-network: driver: bridge # Use the bridge driver to create an isolated network for the Spark cluster Build Your Docker Images In command prompt run the following command to build your Docker images docker-compose -p bitnami-spark-cluster build There will be two images: One for the Master and the other for the worker nodes. The images section will also show one dangling image. Ignore it. Run the containers using the images Now run the following: docker-compose -p bitnami-spark-cluster up -d You should see three containers running inside a compose stack. And a volume will be created and shown in teh Volumes section: The volume will be mounted to all the three containers: Check Permissions for /user/hive/warehouse The /user/hive/warehouse folder is the mounted directory we created. The user dwdas should have the correct permissions for this folder in all three containers. To check this, connect to each container through the terminal using either: docker exec -it spark-master bash Or through Docker Desktop's EXEC tab. Then run: ls -ld /user/hive/warehouse # The permission should be rwx for dwdas Note: If you don't have rwx or owner permissions for the warehouse folder and see an output like this: You will need to either provide rwx permission for dwdas or make dwdas the owner of the folder: chown dwdas:dwdas /user/hive/warehouse If that doesn't work, you can use: chmod 777 /user/hive/warehouse Connect to SparkSQL using spark-sql CLI Open any container's terminal(Master/worker) and input spark-sql . Let's create a simple table and see query it. paste the following in the terminal: CREATE TABLE Hollywood (name STRING); INSERT INTO Hollywood VALUES ('Inception'), ('Titanic'); SELECT * FROM Hollywood; A table will be created in SPARK as extended tabble and you will be able to see the select result: DESCRIBE EXTENDED HOLLYWOOD Will give this result: Note: its a manged table in location /home/dwdas/spark-warehouse/ Connect to a container and create a spark session Open VS Code and use Dev Containers to attach to the running Master container. Then, open a Jupyter notebook and run the following commands. To know how to connect VS code to a container, refer to my article here . from pyspark.sql import SparkSession # Initialize the SparkSession ## Note: file:///user/hive/warehouse is the way and not /user/hive/warehouse. Add file: Else, wont be able to find the directory spark = SparkSession.builder \\ .appName(\"HiveExample\") \\ .config(\"spark.sql.warehouse.dir\", \"file:///user/hive/warehouse\") \\ .config(\"spark.sql.legacy.createHiveTableByDefault\", \"false\") \\ .enableHiveSupport() \\ .getOrCreate() # Explanation: # - `spark.sql.warehouse.dir`: Specifies the default location for managed databases and tables. # - `spark.sql.legacy.createHiveTableByDefault`: Ensures that Hive tables are not created by default unless explicitly specified. # - `enableHiveSupport()`: Enables Hive support, allowing Spark to leverage Hive Metastore, run HiveQL queries, and use Hive functions. # Sample data data = [(\"Kim Jong Obama\", 28), (\"Vladimir Trump\", 35)] columns = [\"Name\", \"Age\"] # Create a DataFrame df = spark.createDataFrame(data, columns) # Save the DataFrame as a managed table df.write.mode(\"overwrite\").saveAsTable(\"people\") # Verify the table creation spark.sql(\"SELECT * FROM people\").show() # Additional Information: # - Hive Metastore Integration: Spark can directly access the Hive Metastore, providing a unified metadata layer for Spark and Hive. # - HiveQL Queries: You can run HiveQL queries using Spark SQL, using the familiar Hive syntax. # - Hive Functions: Spark supports Hive's built-in functions in Spark SQL queries. # - Table Management: Spark can read from and write to Hive-managed tables, including creating, dropping, and altering tables. # - Compatibility with Hive Data Formats: Spark can read from and write to Hive's data formats like ORC and Parquet. # - Access to Hive UDFs: User-defined functions created in Hive can be used within Spark SQL queries. If everything works fine, we will be able to see the spark internal table created! Common Errors I will update this section when I get time.","title":"Bitnami Spark"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/#create-a-spark-cluster-using-bitnami-spark-image","text":"Here I will show you how you can setup a Spark Cluster, 1 master, 2 workers node. We will use the Bitnami Docker image for this. This cluster works wonderfully and the easiest of all setups. Make sure Docker is installed and running on your machine. You can Download Docker Desktop for windows from here.","title":"Create a Spark cluster using Bitnami Spark Image"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/#follow-just-three-steps-below-to-create-the-setup","text":"Create a file docker-compose.yml with the content from the docker-compose section. Create a file Dockerfile , with the content from Dockerfile . Note Dockerfile has NO extension . Now open Command prompt and run the following commands bash docker network create dasnet docker-compose -p bitnami-spark-cluster build docker-compose -p bitnami-spark-cluster up -d Open the Docker app and navigate to the container section. The containers should be up and running.","title":"Follow just three steps below to create the setup"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/#dockerfile-and-docker-compose","text":"","title":"Dockerfile and docker-compose"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/#dockerfile","text":"Save the content below in file, Dockerfile(no extension) # Use the official Bitnami Spark image as the base image # To use the latest version just replace the line below with FROM bitnami/spark:latest FROM bitnami/spark:3.5 # Switch to root user to install necessary packages and set permissions USER root # Install sudo package RUN apt-get update && apt-get install -y sudo # Add a non-root user named dwdas with a home directory and bash shell RUN useradd -ms /bin/bash dwdas # Set the password for dwdas as Passw0rd RUN echo \"dwdas:Passw0rd\" | chpasswd # Add the user to the sudo group and configure sudoers file to allow passwordless sudo RUN adduser dwdas sudo RUN echo \"dwdas ALL=(ALL) NOPASSWD:ALL\" >> /etc/sudoers # Ensure dwdas has write permissions to necessary directories and files RUN mkdir -p /opt/bitnami/spark/tmp && chown -R dwdas:dwdas /opt/bitnami/spark/tmp RUN chown -R dwdas:dwdas /opt/bitnami/spark/conf RUN chown -R dwdas:dwdas /opt/bitnami/spark/work RUN chown -R dwdas:dwdas /opt/bitnami/spark/logs # Switch back to the non-root user USER dwdas # Set the working directory WORKDIR /home/dwdas","title":"Dockerfile"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/#docker-composeyml","text":"Here is the content for the docker-compose.yml file. Simply copy the contents into a file named docker-compose.yml in your folder. version: '3.9' # Specify the version of Docker Compose syntax. Latest is 3.9 on Aug 2024 services: # Define the services (containers) that make up your application master: build: context: . # Build context is the current directory (where the Dockerfile is located) dockerfile: Dockerfile # Dockerfile to use for building the image image: bitnami-spark-master # Name the image for the master node container_name: master # Set a custom name for the master container environment: - SPARK_MODE=master # Environment variable to set the Spark mode to master - SPARK_RPC_AUTHENTICATION_ENABLED=no - SPARK_RPC_ENCRYPTION_ENABLED=no - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no - SPARK_SSL_ENABLED=no - SPARK_USER=spark ports: # Mapping ports: <Host_Port>:<Container_Port> # Use rare ports on the host to avoid conflicts, while using standard ports inside the container - \"16080:8080\" # Map a rare port on the host (16080) to the standard port 8080 on the container for Spark Master web UI - \"17077:7077\" # Map a rare port on the host (17077) to the standard port 7077 on the container for Spark Master communication volumes: - spark-warehouse:/user/hive/warehouse # Mount the shared volume for Hive warehouse networks: - dasnet # Connect the master container to the defined network worker1: build: context: . # Build context is the current directory (where the Dockerfile is located) dockerfile: Dockerfile # Dockerfile to use for building the image image: bitnami-spark-worker # Name the image for the worker node container_name: worker1 # Set a custom name for the first worker container environment: - SPARK_MODE=worker # Environment variable to set the Spark mode to worker - SPARK_MASTER_URL=spark://master:7077 # URL for the worker to connect to the master, using the standard port 7077 - SPARK_WORKER_MEMORY=2G # Set the memory allocated for the worker - SPARK_WORKER_CORES=2 # Set the number of CPU cores allocated for the worker - SPARK_RPC_AUTHENTICATION_ENABLED=no - SPARK_RPC_ENCRYPTION_ENABLED=no - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no - SPARK_SSL_ENABLED=no - SPARK_USER=spark depends_on: - master # Ensure that the master service is started before this worker ports: # Mapping ports: <Host_Port>:<Container_Port> - \"16002:8081\" # Map a rare port on the host (16002) to the standard port 8081 on the container for Spark Worker 1 web UI volumes: - spark-warehouse:/user/hive/warehouse # Mount the shared volume for Hive warehouse networks: - dasnet # Connect the worker container to the defined network worker2: build: context: . # Build context is the current directory (where the Dockerfile is located) dockerfile: Dockerfile # Dockerfile to use for building the image image: bitnami-spark-worker # Name the image for the worker node container_name: worker2 # Set a custom name for the second worker container environment: - SPARK_MODE=worker # Environment variable to set the Spark mode to worker - SPARK_MASTER_URL=spark://master:7077 # URL for the worker to connect to the master, using the standard port 7077 - SPARK_WORKER_MEMORY=2G # Set the memory allocated for the worker - SPARK_WORKER_CORES=2 # Set the number of CPU cores allocated for the worker - SPARK_RPC_AUTHENTICATION_ENABLED=no - SPARK_RPC_ENCRYPTION_ENABLED=no - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no - SPARK_SSL_ENABLED=no - SPARK_USER=spark depends_on: - master # Ensure that the master service is started before this worker ports: # Mapping ports: <Host_Port>:<Container_Port> - \"16004:8082\" # Map a rare port on the host (16004) to a different standard port 8082 on the container for Spark Worker 2 web UI volumes: - spark-warehouse:/user/hive/warehouse # Mount the shared volume for Hive warehouse networks: - dasnet # Connect the worker container to the defined network volumes: spark-warehouse: driver: local # Use the local driver to create a shared volume for the warehouse networks: dasnet: external: true # Use the existing 'dasnet' network, created externally via 'docker network create dasnet'","title":"docker-compose.yml"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/#how-to-add-an-extra-node-to-the-cluster","text":"To add an extra node just copy paste the contents worker2 and replace the values like `container_name: worker3` `ports: 16005:8083` etc.","title":"How to add an extra node to the cluster"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/#configuration-reference","text":"Configuration Item Value Base Image bitnami/spark:3.5 Spark Version 3.5.2 Python Version 3.12 Java Version OpenJDK 17.0.12 Environment Variables SPARK_MODE , SPARK_USER , SPARK_MASTER_URL , SPARK_WORKER_MEMORY , SPARK_WORKER_CORES , SPARK_RPC_AUTHENTICATION_ENABLED=no , SPARK_RPC_ENCRYPTION_ENABLED=no , SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no , SPARK_SSL_ENABLED=no Java Home /opt/bitnami/java Spark Home /opt/bitnami/spark Python Path /opt/bitnami/spark/python/ Pyspark Installation Location /opt/bitnami/spark/python/pyspark Users dwdas with sudo privileges, spark (default user) User Directory /home/dwdas Ports Master: 16080:8080 (Web UI), 17077:7077 (RPC); Workers: 16002:8081 (Web UI worker1), 16004:8082 (Web UI worker2) Master http://localhost:16080/ , Worker1 http://localhost:16002/ , Worker2 http://localhost:16004/ Volumes /opt/bitnami/spark/tmp , /opt/bitnami/spark/conf , /opt/bitnami/spark/work , /opt/bitnami/spark/logs , spark-warehouse:/user/hive/warehouse Network Configuration Custom Docker network dasnet Entry Point /opt/bitnami/scripts/spark/entrypoint.sh CMD /opt/bitnami/scripts/spark/run.sh Spark Web UI Ports Master: 8080 , Worker 1: 8081 , Worker 2: 8082 Spark RPC Port 7077 (mapped to 17077 on host) Spark SSL Configuration SPARK_SSL_ENABLED=no Spark Authentication SPARK_RPC_AUTHENTICATION_ENABLED=no Spark Configuration Files /opt/bitnami/spark/conf spark-sql CLI Just use spark-sql . Location: /opt/bitnami/spark/bin/spark-sql spark-shell CLI Just use spark-shell . Location: /opt/bitnami/spark/bin/spark-shell","title":"Configuration Reference"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/#bitnami-spark-reference","text":"You can check out the home directory for the Bitnami Spark Container here. It has all the documentation, configuration files, and version details you might need. If you want to see how the base image is built, you can look at their Dockerfile . Bitnami makes sure to keep up with any changes from the source and quickly rolls out new versions using their automated systems. This means you get the latest fixes and features without any delay. Whether you're using containers, virtual machines, or cloud images, Bitnami keeps everything consistent, making it easy to switch formats based on what your project needs. All Bitnami images are based on minideb , which is a lightweight Debian-based container image, or scratch , which is an empty image\u2014so you get a small base that\u2019s easy to work with. These containers are set up as non-root by default, but I\u2019ve added myself as a root user.","title":"Bitnami Spark Reference"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/#bitnami-spark-cluster-with-shared-volume","text":"Here, I will show you how to set up a Spark Cluster with the following Details: Configuration Detail Value Image bitnami/spark:latest (Spark 3.5.1), OS_FLAVOUR=debian-12 Spark Mode 1 Master + 2 Workers Spark Submission URL spark://spark-master:7077 Spark URLs Local Master http://localhost:8080/ , Worker 1 http://localhost:8081/ , Worker 2 http://localhost:8082/ Spark Worker Specs Memory 2G, Cores 2 Common Mounted Volumes spark-warehouse:/user/hive/warehouse to all nodes Mounted Folder Details Temp Dir: /opt/bitnami/spark/tmp , Spark Conf: /opt/bitnami/spark/conf , /opt/bitnami/spark/work , Logs Dir: /opt/bitnami/spark/logs Networks spark-network (bridge network) User dwdas (Password: Passw0rd, Sudo Privileges: NOPASSWD:ALL ), Working dir: /home/dwdas Java Version OpenJDK 17.0.11 (LTS) Python Version Python 3.11.9 JAVA_HOME /opt/bitnami/java SPARK_HOME /opt/bitnami/spark PYTHONPATH /opt/bitnami/spark/python/ spark-sql CLI Just use spark-sql . Location: /opt/bitnami/spark/bin/spark-sql spark-shell CLI Just use spark-shell . Location: /opt/bitnami/spark/bin/spark-shell","title":"Bitnami Spark Cluster with Shared Volume"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/#steps-to-create-the-environment","text":"","title":"Steps to create the environment"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/#create-the-dockerfile","text":"Create a Dockerfile.txt with the contents below(remove .txt later). # Use the official Bitnami Spark image as the base image FROM bitnami/spark:latest # Switch to root user to install necessary packages and set permissions USER root # Install sudo package RUN apt-get update && apt-get install -y sudo # Add a non-root user named dwdas with a home directory and bash shell RUN useradd -ms /bin/bash dwdas # Set the password for dwdas as Passw0rd RUN echo \"dwdas:Passw0rd\" | chpasswd # Add the user to the sudo group and configure sudoers file to allow passwordless sudo RUN adduser dwdas sudo RUN echo \"dwdas ALL=(ALL) NOPASSWD:ALL\" >> /etc/sudoers # Ensure dwdas has write permissions to necessary directories and files RUN mkdir -p /opt/bitnami/spark/tmp && chown -R dwdas:dwdas /opt/bitnami/spark/tmp RUN chown -R dwdas:dwdas /opt/bitnami/spark/conf RUN chown -R dwdas:dwdas /opt/bitnami/spark/work RUN chown -R dwdas:dwdas /opt/bitnami/spark/logs # Switch back to the non-root user USER dwdas # Set the working directory WORKDIR /home/dwdas","title":"Create the Dockerfile"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/#create-the-docker-composeyml-file","text":"In the same folder, create a docker-compose.yml with the content below. version: '3' # Specify the version of Docker Compose syntax services: # Define the services (containers) that make up your application spark-master: build: context: . # Build context is the current directory (where the Dockerfile is located) dockerfile: Dockerfile # Dockerfile to use for building the image image: bitnami-spark-master # Name the image for the master node container_name: spark-master # Set a custom name for the master container environment: - SPARK_MODE=master # Environment variable to set the Spark mode to master ports: - \"8080:8080\" # Map port 8080 on the host to port 8080 on the container for Spark Master web UI - \"7077:7077\" # Map port 7077 on the host to port 7077 on the container for Spark Master volumes: - spark-warehouse:/user/hive/warehouse # Mount the shared volume for Hive warehouse networks: - spark-network # Connect the master container to the defined network spark-worker-1: build: context: . # Build context is the current directory (where the Dockerfile is located) dockerfile: Dockerfile # Dockerfile to use for building the image image: bitnami-spark-worker # Name the image for the worker node container_name: spark-worker-1 # Set a custom name for the first worker container environment: - SPARK_MODE=worker # Environment variable to set the Spark mode to worker - SPARK_MASTER_URL=spark://spark-master:7077 # URL for the worker to connect to the master - SPARK_WORKER_MEMORY=2G # Set the memory allocated for the worker - SPARK_WORKER_CORES=2 # Set the number of CPU cores allocated for the worker depends_on: - spark-master # Ensure that the master service is started before this worker ports: - \"8081:8081\" # Map port 8081 on the host to port 8081 on the container for Spark Worker 1 web UI volumes: - spark-warehouse:/user/hive/warehouse # Mount the shared volume for Hive warehouse networks: - spark-network # Connect the worker container to the defined network spark-worker-2: build: context: . # Build context is the current directory (where the Dockerfile is located) dockerfile: Dockerfile # Dockerfile to use for building the image image: bitnami-spark-worker # Name the image for the worker node container_name: spark-worker-2 # Set a custom name for the second worker container environment: - SPARK_MODE=worker # Environment variable to set the Spark mode to worker - SPARK_MASTER_URL=spark://spark-master:7077 # URL for the worker to connect to the master - SPARK_WORKER_MEMORY=2G # Set the memory allocated for the worker - SPARK_WORKER_CORES=2 # Set the number of CPU cores allocated for the worker depends_on: - spark-master # Ensure that the master service is started before this worker ports: - \"8082:8081\" # Map port 8082 on the host to port 8081 on the container for Spark Worker 2 web UI volumes: - spark-warehouse:/user/hive/warehouse # Mount the shared volume for Hive warehouse networks: - spark-network # Connect the worker container to the defined network volumes: spark-warehouse: driver: local # Use the local driver to create a shared volume for the warehouse networks: spark-network: driver: bridge # Use the bridge driver to create an isolated network for the Spark cluster","title":"Create the docker-compose.yml file"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/#build-your-docker-images","text":"In command prompt run the following command to build your Docker images docker-compose -p bitnami-spark-cluster build There will be two images: One for the Master and the other for the worker nodes. The images section will also show one dangling image. Ignore it.","title":"Build Your Docker Images"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/#run-the-containers-using-the-images","text":"Now run the following: docker-compose -p bitnami-spark-cluster up -d You should see three containers running inside a compose stack. And a volume will be created and shown in teh Volumes section: The volume will be mounted to all the three containers:","title":"Run the containers using the images"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/#check-permissions-for-userhivewarehouse","text":"The /user/hive/warehouse folder is the mounted directory we created. The user dwdas should have the correct permissions for this folder in all three containers. To check this, connect to each container through the terminal using either: docker exec -it spark-master bash Or through Docker Desktop's EXEC tab. Then run: ls -ld /user/hive/warehouse # The permission should be rwx for dwdas Note: If you don't have rwx or owner permissions for the warehouse folder and see an output like this: You will need to either provide rwx permission for dwdas or make dwdas the owner of the folder: chown dwdas:dwdas /user/hive/warehouse If that doesn't work, you can use: chmod 777 /user/hive/warehouse","title":"Check Permissions for /user/hive/warehouse"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/#connect-to-sparksql-using-spark-sql-cli","text":"Open any container's terminal(Master/worker) and input spark-sql . Let's create a simple table and see query it. paste the following in the terminal: CREATE TABLE Hollywood (name STRING); INSERT INTO Hollywood VALUES ('Inception'), ('Titanic'); SELECT * FROM Hollywood; A table will be created in SPARK as extended tabble and you will be able to see the select result: DESCRIBE EXTENDED HOLLYWOOD Will give this result: Note: its a manged table in location /home/dwdas/spark-warehouse/","title":"Connect to SparkSQL using spark-sql CLI"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/#connect-to-a-container-and-create-a-spark-session","text":"Open VS Code and use Dev Containers to attach to the running Master container. Then, open a Jupyter notebook and run the following commands. To know how to connect VS code to a container, refer to my article here . from pyspark.sql import SparkSession # Initialize the SparkSession ## Note: file:///user/hive/warehouse is the way and not /user/hive/warehouse. Add file: Else, wont be able to find the directory spark = SparkSession.builder \\ .appName(\"HiveExample\") \\ .config(\"spark.sql.warehouse.dir\", \"file:///user/hive/warehouse\") \\ .config(\"spark.sql.legacy.createHiveTableByDefault\", \"false\") \\ .enableHiveSupport() \\ .getOrCreate() # Explanation: # - `spark.sql.warehouse.dir`: Specifies the default location for managed databases and tables. # - `spark.sql.legacy.createHiveTableByDefault`: Ensures that Hive tables are not created by default unless explicitly specified. # - `enableHiveSupport()`: Enables Hive support, allowing Spark to leverage Hive Metastore, run HiveQL queries, and use Hive functions. # Sample data data = [(\"Kim Jong Obama\", 28), (\"Vladimir Trump\", 35)] columns = [\"Name\", \"Age\"] # Create a DataFrame df = spark.createDataFrame(data, columns) # Save the DataFrame as a managed table df.write.mode(\"overwrite\").saveAsTable(\"people\") # Verify the table creation spark.sql(\"SELECT * FROM people\").show() # Additional Information: # - Hive Metastore Integration: Spark can directly access the Hive Metastore, providing a unified metadata layer for Spark and Hive. # - HiveQL Queries: You can run HiveQL queries using Spark SQL, using the familiar Hive syntax. # - Hive Functions: Spark supports Hive's built-in functions in Spark SQL queries. # - Table Management: Spark can read from and write to Hive-managed tables, including creating, dropping, and altering tables. # - Compatibility with Hive Data Formats: Spark can read from and write to Hive's data formats like ORC and Parquet. # - Access to Hive UDFs: User-defined functions created in Hive can be used within Spark SQL queries. If everything works fine, we will be able to see the spark internal table created!","title":"Connect to a container and create a spark session"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/#common-errors","text":"I will update this section when I get time.","title":"Common Errors"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.3_VSCode_Docker_Connection/","text":"Connect to docker container from Visual Studio Code Summary of Steps Install Dev Containers Extension Attach to the running container Install Jupyter notebook support extension in conatiner Install Python Support Extension in Container Install ipykernel Install py4j(if required) Errors failed: mkdir -p /.vscode-server Correct Method Wrong Resolution Add user:root method docker run -u root method No Kernel Visible Py4J Error Connect to docker container from Visual Studio Code Here, I will show you how to connect to a container using VS Code to run Python code, create Jupyter notebooks, and more. This setup is very helpful. Containers are essentially Linux OS environments, and you can't log into them directly to install VS Code. The connection is mainly done using the VS Code Dev Containers extension, which is the key component for connectivity. The following sections provide detailed steps to guide you through the process. Summary of Steps Install VS Code Dev Containers Extension on your local machine. Open a terminal and run the following commands in the container: bash sudo su mkdir -p /.vscode-server chmod -R 777 /.vscode-server Attach to the Running Container: Open a remote window from the bottom left corner in VS Code. Install Jupyter & Python extensions in the container. Install Required Python Packages: Open a terminal and run the following commands in the container: bash sudo su pip install ipykernel pip install py4j Install Dev Containers Extension In Visual Studio Code press Ctrl+Shift+X , search Dev Containers and install . Attach to the running container Click the Open Remote Window button in the bottom-left corner of VS Code and select Attach to Running Container from the command palette that appears. Pick your active container from the presented list. Note: Here you will encounter like this, go to the errors section to resolve it Install Jupyter notebook support extension in conatiner Go to extensions(left pane), search Jupyter, click on Install in container Install Python Support Extension in Container Go to extensions(left pane), search Python, click on Install in container Install ipykernel Connect to the container from terminal(or EXEC in Docker container) and run this command: sudo su pip install ipykernel Also, if you try to run a jupyter notebook, using the steps below You may be prompted to isntall the extension: Install py4j(if required) Errors failed: mkdir -p /.vscode-server When trying to attach to a Docker container using the VSCode Dev extension, you may encounter an error during the Installing VS Code Server step : Reason: Insufficient permissions for VSCode to create a folder .vscode-server inside the root folder. It cannot create this folder: /.vscode-server . For example, when it runs this command: mkdir -p /root/.vscode-server/bin/ Note: When you attach to a running container, the Dev Container extension installs a remote server in the .vscode-server folder, defaulting to the root location. You can change this property by using: User Settings (JSON): Press Ctrl+Shift+P > \"Preferences: Open User Settings (JSON)\". Add: \"remote.SSH.serverInstallPath\": { \"<host>\": \"/test/location\" } Settings UI: Go to File > Preferences > Settings, filter by @ext:ms-vscode-remote.remote-ssh install , and under \"Server Install Path\" > Add Item with Item = <host> and Value = /test/location . Correct Method The correct method is to create the folder using the root user and provide permissions to it for the normal user. This way, you can create a normal container without needing to add the root user to the container. To achieve this run the following commands in the container and then try to connect VS code again: sudo su mkdir -p /.vscode-server chmod -R 777 /.vscode-server sometimes, e.g. in debian, just su Note: You may not always have su access or password. To resolve it you may have to create a Dockerfile and users inside it with elevated permission. Refer to my BitnamiSparkCluster article to create such containers. Wrong Resolution Add user:root method If you use a docker-compose file to create the containers you can add user: root to every container. docker run -u root method Alternaively, you can start the container with root. This is feasible only for single containers. docker run -u root -it --name myCont theImageFileName /bin/bash No Kernel Visible Install the Jupyter extension in the container. Also, go to the Docker container and install ipykernel: sudo su pip install ipykernel Then it will detect the kernel automatically. Py4J Error While running code, you may get this error: Install py4j in the container to resolve it: sudo su pip install py4j","title":"VSCode-Docker-Connection"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.3_VSCode_Docker_Connection/#connect-to-docker-container-from-visual-studio-code","text":"Here, I will show you how to connect to a container using VS Code to run Python code, create Jupyter notebooks, and more. This setup is very helpful. Containers are essentially Linux OS environments, and you can't log into them directly to install VS Code. The connection is mainly done using the VS Code Dev Containers extension, which is the key component for connectivity. The following sections provide detailed steps to guide you through the process.","title":"Connect to docker container from Visual Studio Code"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.3_VSCode_Docker_Connection/#summary-of-steps","text":"Install VS Code Dev Containers Extension on your local machine. Open a terminal and run the following commands in the container: bash sudo su mkdir -p /.vscode-server chmod -R 777 /.vscode-server Attach to the Running Container: Open a remote window from the bottom left corner in VS Code. Install Jupyter & Python extensions in the container. Install Required Python Packages: Open a terminal and run the following commands in the container: bash sudo su pip install ipykernel pip install py4j","title":"Summary of Steps"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.3_VSCode_Docker_Connection/#install-dev-containers-extension","text":"In Visual Studio Code press Ctrl+Shift+X , search Dev Containers and install .","title":"Install Dev Containers Extension"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.3_VSCode_Docker_Connection/#attach-to-the-running-container","text":"Click the Open Remote Window button in the bottom-left corner of VS Code and select Attach to Running Container from the command palette that appears. Pick your active container from the presented list. Note: Here you will encounter like this, go to the errors section to resolve it","title":"Attach to the running container"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.3_VSCode_Docker_Connection/#install-jupyter-notebook-support-extension-in-conatiner","text":"Go to extensions(left pane), search Jupyter, click on Install in container","title":"Install Jupyter notebook support extension in conatiner"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.3_VSCode_Docker_Connection/#install-python-support-extension-in-container","text":"Go to extensions(left pane), search Python, click on Install in container","title":"Install Python Support Extension in Container"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.3_VSCode_Docker_Connection/#install-ipykernel","text":"Connect to the container from terminal(or EXEC in Docker container) and run this command: sudo su pip install ipykernel Also, if you try to run a jupyter notebook, using the steps below You may be prompted to isntall the extension:","title":"Install ipykernel"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.3_VSCode_Docker_Connection/#install-py4jif-required","text":"","title":"Install py4j(if required)"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.3_VSCode_Docker_Connection/#errors","text":"","title":"Errors"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.3_VSCode_Docker_Connection/#failed-mkdir-p-vscode-server","text":"When trying to attach to a Docker container using the VSCode Dev extension, you may encounter an error during the Installing VS Code Server step : Reason: Insufficient permissions for VSCode to create a folder .vscode-server inside the root folder. It cannot create this folder: /.vscode-server . For example, when it runs this command: mkdir -p /root/.vscode-server/bin/ Note: When you attach to a running container, the Dev Container extension installs a remote server in the .vscode-server folder, defaulting to the root location. You can change this property by using: User Settings (JSON): Press Ctrl+Shift+P > \"Preferences: Open User Settings (JSON)\". Add: \"remote.SSH.serverInstallPath\": { \"<host>\": \"/test/location\" } Settings UI: Go to File > Preferences > Settings, filter by @ext:ms-vscode-remote.remote-ssh install , and under \"Server Install Path\" > Add Item with Item = <host> and Value = /test/location .","title":"failed: mkdir -p /.vscode-server"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.3_VSCode_Docker_Connection/#correct-method","text":"The correct method is to create the folder using the root user and provide permissions to it for the normal user. This way, you can create a normal container without needing to add the root user to the container. To achieve this run the following commands in the container and then try to connect VS code again: sudo su mkdir -p /.vscode-server chmod -R 777 /.vscode-server sometimes, e.g. in debian, just su Note: You may not always have su access or password. To resolve it you may have to create a Dockerfile and users inside it with elevated permission. Refer to my BitnamiSparkCluster article to create such containers.","title":"Correct Method"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.3_VSCode_Docker_Connection/#wrong-resolution","text":"","title":"Wrong Resolution"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.3_VSCode_Docker_Connection/#add-userroot-method","text":"If you use a docker-compose file to create the containers you can add user: root to every container.","title":"Add user:root method"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.3_VSCode_Docker_Connection/#docker-run-u-root-method","text":"Alternaively, you can start the container with root. This is feasible only for single containers. docker run -u root -it --name myCont theImageFileName /bin/bash","title":"docker run -u root method"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.3_VSCode_Docker_Connection/#no-kernel-visible","text":"Install the Jupyter extension in the container. Also, go to the Docker container and install ipykernel: sudo su pip install ipykernel Then it will detect the kernel automatically.","title":"No Kernel Visible"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.3_VSCode_Docker_Connection/#py4j-error","text":"While running code, you may get this error: Install py4j in the container to resolve it: sudo su pip install py4j","title":"Py4J Error"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/","text":"Setup a complete data warehouse with Spark cluster \\& external Hive(MSSQL Metastore) For the busy people How was the setup created Bit Background About Bitnami Spark Container Their Dockerfile Non-Root Containers The User 1001 Folder Structure Spark container setup details MSSQL 2019 container setup details Hive Server container setup details Hive-MSSQL Connection spark-defaults.conf \\& hive-site.xml spark-defaults.conf \\& hive-site.xml spark-defaults.conf important details hive-site important details Repository and Metastore Folder Creation Testing When will be managed or external tables created? Errors Server Details Setup a complete data warehouse with Spark cluster & external Hive(MSSQL Metastore) Here, I will show you how to create a complete warehouse setup with a Spark Cluster and a standalone Hive with an external metastore database (MSSQL). The setup will use 5 containers: Service URL/Connection Image Spark Master http://localhost:16789/ spark:3.5.1-debian-12-r7 Spark Worker 1 http://localhost:16791/ spark:3.5.1-debian-12-r7 Spark Worker 2 http://localhost:16792/ spark:3.5.1-debian-12-r7 SQL Server (SSMS) localhost,1433 , SQL Server Auth: dwdas/Passw0rd mcr.microsoft.com/mssql/server:2019-latest Hive Metastore thrift://hive-metastore:9083 apache/hive:4.0.0 For the busy people Follow these steps to get your setup ready: Download the zip file on your laptop. Go to Step1xx , Step2xx , and Step3xx folders and run run.bat in each. This will create a setup like the one shown below: A shared volume should be present in the volumes with the mapping shown below: Here are some important details about the setup: How was the setup created Bit Background About Bitnami Spark Container Their Dockerfile The official Dockerfile used by Bitnami to create their Spark container can be found on GitHub . Some key parts of the file are: RUN chown -R 1001:1001 /opt/bitnami/spark WORKDIR /opt/bitnami/spark USER 1001 ENTRYPOINT [ \"/opt/bitnami/scripts/spark/entrypoint.sh\" ] CMD [ \"/opt/bitnami/scripts/spark/run.sh\" ] Non-Root Containers By default, Bitnami containers run as non-root users. This means when you log into the container, you are not the root user. The container itself runs as a non-root user, but you can change this by specifying User: root in your Dockerfile or Docker-compose. More details can be found here . The User 1001 Bitnami Spark containers use the non-root user ID 1001. When you start the container and log in, you will be user 1001. It's important that this user has access to all required directories and volumes. Ensure necessary permissions are set when building the container. Folder Structure Bitnami containers store application files in /opt/bitnami/APPNAME/ . For example, Spark files are located in /opt/bitnami/spark . More details can be found in the Bitnami directory structure documentation . Spark container setup details First, I created the Spark cluster using the Bitnami Spark image bitnami/spark:3.5.1-debian-12-r7 . I chose Bitnami because it is popular and offers a constant version for stability. However, you can also use bitnami/spark:latest . I used a Dockerfile and docker-compose approach. Created a spark-defaults.conf with just one setting: text spark.sql.warehouse.dir = /data/spark-warehouse Created a Dockerfile with the following important activities: Created a /data folder. Copied the custom spark-defaults.conf from laptop to /conf . Set the root password and provided user 1001 permissions to /data folder. Created a docker-compose.yml file that included these changes(rest are usual: ```yaml volumes: shared-data:/data ./spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf environment: SPARK_MODE=master # Environment variable to set the Spark mode to master SPARK_LOCAL_DIRS=/data/tmp # Local directories for Spark ``` Finally, created a run.bat to do the following: ```bat REM Create the Docker volume docker volume create shared-data REM Build and run the Docker Compose services docker-compose -p bitnami-spark-cluster build docker-compose -p bitnami-spark-cluster up -d ``` MSSQL 2019 container setup details Setting up the MSSQL container was more straightforward and required less customization than the Spark cluster. I used a Dockerfile and docker-compose approach. Created a Dockerfile with the following important activities: Created a user dwdas with the password Passw0rd . Set the root user password to Passw0rd . Created a docker-compose.yml file that included these changes: ```yaml ports: \"1433:1433\" # Map port 1433 of the host to port 1433 of the container for MSSQL communication. environment: SA_PASSWORD=Passw0rd # Set the system administrator password for MSSQL. networks: - spark-network ``` Finally, created a run.bat to do the following: ```bat REM Create the Docker volumes. This is mapped to all Spark containers and the Hive container. docker volume create shared-data REM Build and run the Docker Compose services docker-compose -p bitnami-spark-cluster build docker-compose -p bitnami-spark-cluster up -d REM Post-Setup: Create MSSQL user and database docker exec -it mssql-container-name /opt/mssql-tools/bin/sqlcmd -S localhost -U SA -P \"Passw0rd\" -Q \"CREATE LOGIN dwdas WITH PASSWORD='Passw0rd'; CREATE DATABASE hive_metastore;\" ``` run.bat: After the container started, I created an MSSQL user dwdas with the password Passw0rd and an empty database hive_metastore . This was done inside the run.bat file. Hive Server container setup details Setting up the Hive server can be very tricky. I used the official Apache Hive image with a Dockerfile and docker-compose approach. Download JDBC Driver : Downloaded sqljdbc_7.2.2.0_enu.tar.gz from Microsoft Download Center . Extracted the mssql-jdbc-7.2.2.jre8.jar driver to the current folder. Create a custom hive-site.xml : Created a custom hive-site.xml with MSSQL connection information and directory configurations. Refer to the Step3-Hive/hive-site.xml in the setup folder for more details. Create a Dockerfile : Refer to the dockerfile in Step3-Hive folder for more details. Apart from usual activity these two steps are most immortant. I.e. placing the driver to the /lib and hive-site.xml to conf. dockerfile COPY ./mssql-jdbc-7.2.2.jre8.jar /opt/hive/lib/ COPY ./hive-site.xml /opt/hive/conf/ Docker-compose Configuration : Apart from usual stuff the most important was to the environment variable DB_DRIVER=mssql . Run.bat : The run.bat script builds the images, runs the container. Hive-MSSQL Connection Apache Hive containers usually use a startup script called '/entrypoint.sh'. This script is set to use Derby ( ${DB_DRIVER:=derby} ) database by default. To use MSSQL instead: Change the DB_DRIVER environment variable to 'mssql'. Create an empty database called 'hive_metastore' on your MSSQL server. Put your MSSQL connection and db details in the 'hive-site.xml' file. Add the MSSQL driver file (mssql-jdbc-7.2.2.jre8.jar) to the '/lib' folder. /entrypoint.sh #!/bin/bash set -x : ${DB_DRIVER:=derby} SKIP_SCHEMA_INIT=\"${IS_RESUME:-false}\" function initialize_hive { COMMAND=\"-initOrUpgradeSchema\" if [ \"$(echo \"$HIVE_VER\" | cut -d '.' -f1)\" -lt \"4\" ]; then COMMAND=\"-${SCHEMA_COMMAND:-initSchema}\" fi $HIVE_HOME/bin/schematool -dbType $DB_DRIVER $COMMAND if [ $? -eq 0 ]; then echo \"Initialized schema successfully..\" else echo \"Schema initialization failed!\" exit 1 fi } # Additional script content... So, how I did this: During mssql creation I already creatd the hive_metastore empty database and the user The dockerfile managed the copying of the driver and the custom hive-site.xml dockerfile COPY ./mssql-jdbc-7.2.2.jre8.jar /opt/hive/lib/ COPY ./hive-site.xml /opt/hive/conf/ docker-compose managed set the DB_DRIVER=mssql Note: An alternative approach to achieve the same feature would be to: Let the container start with the default settings. Open /entrypoint.sh and change ${DB_DRIVER:=mssql} . Run su -c \"chmod 777 /entrypoint.sh\" with the password Passw0rd as the permission will change. Copy hive-site.xml containing MSSQL connection info to /opt/hive/conf/ . Restart the container. spark-defaults.conf & hive-site.xml Only two configureation files were touched. Spark-defaults.conf in spark server and hive-site.xml in hive server. spark-defaults.conf & hive-site.xml spark-defaults.conf important details Here, only one value was added. The same file is present in all the spark containres. spark.sql.warehouse.dir=/data/spark-warehouse hive-site important details <property> <name>hive.metastore.warehouse.dir</name> <value>/data/hive-warehouse</value> </property> <property> <name>javax.jdo.option.ConnectionDriverName</name> <value>com.microsoft.sqlserver.jdbc.SQLServerDriver</value> </property> <property> <name>javax.jdo.option.ConnectionURL</name> <value>jdbc:sqlserver://mssql:1433;DatabaseName=hive_metastore;</value> </property> <property> <property> <name>hive.metastore.uris</name> <value>thrift://hive-metastore:9083</value> </property> <property> <name>hive.metastore.db.type</name> <value>mssql</value> <description> Expects one of [derby, oracle, mysql, mssql, postgres]. </description> </property> Repository and Metastore Folder Creation We have two options to decide on the folder structure: Our choice: Separate warehouses for Spark and Hive 1. Keep Hive warehouse as is: - In hive-site.xml : xml <property> <name>hive.metastore.warehouse.dir</name> <value>/data/hive/warehouse</value> </property> 2. Set a different warehouse for Spark: - In Spark configuration: spark.sql.warehouse.dir=/data/spark/warehouse /data \u251c\u2500\u2500 hive-warehouse/ \u2502 \u2514\u2500\u2500 (Hive managed tables) \u251c\u2500\u2500 spark-warehouse/ \u2502 \u2514\u2500\u2500 (Spark-managed tables) Option 2: Use Hive Warehouse for both Spark and Hive 1. Configure Spark to use the Hive metastore: - Set spark.sql.hive.metastore.version and spark.sql.hive.metastore.jars in Spark configuration. 2. Use the same warehouse directory for both: - In hive-site.xml : xml <property> <name>hive.metastore.warehouse.dir</name> <value>/data/spark-warehouse</value> </property> - In Spark configuration. Using( spark-defaults.conf ) or setting during session etc. spark.sql.warehouse.dir=/data/spark-warehouse Testing Testing Hive Go the the hive container and type hive . You should see output lilke this: Testing spark-shell Go to any of hte spark containers's terminal and key in spark-shell , you should see output liek this: Testing spark-sql Go to any of hte spark containers's terminal and key in spark-sql , you should see output liek this: When will be managed or external tables created? When configuring Spark to work with Hive, the type of table created (managed vs. external) depends on specific settings in your Spark configuration. If spark-defaults.xml includes: spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083 Then, spark will create all tables as EXTERNAL . Regardless of whatever settigngs you try. If spark-defaults.xml does not include: spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083 Then tables are created as MANAGED by default and stored in the directory specified by spark.sql.warehouse.dir . You can still create Hive tables stored in the Hive warehouse. To do this, include: .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\ .enableHiveSupport() \\ In this case, the tables will be EXTERNAL and stored in the directory specified by hive. metastore.warehouse.dir in hive-site.xml Errors If DB_DRIVER=derby in env var but hive-site.xml has no mssql connection: If you are unable to create folders or perform any operation. It could be the user 1001 is not having enough permissions. This is a typical user Bitnami spark configures to run the container. Server Details Configuration Details Hive Metastore URI spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083 User and Database User dwdas (PASSWORD= Passw0rd ), Database hive_metastore . hive-site.xml Details hive.metastore.warehouse.dir /user/hive/warehouse javax.jdo.option.ConnectionURL jdbc:sqlserver://mssql:1433;databaseName=hive_metastore spark-defaults.conf Details spark.sql.warehouse.dir /data/spark-warehouse Service Configuration Details Spark Server Configuration Image spark:3.5.1-debian-12-r7 User running container 1001:1001 Environment Variables JAVA_HOME=/opt/bitnami/java PYTHONPATH=/opt/bitnami/spark/python/ SPARK_HOME=/opt/bitnami/spark SPARK_USER=spark root: Passw0rd Mounted Volume shared-data:/data SQL Server Configuration Environment Variables SA_PASSWORD=Passw0rd Server name: mssql /opt/mssql/bin/sqlservr Hive Configuration Environment Variables HIVE_HOME=/opt/hive SERVICE_NAME=metastore DB_DRIVER=mssql TEZ_HOME=/opt/tez HIVE_VER=4.0.0 JAVA_HOME=/usr/local/openjdk-8 PWD=/home/dwdas HADOOP_HOME=/opt/hadoop Thrift Server URL thrift://hive-metastore:9083","title":"Spark-Hive_MSSQL"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#setup-a-complete-data-warehouse-with-spark-cluster-external-hivemssql-metastore","text":"Here, I will show you how to create a complete warehouse setup with a Spark Cluster and a standalone Hive with an external metastore database (MSSQL). The setup will use 5 containers: Service URL/Connection Image Spark Master http://localhost:16789/ spark:3.5.1-debian-12-r7 Spark Worker 1 http://localhost:16791/ spark:3.5.1-debian-12-r7 Spark Worker 2 http://localhost:16792/ spark:3.5.1-debian-12-r7 SQL Server (SSMS) localhost,1433 , SQL Server Auth: dwdas/Passw0rd mcr.microsoft.com/mssql/server:2019-latest Hive Metastore thrift://hive-metastore:9083 apache/hive:4.0.0","title":"Setup a complete data warehouse with Spark cluster &amp; external Hive(MSSQL Metastore)"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#for-the-busy-people","text":"Follow these steps to get your setup ready: Download the zip file on your laptop. Go to Step1xx , Step2xx , and Step3xx folders and run run.bat in each. This will create a setup like the one shown below: A shared volume should be present in the volumes with the mapping shown below: Here are some important details about the setup:","title":"For the busy people"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#how-was-the-setup-created","text":"","title":"How was the setup created"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#bit-background-about-bitnami-spark-container","text":"","title":"Bit Background About Bitnami Spark Container"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#their-dockerfile","text":"The official Dockerfile used by Bitnami to create their Spark container can be found on GitHub . Some key parts of the file are: RUN chown -R 1001:1001 /opt/bitnami/spark WORKDIR /opt/bitnami/spark USER 1001 ENTRYPOINT [ \"/opt/bitnami/scripts/spark/entrypoint.sh\" ] CMD [ \"/opt/bitnami/scripts/spark/run.sh\" ]","title":"Their Dockerfile"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#non-root-containers","text":"By default, Bitnami containers run as non-root users. This means when you log into the container, you are not the root user. The container itself runs as a non-root user, but you can change this by specifying User: root in your Dockerfile or Docker-compose. More details can be found here .","title":"Non-Root Containers"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#the-user-1001","text":"Bitnami Spark containers use the non-root user ID 1001. When you start the container and log in, you will be user 1001. It's important that this user has access to all required directories and volumes. Ensure necessary permissions are set when building the container.","title":"The User 1001"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#folder-structure","text":"Bitnami containers store application files in /opt/bitnami/APPNAME/ . For example, Spark files are located in /opt/bitnami/spark . More details can be found in the Bitnami directory structure documentation .","title":"Folder Structure"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#spark-container-setup-details","text":"First, I created the Spark cluster using the Bitnami Spark image bitnami/spark:3.5.1-debian-12-r7 . I chose Bitnami because it is popular and offers a constant version for stability. However, you can also use bitnami/spark:latest . I used a Dockerfile and docker-compose approach. Created a spark-defaults.conf with just one setting: text spark.sql.warehouse.dir = /data/spark-warehouse Created a Dockerfile with the following important activities: Created a /data folder. Copied the custom spark-defaults.conf from laptop to /conf . Set the root password and provided user 1001 permissions to /data folder. Created a docker-compose.yml file that included these changes(rest are usual: ```yaml volumes: shared-data:/data ./spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf environment: SPARK_MODE=master # Environment variable to set the Spark mode to master SPARK_LOCAL_DIRS=/data/tmp # Local directories for Spark ``` Finally, created a run.bat to do the following: ```bat REM Create the Docker volume docker volume create shared-data REM Build and run the Docker Compose services docker-compose -p bitnami-spark-cluster build docker-compose -p bitnami-spark-cluster up -d ```","title":"Spark container setup details"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#mssql-2019-container-setup-details","text":"Setting up the MSSQL container was more straightforward and required less customization than the Spark cluster. I used a Dockerfile and docker-compose approach. Created a Dockerfile with the following important activities: Created a user dwdas with the password Passw0rd . Set the root user password to Passw0rd . Created a docker-compose.yml file that included these changes: ```yaml ports: \"1433:1433\" # Map port 1433 of the host to port 1433 of the container for MSSQL communication. environment: SA_PASSWORD=Passw0rd # Set the system administrator password for MSSQL. networks: - spark-network ``` Finally, created a run.bat to do the following: ```bat REM Create the Docker volumes. This is mapped to all Spark containers and the Hive container. docker volume create shared-data REM Build and run the Docker Compose services docker-compose -p bitnami-spark-cluster build docker-compose -p bitnami-spark-cluster up -d REM Post-Setup: Create MSSQL user and database docker exec -it mssql-container-name /opt/mssql-tools/bin/sqlcmd -S localhost -U SA -P \"Passw0rd\" -Q \"CREATE LOGIN dwdas WITH PASSWORD='Passw0rd'; CREATE DATABASE hive_metastore;\" ``` run.bat: After the container started, I created an MSSQL user dwdas with the password Passw0rd and an empty database hive_metastore . This was done inside the run.bat file.","title":"MSSQL 2019 container setup details"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#hive-server-container-setup-details","text":"Setting up the Hive server can be very tricky. I used the official Apache Hive image with a Dockerfile and docker-compose approach. Download JDBC Driver : Downloaded sqljdbc_7.2.2.0_enu.tar.gz from Microsoft Download Center . Extracted the mssql-jdbc-7.2.2.jre8.jar driver to the current folder. Create a custom hive-site.xml : Created a custom hive-site.xml with MSSQL connection information and directory configurations. Refer to the Step3-Hive/hive-site.xml in the setup folder for more details. Create a Dockerfile : Refer to the dockerfile in Step3-Hive folder for more details. Apart from usual activity these two steps are most immortant. I.e. placing the driver to the /lib and hive-site.xml to conf. dockerfile COPY ./mssql-jdbc-7.2.2.jre8.jar /opt/hive/lib/ COPY ./hive-site.xml /opt/hive/conf/ Docker-compose Configuration : Apart from usual stuff the most important was to the environment variable DB_DRIVER=mssql . Run.bat : The run.bat script builds the images, runs the container.","title":"Hive Server container setup details"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#hive-mssql-connection","text":"Apache Hive containers usually use a startup script called '/entrypoint.sh'. This script is set to use Derby ( ${DB_DRIVER:=derby} ) database by default. To use MSSQL instead: Change the DB_DRIVER environment variable to 'mssql'. Create an empty database called 'hive_metastore' on your MSSQL server. Put your MSSQL connection and db details in the 'hive-site.xml' file. Add the MSSQL driver file (mssql-jdbc-7.2.2.jre8.jar) to the '/lib' folder. /entrypoint.sh #!/bin/bash set -x : ${DB_DRIVER:=derby} SKIP_SCHEMA_INIT=\"${IS_RESUME:-false}\" function initialize_hive { COMMAND=\"-initOrUpgradeSchema\" if [ \"$(echo \"$HIVE_VER\" | cut -d '.' -f1)\" -lt \"4\" ]; then COMMAND=\"-${SCHEMA_COMMAND:-initSchema}\" fi $HIVE_HOME/bin/schematool -dbType $DB_DRIVER $COMMAND if [ $? -eq 0 ]; then echo \"Initialized schema successfully..\" else echo \"Schema initialization failed!\" exit 1 fi } # Additional script content... So, how I did this: During mssql creation I already creatd the hive_metastore empty database and the user The dockerfile managed the copying of the driver and the custom hive-site.xml dockerfile COPY ./mssql-jdbc-7.2.2.jre8.jar /opt/hive/lib/ COPY ./hive-site.xml /opt/hive/conf/ docker-compose managed set the DB_DRIVER=mssql Note: An alternative approach to achieve the same feature would be to: Let the container start with the default settings. Open /entrypoint.sh and change ${DB_DRIVER:=mssql} . Run su -c \"chmod 777 /entrypoint.sh\" with the password Passw0rd as the permission will change. Copy hive-site.xml containing MSSQL connection info to /opt/hive/conf/ . Restart the container.","title":"Hive-MSSQL Connection"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#spark-defaultsconf-hive-sitexml","text":"Only two configureation files were touched. Spark-defaults.conf in spark server and hive-site.xml in hive server.","title":"spark-defaults.conf &amp; hive-site.xml"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#spark-defaultsconf-hive-sitexml_1","text":"","title":"spark-defaults.conf &amp; hive-site.xml"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#spark-defaultsconf-important-details","text":"Here, only one value was added. The same file is present in all the spark containres. spark.sql.warehouse.dir=/data/spark-warehouse","title":"spark-defaults.conf important details"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#hive-site-important-details","text":"<property> <name>hive.metastore.warehouse.dir</name> <value>/data/hive-warehouse</value> </property> <property> <name>javax.jdo.option.ConnectionDriverName</name> <value>com.microsoft.sqlserver.jdbc.SQLServerDriver</value> </property> <property> <name>javax.jdo.option.ConnectionURL</name> <value>jdbc:sqlserver://mssql:1433;DatabaseName=hive_metastore;</value> </property> <property> <property> <name>hive.metastore.uris</name> <value>thrift://hive-metastore:9083</value> </property> <property> <name>hive.metastore.db.type</name> <value>mssql</value> <description> Expects one of [derby, oracle, mysql, mssql, postgres]. </description> </property>","title":"hive-site important details"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#repository-and-metastore-folder-creation","text":"We have two options to decide on the folder structure: Our choice: Separate warehouses for Spark and Hive 1. Keep Hive warehouse as is: - In hive-site.xml : xml <property> <name>hive.metastore.warehouse.dir</name> <value>/data/hive/warehouse</value> </property> 2. Set a different warehouse for Spark: - In Spark configuration: spark.sql.warehouse.dir=/data/spark/warehouse /data \u251c\u2500\u2500 hive-warehouse/ \u2502 \u2514\u2500\u2500 (Hive managed tables) \u251c\u2500\u2500 spark-warehouse/ \u2502 \u2514\u2500\u2500 (Spark-managed tables) Option 2: Use Hive Warehouse for both Spark and Hive 1. Configure Spark to use the Hive metastore: - Set spark.sql.hive.metastore.version and spark.sql.hive.metastore.jars in Spark configuration. 2. Use the same warehouse directory for both: - In hive-site.xml : xml <property> <name>hive.metastore.warehouse.dir</name> <value>/data/spark-warehouse</value> </property> - In Spark configuration. Using( spark-defaults.conf ) or setting during session etc. spark.sql.warehouse.dir=/data/spark-warehouse","title":"Repository and Metastore Folder Creation"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#testing","text":"Testing Hive Go the the hive container and type hive . You should see output lilke this: Testing spark-shell Go to any of hte spark containers's terminal and key in spark-shell , you should see output liek this: Testing spark-sql Go to any of hte spark containers's terminal and key in spark-sql , you should see output liek this:","title":"Testing"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#when-will-be-managed-or-external-tables-created","text":"When configuring Spark to work with Hive, the type of table created (managed vs. external) depends on specific settings in your Spark configuration. If spark-defaults.xml includes: spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083 Then, spark will create all tables as EXTERNAL . Regardless of whatever settigngs you try. If spark-defaults.xml does not include: spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083 Then tables are created as MANAGED by default and stored in the directory specified by spark.sql.warehouse.dir . You can still create Hive tables stored in the Hive warehouse. To do this, include: .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\ .enableHiveSupport() \\ In this case, the tables will be EXTERNAL and stored in the directory specified by hive. metastore.warehouse.dir in hive-site.xml","title":"When will be managed or external tables created?"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#errors","text":"If DB_DRIVER=derby in env var but hive-site.xml has no mssql connection: If you are unable to create folders or perform any operation. It could be the user 1001 is not having enough permissions. This is a typical user Bitnami spark configures to run the container.","title":"Errors"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/#server-details","text":"Configuration Details Hive Metastore URI spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083 User and Database User dwdas (PASSWORD= Passw0rd ), Database hive_metastore . hive-site.xml Details hive.metastore.warehouse.dir /user/hive/warehouse javax.jdo.option.ConnectionURL jdbc:sqlserver://mssql:1433;databaseName=hive_metastore spark-defaults.conf Details spark.sql.warehouse.dir /data/spark-warehouse Service Configuration Details Spark Server Configuration Image spark:3.5.1-debian-12-r7 User running container 1001:1001 Environment Variables JAVA_HOME=/opt/bitnami/java PYTHONPATH=/opt/bitnami/spark/python/ SPARK_HOME=/opt/bitnami/spark SPARK_USER=spark root: Passw0rd Mounted Volume shared-data:/data SQL Server Configuration Environment Variables SA_PASSWORD=Passw0rd Server name: mssql /opt/mssql/bin/sqlservr Hive Configuration Environment Variables HIVE_HOME=/opt/hive SERVICE_NAME=metastore DB_DRIVER=mssql TEZ_HOME=/opt/tez HIVE_VER=4.0.0 JAVA_HOME=/usr/local/openjdk-8 PWD=/home/dwdas HADOOP_HOME=/opt/hadoop Thrift Server URL thrift://hive-metastore:9083","title":"Server Details"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/","text":"Create a Seven-Node Hadoop Container on Docker Introduction Prerequisites Files Used Step 1: Prepare the Docker Environment Step 2: Build and Start the Cluster Step 3: Verify the Setup Conclusion Config reference Reference Create a Single-Node Hadoop Container on Docker Quick Steps for the Busy People Steps to build the container using ready-to-use files Check the setup Detailed steps - building the setup, file by file Details of the files used Building, Running, and Testing the Setup How to test the setup? Setup details Appendix Common Errors and Their Solutions Create a Seven-Node Hadoop Container on Docker Introduction In this guide, I'll walk through the process of setting up an Apache Hadoop cluster using Docker containers. This setup is ideal for development and testing purposes on your local machine. I'll cover the configuration files, container setup, and how to verify that the cluster is functioning correctly. For busy people: Download and unzip the file to a folder CD and run the following commands bash docker-compose build docker-compose up -d You will have a full-fledged Hadoop setup Prerequisites Docker installed on your machine. Basic knowledge of Docker and Hadoop. Files Used Dockerfile : Defines the environment and how the Hadoop services will be run inside the Docker containers. docker-compose.yml : Manages the multi-container application, ensuring all necessary Hadoop services are launched and networked correctly. entrypoint.sh : A script to start the appropriate Hadoop service based on the container's role (e.g., NameNode, DataNode). Important : If you create the entrypoint.sh file on Windows, you must convert it to Unix format using a tool like Toolslick DOS to Unix Converter before using it in your Docker environment. Step 1: Prepare the Docker Environment Dockerfile : The Dockerfile sets up the Java runtime and Hadoop environment. Here's the Dockerfile used: ```Dockerfile # Use Java 8 runtime as base image FROM openjdk:8-jdk # Set environment variables ENV HADOOP_VERSION=2.7.7 ENV HADOOP_HOME=/usr/local/hadoop ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop ENV PATH=$PATH:$HADOOP_HOME/bin # Install Hadoop RUN wget https://archive.apache.org/dist/hadoop/core/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz && \\ tar -xzvf hadoop-$HADOOP_VERSION.tar.gz && \\ mv hadoop-$HADOOP_VERSION $HADOOP_HOME && \\ rm hadoop-$HADOOP_VERSION.tar.gz # Copy and set entrypoint script # Note: If entrypoint.sh is created on Windows, convert it to Unix format using dos2unix website. COPY entrypoint.sh /entrypoint.sh RUN chmod +x /entrypoint.sh # Expose Hadoop ports EXPOSE 50070 8088 9000 9864 9870 9866 9867 # Set entrypoint ENTRYPOINT [\"/entrypoint.sh\"] ``` docker-compose.yml : The docker-compose.yml file orchestrates the Hadoop cluster by defining services like NameNode, DataNode, and ResourceManager. ```yaml version: '3.8' services: namenode: build: . container_name: namenode hostname: namenode environment: - CLUSTER_NAME=my-hadoop-cluster volumes: - namenode_data:/hadoop/dfs/name ports: - \"29070:50070\" # HDFS NameNode Web UI on port 29070 - \"29870:9870\" # NameNode Web UI port on port 29870 - \"29000:9000\" # HDFS port on port 29000 command: namenode networks: - dasnet secondarynamenode: build: . container_name: secondarynamenode hostname: secondarynamenode volumes: - secondarynamenode_data:/hadoop/dfs/secondary command: secondarynamenode networks: - dasnet datanode1: build: . container_name: datanode1 hostname: datanode1 volumes: - datanode1_data:/hadoop/dfs/data command: datanode networks: - dasnet datanode2: build: . container_name: datanode2 hostname: datanode2 volumes: - datanode2_data:/hadoop/dfs/data command: datanode networks: - dasnet resourcemanager: build: . container_name: resourcemanager hostname: resourcemanager ports: - \"28088:8088\" # ResourceManager Web UI on port 28088 command: resourcemanager networks: - dasnet nodemanager1: build: . container_name: nodemanager1 hostname: nodemanager1 ports: - \"29864:9864\" # NodeManager Web UI on port 29864 command: nodemanager networks: - dasnet historyserver: build: . container_name: historyserver hostname: historyserver ports: - \"29866:9866\" # HistoryServer Web UI on port 29866 - \"29867:9867\" # Additional service on port 29867 command: historyserver networks: - dasnet volumes: namenode_data: secondarynamenode_data: datanode1_data: datanode2_data: networks: dasnet: external: true ``` entrypoint.sh : This script starts the appropriate Hadoop service based on the container\u2019s role. Below is the script: bash #!/bin/bash # Format namenode if necessary if [ \"$1\" == \"namenode\" ]; then $HADOOP_HOME/bin/hdfs namenode -format -force -nonInteractive fi # Start SSH service service ssh start # Start Hadoop service based on the role if [ \"$1\" == \"namenode\" ]; then $HADOOP_HOME/sbin/hadoop-daemon.sh start namenode elif [ \"$1\" == \"datanode\" ]; then $HADOOP_HOME/sbin/hadoop-daemon.sh start datanode elif [ \"$1\" == \"secondarynamenode\" ]; then $HADOOP_HOME/sbin/hadoop-daemon.sh start secondarynamenode elif [ \"$1\" == \"resourcemanager\" ]; then $HADOOP_HOME/sbin/yarn-daemon.sh start resourcemanager elif [ \"$1\" == \"nodemanager\" ]; then $HADOOP_HOME/sbin/yarn-daemon.sh start nodemanager elif [ \"$1\" == \"historyserver\" ]; then $HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver fi # Keep the container running tail -f /dev/null Note : Convert entrypoint.sh to Unix format if it's created on Windows using Toolslick DOS to Unix Converter . Step 2: Build and Start the Cluster Build the Docker Images : Navigate to the directory containing the Dockerfile , docker-compose.yml , and entrypoint.sh files. Run the following command to build the Docker images: bash docker-compose build Start the Cluster : Start the cluster using the following command: bash docker-compose up -d Step 3: Verify the Setup Access Hadoop Web UIs : NameNode Web UI : http://localhost:29870 HDFS NameNode Web UI : http://localhost:29070 ResourceManager Web UI : http://localhost:28088 NodeManager Web UI : http://localhost:29864 HistoryServer Web UI : http://localhost:29866 These interfaces will allow you to monitor the status of your Hadoop cluster and the jobs running on it. Run a Test Job : Create Input Directory in HDFS : bash docker exec -it namenode /bin/bash hdfs dfs -mkdir -p /input echo \"Hello Hadoop\" > /tmp/sample.txt hdfs dfs -put /tmp/sample.txt /input/ Run the WordCount Job : bash hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar wordcount /input /output Check the Output : bash hdfs dfs -cat /output/part-r-00000 Expected output: Hadoop 1 Hello 1 Conclusion Remember to convert any scripts created on Windows to Unix format before using them in your Docker containers to avoid potential issues. Happy coding! Config reference Element Location/Value Description Hadoop Installation Dir /usr/local/hadoop The directory where Hadoop is installed inside the Docker containers ( HADOOP_HOME ). Hadoop Config Dir /usr/local/hadoop/etc/hadoop Directory containing Hadoop configuration files ( HADOOP_CONF_DIR ). HDFS Data Directory /hadoop/dfs/name (NameNode), /hadoop/dfs/data (DataNode) Directories used to store HDFS data, mapped to Docker volumes for persistence. Mapped Ports See docker-compose.yml Ports mapped between host and container for accessing Hadoop Web UIs. NameNode Web UI http://localhost:29870 Access URL for NameNode Web UI from the host machine. HDFS NameNode UI http://localhost:29070 Access URL for HDFS NameNode Web UI from the host machine. ResourceManager Web UI http://localhost:28088 Access URL for YARN ResourceManager Web UI from the host machine. NodeManager Web UI http://localhost:29864 Access URL for YARN NodeManager Web UI from the host machine. HistoryServer Web UI http://localhost:29866 Access URL for MapReduce Job HistoryServer Web UI from the host machine. HDFS Input Directory /input in HDFS Directory where input files for MapReduce jobs are stored in HDFS. HDFS Output Directory /output in HDFS Directory where output files from MapReduce jobs are stored in HDFS. Reference Official Hadoop Link Create a Single-Node Hadoop Container on Docker Setting up Hadoop can be quite confusing, especially if you're new to it. The best ready-to-use setups used to be provided by Cloudera, their current docker image(as on Aug 2024) has many issues. In this guide, I\u2019ll help you create a pure Hadoop container with just one node. This guide is divided into two sections: one for quick steps and another for a detailed setup. The container setup has been thoroughly tested and can be created easily. Quick Steps for the Busy People If you\u2019re short on time and don\u2019t want to go through the entire process of setting everything up manually, this section is for you. Just follow the instructions here, use the ready-to-use files, and your container will be up and running in a few minutes! Steps to build the container using ready-to-use files Note: The `hadoop-3.4.0.tar.gz` file is not included in the zip because of its large size. I\u2019ve set up the Dockerfile to download this file automatically from the Apache website if it\u2019s not found in the folder. However, sometimes these download links change. If you encounter any issues with the automatic download, you can manually download the `hadoop-3.4.0.tar.gz` file from this link . If this link doesn\u2019t work, simply search online for the file, download it, and place it in the folder. You don\u2019t need to change anything in the Dockerfile; it will work as long as the file is named `hadoop-3.4.0.tar.gz`. If you create a .sh file or any file on Windows and plan to use it in Linux, make sure you convert it to a Linux-friendly format using this site . Other methods may not work as well. It's important to convert the file, or else it might cause many unexpected errors. Download the hadoop-singlenode.zip file. Unzip it to a folder on your laptop. Open command prompt/terminal and cd to the folder where you unziped the files bash cd path_to_unzipped_folder - Build the doccker image from the Dockerfile. There is a dot bash docker build -t hadoop-singlenode . - Run the container from the built image bash docker run -it --name hadoop-singlenode --network dasnet -p 9870:9870 -p 8088:8088 -v namenode-data:/hadoop/dfs/namenode -v datanode-data:/hadoop/dfs/datanode -v secondarynamenode-data:/hadoop/dfs/secondarynamenode hadoop-singlenode - From inside the running container, start the Hadoop services: bash sudo service ssh start $HADOOP_HOME/sbin/start-dfs.sh $HADOOP_HOME/sbin/start-yarn.sh Check the setup Once the services are up you can access the Hadoop links: HDFS NameNode Web UI: http://localhost:9870 YARN ResourceManager Web UI: http://localhost:8088 Detailed steps - building the setup, file by file If you want to understand how to build the Docker container from scratch or make custom modifications, follow these steps. Details of the files used The setup uses 6 files, Dockerfile , core-site.xml , hdfs-site.xml , mapred-site.xml , yarn-site.xml and hadoop-env.sh Dockerfile: This is the main file. We only use Dockerfile for our setup and no docker-compose.yml. But, you can add a docker-compose if you need. The filename is Dockerfile (no extension) and has the following content: ```bash Use a base image with Java 8 FROM openjdk:8-jdk Set environment variables for Hadoop ENV HADOOP_VERSION=3.4.0 ENV HADOOP_HOME=/usr/local/hadoop ENV JAVA_HOME=/usr/local/openjdk-8 ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin ENV HDFS_NAMENODE_USER=dwdas ENV HDFS_DATANODE_USER=dwdas ENV HDFS_SECONDARYNAMENODE_USER=dwdas ENV YARN_RESOURCEMANAGER_USER=dwdas ENV YARN_NODEMANAGER_USER=dwdas Install necessary packages RUN apt-get update && \\ apt-get install -y ssh rsync sudo wget && \\ apt-get clean Set root password RUN echo \"root:Passw0rd\" | chpasswd Create a user 'dwdas' with sudo privileges RUN useradd -m -s /bin/bash dwdas && \\ echo \"dwdas:Passw0rd\" | chpasswd && \\ usermod -aG sudo dwdas && \\ echo \"dwdas ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers Optional: Download and extract Hadoop tarball if not already provided COPY hadoop-${HADOOP_VERSION}.tar.gz /tmp/ RUN if [ ! -f /tmp/hadoop-${HADOOP_VERSION}.tar.gz ]; then \\ wget https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz -O /tmp/hadoop-${HADOOP_VERSION}.tar.gz; \\ fi && \\ tar -xzf /tmp/hadoop-${HADOOP_VERSION}.tar.gz -C /usr/local && \\ mv /usr/local/hadoop-${HADOOP_VERSION} $HADOOP_HOME && \\ rm /tmp/hadoop-${HADOOP_VERSION}.tar.gz Configure Hadoop - Create necessary directories on Docker volumes RUN mkdir -p /hadoop/dfs/namenode && \\ mkdir -p /hadoop/dfs/datanode && \\ mkdir -p /hadoop/dfs/secondarynamenode Copy configuration files COPY core-site.xml $HADOOP_HOME/etc/hadoop/ COPY hdfs-site.xml $HADOOP_HOME/etc/hadoop/ COPY mapred-site.xml $HADOOP_HOME/etc/hadoop/ COPY yarn-site.xml $HADOOP_HOME/etc/hadoop/ Copy and configure hadoop-env.sh. JAVA_HOME MUST be set here also. COPY hadoop-env.sh $HADOOP_HOME/etc/hadoop/hadoop-env.sh Set ownership for Hadoop directories and volumes to 'dwdas' RUN chown -R dwdas:dwdas $HADOOP_HOME /hadoop/dfs/namenode /hadoop/dfs/datanode /hadoop/dfs/secondarynamenode Switch to the dwdas user for all subsequent operations USER dwdas Create the .ssh directory and set permissions RUN mkdir -p /home/dwdas/.ssh && \\ chmod 700 /home/dwdas/.ssh Generate SSH keys for passwordless SSH login and configure SSH RUN ssh-keygen -t rsa -P '' -f /home/dwdas/.ssh/id_rsa && \\ cat /home/dwdas/.ssh/id_rsa.pub >> /home/dwdas/.ssh/authorized_keys && \\ chmod 600 /home/dwdas/.ssh/authorized_keys && \\ echo \"Host localhost\" >> /home/dwdas/.ssh/config && \\ echo \" StrictHostKeyChecking no\" >> /home/dwdas/.ssh/config && \\ chmod 600 /home/dwdas/.ssh/config Format HDFS as 'dwdas' user RUN $HADOOP_HOME/bin/hdfs namenode -format Expose the necessary ports for Hadoop services EXPOSE 9870 8088 19888 Set the container to start in the dwdas user's home directory WORKDIR /home/dwdas Set the container to start with a bash shell CMD [\"bash\"] - **core-site.xml**: Configures the default filesystem and permissions. xml fs.defaultFS hdfs://localhost:9000 dfs.permissions false ``` hdfs-site.xml : Configures the NameNode and DataNode directories. xml <configuration> <property> <name>dfs.namenode.name.dir</name> <value>file:///usr/local/hadoop/tmp/hdfs/namenode</value> </property> <property> <name>dfs.datanode.data.dir</name> <value>file:///usr/local/hadoop/tmp/hdfs/datanode</value> </property> <property> <name>dfs.replication</name> <value>1</value> </property> <property> <name>dfs.permissions.superusergroup</name> <value>dwdas</value> </property> <property> <name>dfs.cluster.administrators</name> <value>dwdas</value> </property> </configuration> mapred-site.xml : Configures the MapReduce framework to use YARN. xml <configuration> <property> <name>mapreduce.framework.name</name> <value>yarn</value> </property> <property> <name>mapred.job.tracker</name> <value>hadoop-master:9001</value> </property> </configuration> yarn-site.xml : Configures YARN services. xml <configuration> <property> <name>yarn.nodemanager.aux-services</name> <value>mapreduce_shuffle</value> </property> <property> <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name> <value>org.apache.hadoop.mapred.ShuffleHandler</value> </property> </configuration> hadoop-env.sh: I haven\u2019t included this entire file because it\u2019s large, and there\u2019s only one change you need to make. Refer to the hadoop-env.sh file in the zip folder. The only modification required is shown below. Note: This step is crucial. Without this, Hadoop will give an error saying it can\u2019t find JAVA_HOME , even if you\u2019ve already set it as an environment variable. This change follows Apache's instructions. bash export JAVA_HOME=/usr/local/openjdk-8 Building, Running, and Testing the Setup The process for building, running, and testing the setup is the same as described in the Quick Steps section . Simply navigate to the folder, build the container, and then run it as before. How to test the setup? The table below shows how various components and functionalities could be tested: Category Action Command/URL What to Look For Verify HDFS Check HDFS status hdfs dfsadmin -report A detailed report on the HDFS cluster, showing live nodes, configured capacity, used space, etc. Browse HDFS NameNode Web UI http://localhost:9870 The HDFS NameNode web interface should load, showing the health of the file system. Create a directory hdfs dfs -mkdir /test No errors, and the directory /test should be created successfully in HDFS. List directory contents hdfs dfs -ls / The newly created /test directory should be listed. Verify YARN Check YARN NodeManager status yarn node -list A list of nodes managed by YARN, showing their status (e.g., healthy, active). Browse YARN ResourceManager Web UI http://localhost:8088 The YARN ResourceManager web interface should load, showing job and node statuses. Verify Hadoop Services Check running services jps List of Java processes such as NameNode , DataNode , ResourceManager , and NodeManager . Test MapReduce Run a test MapReduce job hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar wordcount /test /output The MapReduce job should complete successfully, creating an output directory in HDFS. Verify MapReduce output hdfs dfs -ls /output The /output directory should contain the results of the MapReduce job. Setup details Component Item Location/Value Description Hadoop Installation Directory /usr/local/hadoop Hadoop installation directory. Version 3.4.0 Hadoop version. Core Config $HADOOP_HOME/etc/hadoop/core-site.xml Core configuration file. HDFS Config $HADOOP_HOME/etc/hadoop/hdfs-site.xml HDFS configuration file. MapReduce Config $HADOOP_HOME/etc/hadoop/mapred-site.xml MapReduce configuration file. YARN Config $HADOOP_HOME/etc/hadoop/yarn-site.xml YARN configuration file. Env Variables $HADOOP_HOME/etc/hadoop/hadoop-env.sh Hadoop environment variables. HDFS Directories NameNode /hadoop/dfs/namenode HDFS NameNode data directory (Docker volume). DataNode /hadoop/dfs/datanode HDFS DataNode data directory (Docker volume). Secondary NameNode /hadoop/dfs/secondarynamenode Secondary NameNode directory (Docker volume). Environment Variables Hadoop Home /usr/local/hadoop Path to Hadoop installation. Java Home /usr/local/openjdk-8 Path to Java installation. System Path $PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin Updated path including Hadoop binaries. HDFS_NAMENODE_USER dwdas User for NameNode service. HDFS_DATANODE_USER dwdas User for DataNode service. HDFS_SECONDARYNAMENODE_USER dwdas User for Secondary NameNode service. YARN_RESOURCEMANAGER_USER dwdas User for ResourceManager service. YARN_NODEMANAGER_USER dwdas User for NodeManager service. Networking Docker Network dasnet Docker network for the Hadoop container. Ports Mapped HDFS NameNode UI 9870:9870 Port mapping for HDFS NameNode web interface. YARN ResourceManager UI 8088:8088 Port mapping for YARN ResourceManager web interface. Appendix Alternate command - Start the Namenode sh $HADOOP_HOME/sbin/hadoop-daemon.sh start namenode Alternate command - Start the Datanode sh $HADOOP_HOME/sbin/hadoop-daemon.sh start datanode Enter jps to see all the hadoop services Command to get a report on the Hadoop setup sh hdfs dfsadmin -report Common Errors and Their Solutions Port Binding Error : Error : Ports are not available: exposing port TCP 0.0.0.0:50070 -> 0.0.0.0:0: listen tcp 0.0.0.0:50070: bind: An attempt was made to access a socket in a way forbidden by its access permissions. Solution : Make sure that the ports you\u2019re using aren\u2019t already in use by other services. You can check and stop any process using these ports with these commands: sh netstat -aon | findstr :50070 taskkill /PID <PID> /F Or you can change the port numbers in the Dockerfile and docker run command. Permission Denied Error : Error : ERROR: Attempting to operate on hdfs namenode as root but there is no HDFS_NAMENODE_USER defined. Solution : Hadoop services should not run as the root user. The Dockerfile sets up a non-root user ( dwdas ) to run Hadoop and Hive services: Dockerfile ENV HDFS_NAMENODE_USER=dwdas ENV HDFS_DATANODE_USER=dwdas ENV HDFS_SECONDARYNAMENODE_USER=dwdas ENV YARN_RESOURCEMANAGER_USER=dwdas ENV YARN_NODEMANAGER_USER=dwdas Multiple SLF4J Bindings Error : Error : SLF4J: Class path contains multiple SLF4J bindings. Solution : This is usually just a warning, not an error. It means there are multiple SLF4J bindings in the classpath. It can generally be ignored, but if it causes issues, you might need to clean up the classpath by removing conflicting SLF4J jars.","title":"Hadoop Cluster"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#create-a-seven-node-hadoop-container-on-docker","text":"","title":"Create a Seven-Node Hadoop Container on Docker"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#introduction","text":"In this guide, I'll walk through the process of setting up an Apache Hadoop cluster using Docker containers. This setup is ideal for development and testing purposes on your local machine. I'll cover the configuration files, container setup, and how to verify that the cluster is functioning correctly. For busy people: Download and unzip the file to a folder CD and run the following commands bash docker-compose build docker-compose up -d You will have a full-fledged Hadoop setup","title":"Introduction"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#prerequisites","text":"Docker installed on your machine. Basic knowledge of Docker and Hadoop.","title":"Prerequisites"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#files-used","text":"Dockerfile : Defines the environment and how the Hadoop services will be run inside the Docker containers. docker-compose.yml : Manages the multi-container application, ensuring all necessary Hadoop services are launched and networked correctly. entrypoint.sh : A script to start the appropriate Hadoop service based on the container's role (e.g., NameNode, DataNode). Important : If you create the entrypoint.sh file on Windows, you must convert it to Unix format using a tool like Toolslick DOS to Unix Converter before using it in your Docker environment.","title":"Files Used"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#step-1-prepare-the-docker-environment","text":"Dockerfile : The Dockerfile sets up the Java runtime and Hadoop environment. Here's the Dockerfile used: ```Dockerfile # Use Java 8 runtime as base image FROM openjdk:8-jdk # Set environment variables ENV HADOOP_VERSION=2.7.7 ENV HADOOP_HOME=/usr/local/hadoop ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop ENV PATH=$PATH:$HADOOP_HOME/bin # Install Hadoop RUN wget https://archive.apache.org/dist/hadoop/core/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz && \\ tar -xzvf hadoop-$HADOOP_VERSION.tar.gz && \\ mv hadoop-$HADOOP_VERSION $HADOOP_HOME && \\ rm hadoop-$HADOOP_VERSION.tar.gz # Copy and set entrypoint script # Note: If entrypoint.sh is created on Windows, convert it to Unix format using dos2unix website. COPY entrypoint.sh /entrypoint.sh RUN chmod +x /entrypoint.sh # Expose Hadoop ports EXPOSE 50070 8088 9000 9864 9870 9866 9867 # Set entrypoint ENTRYPOINT [\"/entrypoint.sh\"] ``` docker-compose.yml : The docker-compose.yml file orchestrates the Hadoop cluster by defining services like NameNode, DataNode, and ResourceManager. ```yaml version: '3.8' services: namenode: build: . container_name: namenode hostname: namenode environment: - CLUSTER_NAME=my-hadoop-cluster volumes: - namenode_data:/hadoop/dfs/name ports: - \"29070:50070\" # HDFS NameNode Web UI on port 29070 - \"29870:9870\" # NameNode Web UI port on port 29870 - \"29000:9000\" # HDFS port on port 29000 command: namenode networks: - dasnet secondarynamenode: build: . container_name: secondarynamenode hostname: secondarynamenode volumes: - secondarynamenode_data:/hadoop/dfs/secondary command: secondarynamenode networks: - dasnet datanode1: build: . container_name: datanode1 hostname: datanode1 volumes: - datanode1_data:/hadoop/dfs/data command: datanode networks: - dasnet datanode2: build: . container_name: datanode2 hostname: datanode2 volumes: - datanode2_data:/hadoop/dfs/data command: datanode networks: - dasnet resourcemanager: build: . container_name: resourcemanager hostname: resourcemanager ports: - \"28088:8088\" # ResourceManager Web UI on port 28088 command: resourcemanager networks: - dasnet nodemanager1: build: . container_name: nodemanager1 hostname: nodemanager1 ports: - \"29864:9864\" # NodeManager Web UI on port 29864 command: nodemanager networks: - dasnet historyserver: build: . container_name: historyserver hostname: historyserver ports: - \"29866:9866\" # HistoryServer Web UI on port 29866 - \"29867:9867\" # Additional service on port 29867 command: historyserver networks: - dasnet volumes: namenode_data: secondarynamenode_data: datanode1_data: datanode2_data: networks: dasnet: external: true ``` entrypoint.sh : This script starts the appropriate Hadoop service based on the container\u2019s role. Below is the script: bash #!/bin/bash # Format namenode if necessary if [ \"$1\" == \"namenode\" ]; then $HADOOP_HOME/bin/hdfs namenode -format -force -nonInteractive fi # Start SSH service service ssh start # Start Hadoop service based on the role if [ \"$1\" == \"namenode\" ]; then $HADOOP_HOME/sbin/hadoop-daemon.sh start namenode elif [ \"$1\" == \"datanode\" ]; then $HADOOP_HOME/sbin/hadoop-daemon.sh start datanode elif [ \"$1\" == \"secondarynamenode\" ]; then $HADOOP_HOME/sbin/hadoop-daemon.sh start secondarynamenode elif [ \"$1\" == \"resourcemanager\" ]; then $HADOOP_HOME/sbin/yarn-daemon.sh start resourcemanager elif [ \"$1\" == \"nodemanager\" ]; then $HADOOP_HOME/sbin/yarn-daemon.sh start nodemanager elif [ \"$1\" == \"historyserver\" ]; then $HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver fi # Keep the container running tail -f /dev/null Note : Convert entrypoint.sh to Unix format if it's created on Windows using Toolslick DOS to Unix Converter .","title":"Step 1: Prepare the Docker Environment"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#step-2-build-and-start-the-cluster","text":"Build the Docker Images : Navigate to the directory containing the Dockerfile , docker-compose.yml , and entrypoint.sh files. Run the following command to build the Docker images: bash docker-compose build Start the Cluster : Start the cluster using the following command: bash docker-compose up -d","title":"Step 2: Build and Start the Cluster"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#step-3-verify-the-setup","text":"Access Hadoop Web UIs : NameNode Web UI : http://localhost:29870 HDFS NameNode Web UI : http://localhost:29070 ResourceManager Web UI : http://localhost:28088 NodeManager Web UI : http://localhost:29864 HistoryServer Web UI : http://localhost:29866 These interfaces will allow you to monitor the status of your Hadoop cluster and the jobs running on it. Run a Test Job : Create Input Directory in HDFS : bash docker exec -it namenode /bin/bash hdfs dfs -mkdir -p /input echo \"Hello Hadoop\" > /tmp/sample.txt hdfs dfs -put /tmp/sample.txt /input/ Run the WordCount Job : bash hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar wordcount /input /output Check the Output : bash hdfs dfs -cat /output/part-r-00000 Expected output: Hadoop 1 Hello 1","title":"Step 3: Verify the Setup"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#conclusion","text":"Remember to convert any scripts created on Windows to Unix format before using them in your Docker containers to avoid potential issues. Happy coding!","title":"Conclusion"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#config-reference","text":"Element Location/Value Description Hadoop Installation Dir /usr/local/hadoop The directory where Hadoop is installed inside the Docker containers ( HADOOP_HOME ). Hadoop Config Dir /usr/local/hadoop/etc/hadoop Directory containing Hadoop configuration files ( HADOOP_CONF_DIR ). HDFS Data Directory /hadoop/dfs/name (NameNode), /hadoop/dfs/data (DataNode) Directories used to store HDFS data, mapped to Docker volumes for persistence. Mapped Ports See docker-compose.yml Ports mapped between host and container for accessing Hadoop Web UIs. NameNode Web UI http://localhost:29870 Access URL for NameNode Web UI from the host machine. HDFS NameNode UI http://localhost:29070 Access URL for HDFS NameNode Web UI from the host machine. ResourceManager Web UI http://localhost:28088 Access URL for YARN ResourceManager Web UI from the host machine. NodeManager Web UI http://localhost:29864 Access URL for YARN NodeManager Web UI from the host machine. HistoryServer Web UI http://localhost:29866 Access URL for MapReduce Job HistoryServer Web UI from the host machine. HDFS Input Directory /input in HDFS Directory where input files for MapReduce jobs are stored in HDFS. HDFS Output Directory /output in HDFS Directory where output files from MapReduce jobs are stored in HDFS.","title":"Config reference"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#reference","text":"Official Hadoop Link","title":"Reference"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#create-a-single-node-hadoop-container-on-docker","text":"Setting up Hadoop can be quite confusing, especially if you're new to it. The best ready-to-use setups used to be provided by Cloudera, their current docker image(as on Aug 2024) has many issues. In this guide, I\u2019ll help you create a pure Hadoop container with just one node. This guide is divided into two sections: one for quick steps and another for a detailed setup. The container setup has been thoroughly tested and can be created easily.","title":"Create a Single-Node Hadoop Container on Docker"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#quick-steps-for-the-busy-people","text":"If you\u2019re short on time and don\u2019t want to go through the entire process of setting everything up manually, this section is for you. Just follow the instructions here, use the ready-to-use files, and your container will be up and running in a few minutes!","title":"Quick Steps for the Busy People"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#steps-to-build-the-container-using-ready-to-use-files","text":"Note: The `hadoop-3.4.0.tar.gz` file is not included in the zip because of its large size. I\u2019ve set up the Dockerfile to download this file automatically from the Apache website if it\u2019s not found in the folder. However, sometimes these download links change. If you encounter any issues with the automatic download, you can manually download the `hadoop-3.4.0.tar.gz` file from this link . If this link doesn\u2019t work, simply search online for the file, download it, and place it in the folder. You don\u2019t need to change anything in the Dockerfile; it will work as long as the file is named `hadoop-3.4.0.tar.gz`. If you create a .sh file or any file on Windows and plan to use it in Linux, make sure you convert it to a Linux-friendly format using this site . Other methods may not work as well. It's important to convert the file, or else it might cause many unexpected errors. Download the hadoop-singlenode.zip file. Unzip it to a folder on your laptop. Open command prompt/terminal and cd to the folder where you unziped the files bash cd path_to_unzipped_folder - Build the doccker image from the Dockerfile. There is a dot bash docker build -t hadoop-singlenode . - Run the container from the built image bash docker run -it --name hadoop-singlenode --network dasnet -p 9870:9870 -p 8088:8088 -v namenode-data:/hadoop/dfs/namenode -v datanode-data:/hadoop/dfs/datanode -v secondarynamenode-data:/hadoop/dfs/secondarynamenode hadoop-singlenode - From inside the running container, start the Hadoop services: bash sudo service ssh start $HADOOP_HOME/sbin/start-dfs.sh $HADOOP_HOME/sbin/start-yarn.sh","title":"Steps to build the container using ready-to-use files"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#check-the-setup","text":"Once the services are up you can access the Hadoop links: HDFS NameNode Web UI: http://localhost:9870 YARN ResourceManager Web UI: http://localhost:8088","title":"Check the setup"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#detailed-steps-building-the-setup-file-by-file","text":"If you want to understand how to build the Docker container from scratch or make custom modifications, follow these steps.","title":"Detailed steps - building the setup, file by file"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#details-of-the-files-used","text":"The setup uses 6 files, Dockerfile , core-site.xml , hdfs-site.xml , mapred-site.xml , yarn-site.xml and hadoop-env.sh Dockerfile: This is the main file. We only use Dockerfile for our setup and no docker-compose.yml. But, you can add a docker-compose if you need. The filename is Dockerfile (no extension) and has the following content: ```bash","title":"Details of the files used"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#use-a-base-image-with-java-8","text":"FROM openjdk:8-jdk","title":"Use a base image with Java 8"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#set-environment-variables-for-hadoop","text":"ENV HADOOP_VERSION=3.4.0 ENV HADOOP_HOME=/usr/local/hadoop ENV JAVA_HOME=/usr/local/openjdk-8 ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin ENV HDFS_NAMENODE_USER=dwdas ENV HDFS_DATANODE_USER=dwdas ENV HDFS_SECONDARYNAMENODE_USER=dwdas ENV YARN_RESOURCEMANAGER_USER=dwdas ENV YARN_NODEMANAGER_USER=dwdas","title":"Set environment variables for Hadoop"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#install-necessary-packages","text":"RUN apt-get update && \\ apt-get install -y ssh rsync sudo wget && \\ apt-get clean","title":"Install necessary packages"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#set-root-password","text":"RUN echo \"root:Passw0rd\" | chpasswd","title":"Set root password"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#create-a-user-dwdas-with-sudo-privileges","text":"RUN useradd -m -s /bin/bash dwdas && \\ echo \"dwdas:Passw0rd\" | chpasswd && \\ usermod -aG sudo dwdas && \\ echo \"dwdas ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers","title":"Create a user 'dwdas' with sudo privileges"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#optional-download-and-extract-hadoop-tarball-if-not-already-provided","text":"COPY hadoop-${HADOOP_VERSION}.tar.gz /tmp/ RUN if [ ! -f /tmp/hadoop-${HADOOP_VERSION}.tar.gz ]; then \\ wget https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz -O /tmp/hadoop-${HADOOP_VERSION}.tar.gz; \\ fi && \\ tar -xzf /tmp/hadoop-${HADOOP_VERSION}.tar.gz -C /usr/local && \\ mv /usr/local/hadoop-${HADOOP_VERSION} $HADOOP_HOME && \\ rm /tmp/hadoop-${HADOOP_VERSION}.tar.gz","title":"Optional: Download and extract Hadoop tarball if not already provided"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#configure-hadoop-create-necessary-directories-on-docker-volumes","text":"RUN mkdir -p /hadoop/dfs/namenode && \\ mkdir -p /hadoop/dfs/datanode && \\ mkdir -p /hadoop/dfs/secondarynamenode","title":"Configure Hadoop - Create necessary directories on Docker volumes"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#copy-configuration-files","text":"COPY core-site.xml $HADOOP_HOME/etc/hadoop/ COPY hdfs-site.xml $HADOOP_HOME/etc/hadoop/ COPY mapred-site.xml $HADOOP_HOME/etc/hadoop/ COPY yarn-site.xml $HADOOP_HOME/etc/hadoop/","title":"Copy configuration files"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#copy-and-configure-hadoop-envsh-java_home-must-be-set-here-also","text":"COPY hadoop-env.sh $HADOOP_HOME/etc/hadoop/hadoop-env.sh","title":"Copy and configure hadoop-env.sh. JAVA_HOME MUST be set here also."},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#set-ownership-for-hadoop-directories-and-volumes-to-dwdas","text":"RUN chown -R dwdas:dwdas $HADOOP_HOME /hadoop/dfs/namenode /hadoop/dfs/datanode /hadoop/dfs/secondarynamenode","title":"Set ownership for Hadoop directories and volumes to 'dwdas'"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#switch-to-the-dwdas-user-for-all-subsequent-operations","text":"USER dwdas","title":"Switch to the dwdas user for all subsequent operations"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#create-the-ssh-directory-and-set-permissions","text":"RUN mkdir -p /home/dwdas/.ssh && \\ chmod 700 /home/dwdas/.ssh","title":"Create the .ssh directory and set permissions"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#generate-ssh-keys-for-passwordless-ssh-login-and-configure-ssh","text":"RUN ssh-keygen -t rsa -P '' -f /home/dwdas/.ssh/id_rsa && \\ cat /home/dwdas/.ssh/id_rsa.pub >> /home/dwdas/.ssh/authorized_keys && \\ chmod 600 /home/dwdas/.ssh/authorized_keys && \\ echo \"Host localhost\" >> /home/dwdas/.ssh/config && \\ echo \" StrictHostKeyChecking no\" >> /home/dwdas/.ssh/config && \\ chmod 600 /home/dwdas/.ssh/config","title":"Generate SSH keys for passwordless SSH login and configure SSH"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#format-hdfs-as-dwdas-user","text":"RUN $HADOOP_HOME/bin/hdfs namenode -format","title":"Format HDFS as 'dwdas' user"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#expose-the-necessary-ports-for-hadoop-services","text":"EXPOSE 9870 8088 19888","title":"Expose the necessary ports for Hadoop services"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#set-the-container-to-start-in-the-dwdas-users-home-directory","text":"WORKDIR /home/dwdas","title":"Set the container to start in the dwdas user's home directory"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#set-the-container-to-start-with-a-bash-shell","text":"CMD [\"bash\"] - **core-site.xml**: Configures the default filesystem and permissions. xml fs.defaultFS hdfs://localhost:9000 dfs.permissions false ``` hdfs-site.xml : Configures the NameNode and DataNode directories. xml <configuration> <property> <name>dfs.namenode.name.dir</name> <value>file:///usr/local/hadoop/tmp/hdfs/namenode</value> </property> <property> <name>dfs.datanode.data.dir</name> <value>file:///usr/local/hadoop/tmp/hdfs/datanode</value> </property> <property> <name>dfs.replication</name> <value>1</value> </property> <property> <name>dfs.permissions.superusergroup</name> <value>dwdas</value> </property> <property> <name>dfs.cluster.administrators</name> <value>dwdas</value> </property> </configuration> mapred-site.xml : Configures the MapReduce framework to use YARN. xml <configuration> <property> <name>mapreduce.framework.name</name> <value>yarn</value> </property> <property> <name>mapred.job.tracker</name> <value>hadoop-master:9001</value> </property> </configuration> yarn-site.xml : Configures YARN services. xml <configuration> <property> <name>yarn.nodemanager.aux-services</name> <value>mapreduce_shuffle</value> </property> <property> <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name> <value>org.apache.hadoop.mapred.ShuffleHandler</value> </property> </configuration> hadoop-env.sh: I haven\u2019t included this entire file because it\u2019s large, and there\u2019s only one change you need to make. Refer to the hadoop-env.sh file in the zip folder. The only modification required is shown below. Note: This step is crucial. Without this, Hadoop will give an error saying it can\u2019t find JAVA_HOME , even if you\u2019ve already set it as an environment variable. This change follows Apache's instructions. bash export JAVA_HOME=/usr/local/openjdk-8","title":"Set the container to start with a bash shell"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#building-running-and-testing-the-setup","text":"The process for building, running, and testing the setup is the same as described in the Quick Steps section . Simply navigate to the folder, build the container, and then run it as before.","title":"Building, Running, and Testing the Setup"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#how-to-test-the-setup","text":"The table below shows how various components and functionalities could be tested: Category Action Command/URL What to Look For Verify HDFS Check HDFS status hdfs dfsadmin -report A detailed report on the HDFS cluster, showing live nodes, configured capacity, used space, etc. Browse HDFS NameNode Web UI http://localhost:9870 The HDFS NameNode web interface should load, showing the health of the file system. Create a directory hdfs dfs -mkdir /test No errors, and the directory /test should be created successfully in HDFS. List directory contents hdfs dfs -ls / The newly created /test directory should be listed. Verify YARN Check YARN NodeManager status yarn node -list A list of nodes managed by YARN, showing their status (e.g., healthy, active). Browse YARN ResourceManager Web UI http://localhost:8088 The YARN ResourceManager web interface should load, showing job and node statuses. Verify Hadoop Services Check running services jps List of Java processes such as NameNode , DataNode , ResourceManager , and NodeManager . Test MapReduce Run a test MapReduce job hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar wordcount /test /output The MapReduce job should complete successfully, creating an output directory in HDFS. Verify MapReduce output hdfs dfs -ls /output The /output directory should contain the results of the MapReduce job.","title":"How to test the setup?"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#setup-details","text":"Component Item Location/Value Description Hadoop Installation Directory /usr/local/hadoop Hadoop installation directory. Version 3.4.0 Hadoop version. Core Config $HADOOP_HOME/etc/hadoop/core-site.xml Core configuration file. HDFS Config $HADOOP_HOME/etc/hadoop/hdfs-site.xml HDFS configuration file. MapReduce Config $HADOOP_HOME/etc/hadoop/mapred-site.xml MapReduce configuration file. YARN Config $HADOOP_HOME/etc/hadoop/yarn-site.xml YARN configuration file. Env Variables $HADOOP_HOME/etc/hadoop/hadoop-env.sh Hadoop environment variables. HDFS Directories NameNode /hadoop/dfs/namenode HDFS NameNode data directory (Docker volume). DataNode /hadoop/dfs/datanode HDFS DataNode data directory (Docker volume). Secondary NameNode /hadoop/dfs/secondarynamenode Secondary NameNode directory (Docker volume). Environment Variables Hadoop Home /usr/local/hadoop Path to Hadoop installation. Java Home /usr/local/openjdk-8 Path to Java installation. System Path $PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin Updated path including Hadoop binaries. HDFS_NAMENODE_USER dwdas User for NameNode service. HDFS_DATANODE_USER dwdas User for DataNode service. HDFS_SECONDARYNAMENODE_USER dwdas User for Secondary NameNode service. YARN_RESOURCEMANAGER_USER dwdas User for ResourceManager service. YARN_NODEMANAGER_USER dwdas User for NodeManager service. Networking Docker Network dasnet Docker network for the Hadoop container. Ports Mapped HDFS NameNode UI 9870:9870 Port mapping for HDFS NameNode web interface. YARN ResourceManager UI 8088:8088 Port mapping for YARN ResourceManager web interface.","title":"Setup details"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#appendix","text":"Alternate command - Start the Namenode sh $HADOOP_HOME/sbin/hadoop-daemon.sh start namenode Alternate command - Start the Datanode sh $HADOOP_HOME/sbin/hadoop-daemon.sh start datanode Enter jps to see all the hadoop services Command to get a report on the Hadoop setup sh hdfs dfsadmin -report","title":"Appendix"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/#common-errors-and-their-solutions","text":"Port Binding Error : Error : Ports are not available: exposing port TCP 0.0.0.0:50070 -> 0.0.0.0:0: listen tcp 0.0.0.0:50070: bind: An attempt was made to access a socket in a way forbidden by its access permissions. Solution : Make sure that the ports you\u2019re using aren\u2019t already in use by other services. You can check and stop any process using these ports with these commands: sh netstat -aon | findstr :50070 taskkill /PID <PID> /F Or you can change the port numbers in the Dockerfile and docker run command. Permission Denied Error : Error : ERROR: Attempting to operate on hdfs namenode as root but there is no HDFS_NAMENODE_USER defined. Solution : Hadoop services should not run as the root user. The Dockerfile sets up a non-root user ( dwdas ) to run Hadoop and Hive services: Dockerfile ENV HDFS_NAMENODE_USER=dwdas ENV HDFS_DATANODE_USER=dwdas ENV HDFS_SECONDARYNAMENODE_USER=dwdas ENV YARN_RESOURCEMANAGER_USER=dwdas ENV YARN_NODEMANAGER_USER=dwdas Multiple SLF4J Bindings Error : Error : SLF4J: Class path contains multiple SLF4J bindings. Solution : This is usually just a warning, not an error. It means there are multiple SLF4J bindings in the classpath. It can generally be ignored, but if it causes issues, you might need to clean up the classpath by removing conflicting SLF4J jars.","title":"Common Errors and Their Solutions"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/","text":"Setting Up a Hadoop, Hive, and Presto Cluster Using Docker Compose This guide will walk you through setting up a Hadoop ecosystem including HDFS, YARN, Hive, and Presto using Docker Compose. This setup will allow you to perform distributed data processing and SQL-based querying on large datasets. The services are containerized, making it easy to deploy and manage. Overview of the Setup This setup involves several Docker containers, each providing a different service in the Hadoop ecosystem: HDFS : Provides a distributed file system that stores data across multiple nodes. YARN : Manages resources and job scheduling across the cluster. Hive : A data warehouse that allows SQL-like querying on data stored in HDFS. Presto : A distributed SQL query engine that allows querying large datasets from multiple sources. The entire setup can be managed using Docker Compose, which simplifies the orchestration of multiple containers. Using the Setup To use this setup, follow these steps: Download the Zip File : Download the provided zip file that contains all the necessary configuration files and scripts. Unzip the File : Extract the contents of the zip file to a directory of your choice: bash unzip hadoop-hive-presto-setup.zip cd hadoop-hive-presto-setup Run the Setup : Start the entire setup by running the following command: bash docker-compose up -d This command will spin up all the necessary containers and services. Access the Services : You can access the various services via their respective ports as described below. Description of Services and Ports Description of Services, Ports, and Folder Paths Below is a detailed description of the services included in this setup, along with the ports used, relevant folder paths, and user access information. This will help both users and administrators to understand and manage the setup effectively. Service Port Description Folder Path User Access Details Namenode (HDFS) 50070 Web UI for monitoring the HDFS Namenode. /hadoop/dfs/name root Used for monitoring the HDFS cluster, filesystem namespace management. Datanode (HDFS) 50075 Web UI for monitoring Datanode data transfer. /hadoop/dfs/data root Stores actual data blocks, communicates with the Namenode. ResourceManager (YARN) 8088 Web UI for monitoring resource allocation and job scheduling. /rmstate root Manages resources across the cluster, schedules and monitors jobs. NodeManager (YARN) 8042 Web UI for monitoring resource usage on individual nodes. /app-logs root Manages containers on each node, monitors resource usage. HistoryServer (YARN) 19888 Web UI for accessing logs and statistics of completed MapReduce jobs. /hadoop/yarn/timeline root Provides access to history logs and statistics of completed jobs. HiveServer2 (Hive) 10000 Service for executing SQL-based queries via JDBC/ODBC. /user/hive/warehouse hive Used for running SQL queries on data stored in HDFS via Hive. Hive Metastore 9083 Service for managing metadata for Hive tables. /user/hive/warehouse (metadata) hive Stores metadata for Hive tables, accessed by HiveServer2 and clients. Hive Metastore PostgreSQL N/A Internal PostgreSQL database for Hive Metastore metadata storage. /var/lib/postgresql/data (inside PostgreSQL container) postgres Stores metadata about Hive tables, partitions, etc., used by the Metastore. Presto Coordinator 8081 Web UI for monitoring queries and worker nodes in Presto. N/A root Provides a distributed SQL query engine to run queries across data sources. Hive Table and Data Location Hive Tables : The tables created in Hive are stored in HDFS under the directory /user/hive/warehouse/ . HDFS Data : All data stored in HDFS, including Hive table data, is managed by the Namenode and Datanode services. The default directory for Hive tables is /user/hive/warehouse/ . Users and Access root : The root user has access to all the Hadoop services and directories. hive : The hive user has access to Hive-specific directories and can execute SQL queries via HiveServer2. postgres : The postgres user is the database administrator for the PostgreSQL database used by the Hive Metastore. Testing the Setup 1. Connecting to Hive To test the Hive setup, you can connect to HiveServer2 using Beeline: docker exec -it hive-server2 beeline -u jdbc:hive2://localhost:10000 -n hive -p hive 2. Creating and Managing Hive Tables Once connected to Hive, you can create databases, tables, and insert data: CREATE DATABASE test_db; USE test_db; CREATE TABLE test_table (id INT, name STRING); INSERT INTO test_table VALUES (1, 'Hadoop'), (2, 'Hive'); SELECT * FROM test_table; 3. Checking HDFS To verify that the data is stored correctly in HDFS, you can use the following command: docker exec -it namenode hadoop fs -ls /user/hive/warehouse/test_db.db/test_table This command will list the files in the specified Hive table directory in HDFS. 4. Monitoring Services HDFS Namenode Web UI : http://localhost:50070 YARN ResourceManager Web UI : http://localhost:8088 YARN NodeManager Web UI : http://localhost:8042 YARN HistoryServer Web UI : http://localhost:19888 Presto Web UI : http://localhost:8081 These interfaces provide detailed information about the health and status of the services running in your cluster. Testing the Setup To ensure that your setup is working correctly, you can perform the following tests: Connect to Hive : Use the Beeline CLI to connect to HiveServer2: bash docker exec -it hive-server2 beeline -u jdbc:hive2://localhost:10000 -n hive -p hive Once connected, create a database and a table: sql CREATE DATABASE test_db; USE test_db; CREATE TABLE test_table (id INT, name STRING); Insert some data into the table: sql INSERT INTO test_table VALUES (1, 'Hadoop'), (2, 'Hive'); Query the data to verify the setup: sql SELECT * FROM test_table; Check HDFS : Ensure that the data was correctly stored in HDFS: bash docker exec -it namenode hadoop fs -ls /user/hive/warehouse/test_db.db/test_table You should see the files corresponding to the inserted data. Monitor YARN Jobs : Visit the ResourceManager Web UI at http://localhost:8088 to monitor job execution. Access the Presto UI : Visit the Presto Web UI at http://localhost:8081 to monitor queries and worker nodes. Conclusion This setup provides a robust environment for distributed data processing and querying using Hadoop, Hive, and Presto. By containerizing these services, you can easily manage and scale your data processing infrastructure. Testing the setup with sample data ensures that all components are correctly configured and working together seamlessly. Connecting with Spark Cluster As we are using bitnami spark cluster the conf direcotory is in . Put the following hive-conf.xml <?xml version=\"1.0\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <!-- hive-site.xml for Spark to connect with Hive This configuration enables Spark to interact with Hive Metastore and HDFS. --> <configuration> <!-- Configures the JDBC connection URL for the Hive Metastore. The hostname and port must match your Docker Compose setup. --> <property> <name>javax.jdo.option.ConnectionURL</name> <value>jdbc:postgresql://hive-metastore-postgresql/metastore</value> <description>JDBC connection URL for the Hive Metastore database.</description> </property> <!-- Specifies the JDBC driver class for PostgreSQL. --> <property> <name>javax.jdo.option.ConnectionDriverName</name> <value>org.postgresql.Driver</value> <description>JDBC driver class for connecting to the PostgreSQL database.</description> </property> <!-- The username and password for connecting to the Hive Metastore database. These credentials should match those used in your PostgreSQL setup. --> <property> <name>javax.jdo.option.ConnectionUserName</name> <value>hive</value> <description>Username for connecting to the PostgreSQL database.</description> </property> <property> <name>javax.jdo.option.ConnectionPassword</name> <value>hive</value> <description>Password for connecting to the PostgreSQL database.</description> </property> <!-- Configures the URI for the Hive Metastore service. This URI must match the service in your Hive setup. --> <property> <name>hive.metastore.uris</name> <value>thrift://hive-metastore:9083</value> <description>Thrift URI for the Hive Metastore service.</description> </property> <!-- The location of the Hive warehouse directory in HDFS. It should match the configuration used in your Hive setup. --> <property> <name>hive.metastore.warehouse.dir</name> <value>/user/hive/warehouse</value> <description>Location of the Hive warehouse directory in HDFS.</description> </property> <!-- Enabling Hive support in Spark. This setting allows Spark to interact with Hives Metastore. --> <property> <name>spark.sql.catalogImplementation</name> <value>hive</value> <description>Enables Hive support in Spark.</description> </property> <!-- Configuring dynamic partitioning in Spark when writing data to Hive tables. --> <property> <name>hive.exec.dynamic.partition</name> <value>true</value> <description>Enables dynamic partitioning in Spark for Hive tables.</description> </property> <property> <name>hive.exec.dynamic.partition.mode</name> <value>nonstrict</value> <description>Allows dynamic partitioning without requiring explicit partition columns.</description> </property> <!-- HDFS Configuration: Telling Spark where to find the HDFS Namenode. The Namenode is typically accessed at port 8020 in Hadoop setups. --> <property> <name>fs.defaultFS</name> <value>hdfs://namenode:8020</value> <description>URI of the HDFS Namenode.</description> </property> </configuration> from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"Spark-Hive Test\") \\ .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:8020/user/hive/warehouse\") \\ .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\ .enableHiveSupport() \\ .getOrCreate() spark.sql(\"SHOW DATABASES\").show() Further reading Create the first project Download the keggel dataset to a folder retailkegraw Copy the folder to the container /home/dwdas Frst create a virtual enviornment. It resolves many permission issues and makes work very smooth. It's not just an improvement it helps work with pyspark easier. mv C:\\Users\\dwaip\\Desktop\\retailkegraw master:/home/dwdas dwdas@1478d9ddd5d8: /home/dwdas$ python -m venv keglrtl-venv source keglrtl-venv/bin/activate https://medium.com/@madtopcoder/putting-hadoop-hive-and-spark-together-for-the-first-time-bf44262575bd Error Description Possible Reason How to Troubleshoot Resolution SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta Spark could not find the Delta Lake source. Missing Delta Lake libraries or incorrect configuration. Ensure Delta Lake is installed and correctly configured in the Spark session. Installed delta-spark and configured Spark with the appropriate Delta Lake JARs. java.io.FileNotFoundException: /opt/bitnami/spark/.ivy2/cache/resolved-org.apache.spark-spark-submit-parent... Spark was unable to write to the Ivy cache directory. Permission issues in the directory. Check and adjust directory permissions or set a custom Ivy cache directory. Adjusted permissions or set a custom Ivy cache directory with write permissions. Permission denied: '/.local/lib' Insufficient permissions to install Python packages globally. Trying to install packages globally without sufficient privileges. Use pip install --user or run with sudo -H . Installed with --user flag or sudo -H . java.lang.NoClassDefFoundError: scala/collection/SeqOps A Scala version mismatch where a dependency required Scala 2.13. Using libraries compiled with Scala 2.13 with Spark compiled for Scala 2.12. Ensure all libraries match Scala 2.12 and clear any Scala 2.13 JARs. Used Scala 2.12-compatible Delta Lake JARs and cleared cached Scala 2.13 JARs. java.lang.NoClassDefFoundError: scala/collection/IterableOnce Another Scala version mismatch due to Scala 2.13 libraries. Similar to the SeqOps error; mixing Scala 2.13 libraries with a Scala 2.12 Spark environment. Check library versions, ensure consistency, and clear any Scala 2.13 dependencies. Aligned all dependencies with Scala 2.12 and cleared conflicting JARs. Py4JJavaError: An error occurred while calling o37.applyModifiableSettings. Spark encountered issues applying settings due to a version mismatch. Likely caused by using incompatible versions of libraries or JARs. Review and match versions of all libraries with Spark\u2019s Scala version. Ensured correct Delta Lake version for Scala 2.12 and cleared cache. Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions. Spark was unable to use Delta Lake extensions. Incorrect or missing dependencies, likely due to Scala version mismatch. Ensure the Delta Lake JAR matches the Scala version used by Spark. Used the correct Delta Lake JARs for Scala 2.12 and Spark 3.5.2. rm -rf ~/.ivy2/cache/io.delta rm -rf ~/.ivy2/jars/io.delta rm -rf ~/.m2/repository/io/delta Commands used to clear cached JAR files. Old or conflicting cached dependencies causing version conflicts. Clear Ivy or Maven cache to remove outdated or conflicting JARs. Removed cached JARs to prevent conflicts and ensure fresh downloads. from pyspark.sql import SparkSession from delta import configure_spark_with_delta_pip # Build the Spark session with Delta Lake support # - config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\"): Adds Delta Lake SQL extensions to Spark, allowing it to recognize and work with Delta Lake features. # - config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\"): Configures Spark to use Delta Lake as the default catalog for managing tables. # - config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\"): Specifies the Maven coordinates for the Delta Lake library compatible with Scala 2.12 and Spark 3.5.2. This ensures the necessary Delta Lake JARs are included in the Spark session. builder = SparkSession.builder.appName(\"DeltaTutorial\") \\ .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\ .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\ .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\") # Set up Spark to work with Delta Lake # - configure_spark_with_delta_pip(builder): This function from Delta Lake makes sure Spark is ready to use Delta features. spark = configure_spark_with_delta_pip(builder).getOrCreate() spark.sparkContext.setLogLevel(\"ERROR\") df = spark.createDataFrame(data, [\"id\", \"value\"]) # - save(\"/tmp/delta-table\"): Specifies the location where the Delta table will be saved. In this case, it's saved in the `/tmp/delta-table` directory. df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\") df_read = spark.read.format(\"delta\").load(\"/tmp/delta-table\") df_read.show()","title":"bde2020-Hive-Hadoop-Presto"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/#setting-up-a-hadoop-hive-and-presto-cluster-using-docker-compose","text":"This guide will walk you through setting up a Hadoop ecosystem including HDFS, YARN, Hive, and Presto using Docker Compose. This setup will allow you to perform distributed data processing and SQL-based querying on large datasets. The services are containerized, making it easy to deploy and manage.","title":"Setting Up a Hadoop, Hive, and Presto Cluster Using Docker Compose"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/#overview-of-the-setup","text":"This setup involves several Docker containers, each providing a different service in the Hadoop ecosystem: HDFS : Provides a distributed file system that stores data across multiple nodes. YARN : Manages resources and job scheduling across the cluster. Hive : A data warehouse that allows SQL-like querying on data stored in HDFS. Presto : A distributed SQL query engine that allows querying large datasets from multiple sources. The entire setup can be managed using Docker Compose, which simplifies the orchestration of multiple containers.","title":"Overview of the Setup"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/#using-the-setup","text":"To use this setup, follow these steps: Download the Zip File : Download the provided zip file that contains all the necessary configuration files and scripts. Unzip the File : Extract the contents of the zip file to a directory of your choice: bash unzip hadoop-hive-presto-setup.zip cd hadoop-hive-presto-setup Run the Setup : Start the entire setup by running the following command: bash docker-compose up -d This command will spin up all the necessary containers and services. Access the Services : You can access the various services via their respective ports as described below.","title":"Using the Setup"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/#description-of-services-and-ports","text":"","title":"Description of Services and Ports"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/#description-of-services-ports-and-folder-paths","text":"Below is a detailed description of the services included in this setup, along with the ports used, relevant folder paths, and user access information. This will help both users and administrators to understand and manage the setup effectively. Service Port Description Folder Path User Access Details Namenode (HDFS) 50070 Web UI for monitoring the HDFS Namenode. /hadoop/dfs/name root Used for monitoring the HDFS cluster, filesystem namespace management. Datanode (HDFS) 50075 Web UI for monitoring Datanode data transfer. /hadoop/dfs/data root Stores actual data blocks, communicates with the Namenode. ResourceManager (YARN) 8088 Web UI for monitoring resource allocation and job scheduling. /rmstate root Manages resources across the cluster, schedules and monitors jobs. NodeManager (YARN) 8042 Web UI for monitoring resource usage on individual nodes. /app-logs root Manages containers on each node, monitors resource usage. HistoryServer (YARN) 19888 Web UI for accessing logs and statistics of completed MapReduce jobs. /hadoop/yarn/timeline root Provides access to history logs and statistics of completed jobs. HiveServer2 (Hive) 10000 Service for executing SQL-based queries via JDBC/ODBC. /user/hive/warehouse hive Used for running SQL queries on data stored in HDFS via Hive. Hive Metastore 9083 Service for managing metadata for Hive tables. /user/hive/warehouse (metadata) hive Stores metadata for Hive tables, accessed by HiveServer2 and clients. Hive Metastore PostgreSQL N/A Internal PostgreSQL database for Hive Metastore metadata storage. /var/lib/postgresql/data (inside PostgreSQL container) postgres Stores metadata about Hive tables, partitions, etc., used by the Metastore. Presto Coordinator 8081 Web UI for monitoring queries and worker nodes in Presto. N/A root Provides a distributed SQL query engine to run queries across data sources.","title":"Description of Services, Ports, and Folder Paths"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/#hive-table-and-data-location","text":"Hive Tables : The tables created in Hive are stored in HDFS under the directory /user/hive/warehouse/ . HDFS Data : All data stored in HDFS, including Hive table data, is managed by the Namenode and Datanode services. The default directory for Hive tables is /user/hive/warehouse/ .","title":"Hive Table and Data Location"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/#users-and-access","text":"root : The root user has access to all the Hadoop services and directories. hive : The hive user has access to Hive-specific directories and can execute SQL queries via HiveServer2. postgres : The postgres user is the database administrator for the PostgreSQL database used by the Hive Metastore.","title":"Users and Access"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/#testing-the-setup","text":"","title":"Testing the Setup"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/#1-connecting-to-hive","text":"To test the Hive setup, you can connect to HiveServer2 using Beeline: docker exec -it hive-server2 beeline -u jdbc:hive2://localhost:10000 -n hive -p hive","title":"1. Connecting to Hive"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/#2-creating-and-managing-hive-tables","text":"Once connected to Hive, you can create databases, tables, and insert data: CREATE DATABASE test_db; USE test_db; CREATE TABLE test_table (id INT, name STRING); INSERT INTO test_table VALUES (1, 'Hadoop'), (2, 'Hive'); SELECT * FROM test_table;","title":"2. Creating and Managing Hive Tables"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/#3-checking-hdfs","text":"To verify that the data is stored correctly in HDFS, you can use the following command: docker exec -it namenode hadoop fs -ls /user/hive/warehouse/test_db.db/test_table This command will list the files in the specified Hive table directory in HDFS.","title":"3. Checking HDFS"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/#4-monitoring-services","text":"HDFS Namenode Web UI : http://localhost:50070 YARN ResourceManager Web UI : http://localhost:8088 YARN NodeManager Web UI : http://localhost:8042 YARN HistoryServer Web UI : http://localhost:19888 Presto Web UI : http://localhost:8081 These interfaces provide detailed information about the health and status of the services running in your cluster.","title":"4. Monitoring Services"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/#testing-the-setup_1","text":"To ensure that your setup is working correctly, you can perform the following tests: Connect to Hive : Use the Beeline CLI to connect to HiveServer2: bash docker exec -it hive-server2 beeline -u jdbc:hive2://localhost:10000 -n hive -p hive Once connected, create a database and a table: sql CREATE DATABASE test_db; USE test_db; CREATE TABLE test_table (id INT, name STRING); Insert some data into the table: sql INSERT INTO test_table VALUES (1, 'Hadoop'), (2, 'Hive'); Query the data to verify the setup: sql SELECT * FROM test_table; Check HDFS : Ensure that the data was correctly stored in HDFS: bash docker exec -it namenode hadoop fs -ls /user/hive/warehouse/test_db.db/test_table You should see the files corresponding to the inserted data. Monitor YARN Jobs : Visit the ResourceManager Web UI at http://localhost:8088 to monitor job execution. Access the Presto UI : Visit the Presto Web UI at http://localhost:8081 to monitor queries and worker nodes.","title":"Testing the Setup"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/#conclusion","text":"This setup provides a robust environment for distributed data processing and querying using Hadoop, Hive, and Presto. By containerizing these services, you can easily manage and scale your data processing infrastructure. Testing the setup with sample data ensures that all components are correctly configured and working together seamlessly.","title":"Conclusion"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/#connecting-with-spark-cluster","text":"As we are using bitnami spark cluster the conf direcotory is in . Put the following hive-conf.xml <?xml version=\"1.0\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <!-- hive-site.xml for Spark to connect with Hive This configuration enables Spark to interact with Hive Metastore and HDFS. --> <configuration> <!-- Configures the JDBC connection URL for the Hive Metastore. The hostname and port must match your Docker Compose setup. --> <property> <name>javax.jdo.option.ConnectionURL</name> <value>jdbc:postgresql://hive-metastore-postgresql/metastore</value> <description>JDBC connection URL for the Hive Metastore database.</description> </property> <!-- Specifies the JDBC driver class for PostgreSQL. --> <property> <name>javax.jdo.option.ConnectionDriverName</name> <value>org.postgresql.Driver</value> <description>JDBC driver class for connecting to the PostgreSQL database.</description> </property> <!-- The username and password for connecting to the Hive Metastore database. These credentials should match those used in your PostgreSQL setup. --> <property> <name>javax.jdo.option.ConnectionUserName</name> <value>hive</value> <description>Username for connecting to the PostgreSQL database.</description> </property> <property> <name>javax.jdo.option.ConnectionPassword</name> <value>hive</value> <description>Password for connecting to the PostgreSQL database.</description> </property> <!-- Configures the URI for the Hive Metastore service. This URI must match the service in your Hive setup. --> <property> <name>hive.metastore.uris</name> <value>thrift://hive-metastore:9083</value> <description>Thrift URI for the Hive Metastore service.</description> </property> <!-- The location of the Hive warehouse directory in HDFS. It should match the configuration used in your Hive setup. --> <property> <name>hive.metastore.warehouse.dir</name> <value>/user/hive/warehouse</value> <description>Location of the Hive warehouse directory in HDFS.</description> </property> <!-- Enabling Hive support in Spark. This setting allows Spark to interact with Hives Metastore. --> <property> <name>spark.sql.catalogImplementation</name> <value>hive</value> <description>Enables Hive support in Spark.</description> </property> <!-- Configuring dynamic partitioning in Spark when writing data to Hive tables. --> <property> <name>hive.exec.dynamic.partition</name> <value>true</value> <description>Enables dynamic partitioning in Spark for Hive tables.</description> </property> <property> <name>hive.exec.dynamic.partition.mode</name> <value>nonstrict</value> <description>Allows dynamic partitioning without requiring explicit partition columns.</description> </property> <!-- HDFS Configuration: Telling Spark where to find the HDFS Namenode. The Namenode is typically accessed at port 8020 in Hadoop setups. --> <property> <name>fs.defaultFS</name> <value>hdfs://namenode:8020</value> <description>URI of the HDFS Namenode.</description> </property> </configuration> from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"Spark-Hive Test\") \\ .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:8020/user/hive/warehouse\") \\ .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\ .enableHiveSupport() \\ .getOrCreate() spark.sql(\"SHOW DATABASES\").show()","title":"Connecting with Spark Cluster"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/#further-reading","text":"","title":"Further reading"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/#create-the-first-project","text":"Download the keggel dataset to a folder retailkegraw Copy the folder to the container /home/dwdas Frst create a virtual enviornment. It resolves many permission issues and makes work very smooth. It's not just an improvement it helps work with pyspark easier. mv C:\\Users\\dwaip\\Desktop\\retailkegraw master:/home/dwdas dwdas@1478d9ddd5d8: /home/dwdas$ python -m venv keglrtl-venv source keglrtl-venv/bin/activate https://medium.com/@madtopcoder/putting-hadoop-hive-and-spark-together-for-the-first-time-bf44262575bd Error Description Possible Reason How to Troubleshoot Resolution SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta Spark could not find the Delta Lake source. Missing Delta Lake libraries or incorrect configuration. Ensure Delta Lake is installed and correctly configured in the Spark session. Installed delta-spark and configured Spark with the appropriate Delta Lake JARs. java.io.FileNotFoundException: /opt/bitnami/spark/.ivy2/cache/resolved-org.apache.spark-spark-submit-parent... Spark was unable to write to the Ivy cache directory. Permission issues in the directory. Check and adjust directory permissions or set a custom Ivy cache directory. Adjusted permissions or set a custom Ivy cache directory with write permissions. Permission denied: '/.local/lib' Insufficient permissions to install Python packages globally. Trying to install packages globally without sufficient privileges. Use pip install --user or run with sudo -H . Installed with --user flag or sudo -H . java.lang.NoClassDefFoundError: scala/collection/SeqOps A Scala version mismatch where a dependency required Scala 2.13. Using libraries compiled with Scala 2.13 with Spark compiled for Scala 2.12. Ensure all libraries match Scala 2.12 and clear any Scala 2.13 JARs. Used Scala 2.12-compatible Delta Lake JARs and cleared cached Scala 2.13 JARs. java.lang.NoClassDefFoundError: scala/collection/IterableOnce Another Scala version mismatch due to Scala 2.13 libraries. Similar to the SeqOps error; mixing Scala 2.13 libraries with a Scala 2.12 Spark environment. Check library versions, ensure consistency, and clear any Scala 2.13 dependencies. Aligned all dependencies with Scala 2.12 and cleared conflicting JARs. Py4JJavaError: An error occurred while calling o37.applyModifiableSettings. Spark encountered issues applying settings due to a version mismatch. Likely caused by using incompatible versions of libraries or JARs. Review and match versions of all libraries with Spark\u2019s Scala version. Ensured correct Delta Lake version for Scala 2.12 and cleared cache. Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions. Spark was unable to use Delta Lake extensions. Incorrect or missing dependencies, likely due to Scala version mismatch. Ensure the Delta Lake JAR matches the Scala version used by Spark. Used the correct Delta Lake JARs for Scala 2.12 and Spark 3.5.2. rm -rf ~/.ivy2/cache/io.delta rm -rf ~/.ivy2/jars/io.delta rm -rf ~/.m2/repository/io/delta Commands used to clear cached JAR files. Old or conflicting cached dependencies causing version conflicts. Clear Ivy or Maven cache to remove outdated or conflicting JARs. Removed cached JARs to prevent conflicts and ensure fresh downloads. from pyspark.sql import SparkSession from delta import configure_spark_with_delta_pip # Build the Spark session with Delta Lake support # - config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\"): Adds Delta Lake SQL extensions to Spark, allowing it to recognize and work with Delta Lake features. # - config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\"): Configures Spark to use Delta Lake as the default catalog for managing tables. # - config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\"): Specifies the Maven coordinates for the Delta Lake library compatible with Scala 2.12 and Spark 3.5.2. This ensures the necessary Delta Lake JARs are included in the Spark session. builder = SparkSession.builder.appName(\"DeltaTutorial\") \\ .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\ .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\ .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\") # Set up Spark to work with Delta Lake # - configure_spark_with_delta_pip(builder): This function from Delta Lake makes sure Spark is ready to use Delta features. spark = configure_spark_with_delta_pip(builder).getOrCreate() spark.sparkContext.setLogLevel(\"ERROR\") df = spark.createDataFrame(data, [\"id\", \"value\"]) # - save(\"/tmp/delta-table\"): Specifies the location where the Delta table will be saved. In this case, it's saved in the `/tmp/delta-table` directory. df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\") df_read = spark.read.format(\"delta\").load(\"/tmp/delta-table\") df_read.show()","title":"Create the first project"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.7_Hadoop_Hive_SingleNode_MySQL/","text":"Step 1: Updated entrypoint.sh #!/bin/bash # Exit immediately if a command exits with a non-zero status set -e # Function to start SSH service start_ssh() { echo \"Starting SSH service...\" service ssh start } # Function to start MySQL service and initialize metastore start_mysql() { echo \"Starting MySQL service...\" service mysql start echo \"Configuring MySQL for Hive Metastore...\" mysql -uroot -pPassw0rd <<EOF CREATE DATABASE IF NOT EXISTS metastore; CREATE USER IF NOT EXISTS 'hive'@'localhost' IDENTIFIED BY 'Passw0rd'; GRANT ALL PRIVILEGES ON metastore.* TO 'hive'@'localhost'; FLUSH PRIVILEGES; EOF } # Function to format HDFS namenode format_namenode() { if [ ! -d \"/usr/local/hadoop/tmp/hdfs/namenode/current\" ]; then echo \"Formatting HDFS namenode...\" hdfs namenode -format -force -nonInteractive else echo \"HDFS namenode already formatted.\" fi } # Function to start HDFS services start_hdfs() { echo \"Starting HDFS services...\" start-dfs.sh } # Function to start YARN services start_yarn() { echo \"Starting YARN services...\" start-yarn.sh } # Function to start MapReduce JobHistory Server start_historyserver() { echo \"Starting MapReduce JobHistory Server...\" mr-jobhistory-daemon.sh start historyserver } # Function to start Hive services start_hive() { echo \"Starting Hive Metastore...\" nohup hive --service metastore > /tmp/hive-metastore.log 2>&1 & echo \"Starting HiveServer2...\" nohup hive --service hiveserver2 > /tmp/hive-server2.log 2>&1 & } # Function to keep the container running keep_container_alive() { echo \"All services started. Tail logs to keep the container running...\" tail -f /dev/null } # Execute functions in order start_ssh start_mysql format_namenode start_hdfs start_yarn start_historyserver start_hive keep_container_alive Explanation of the Script: set -e : Ensures that the script exits immediately if any command fails. start_ssh : Starts the SSH service required by Hadoop. start_mysql : Starts MySQL service and configures the Hive metastore database with user hive and password Passw0rd . format_namenode : Checks if the namenode is already formatted; if not, formats it. start_hdfs : Starts HDFS services including namenode and datanode. start_yarn : Starts YARN services including resourcemanager and nodemanager. start_historyserver : Starts the MapReduce JobHistory Server. start_hive : Starts Hive Metastore and HiveServer2 services in the background and redirects logs to /tmp/ . keep_container_alive : Keeps the container running by tailing /dev/null . Step 2: Update Dockerfile Ensure your Dockerfile correctly copies the entrypoint.sh script and sets appropriate permissions. # Use a base image with Java FROM openjdk:8-jdk # Set environment variables ENV HADOOP_VERSION=3.4.0 ENV HIVE_VERSION=4.0.0 ENV MYSQL_ROOT_PASSWORD=Passw0rd ENV HADOOP_HOME=/usr/local/hadoop ENV HIVE_HOME=/usr/local/hive ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin # Install necessary packages RUN apt-get update && \\ apt-get install -y ssh rsync mysql-server wget && \\ apt-get clean # Create a user RUN useradd -ms /bin/bash dwdas # Switch to user USER dwdas WORKDIR /home/dwdas # Download and extract Hadoop RUN wget https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz && \\ tar -xzf hadoop-${HADOOP_VERSION}.tar.gz && \\ mv hadoop-${HADOOP_VERSION} $HADOOP_HOME && \\ rm hadoop-${HADOOP_VERSION}.tar.gz # Download and extract Hive RUN wget https://downloads.apache.org/hive/hive-${HIVE_VERSION}/apache-hive-${HIVE_VERSION}-bin.tar.gz && \\ tar -xzf apache-hive-${HIVE_VERSION}-bin.tar.gz && \\ mv apache-hive-${HIVE_VERSION}-bin $HIVE_HOME && \\ rm apache-hive-${HIVE_VERSION}-bin.tar.gz # Copy configuration files COPY --chown=dwdas:dwdas core-site.xml $HADOOP_HOME/etc/hadoop/ COPY --chown=dwdas:dwdas hdfs-site.xml $HADOOP_HOME/etc/hadoop/ COPY --chown=dwdas:dwdas mapred-site.xml $HADOOP_HOME/etc/hadoop/ COPY --chown=dwdas:dwdas yarn-site.xml $HADOOP_HOME/etc/hadoop/ COPY --chown=dwdas:dwdas hive-site.xml $HIVE_HOME/conf/ # Copy MySQL JDBC driver RUN wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.31.tar.gz && \\ tar -xzf mysql-connector-java-8.0.31.tar.gz && \\ cp mysql-connector-java-8.0.31/mysql-connector-java-8.0.31.jar $HIVE_HOME/lib/ && \\ cp mysql-connector-java-8.0.31/mysql-connector-java-8.0.31.jar $HADOOP_HOME/share/hadoop/common/lib/ && \\ rm -rf mysql-connector-java-8.0.31* # Set permissions RUN chmod +x $HADOOP_HOME/bin/* && \\ chmod +x $HADOOP_HOME/sbin/* && \\ chmod +x $HIVE_HOME/bin/* # Switch back to root to start services USER root # Copy and set permissions for entrypoint.sh COPY entrypoint.sh /entrypoint.sh RUN chmod +x /entrypoint.sh # Expose necessary ports EXPOSE 50070 8088 10000 10002 19888 3306 # Start services CMD [\"/entrypoint.sh\"] Explanation of the Dockerfile: Base Image : Uses openjdk:8-jdk as the base image. Environment Variables : Sets necessary environment variables for Hadoop and Hive. Package Installation : Installs SSH, rsync, MySQL server, and wget. User Creation : Creates a user dwdas and sets the working directory. Downloading Hadoop and Hive : Downloads and extracts Hadoop and Hive binaries. Configuration Files : Copies necessary configuration files into their respective directories. MySQL JDBC Driver : Downloads and places the MySQL JDBC driver in Hive and Hadoop lib directories. Permissions : Ensures all binaries and scripts have the correct execution permissions. Entrypoint Script : Copies the updated entrypoint.sh into the image and sets execution permission. Exposed Ports : Exposes all necessary ports for accessing services. CMD : Sets the default command to execute entrypoint.sh when the container starts. Step 3: Configuration Files Ensure that your configuration files ( core-site.xml , hdfs-site.xml , mapred-site.xml , yarn-site.xml , and hive-site.xml ) are correctly set up. Example hive-site.xml <configuration> <property> <name>javax.jdo.option.ConnectionURL</name> <value>jdbc:mysql://localhost:3306/metastore?useSSL=false&amp;createDatabaseIfNotExist=true</value> </property> <property> <name>javax.jdo.option.ConnectionDriverName</name> <value>com.mysql.cj.jdbc.Driver</value> </property> <property> <name>javax.jdo.option.ConnectionUserName</name> <value>hive</value> </property> <property> <name>javax.jdo.option.ConnectionPassword</name> <value>Passw0rd</value> </property> <property> <name>hive.metastore.warehouse.dir</name> <value>/user/hive/warehouse</value> </property> <property> <name>datanucleus.autoCreateSchema</name> <value>true</value> </property> <property> <name>datanucleus.autoCreateTables</name> <value>true</value> </property> <property> <name>datanucleus.fixedDatastore</name> <value>false</value> </property> <property> <name>hive.server2.thrift.port</name> <value>10000</value> </property> </configuration> Note : Ensure all other Hadoop configuration files are correctly set according to your environment and requirements. Step 4: Update docker-compose.yml version: '3.8' services: hadoop-hive-mysql: build: . container_name: hadoop-hive-mysql hostname: hadoop-hive-mysql environment: - MYSQL_ROOT_PASSWORD=Passw0rd volumes: - namenode_data:/usr/local/hadoop/tmp/hdfs/namenode - datanode_data:/usr/local/hadoop/tmp/hdfs/datanode - hive_warehouse:/user/hive/warehouse - mysql_data:/var/lib/mysql ports: - \"29070:50070\" # HDFS NameNode Web UI - \"28088:8088\" # YARN ResourceManager Web UI - \"21000:10000\" # HiveServer2 - \"21002:10002\" # HiveServer2 Thrift HTTP - \"21988:19888\" # MapReduce JobHistory - \"23306:3306\" # MySQL networks: - dasnet volumes: namenode_data: datanode_data: hive_warehouse: mysql_data: networks: dasnet: external: true Explanation of docker-compose.yml : Services : Defines a single service hadoop-hive-mysql . Volumes : Uses Docker volumes for persistent storage of HDFS namenode, datanode, Hive warehouse, and MySQL data. Ports : Maps container ports to host ports using rare ports to avoid conflicts. Networks : Connects to an external network dasnet . Ensure this network exists or remove external: true to let Docker create it. Step 5: Building and Running the Container Build the Docker Image : bash docker-compose build Start the Container : bash docker-compose up -d Check Running Services : HDFS NameNode Web UI : http://localhost:29070 YARN ResourceManager Web UI : http://localhost:28088 MapReduce JobHistory Web UI : http://localhost:21988 Connecting to Hive : Using Beeline : bash beeline -u jdbc:hive2://localhost:21000 -n hive -p Passw0rd Using JDBC : jdbc:hive2://localhost:21000/default Connecting to MySQL : From Host Machine : bash mysql -h 127.0.0.1 -P 23306 -u root -pPassw0rd Inspect Logs : Hive Logs : bash docker exec -it hadoop-hive-mysql bash tail -f /tmp/hive-metastore.log tail -f /tmp/hive-server2.log Hadoop Logs : bash docker exec -it hadoop-hive-mysql bash hdfs dfsadmin -report yarn node -list Troubleshooting Ports Already in Use : If any of the mapped ports are already in use on your host machine, modify them in the docker-compose.yml file. Service Failures : Check respective logs inside the container for detailed error messages. Network Issues : Ensure that the Docker network dasnet exists or remove the external specification to let Docker manage it.","title":"Hive-Hadoop-MySQL-SingleNode"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.7_Hadoop_Hive_SingleNode_MySQL/#step-1-updated-entrypointsh","text":"#!/bin/bash # Exit immediately if a command exits with a non-zero status set -e # Function to start SSH service start_ssh() { echo \"Starting SSH service...\" service ssh start } # Function to start MySQL service and initialize metastore start_mysql() { echo \"Starting MySQL service...\" service mysql start echo \"Configuring MySQL for Hive Metastore...\" mysql -uroot -pPassw0rd <<EOF CREATE DATABASE IF NOT EXISTS metastore; CREATE USER IF NOT EXISTS 'hive'@'localhost' IDENTIFIED BY 'Passw0rd'; GRANT ALL PRIVILEGES ON metastore.* TO 'hive'@'localhost'; FLUSH PRIVILEGES; EOF } # Function to format HDFS namenode format_namenode() { if [ ! -d \"/usr/local/hadoop/tmp/hdfs/namenode/current\" ]; then echo \"Formatting HDFS namenode...\" hdfs namenode -format -force -nonInteractive else echo \"HDFS namenode already formatted.\" fi } # Function to start HDFS services start_hdfs() { echo \"Starting HDFS services...\" start-dfs.sh } # Function to start YARN services start_yarn() { echo \"Starting YARN services...\" start-yarn.sh } # Function to start MapReduce JobHistory Server start_historyserver() { echo \"Starting MapReduce JobHistory Server...\" mr-jobhistory-daemon.sh start historyserver } # Function to start Hive services start_hive() { echo \"Starting Hive Metastore...\" nohup hive --service metastore > /tmp/hive-metastore.log 2>&1 & echo \"Starting HiveServer2...\" nohup hive --service hiveserver2 > /tmp/hive-server2.log 2>&1 & } # Function to keep the container running keep_container_alive() { echo \"All services started. Tail logs to keep the container running...\" tail -f /dev/null } # Execute functions in order start_ssh start_mysql format_namenode start_hdfs start_yarn start_historyserver start_hive keep_container_alive Explanation of the Script: set -e : Ensures that the script exits immediately if any command fails. start_ssh : Starts the SSH service required by Hadoop. start_mysql : Starts MySQL service and configures the Hive metastore database with user hive and password Passw0rd . format_namenode : Checks if the namenode is already formatted; if not, formats it. start_hdfs : Starts HDFS services including namenode and datanode. start_yarn : Starts YARN services including resourcemanager and nodemanager. start_historyserver : Starts the MapReduce JobHistory Server. start_hive : Starts Hive Metastore and HiveServer2 services in the background and redirects logs to /tmp/ . keep_container_alive : Keeps the container running by tailing /dev/null .","title":"Step 1: Updated entrypoint.sh"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.7_Hadoop_Hive_SingleNode_MySQL/#step-2-update-dockerfile","text":"Ensure your Dockerfile correctly copies the entrypoint.sh script and sets appropriate permissions. # Use a base image with Java FROM openjdk:8-jdk # Set environment variables ENV HADOOP_VERSION=3.4.0 ENV HIVE_VERSION=4.0.0 ENV MYSQL_ROOT_PASSWORD=Passw0rd ENV HADOOP_HOME=/usr/local/hadoop ENV HIVE_HOME=/usr/local/hive ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin # Install necessary packages RUN apt-get update && \\ apt-get install -y ssh rsync mysql-server wget && \\ apt-get clean # Create a user RUN useradd -ms /bin/bash dwdas # Switch to user USER dwdas WORKDIR /home/dwdas # Download and extract Hadoop RUN wget https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz && \\ tar -xzf hadoop-${HADOOP_VERSION}.tar.gz && \\ mv hadoop-${HADOOP_VERSION} $HADOOP_HOME && \\ rm hadoop-${HADOOP_VERSION}.tar.gz # Download and extract Hive RUN wget https://downloads.apache.org/hive/hive-${HIVE_VERSION}/apache-hive-${HIVE_VERSION}-bin.tar.gz && \\ tar -xzf apache-hive-${HIVE_VERSION}-bin.tar.gz && \\ mv apache-hive-${HIVE_VERSION}-bin $HIVE_HOME && \\ rm apache-hive-${HIVE_VERSION}-bin.tar.gz # Copy configuration files COPY --chown=dwdas:dwdas core-site.xml $HADOOP_HOME/etc/hadoop/ COPY --chown=dwdas:dwdas hdfs-site.xml $HADOOP_HOME/etc/hadoop/ COPY --chown=dwdas:dwdas mapred-site.xml $HADOOP_HOME/etc/hadoop/ COPY --chown=dwdas:dwdas yarn-site.xml $HADOOP_HOME/etc/hadoop/ COPY --chown=dwdas:dwdas hive-site.xml $HIVE_HOME/conf/ # Copy MySQL JDBC driver RUN wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.31.tar.gz && \\ tar -xzf mysql-connector-java-8.0.31.tar.gz && \\ cp mysql-connector-java-8.0.31/mysql-connector-java-8.0.31.jar $HIVE_HOME/lib/ && \\ cp mysql-connector-java-8.0.31/mysql-connector-java-8.0.31.jar $HADOOP_HOME/share/hadoop/common/lib/ && \\ rm -rf mysql-connector-java-8.0.31* # Set permissions RUN chmod +x $HADOOP_HOME/bin/* && \\ chmod +x $HADOOP_HOME/sbin/* && \\ chmod +x $HIVE_HOME/bin/* # Switch back to root to start services USER root # Copy and set permissions for entrypoint.sh COPY entrypoint.sh /entrypoint.sh RUN chmod +x /entrypoint.sh # Expose necessary ports EXPOSE 50070 8088 10000 10002 19888 3306 # Start services CMD [\"/entrypoint.sh\"] Explanation of the Dockerfile: Base Image : Uses openjdk:8-jdk as the base image. Environment Variables : Sets necessary environment variables for Hadoop and Hive. Package Installation : Installs SSH, rsync, MySQL server, and wget. User Creation : Creates a user dwdas and sets the working directory. Downloading Hadoop and Hive : Downloads and extracts Hadoop and Hive binaries. Configuration Files : Copies necessary configuration files into their respective directories. MySQL JDBC Driver : Downloads and places the MySQL JDBC driver in Hive and Hadoop lib directories. Permissions : Ensures all binaries and scripts have the correct execution permissions. Entrypoint Script : Copies the updated entrypoint.sh into the image and sets execution permission. Exposed Ports : Exposes all necessary ports for accessing services. CMD : Sets the default command to execute entrypoint.sh when the container starts.","title":"Step 2: Update Dockerfile"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.7_Hadoop_Hive_SingleNode_MySQL/#step-3-configuration-files","text":"Ensure that your configuration files ( core-site.xml , hdfs-site.xml , mapred-site.xml , yarn-site.xml , and hive-site.xml ) are correctly set up.","title":"Step 3: Configuration Files"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.7_Hadoop_Hive_SingleNode_MySQL/#example-hive-sitexml","text":"<configuration> <property> <name>javax.jdo.option.ConnectionURL</name> <value>jdbc:mysql://localhost:3306/metastore?useSSL=false&amp;createDatabaseIfNotExist=true</value> </property> <property> <name>javax.jdo.option.ConnectionDriverName</name> <value>com.mysql.cj.jdbc.Driver</value> </property> <property> <name>javax.jdo.option.ConnectionUserName</name> <value>hive</value> </property> <property> <name>javax.jdo.option.ConnectionPassword</name> <value>Passw0rd</value> </property> <property> <name>hive.metastore.warehouse.dir</name> <value>/user/hive/warehouse</value> </property> <property> <name>datanucleus.autoCreateSchema</name> <value>true</value> </property> <property> <name>datanucleus.autoCreateTables</name> <value>true</value> </property> <property> <name>datanucleus.fixedDatastore</name> <value>false</value> </property> <property> <name>hive.server2.thrift.port</name> <value>10000</value> </property> </configuration> Note : Ensure all other Hadoop configuration files are correctly set according to your environment and requirements.","title":"Example hive-site.xml"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.7_Hadoop_Hive_SingleNode_MySQL/#step-4-update-docker-composeyml","text":"version: '3.8' services: hadoop-hive-mysql: build: . container_name: hadoop-hive-mysql hostname: hadoop-hive-mysql environment: - MYSQL_ROOT_PASSWORD=Passw0rd volumes: - namenode_data:/usr/local/hadoop/tmp/hdfs/namenode - datanode_data:/usr/local/hadoop/tmp/hdfs/datanode - hive_warehouse:/user/hive/warehouse - mysql_data:/var/lib/mysql ports: - \"29070:50070\" # HDFS NameNode Web UI - \"28088:8088\" # YARN ResourceManager Web UI - \"21000:10000\" # HiveServer2 - \"21002:10002\" # HiveServer2 Thrift HTTP - \"21988:19888\" # MapReduce JobHistory - \"23306:3306\" # MySQL networks: - dasnet volumes: namenode_data: datanode_data: hive_warehouse: mysql_data: networks: dasnet: external: true Explanation of docker-compose.yml : Services : Defines a single service hadoop-hive-mysql . Volumes : Uses Docker volumes for persistent storage of HDFS namenode, datanode, Hive warehouse, and MySQL data. Ports : Maps container ports to host ports using rare ports to avoid conflicts. Networks : Connects to an external network dasnet . Ensure this network exists or remove external: true to let Docker create it.","title":"Step 4: Update docker-compose.yml"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.7_Hadoop_Hive_SingleNode_MySQL/#step-5-building-and-running-the-container","text":"Build the Docker Image : bash docker-compose build Start the Container : bash docker-compose up -d Check Running Services : HDFS NameNode Web UI : http://localhost:29070 YARN ResourceManager Web UI : http://localhost:28088 MapReduce JobHistory Web UI : http://localhost:21988 Connecting to Hive : Using Beeline : bash beeline -u jdbc:hive2://localhost:21000 -n hive -p Passw0rd Using JDBC : jdbc:hive2://localhost:21000/default Connecting to MySQL : From Host Machine : bash mysql -h 127.0.0.1 -P 23306 -u root -pPassw0rd Inspect Logs : Hive Logs : bash docker exec -it hadoop-hive-mysql bash tail -f /tmp/hive-metastore.log tail -f /tmp/hive-server2.log Hadoop Logs : bash docker exec -it hadoop-hive-mysql bash hdfs dfsadmin -report yarn node -list","title":"Step 5: Building and Running the Container"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.7_Hadoop_Hive_SingleNode_MySQL/#troubleshooting","text":"Ports Already in Use : If any of the mapped ports are already in use on your host machine, modify them in the docker-compose.yml file. Service Failures : Check respective logs inside the container for detailed error messages. Network Issues : Ensure that the Docker network dasnet exists or remove the external specification to let Docker manage it.","title":"Troubleshooting"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.8_Hive-ApacheOfficial/","text":"Apache Official - Hive Metastore, Postgress and HiveServer2 Setup On Docker Overview This steup is based on Official Hive docker site . There is almost no customization, except the addtion of network dasnet. The setup consists of few docker commands run using CMD/Terminal. That's it. The only issue with this setup is when you restart HiveServer2, it gives the error: \"HiveServer2 running as process 7. Stop it first.\" Other than that, the setup is great. Just rerun the Docker command for HiveServer2 to create a new container. The metastore and PostgreSQL work fine without needing a restart. Steps to create the setup Just download the zip file . Unzip it and run the run.bat It will create three conatiners like this: Or, if you want to run it manually here is the CMD script(just save it as anyname.bat and run it. That's all.): REM Das Sep-04-2024: This CDs to the current folder cd /d %~dp0 REM Step 1: Create a Docker volume docker volume create warehouse REM Das: We create a postgres container docker run -d --network dasnet --name postgres -e POSTGRES_DB=metastore_db -e POSTGRES_USER=hive -e POSTGRES_PASSWORD=password postgres:13 REM Das: Create metastore-standalone container, connecting to Postgres and initializing the database docker run -d --network dasnet -p 9083:9083 --env SERVICE_NAME=metastore --env DB_DRIVER=postgres --env SERVICE_OPTS=\"-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore_db -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=password\" --mount source=warehouse,target=/opt/hive/data/warehouse --name metastore apache/hive:4.0.0-alpha-2 REM Das: Create HiveServer2 container which will connect to the metastore docker run -d --network dasnet -p 10000:10000 -p 10002:10002 --env SERVICE_NAME=hiveserver2 --env SERVICE_OPTS=\"-Dhive.metastore.uris=thrift://metastore:9083\" --mount source=warehouse,target=/opt/hive/data/warehouse --env IS_RESUME=\"true\" --name hiveserver2 apache/hive:4.0.0-alpha-2 REM Keep the window open so that the user can review the output pause Steps to verify the Setup To check the connectivity between all components: docker ps You should see the postgres , metastore , and hiveserver2 containers running. Connect to Beeline on HiveServer2: docker exec -it hiveserver2 /bin/bash beeline -u 'jdbc:hive2://hiveserver2:10000/' Create a sample table: CREATE TABLE movies (id INT, name STRING); INSERT INTO movies VALUES (1, 'Rambo'), (2, 'Mogambo'); SELECT * FROM movies; Exit Beeline: !quit; How data is stored in this setup by default This setup is fully self-contained and enough for a Hive warehouse. The data is saved in a Docker volume, and the metadata is stored in a PostgreSQL server. You can later modify this setup to add a Hadoop Namenode (like in real-world scenarios) or even use Amazon S3 or ADLS for data storage. In this setup, Hive data will be stored in the following locations: Postgres Database : The Postgres container is used to store the metadata (table schemas, databases, etc.) for Hive. This is where the Hive Metastore connects to manage and query metadata. Database: metastore_db , User: hive , Password: password . HDFS for Table Data : We are using the volume warehouse mounted to /opt/hive/data/warehouse in both the Metastore and HiveServer2 containers. This is where Hive stores its table data files. The directory /opt/hive/data/warehouse in the containers corresponds to the warehouse volume on our host system. Metastore Initialization : The Metastore container connects to Postgres and uses the JDBC driver ( postgresql-42.5.1.jar ) to initialize the Hive Metastore database. This ensures that Hive's metadata is managed via Postgres. HiveServer2 : The HiveServer2 container connects to the Metastore (via thrift://metastore:9083 ) to retrieve metadata and execute Hive queries. The table data is stored in the same warehouse directory mounted as a volume, allowing HiveServer2 to access the data files in HDFS. thrift metastore uris for this setup thrift://metastore:9083 : This is the Metastore Thrift URI that other services (like HiveServer2 or Spark) use to connect to the Hive Metastore and retrieve metadata about tables and databases. 9083 is a default port. thrift://hiveserver2:10000 : This URI is for the HiveServer2 service, which is used to execute Hive queries. Applications (like Beeline or JDBC/ODBC clients) connect to this URI to run SQL queries in Hive. 10000 is a default port. How to connect a Spark Session to this setup In this setup, even without explicitly configuring hive-site.xml or adding any special configuration files, Spark can still interact with Hive by creating a Spark session that points to Hive and using Hive's metastore through the Thrift interface. Here's how it works: Create a Spark Session : When you create a Spark session, make sure to specify that it should use Hive. Spark will use the Hive Thrift Server (HiveServer2) to access Hive tables and metadata. Here's how you can create the session: ```python # Create a Spark session to connect with Hive Metastore # 'thrift://metastore:9083' is the URI of the Hive Metastore service in the container. # 9083 is the default port for the Hive Metastore using the Thrift protocol. # Spark will use this to fetch table metadata and manage Hive tables. # Enable Hive support so Spark can read/write Hive tables using Hive's metadata and storage. from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"Spark Hive Example\") \\ .config(\"spark.sql.catalogImplementation\", \"hive\") \\ .config(\"hive.metastore.uris\", \"thrift://metastore:9083\") \\ .enableHiveSupport() \\ .getOrCreate() ``` Create a Table and Store Data : Once the session is created, you can create tables and store data in Hive without needing hive-site.xml . For example, you can create a table and insert data like this: ```python # Create a DataFrame with some sample data data = [(\"Jango\", 30), (\"Jamila\", 25)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Create a table in Hive and write data to it df.write.mode(\"overwrite\").saveAsTable(\"default.person\") # Verify that the table was created and data was stored spark.sql(\"SELECT * FROM default.person\").show() ``` Where is the Data Stored? : The data you insert will be saved in the warehouse directory mounted to /opt/hive/data/warehouse in the HiveServer2 container. This happens automatically since you have the Docker volume set up to persist the data. For example, after creating the person table, the actual data will be stored in a directory like /opt/hive/data/warehouse/default/person inside the hiveserver2 container, and it will persist even if the container restarts. Configuration Reference Component Details PostgreSQL Hive Metastore - Database Name : metastore_db - Username : hive - Password : password - Connection URL : jdbc:postgresql://postgres:5432/metastore_db Schema Tool for Hive Tables - Location : /opt/hive/bin/schematool - Command : /opt/hive/bin/schematool -dbType postgres -initOrUpgradeSchema Hive Configuration Directory /opt/hive/conf Hive Version 4.0.0-alpha-2 HiveServer2 Startup Script /opt/hive/bin/ext/hiveserver2.sh PostgreSQL Container - Network : dasnet - Image : postgres:13 - Command : docker run -d --network dasnet --name postgres -e POSTGRES_DB=metastore_db -e POSTGRES_USER=hive -e POSTGRES_PASSWORD=password postgres:13 Metastore Container - Network : dasnet - Port : 9083 - Warehouse Data Directory : /opt/hive/data/warehouse - JDBC Driver : /opt/hive/lib/postgres.jar - Image : apache/hive:4.0.0-alpha-2 HiveServer2 Container - Network : dasnet - Ports : 10000 (Thrift service), 10002 (additional) - Warehouse Data Directory : /opt/hive/data/warehouse - Metastore URI : thrift://metastore:9083 - Image : apache/hive:4.0.0-alpha-2 Note : This setup works smoothly for the metastore and PostgreSQL combination, but restarting HiveServer2 doesn't work, and you need to recreate the HiveServer2 container alone. Official Hive Docker Hive Files Github Add hue to the system To install Apache Hue and connect it to your existing environment, we'll need to configure Hue to work with Hive and PostgreSQL. Here's a step-by-step guide on how to do this: Step-by-Step Guide to Install and Connect Hue to Hive Create a Docker Network (should be already created): The dasnet network shoudld be already created. If not, create it using command: bash docker network create dasnet Run Hue Docker Container: You can run Hue in a Docker container and connect it to your existing Hive and PostgreSQL setup. Here's the command to run Hue: bash docker run -d --network dasnet -p 8888:8888 --name hue gethue/hue:latest Configure Hue to Connect to Hive Note: You can either edit the hue.ini file directly inside the container or modify it locally and then copy it to the /usr/share/hue/desktop/conf/ directory in the container. After making the changes you can copy the file to container using the followig command(cd to the folder containing the hue.ini file) docker cp ./hue.ini hue:/usr/share/hue/desktop/conf/ To connect Hue to HiveServer2, you need to update the hue.ini configuration file in the Hue container. First, open a terminal in the Hue container: bash docker exec -it hue /bin/bash Navigate to the directory where the hue.ini file is located: bash cd /usr/share/hue/desktop/conf/ Open the hue.ini file using a text editor like nano or vim : bash nano hue.ini Look for the [beeswax] section in the file and add the following lines to point Hue to your HiveServer2 instance: ini [[beeswax]] # HiveServer2 connection settings hive_server_host=localhost hive_server_port=10000 # Optional authentication settings for HiveServer2 auth_username=hive auth_password=password Configure PostgreSQL as Hue's Backend Database To make Hue use your PostgreSQL database (the one used by Hive Metastore), update the database configuration in the hue.ini file. In the same hue.ini file, find the [desktop] section and add the following lines to configure PostgreSQL as the backend database: ini [desktop] [[database]] engine=postgresql_psycopg2 host=postgres port=5432 user=hive password=password name=metastore_db There should only be one `[[database]]` section and one `[[beeswax]]` section in the configuration file, and these sections should always use double brackets (`[[ ]]`). Common errors that occurs if these are not followed are: ERROR: Error in configuration file '/usr/share/hue/desktop/conf/hue.ini': Parsing failed with several errors. Error in configuration file '/usr/share/hue/desktop/conf/hue.ini': Duplicate section name at line 524. Restart the Hue Container, then access it: After making changes to the hue.ini file, exit the editor and container, and restart the Hue container to apply the changes: bash docker restart hue Then access hue using the following command: http://localhost:8888 The credential you choose will be your root credential. For me it is `dwdas/Passw0rd` [Draft DO NOT USE]Connecting To Spark using hive-site.xml Note: 1. SparkConf : If you programmatically set configurations using the SparkConf object in your Spark application, these settings have the highest priority. 2. Flags passed to spark-submit : If you pass configuration options directly when running the Spark job using spark-submit (e.g., --conf spark.executor.memory=4g ), these come next. 3. spark-defaults.conf : This is the default configuration file for Spark, and properties here are considered last. They will only apply if the settings haven't been specified by SparkConf or spark-submit . In this section, we will show how to create a Spark container from scratch and connect it to the existing Hive environment that we already set up using Docker. You will learn step by step how to configure Spark so it can talk to Hive, create tables in Hive, and use PySpark to query and manage those tables. Create and Run the Spark Container : First, we need to create a new Spark container. This Spark container will be on the same network as your Hive and PostgreSQL containers so that they can communicate with each other. Run this command to create the Spark container and connect it to the Docker network ( dasnet ): bash docker run -d --network dasnet --name spark-container -p 4040:4040 -p 8080:8080 -v /path/to/spark-conf:/opt/spark/conf bitnami/spark:latest Configure Hive for Spark : Now, we need to tell Spark where to find Hive. We do this by setting up the hive-site.xml file with the correct information about the Hive metastore. Here\u2019s what the file should look like: xml <configuration> <property> <name>hive.metastore.uris</name> <value>thrift://metastore:9083</value> </property> <property> <name>javax.jdo.option.ConnectionDriverName</name> <value>org.postgresql.Driver</value> </property> <property> <name>javax.jdo.option.ConnectionURL</name> <value>jdbc:postgresql://postgres:5432/metastore_db</value> </property> <property> <name>javax.jdo.option.ConnectionUserName</name> <value>hive</value> </property> <property> <name>javax.jdo.option.ConnectionPassword</name> <value>password</value> </property> </configuration> Copy this file to your Spark container using: bash docker cp /path/to/hive-site.xml spark-container:/opt/spark/conf/hive-site.xml Add PostgreSQL Driver to Spark : Since Hive uses PostgreSQL, you need to add the PostgreSQL driver to the Spark container so that Spark can connect to the Hive metastore. Copy the PostgreSQL driver ( postgresql-42.5.1.jar ) into the Spark container: bash docker cp /path/to/postgresql-42.5.1.jar spark-container:/opt/spark/jars/ Start PySpark with Hive Support : Now, start the PySpark session from the Spark container with Hive support enabled: bash docker exec -it spark-container /bin/bash pyspark --conf spark.sql.catalogImplementation=hive Create and Query Tables Using PySpark : Inside the PySpark shell, you can create tables and query them. Here\u2019s how: Create a table : python spark.sql(\"CREATE TABLE IF NOT EXISTS sample_table (id INT, name STRING)\") Insert data into the table : python spark.sql(\"INSERT INTO sample_table VALUES (1, 'Alice'), (2, 'Bob')\") Query the table : python result = spark.sql(\"SELECT * FROM sample_table\") result.show() Verify the Table in Hive : After creating the table using PySpark, you can check if the table exists in Hive using Beeline: bash docker exec -it hiveserver2 /bin/bash beeline -u 'jdbc:hive2://hiveserver2:10000/' Run this query in Beeline to verify: sql SHOW TABLES; SELECT * FROM sample_table; \u00a9 2024 Das. All Rights Reserved.","title":"Hive Official Setup"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.8_Hive-ApacheOfficial/#apache-official-hive-metastore-postgress-and-hiveserver2-setup-on-docker","text":"","title":"Apache Official - Hive Metastore, Postgress and HiveServer2 Setup On Docker"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.8_Hive-ApacheOfficial/#overview","text":"This steup is based on Official Hive docker site . There is almost no customization, except the addtion of network dasnet. The setup consists of few docker commands run using CMD/Terminal. That's it. The only issue with this setup is when you restart HiveServer2, it gives the error: \"HiveServer2 running as process 7. Stop it first.\" Other than that, the setup is great. Just rerun the Docker command for HiveServer2 to create a new container. The metastore and PostgreSQL work fine without needing a restart.","title":"Overview"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.8_Hive-ApacheOfficial/#steps-to-create-the-setup","text":"Just download the zip file . Unzip it and run the run.bat It will create three conatiners like this: Or, if you want to run it manually here is the CMD script(just save it as anyname.bat and run it. That's all.): REM Das Sep-04-2024: This CDs to the current folder cd /d %~dp0 REM Step 1: Create a Docker volume docker volume create warehouse REM Das: We create a postgres container docker run -d --network dasnet --name postgres -e POSTGRES_DB=metastore_db -e POSTGRES_USER=hive -e POSTGRES_PASSWORD=password postgres:13 REM Das: Create metastore-standalone container, connecting to Postgres and initializing the database docker run -d --network dasnet -p 9083:9083 --env SERVICE_NAME=metastore --env DB_DRIVER=postgres --env SERVICE_OPTS=\"-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore_db -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=password\" --mount source=warehouse,target=/opt/hive/data/warehouse --name metastore apache/hive:4.0.0-alpha-2 REM Das: Create HiveServer2 container which will connect to the metastore docker run -d --network dasnet -p 10000:10000 -p 10002:10002 --env SERVICE_NAME=hiveserver2 --env SERVICE_OPTS=\"-Dhive.metastore.uris=thrift://metastore:9083\" --mount source=warehouse,target=/opt/hive/data/warehouse --env IS_RESUME=\"true\" --name hiveserver2 apache/hive:4.0.0-alpha-2 REM Keep the window open so that the user can review the output pause","title":"Steps to create the setup"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.8_Hive-ApacheOfficial/#steps-to-verify-the-setup","text":"To check the connectivity between all components: docker ps You should see the postgres , metastore , and hiveserver2 containers running. Connect to Beeline on HiveServer2: docker exec -it hiveserver2 /bin/bash beeline -u 'jdbc:hive2://hiveserver2:10000/' Create a sample table: CREATE TABLE movies (id INT, name STRING); INSERT INTO movies VALUES (1, 'Rambo'), (2, 'Mogambo'); SELECT * FROM movies; Exit Beeline: !quit;","title":"Steps to verify the Setup"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.8_Hive-ApacheOfficial/#how-data-is-stored-in-this-setup-by-default","text":"This setup is fully self-contained and enough for a Hive warehouse. The data is saved in a Docker volume, and the metadata is stored in a PostgreSQL server. You can later modify this setup to add a Hadoop Namenode (like in real-world scenarios) or even use Amazon S3 or ADLS for data storage. In this setup, Hive data will be stored in the following locations: Postgres Database : The Postgres container is used to store the metadata (table schemas, databases, etc.) for Hive. This is where the Hive Metastore connects to manage and query metadata. Database: metastore_db , User: hive , Password: password . HDFS for Table Data : We are using the volume warehouse mounted to /opt/hive/data/warehouse in both the Metastore and HiveServer2 containers. This is where Hive stores its table data files. The directory /opt/hive/data/warehouse in the containers corresponds to the warehouse volume on our host system. Metastore Initialization : The Metastore container connects to Postgres and uses the JDBC driver ( postgresql-42.5.1.jar ) to initialize the Hive Metastore database. This ensures that Hive's metadata is managed via Postgres. HiveServer2 : The HiveServer2 container connects to the Metastore (via thrift://metastore:9083 ) to retrieve metadata and execute Hive queries. The table data is stored in the same warehouse directory mounted as a volume, allowing HiveServer2 to access the data files in HDFS.","title":"How data is stored in this setup by default"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.8_Hive-ApacheOfficial/#thrift-metastore-uris-for-this-setup","text":"thrift://metastore:9083 : This is the Metastore Thrift URI that other services (like HiveServer2 or Spark) use to connect to the Hive Metastore and retrieve metadata about tables and databases. 9083 is a default port. thrift://hiveserver2:10000 : This URI is for the HiveServer2 service, which is used to execute Hive queries. Applications (like Beeline or JDBC/ODBC clients) connect to this URI to run SQL queries in Hive. 10000 is a default port.","title":"thrift metastore uris for this setup"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.8_Hive-ApacheOfficial/#how-to-connect-a-spark-session-to-this-setup","text":"In this setup, even without explicitly configuring hive-site.xml or adding any special configuration files, Spark can still interact with Hive by creating a Spark session that points to Hive and using Hive's metastore through the Thrift interface. Here's how it works: Create a Spark Session : When you create a Spark session, make sure to specify that it should use Hive. Spark will use the Hive Thrift Server (HiveServer2) to access Hive tables and metadata. Here's how you can create the session: ```python # Create a Spark session to connect with Hive Metastore # 'thrift://metastore:9083' is the URI of the Hive Metastore service in the container. # 9083 is the default port for the Hive Metastore using the Thrift protocol. # Spark will use this to fetch table metadata and manage Hive tables. # Enable Hive support so Spark can read/write Hive tables using Hive's metadata and storage. from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"Spark Hive Example\") \\ .config(\"spark.sql.catalogImplementation\", \"hive\") \\ .config(\"hive.metastore.uris\", \"thrift://metastore:9083\") \\ .enableHiveSupport() \\ .getOrCreate() ``` Create a Table and Store Data : Once the session is created, you can create tables and store data in Hive without needing hive-site.xml . For example, you can create a table and insert data like this: ```python # Create a DataFrame with some sample data data = [(\"Jango\", 30), (\"Jamila\", 25)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Create a table in Hive and write data to it df.write.mode(\"overwrite\").saveAsTable(\"default.person\") # Verify that the table was created and data was stored spark.sql(\"SELECT * FROM default.person\").show() ``` Where is the Data Stored? : The data you insert will be saved in the warehouse directory mounted to /opt/hive/data/warehouse in the HiveServer2 container. This happens automatically since you have the Docker volume set up to persist the data. For example, after creating the person table, the actual data will be stored in a directory like /opt/hive/data/warehouse/default/person inside the hiveserver2 container, and it will persist even if the container restarts.","title":"How to connect a Spark Session to this setup"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.8_Hive-ApacheOfficial/#configuration-reference","text":"Component Details PostgreSQL Hive Metastore - Database Name : metastore_db - Username : hive - Password : password - Connection URL : jdbc:postgresql://postgres:5432/metastore_db Schema Tool for Hive Tables - Location : /opt/hive/bin/schematool - Command : /opt/hive/bin/schematool -dbType postgres -initOrUpgradeSchema Hive Configuration Directory /opt/hive/conf Hive Version 4.0.0-alpha-2 HiveServer2 Startup Script /opt/hive/bin/ext/hiveserver2.sh PostgreSQL Container - Network : dasnet - Image : postgres:13 - Command : docker run -d --network dasnet --name postgres -e POSTGRES_DB=metastore_db -e POSTGRES_USER=hive -e POSTGRES_PASSWORD=password postgres:13 Metastore Container - Network : dasnet - Port : 9083 - Warehouse Data Directory : /opt/hive/data/warehouse - JDBC Driver : /opt/hive/lib/postgres.jar - Image : apache/hive:4.0.0-alpha-2 HiveServer2 Container - Network : dasnet - Ports : 10000 (Thrift service), 10002 (additional) - Warehouse Data Directory : /opt/hive/data/warehouse - Metastore URI : thrift://metastore:9083 - Image : apache/hive:4.0.0-alpha-2 Note : This setup works smoothly for the metastore and PostgreSQL combination, but restarting HiveServer2 doesn't work, and you need to recreate the HiveServer2 container alone. Official Hive Docker Hive Files Github","title":"Configuration Reference"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.8_Hive-ApacheOfficial/#add-hue-to-the-system","text":"To install Apache Hue and connect it to your existing environment, we'll need to configure Hue to work with Hive and PostgreSQL. Here's a step-by-step guide on how to do this:","title":"Add hue to the system"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.8_Hive-ApacheOfficial/#step-by-step-guide-to-install-and-connect-hue-to-hive","text":"Create a Docker Network (should be already created): The dasnet network shoudld be already created. If not, create it using command: bash docker network create dasnet Run Hue Docker Container: You can run Hue in a Docker container and connect it to your existing Hive and PostgreSQL setup. Here's the command to run Hue: bash docker run -d --network dasnet -p 8888:8888 --name hue gethue/hue:latest Configure Hue to Connect to Hive Note: You can either edit the hue.ini file directly inside the container or modify it locally and then copy it to the /usr/share/hue/desktop/conf/ directory in the container. After making the changes you can copy the file to container using the followig command(cd to the folder containing the hue.ini file) docker cp ./hue.ini hue:/usr/share/hue/desktop/conf/ To connect Hue to HiveServer2, you need to update the hue.ini configuration file in the Hue container. First, open a terminal in the Hue container: bash docker exec -it hue /bin/bash Navigate to the directory where the hue.ini file is located: bash cd /usr/share/hue/desktop/conf/ Open the hue.ini file using a text editor like nano or vim : bash nano hue.ini Look for the [beeswax] section in the file and add the following lines to point Hue to your HiveServer2 instance: ini [[beeswax]] # HiveServer2 connection settings hive_server_host=localhost hive_server_port=10000 # Optional authentication settings for HiveServer2 auth_username=hive auth_password=password Configure PostgreSQL as Hue's Backend Database To make Hue use your PostgreSQL database (the one used by Hive Metastore), update the database configuration in the hue.ini file. In the same hue.ini file, find the [desktop] section and add the following lines to configure PostgreSQL as the backend database: ini [desktop] [[database]] engine=postgresql_psycopg2 host=postgres port=5432 user=hive password=password name=metastore_db There should only be one `[[database]]` section and one `[[beeswax]]` section in the configuration file, and these sections should always use double brackets (`[[ ]]`). Common errors that occurs if these are not followed are: ERROR: Error in configuration file '/usr/share/hue/desktop/conf/hue.ini': Parsing failed with several errors. Error in configuration file '/usr/share/hue/desktop/conf/hue.ini': Duplicate section name at line 524. Restart the Hue Container, then access it: After making changes to the hue.ini file, exit the editor and container, and restart the Hue container to apply the changes: bash docker restart hue Then access hue using the following command: http://localhost:8888 The credential you choose will be your root credential. For me it is `dwdas/Passw0rd`","title":"Step-by-Step Guide to Install and Connect Hue to Hive"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.8_Hive-ApacheOfficial/#draft-do-not-useconnecting-to-spark-using-hive-sitexml","text":"Note: 1. SparkConf : If you programmatically set configurations using the SparkConf object in your Spark application, these settings have the highest priority. 2. Flags passed to spark-submit : If you pass configuration options directly when running the Spark job using spark-submit (e.g., --conf spark.executor.memory=4g ), these come next. 3. spark-defaults.conf : This is the default configuration file for Spark, and properties here are considered last. They will only apply if the settings haven't been specified by SparkConf or spark-submit . In this section, we will show how to create a Spark container from scratch and connect it to the existing Hive environment that we already set up using Docker. You will learn step by step how to configure Spark so it can talk to Hive, create tables in Hive, and use PySpark to query and manage those tables. Create and Run the Spark Container : First, we need to create a new Spark container. This Spark container will be on the same network as your Hive and PostgreSQL containers so that they can communicate with each other. Run this command to create the Spark container and connect it to the Docker network ( dasnet ): bash docker run -d --network dasnet --name spark-container -p 4040:4040 -p 8080:8080 -v /path/to/spark-conf:/opt/spark/conf bitnami/spark:latest Configure Hive for Spark : Now, we need to tell Spark where to find Hive. We do this by setting up the hive-site.xml file with the correct information about the Hive metastore. Here\u2019s what the file should look like: xml <configuration> <property> <name>hive.metastore.uris</name> <value>thrift://metastore:9083</value> </property> <property> <name>javax.jdo.option.ConnectionDriverName</name> <value>org.postgresql.Driver</value> </property> <property> <name>javax.jdo.option.ConnectionURL</name> <value>jdbc:postgresql://postgres:5432/metastore_db</value> </property> <property> <name>javax.jdo.option.ConnectionUserName</name> <value>hive</value> </property> <property> <name>javax.jdo.option.ConnectionPassword</name> <value>password</value> </property> </configuration> Copy this file to your Spark container using: bash docker cp /path/to/hive-site.xml spark-container:/opt/spark/conf/hive-site.xml Add PostgreSQL Driver to Spark : Since Hive uses PostgreSQL, you need to add the PostgreSQL driver to the Spark container so that Spark can connect to the Hive metastore. Copy the PostgreSQL driver ( postgresql-42.5.1.jar ) into the Spark container: bash docker cp /path/to/postgresql-42.5.1.jar spark-container:/opt/spark/jars/ Start PySpark with Hive Support : Now, start the PySpark session from the Spark container with Hive support enabled: bash docker exec -it spark-container /bin/bash pyspark --conf spark.sql.catalogImplementation=hive Create and Query Tables Using PySpark : Inside the PySpark shell, you can create tables and query them. Here\u2019s how: Create a table : python spark.sql(\"CREATE TABLE IF NOT EXISTS sample_table (id INT, name STRING)\") Insert data into the table : python spark.sql(\"INSERT INTO sample_table VALUES (1, 'Alice'), (2, 'Bob')\") Query the table : python result = spark.sql(\"SELECT * FROM sample_table\") result.show() Verify the Table in Hive : After creating the table using PySpark, you can check if the table exists in Hive using Beeline: bash docker exec -it hiveserver2 /bin/bash beeline -u 'jdbc:hive2://hiveserver2:10000/' Run this query in Beeline to verify: sql SHOW TABLES; SELECT * FROM sample_table; \u00a9 2024 Das. All Rights Reserved.","title":"[Draft DO NOT USE]Connecting To Spark using hive-site.xml"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9.1_Hive_Concepts/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Where to Store Hive Warehouse Data in a Distributed System When we are working with Hive in a distributed system, one important thing is where the data should be stored. By \"data,\" we mean the actual table data that you create using Hive. This is different from the metadata, which is stored in the Hive Metastore. Let\u2019s discuss the different options for storing Hive's warehouse data in a distributed system and how it works when you have multiple services like Spark , Hive , and Hadoop running in separate containers or machines. Understanding the Problem In a multi-container or multi-node system (like when you are running Spark, Hive, Hadoop, etc., on separate machines or containers), you need to make sure that all nodes can access the same data . If each node stores its data locally (e.g., /opt/hive/data/warehouse on each machine), the data won\u2019t be shared across all the nodes, which creates problems. Each machine will have its own local data, and other machines won't be able to see or access it. To avoid this, we need to store the data in a shared location that all nodes can access. 1. Use HDFS The most common approach is to use HDFS . HDFS is designed to store data in a distributed way, so all nodes in the cluster can read and write data to the same HDFS location. For example, you would set up Hive\u2019s warehouse directory like this in the hive-site.xml file: <property> <name>hive.metastore.warehouse.dir</name> <value>hdfs://namenode:8020/user/hive/warehouse</value> <description>This is the location where Hive stores table data in HDFS.</description> </property> Here, hdfs://namenode:8020 is the HDFS path where the actual data is stored. This way, all nodes (Spark, Hive, Hadoop, etc.) can access the same data because they are all connected to HDFS. How it works in a distributed system: All your containers or nodes (e.g., Spark, Hive, Hadoop) are configured to use HDFS. When you create a table in Hive, the data is stored in the hdfs://namenode:8020/user/hive/warehouse folder. Whether you're using Spark to query the data or Hive to manage it, everyone accesses the same location in HDFS, which avoids data inconsistency. 2. Use Cloud Storage (Amazon S3, ADLS etc) If you\u2019re running Hive in a cloud environment, you can also use cloud storage services like Amazon S3 or ADLS . These services act like distributed storage systems where all nodes can access the same data, just like HDFS. For example, if you are using Amazon S3 , you can set the warehouse directory in hive-site.xml like this: <property> <name>hive.metastore.warehouse.dir</name> <value>s3a://my-bucket/hive/warehouse</value> <description>This is the location where Hive stores table data in Amazon S3.</description> </property> How it works in a distributed system: All your containers (Spark, Hive, Hadoop) are configured to access S3. When you create a table in Hive, the actual data is stored in the s3a://my-bucket/hive/warehouse directory. Whether you are using Spark, Hive, or any other service, everyone accesses the same location in S3. This works well if you are running your system in the cloud, where distributed storage solutions like S3 or Azure Blob Storage are available. 3. Use NFS (Network File System) or Shared Filesystems If you don\u2019t want to use HDFS or cloud storage, you can set up a Network File System (NFS) or another type of shared filesystem that all nodes can access. For example, you might set up an NFS mount like /mnt/nfs/hive/warehouse , which is shared across all your servers or containers. In this case, the hive-site.xml configuration would look like this: <property> <name>hive.metastore.warehouse.dir</name> <value>/mnt/nfs/hive/warehouse</value> <description>This is the location where Hive stores table data in the shared NFS mount.</description> </property> How it works in a distributed system: All your containers or servers have the same NFS mount point (e.g., /mnt/nfs/hive/warehouse ). When you create a table in Hive, the actual data is stored in the shared directory. All nodes (Spark, Hive, Hadoop, etc.) can read and write data to the same location. This works for on-premises setups where a shared network drive is used to store data. 4. Avoid Using Local File System In a distributed system, don\u2019t use the local file system (like /opt/hive/data/warehouse ). If you store data on the local file system, each node will have its own copy of the data, and this can lead to problems because: Data will be spread across different machines, and there will be no consistency. One machine won\u2019t know about the data on another machine, making it impossible to run distributed queries properly. This option is only fine for single-node setups or local testing, where everything is running on one machine.","title":"Hive Concepts"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9.1_Hive_Concepts/#where-to-store-hive-warehouse-data-in-a-distributed-system","text":"When we are working with Hive in a distributed system, one important thing is where the data should be stored. By \"data,\" we mean the actual table data that you create using Hive. This is different from the metadata, which is stored in the Hive Metastore. Let\u2019s discuss the different options for storing Hive's warehouse data in a distributed system and how it works when you have multiple services like Spark , Hive , and Hadoop running in separate containers or machines.","title":"Where to Store Hive Warehouse Data in a Distributed System"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9.1_Hive_Concepts/#understanding-the-problem","text":"In a multi-container or multi-node system (like when you are running Spark, Hive, Hadoop, etc., on separate machines or containers), you need to make sure that all nodes can access the same data . If each node stores its data locally (e.g., /opt/hive/data/warehouse on each machine), the data won\u2019t be shared across all the nodes, which creates problems. Each machine will have its own local data, and other machines won't be able to see or access it. To avoid this, we need to store the data in a shared location that all nodes can access.","title":"Understanding the Problem"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9.1_Hive_Concepts/#1-use-hdfs","text":"The most common approach is to use HDFS . HDFS is designed to store data in a distributed way, so all nodes in the cluster can read and write data to the same HDFS location. For example, you would set up Hive\u2019s warehouse directory like this in the hive-site.xml file: <property> <name>hive.metastore.warehouse.dir</name> <value>hdfs://namenode:8020/user/hive/warehouse</value> <description>This is the location where Hive stores table data in HDFS.</description> </property> Here, hdfs://namenode:8020 is the HDFS path where the actual data is stored. This way, all nodes (Spark, Hive, Hadoop, etc.) can access the same data because they are all connected to HDFS.","title":"1. Use HDFS"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9.1_Hive_Concepts/#how-it-works-in-a-distributed-system","text":"All your containers or nodes (e.g., Spark, Hive, Hadoop) are configured to use HDFS. When you create a table in Hive, the data is stored in the hdfs://namenode:8020/user/hive/warehouse folder. Whether you're using Spark to query the data or Hive to manage it, everyone accesses the same location in HDFS, which avoids data inconsistency.","title":"How it works in a distributed system:"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9.1_Hive_Concepts/#2-use-cloud-storage-amazon-s3-adls-etc","text":"If you\u2019re running Hive in a cloud environment, you can also use cloud storage services like Amazon S3 or ADLS . These services act like distributed storage systems where all nodes can access the same data, just like HDFS. For example, if you are using Amazon S3 , you can set the warehouse directory in hive-site.xml like this: <property> <name>hive.metastore.warehouse.dir</name> <value>s3a://my-bucket/hive/warehouse</value> <description>This is the location where Hive stores table data in Amazon S3.</description> </property>","title":"2. Use Cloud Storage (Amazon S3, ADLS etc)"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9.1_Hive_Concepts/#how-it-works-in-a-distributed-system_1","text":"All your containers (Spark, Hive, Hadoop) are configured to access S3. When you create a table in Hive, the actual data is stored in the s3a://my-bucket/hive/warehouse directory. Whether you are using Spark, Hive, or any other service, everyone accesses the same location in S3. This works well if you are running your system in the cloud, where distributed storage solutions like S3 or Azure Blob Storage are available.","title":"How it works in a distributed system:"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9.1_Hive_Concepts/#3-use-nfs-network-file-system-or-shared-filesystems","text":"If you don\u2019t want to use HDFS or cloud storage, you can set up a Network File System (NFS) or another type of shared filesystem that all nodes can access. For example, you might set up an NFS mount like /mnt/nfs/hive/warehouse , which is shared across all your servers or containers. In this case, the hive-site.xml configuration would look like this: <property> <name>hive.metastore.warehouse.dir</name> <value>/mnt/nfs/hive/warehouse</value> <description>This is the location where Hive stores table data in the shared NFS mount.</description> </property>","title":"3. Use NFS (Network File System) or Shared Filesystems"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9.1_Hive_Concepts/#how-it-works-in-a-distributed-system_2","text":"All your containers or servers have the same NFS mount point (e.g., /mnt/nfs/hive/warehouse ). When you create a table in Hive, the actual data is stored in the shared directory. All nodes (Spark, Hive, Hadoop, etc.) can read and write data to the same location. This works for on-premises setups where a shared network drive is used to store data.","title":"How it works in a distributed system:"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9.1_Hive_Concepts/#4-avoid-using-local-file-system","text":"In a distributed system, don\u2019t use the local file system (like /opt/hive/data/warehouse ). If you store data on the local file system, each node will have its own copy of the data, and this can lead to problems because: Data will be spread across different machines, and there will be no consistency. One machine won\u2019t know about the data on another machine, making it impossible to run distributed queries properly. This option is only fine for single-node setups or local testing, where everything is running on one machine.","title":"4. Avoid Using Local File System"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9.2_Hadoop_Concepts/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Hadoop Modes of Operation Hadoop can work in three different modes, each suited for different environments and use cases. Let's look at them briefly: 1. Standalone Mode This is the simplest mode of Hadoop. In Standalone Mode , Hadoop runs on a single machine and doesn't use HDFS (Hadoop Distributed File System) . It's mostly used for testing and learning because there's no real distribution of data or processing. No HDFS : Files are stored locally. No distributed processing : Everything happens on one machine. Use Case : Good for testing code or doing small experiments. 2. Pseudo-distributed Mode In Pseudo-distributed Mode , Hadoop runs on a single machine , but it behaves as if it\u2019s a distributed system . Each Hadoop service like NameNode, DataNode , and ResourceManager runs as a separate process on the same machine. HDFS is used : Data is distributed across simulated nodes (which are really just separate processes on the same machine). Single machine but acts distributed . Use Case : For developers who want to test on a single machine but still mimic how Hadoop behaves in a real environment. 3. Fully-Distributed Mode In Fully-Distributed Mode , Hadoop runs across multiple machines. Each service (like NameNode, DataNode , etc.) runs on different machines, and data is truly distributed. Real distribution : Data is spread across many machines. Use Case : This is used in production environments where Hadoop processes large datasets across a cluster of machines. Key Takeaway Standalone : All on one machine, no HDFS. Pseudo-distributed : Simulates distribution on one machine. Fully-distributed : Runs across multiple machines, real-world usage. Every Hadoop architecture soul is fsimage","title":"Hadoop Concepts"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9.2_Hadoop_Concepts/#hadoop-modes-of-operation","text":"Hadoop can work in three different modes, each suited for different environments and use cases. Let's look at them briefly:","title":"Hadoop Modes of Operation"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9.2_Hadoop_Concepts/#1-standalone-mode","text":"This is the simplest mode of Hadoop. In Standalone Mode , Hadoop runs on a single machine and doesn't use HDFS (Hadoop Distributed File System) . It's mostly used for testing and learning because there's no real distribution of data or processing. No HDFS : Files are stored locally. No distributed processing : Everything happens on one machine. Use Case : Good for testing code or doing small experiments.","title":"1. Standalone Mode"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9.2_Hadoop_Concepts/#2-pseudo-distributed-mode","text":"In Pseudo-distributed Mode , Hadoop runs on a single machine , but it behaves as if it\u2019s a distributed system . Each Hadoop service like NameNode, DataNode , and ResourceManager runs as a separate process on the same machine. HDFS is used : Data is distributed across simulated nodes (which are really just separate processes on the same machine). Single machine but acts distributed . Use Case : For developers who want to test on a single machine but still mimic how Hadoop behaves in a real environment.","title":"2. Pseudo-distributed Mode"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9.2_Hadoop_Concepts/#3-fully-distributed-mode","text":"In Fully-Distributed Mode , Hadoop runs across multiple machines. Each service (like NameNode, DataNode , etc.) runs on different machines, and data is truly distributed. Real distribution : Data is spread across many machines. Use Case : This is used in production environments where Hadoop processes large datasets across a cluster of machines.","title":"3. Fully-Distributed Mode"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9.2_Hadoop_Concepts/#key-takeaway","text":"Standalone : All on one machine, no HDFS. Pseudo-distributed : Simulates distribution on one machine. Fully-distributed : Runs across multiple machines, real-world usage.","title":"Key Takeaway"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9.2_Hadoop_Concepts/#every-hadoop-architecture-soul-is-fsimage","text":"","title":"Every Hadoop architecture soul is fsimage"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/","text":"Understanding Dockerfile CMD and ENTRYPOINT Instructions What is ENTRYPOINT? What is CMD? Key Differences Between ENTRYPOINT and CMD Docker PS Error For Our Windows Users: For Our Mac Users: Backup entire docker to your laptop Save Docker Containers, Images, and Volumes on Mac/Linux Save Docker Containers, Images, and Volumes on Windows Docker common errors Orchestration Tools - Docker Swarm vs Kubernetes Docker Swarm Kubernetes Comparing Docker Swarm and Kubernetes K3s, best of both worlds Which One Should You Choose? Summary Common docker commands Running Windows OS as a Container in Docker Switching Docker Desktop to Windows Mode Switching Using Command Line Further Reading Microsoft Base Images Ready-Made Bundles Other Combinations Types of Kubernetes For the busy people: Kubernetes Brands How I push customized Images to Docker Hub(Website) Step 1: Log In to Docker Hub Step 2: Find the Image Names Step 3: Tag the Images Step 4: Push the Images to Docker Hub Conclusion Troubleshooting docker errors Steps to Debug the Issue 1. Check the Container Logs 2. Inspect the Container Status 3. Check for File Permissions and Volume Issues Understanding Dockerfile CMD and ENTRYPOINT Instructions CMD and ENTRYPOINT are important Dockerfile instructions that define what command runs when a Docker container starts. Here, I will try to explain the concepts: What is ENTRYPOINT? ENTRYPOINT sets the main process that will run inside the container. For example: ENTRYPOINT [\"/usr/bin/my-app\"] In this case, /usr/bin/my-app is the process that will run when the container starts. What is CMD? CMD specifies the default arguments for the ENTRYPOINT process. For instance: ENTRYPOINT [\"/usr/bin/my-app\"] CMD [\"help\"] Here, help is passed as an argument to /usr/bin/my-app . Key Differences Between ENTRYPOINT and CMD ENTRYPOINT : Defines the main process to run in the container. CMD : Provides default arguments for the ENTRYPOINT process. Override : CMD can be easily overridden by passing arguments in the docker run command. ENTRYPOINT can be changed using the --entrypoint flag in docker run , but this is rarely necessary. Docker PS Error For Our Windows Users: Verify Docker's Installation Path: Navigate to C:\\Program Files\\Docker\\Docker\\resources\\bin via your command prompt or PowerShell. While you're there, try running docker ps . If it responds, you're in luck! If not, let's move to the next step. Update the System PATH: Sometimes, Windows isn't aware of where Docker is. We'll need to tell it. Open 'System Properties' by right-clicking on the Windows start button and selecting 'System'. Click on 'Advanced system settings', then choose 'Environment Variables'. Locate the PATH variable under 'System Variables'. Click on it and then select 'Edit'. Add a new entry with the path: C:\\Program Files\\Docker\\Docker\\resources\\bin . Confirm with 'OK'. Using PowerShell to Update the PATH: If you're a fan of PowerShell, you can also add the path using the following command: powershell [Environment]::SetEnvironmentVariable(\"PATH\", $env:PATH + \";C:\\Program Files\\Docker\\Docker\\resources\\bin\", \"Machine\") Check if its running now. Just open command prompt and run docker ps . You should get some output. For example: For Our Mac Users: Verify Docker's Installation: Open your terminal and type in docker --version . This ensures that Docker is installed. Is Docker Running? Check if the Docker Desktop application is running. If it's not, fire it up! Update the Shell's PATH: Sometimes, the shell doesn\u2019t know where Docker is located. To fix this: bash echo \"export PATH=/usr/local/bin:$PATH\" >> ~/.bash_profile source ~/.bash_profile Final Check: Close and reopen your terminal, then try docker ps . If all's well, it should work! Backup entire docker to your laptop Save Docker Containers, Images, and Volumes on Mac/Linux To back up Docker containers, images, and volumes on Mac/Linux, you can use the following script: #!/bin/bash # Create directories to store backups mkdir -p docker_image_backups docker_container_backups docker_volume_backups # Backup Docker images for image in $(docker images --format \"{{.Repository}}:{{.Tag}}\"); do sanitized_image_name=$(echo $image | tr / _) docker save -o docker_image_backups/${sanitized_image_name}.tar $image done # Backup Docker containers for container in $(docker ps -a --format \"{{.Names}}\"); do docker export -o docker_container_backups/${container}.tar $container done # Backup Docker volumes for volume in $(docker volume ls --format \"{{.Name}}\"); do docker run --rm -v ${volume}:/volume -v $(pwd)/docker_volume_backups:/backup alpine sh -c \"cd /volume && tar czf /backup/${volume}.tar.gz .\" done # Create a single tarball containing all backups tar czf docker_backup_$(date +%Y%m%d).tar.gz docker_image_backups docker_container_backups docker_volume_backups echo \"Backup completed successfully!\" To run the script: Save the script to a file, e.g., backup_docker.sh . Make the script executable: sh chmod +x backup_docker.sh Run the script: sh ./backup_docker.sh This will create a full backup of all Docker images, containers, and volumes. Save Docker Containers, Images, and Volumes on Windows To back up Docker containers, images, and volumes on Windows, follow these steps: Create a folder and save the following content as backup_docker.ps1 : ```powershell Backup Docker Images docker images -q | ForEach-Object { docker save -o \"$($ ).tar\" $ } Backup Running Containers docker ps -q | ForEach-Object { docker export -o \"$($ ).tar\" $ } Backup Docker Volumes $volumes = docker volume ls -q foreach ($volume in $volumes) { docker run --rm -v ${volume}:/volume -v $(pwd):/backup ubuntu tar cvf /backup/${volume}_backup.tar /volume } Backup Docker Configurations Copy-Item -Path \"C:\\path\\to\\your\\docker\\configurations\" -Destination \"C:\\path\\to\\your\\backup\\location\" -Recurse ``` Open PowerShell with administrative privileges and navigate to the folder you created: powershell cd path\\to\\your\\folder Set the execution policy to allow the script to run temporarily: powershell Set-ExecutionPolicy -ExecutionPolicy Bypass -Scope Process Run the script: powershell .\\backup_docker.ps1 This way you can back up all your Docker containers, images, and volumes to the current folder. Docker common errors HTTP code 500) server error - Ports are not available: exposing port TCP 0.0.0.0:50070 -> 0.0.0.0:0: listen tcp 0.0.0.0:50070: bind: An attempt was made to access a socket in a way forbidden by its access permissions. Execute command net stop winnat docker start <full container name> net start winnat Orchestration Tools - Docker Swarm vs Kubernetes To manage complex applications, many developers use containers. Containers package all the necessary dependencies, making applications portable, fast, secure, scalable, and easy to manage. To handle multiple containers, you need an orchestration tool like Docker Swarm or Kubernetes. Both tools manage containers but have different strengths and weaknesses. This article will help you decide which one is right for you. Docker Swarm Docker Swarm, an open-source orchestration tool by Docker, turns multiple Docker instances into a single virtual host. Here are its key components: Component Description Nodes Individual Docker instances. Services and Tasks The applications you run. Load Balancers Distribute requests across nodes. Advantages of Docker Swarm: - Ease of Use: Simple installation and understanding. - Integration: Works seamlessly with Docker CLI. - Automated Load Balancing: Distributes traffic within the cluster automatically. Disadvantages of Docker Swarm: - Limited Functionality: Less powerful compared to Kubernetes. - Basic Automation: Not as robust as Kubernetes. Kubernetes Kubernetes, developed by Google, offers a more complex structure with nodes, pods, namespaces, and more. Component Description Nodes Worker machines in the cluster. Pods Smallest deployable units, containing one or more containers. Namespaces Logical isolation for resources. Advantages of Kubernetes: - Community Support: Backed by Google, with a large open-source community. - Operating System Support: Works on all OS. - Scalability and Management: Handles large and complex workloads. - Automation and Self-Healing: Automatically scales and repairs itself. - Built-in Monitoring: Includes monitoring tools. - Cloud Support: Available on Google Cloud, Azure, and AWS. Disadvantages of Kubernetes: - Complexity: Difficult to install and learn. - Separate Tools: Requires learning new CLI tools. - Transition: Moving from Docker Swarm can be challenging. - Overhead: Can be overly complex for simple applications. Comparing Docker Swarm and Kubernetes Aspect Docker Swarm Kubernetes Installation Easy to install and use. Works with Docker CLI. Complex installation, separate CLI tools. Application Deployment YAML or Docker Compose for services or microservices. More options like namespaces, pods, and deployments. Availability and Scaling High availability, but no automatic scaling. Highly available, fault-tolerant, self-healing, and automatic scaling. Monitoring Requires third-party tools. Built-in monitoring and third-party integrations. Security Uses TLS for security. Supports multiple security protocols (RBAC, SSL/TLS, secrets management). Load Balancing Automatic load balancing using DNS. Uses tools like Nginx Ingress for load balancing. K3s, best of both worlds K3s , a lightweight version of Kubernetes. It gives you the full Kubernetes API without complexity. It's easy to use and CNCF certified. Consider K3s if you want Kubernetes features with simpler management. Which One Should You Choose? Docker Swarm: Best for beginners and small-scale applications due to its ease of use. Kubernetes: Ideal for complex and large-scale projects requiring robust features and automation. K3s: Suitable if you want Kubernetes features with less complexity. Summary Feature Docker Swarm Kubernetes K3s Component - Nodes: Individual Docker instances. - Services and Tasks: The applications you run. - Load Balancers: Distribute requests across nodes. - Nodes: Worker and master nodes. - Pods: Smallest deployable units. - Namespaces: Virtual clusters. - Config Maps: Manage configuration. - Nodes: Worker and master nodes. - Pods: Smallest deployable units. - Namespaces: Virtual clusters. - Config Maps: Manage configuration. Advantages - Ease of Use: Simple installation and understanding. - Integration: Works seamlessly with Docker CLI. - Automated Load Balancing: Distributes traffic within the cluster automatically. - Community Support: Backed by Google, large open-source community. - Operating System Support: Works on all OS. - Scalability and Management: Handles large and complex workloads. - Automation and Self-Healing: Automatically scales and repairs itself. - Built-in Monitoring: Comes with monitoring tools. - Cloud Support: Available on Google Cloud, Azure, and AWS. - Lightweight: Easier and faster to set up. - Complete Kubernetes API: Offers all features without extra complexity. - CNCF Certified: Ensures compatibility and support. Disadvantages - Limited Functionality: Less powerful compared to Kubernetes. - Basic Automation: Not as robust as Kubernetes. - Complexity: Difficult to install and learn. - Separate Tools: Requires learning new CLI tools. - Transition: Moving from Docker Swarm can be hard. - Overhead: Can be overly complex for simple applications. - Limited Community Support: Smaller user base compared to Kubernetes. - Fewer Integrations: May not support all third-party tools. Application Deployment Deploy services or microservices using YAML or Docker Compose. Offers more options like namespaces, pods, and deployments. Supports deployments using YAML files with simplified configurations. Availability and Scaling High availability, but no automatic scaling. Highly available, fault-tolerant, self-healing, and automatic scaling. Provides high availability and simple scaling mechanisms. Monitoring Requires third-party tools. Built-in monitoring and third-party integrations. Supports basic monitoring with options for third-party integrations. Security Uses TLS for security. Supports multiple security protocols like RBAC, SSL/TLS, secrets management. Provides essential security features with easier management. Load Balancing Automatic load balancing using DNS. Uses tools like Nginx Ingress for load balancing. Simplified load balancing with integrated tools. Common docker commands Purpose Command \ud83c\udfc3 Run docker run <image> \u23f9\ufe0f Stop docker stop <container> \u25b6\ufe0f Start docker start <container> \ud83d\uddd1\ufe0f Remove Ctr docker rm <container> \ud83d\uddbc\ufe0f Remove Img docker rmi <image> \ud83d\udcc3 List Ctrs docker ps \ud83d\uddbc\ufe0f List Imgs docker images \u2b07\ufe0f Pull docker pull <image> \u2328\ufe0f Exec docker exec <container> \ud83c\udfd7\ufe0f Build docker build -t <tag> . \ud83d\udd0a Logs docker logs <container> \ud83d\udd0d Inspect docker inspect <container_or_image> \ud83d\udcca Stats docker stats <container> \ud83d\udcc1 Volume List docker volume ls \ud83c\udd95 Volume Create docker volume create <volume_name> \ud83d\udeae Volume Remove docker volume rm <volume_name> \ud83c\udf10 Network List docker network ls \ud83c\udf09 Network Create docker network create <network> \ud83d\udce1 Network Connect docker network connect <network> <container> \ud83d\udd0c Network Disconnect docker network disconnect <network> <container> \ud83d\udd04 Pull Latest docker pull <image>:latest \ud83d\udeab Build No Cache docker build --no-cache -t <tag> . Running Windows OS as a Container in Docker When you think of containers, you usually picture a small Linux OS. And you\u2019re right! But did you know that containers can run a small Windows OS too? Switching Docker Desktop to Windows Mode For most of your tasks, you've likely been using Docker containers, which are typically Linux-based. Running a Windows container might seem unusual. By default, Docker on your Windows machine operates in 'Linux mode.' To run Windows containers, you'll need to switch from this default Linux mode to Windows mode. And you can switch back to Linux containers easily. Note: When you switch to Windows mode, you won\u2019t be able to see your Linux containers. Switching Using Command Line You can also switch using the command line: & $Env:ProgramFiles\\Docker\\Docker\\DockerCli.exe -SwitchDaemon Further Reading For more information, check out: Windows Containers Documentation Microsoft Base Images Microsoft offers several base images to start building your own container images: Windows : Contains the full set of Windows APIs and system services (excluding server roles). Windows Server : Contains the full set of Windows APIs and system services. Windows Server Core : A smaller image with a subset of Windows Server APIs, including the full .NET framework and most server roles (excluding some like Fax Server). Nano Server : The smallest Windows Server image, supporting .NET Core APIs and some server roles. Ready-Made Bundles Microsoft offers ready-made bundles that combine Windows Server 2022 with apps like MongoDB. For example, if you need MongoDB on Windows Server 2022, you can use this Dockerfile . Other Combinations Other combinations you can create include: aspnet iis Django apache-http-php nginx dotnet35 golang nodejs python python-django rails ruby server-jre-8u51-windows-x64 mongodb mysql redis sqlite sqlserver-express PowerShellDSC_iis-10.0 Now you know that containers are not just small Linux OSs. They can be a mini Windows OS too! Types of Kubernetes Kubernetes is a system for managing containerized applications, but there are many ways to set it up based on your needs. The table below summriazes the various available kubernetes 'brands': For the busy people: Development : Minikube , K3d , Docker Desktop Kubernetes , and Kind are geared toward development and testing. Production : Kubeadm , Rancher , K3s , and managed services like Amazon EKS , Google GKE , and Azure AKS are suited for production environments. Mixed Use : K3s and MicroK8s can be used both in development and production, especially in edge computing and IoT. Kubernetes Brands Here's the revised table with the \"Environment\" column removed: Kubernetes Tool Company/Provider Best For Single Machine Setup Multi-Machine Setup Used By Minikube Kubernetes Community (CNCF) Development Yes No Individual developers, small startups. Kubeadm Kubernetes Community (CNCF) Production Yes (single node) Yes Enterprises, cloud service providers. Rancher SUSE Production Yes (for management) Yes Enterprises, companies managing multi-cloud or hybrid environments. K3s Rancher (SUSE) Development/Production Yes Yes IoT companies, edge computing solutions, small and medium enterprises. K3d Rancher (SUSE) Development Yes No Developers, small companies for testing multi-node setups. Docker Desktop Kubernetes Docker, Inc. Development Yes No Developers using Docker, small teams. MicroK8s Canonical (Ubuntu) Development/Production Yes Yes IoT and edge computing companies, startups. Amazon EKS Amazon Web Services (AWS) Production No Yes Large enterprises, companies using AWS. Google GKE Google Cloud Production No Yes Large enterprises, companies using Google Cloud. Azure AKS Microsoft Azure Production No Yes Enterprises, companies using Azure services. OpenShift Red Hat (IBM) Production Yes (for single node) Yes Enterprises needing integrated CI/CD, large companies in regulated industries. Kind (Kubernetes in Docker) Kubernetes Community (CNCF) Development Yes No Developers, CI/CD pipelines in tech companies. How I push customized Images to Docker Hub(Website) Recently, I set up a Kafka environment using the base images from Confluent. After tweaking and customizing these images to fit my specific needs, I realized that these modified images should be pushed to Docker Hub so I can easily reuse them later or share them with others. I had 9 images in total, so here\u2019s the process I followed. Step 1: Log In to Docker Hub The first thing I did was log in to Docker Hub using my Docker Hub username, dwdas9 . docker login It asked for my Docker Hub username and password, and once I provided those, I was logged in. Step 2: Find the Image Names To push the images, I needed to know their names. I used the following command to list all the Docker images on my local machine: docker images This command gave me a list of all the images, including their names, tags, and IDs. I picked out the relevant images that I had customized. Step 3: Tag the Images Before I could push the images, I had to tag them with my Docker Hub username and the repository name I wanted them to go into. Here\u2019s how I did it for each of the 9 images: docker tag confluentinc/cp-ksqldb-server:6.0.1 dwdas9/cp-ksqldb-server:v6 docker tag confluentinc/cp-kafka-rest:6.0.1 dwdas9/cp-kafka-rest:v6 docker tag confluentinc/cp-schema-registry:6.0.1 dwdas9/cp-schema-registry:v6 docker tag confluentinc/cp-enterprise-control-center:6.0.1 dwdas9/cp-enterprise-control-center:v6 docker tag confluentinc/cp-kafka-connect-base:6.0.1 dwdas9/cp-kafka-connect-base:v6 docker tag confluentinc/cp-server:6.0.1 dwdas9/cp-server:v6 docker tag confluentinc/cp-zookeeper:6.0.1 dwdas9/cp-zookeeper:v6 docker tag confluentinc/ksqldb-examples:6.0.1 dwdas9/ksqldb-examples:v6 docker tag confluentinc/cp-ksqldb-cli:6.0.1 dwdas9/cp-ksqldb-cli:v6 I replaced confluentinc with my username dwdas9 and added a custom tag v6 to each image. Step 4: Push the Images to Docker Hub With the images tagged, the next step was to push them to Docker Hub. I ran the following commands: docker push dwdas9/cp-ksqldb-server:v6 docker push dwdas9/cp-kafka-rest:v6 docker push dwdas9/cp-schema-registry:v6 docker push dwdas9/cp-enterprise-control-center:v6 docker push dwdas9/cp-kafka-connect-base:v6 docker push dwdas9/cp-server:v6 docker push dwdas9/cp-zookeeper:v6 docker push dwdas9/ksqldb-examples:v6 docker push dwdas9/cp-ksqldb-cli:v6 Docker started uploading each image to the repository. Once the upload was complete, I could see all my modified images on Docker Hub under my account. Conclusion And that\u2019s it! Now, my customized Kafka environment images are safely stored on Docker Hub, ready to be pulled down whenever I need them. If you\u2019re working on something similar, these steps should help you push your images too. Troubleshooting docker errors If your Docker container is showing up as orange (in Docker Desktop) or failed to start without giving specific details, it means the container likely encountered an error during startup. Docker doesn\u2019t always show detailed error messages in the UI, but you can retrieve more information using the following methods. Steps to Debug the Issue 1. Check the Container Logs You can check the logs for the failed container to see what went wrong. Run this command to inspect the container logs: docker logs your-container-name This will show you any errors or issues that occurred during the container\u2019s initialization. Look for specific errors related to: - Missing environment variables. - Errors in Spark or system configuration. - Issues with file mounting (e.g., mapping the conf directory). 2. Inspect the Container Status You can inspect the container to get more information about why it failed. Use the docker inspect command: docker inspect your-container-name This will provide detailed information about the container's configuration, including the exit code. Look for anything abnormal in the output, especially the State and ExitCode sections. 3. Check for File Permissions and Volume Issues Sometimes, volume mapping can cause issues, especially when the local directory being mounted doesn\u2019t have the correct permissions, or Docker has trouble accessing it. Make sure that the local mapped-folder folder has the correct permissions for Docker to access it. For example: Check if the directory exists and has read/write permissions: ls -ld mapped-folder If the directory is not accessible by Docker, try giving it the correct permissions: chmod -R 755 mapped-folder","title":"Docker Misc Concepts"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#understanding-dockerfile-cmd-and-entrypoint-instructions","text":"CMD and ENTRYPOINT are important Dockerfile instructions that define what command runs when a Docker container starts. Here, I will try to explain the concepts:","title":"Understanding Dockerfile CMD and ENTRYPOINT Instructions"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#what-is-entrypoint","text":"ENTRYPOINT sets the main process that will run inside the container. For example: ENTRYPOINT [\"/usr/bin/my-app\"] In this case, /usr/bin/my-app is the process that will run when the container starts.","title":"What is ENTRYPOINT?"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#what-is-cmd","text":"CMD specifies the default arguments for the ENTRYPOINT process. For instance: ENTRYPOINT [\"/usr/bin/my-app\"] CMD [\"help\"] Here, help is passed as an argument to /usr/bin/my-app .","title":"What is CMD?"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#key-differences-between-entrypoint-and-cmd","text":"ENTRYPOINT : Defines the main process to run in the container. CMD : Provides default arguments for the ENTRYPOINT process. Override : CMD can be easily overridden by passing arguments in the docker run command. ENTRYPOINT can be changed using the --entrypoint flag in docker run , but this is rarely necessary.","title":"Key Differences Between ENTRYPOINT and CMD"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#docker-ps-error","text":"","title":"Docker PS Error"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#for-our-windows-users","text":"Verify Docker's Installation Path: Navigate to C:\\Program Files\\Docker\\Docker\\resources\\bin via your command prompt or PowerShell. While you're there, try running docker ps . If it responds, you're in luck! If not, let's move to the next step. Update the System PATH: Sometimes, Windows isn't aware of where Docker is. We'll need to tell it. Open 'System Properties' by right-clicking on the Windows start button and selecting 'System'. Click on 'Advanced system settings', then choose 'Environment Variables'. Locate the PATH variable under 'System Variables'. Click on it and then select 'Edit'. Add a new entry with the path: C:\\Program Files\\Docker\\Docker\\resources\\bin . Confirm with 'OK'. Using PowerShell to Update the PATH: If you're a fan of PowerShell, you can also add the path using the following command: powershell [Environment]::SetEnvironmentVariable(\"PATH\", $env:PATH + \";C:\\Program Files\\Docker\\Docker\\resources\\bin\", \"Machine\") Check if its running now. Just open command prompt and run docker ps . You should get some output. For example:","title":"For Our Windows Users:"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#for-our-mac-users","text":"Verify Docker's Installation: Open your terminal and type in docker --version . This ensures that Docker is installed. Is Docker Running? Check if the Docker Desktop application is running. If it's not, fire it up! Update the Shell's PATH: Sometimes, the shell doesn\u2019t know where Docker is located. To fix this: bash echo \"export PATH=/usr/local/bin:$PATH\" >> ~/.bash_profile source ~/.bash_profile Final Check: Close and reopen your terminal, then try docker ps . If all's well, it should work!","title":"For Our Mac Users:"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#backup-entire-docker-to-your-laptop","text":"","title":"Backup entire docker to your laptop"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#save-docker-containers-images-and-volumes-on-maclinux","text":"To back up Docker containers, images, and volumes on Mac/Linux, you can use the following script: #!/bin/bash # Create directories to store backups mkdir -p docker_image_backups docker_container_backups docker_volume_backups # Backup Docker images for image in $(docker images --format \"{{.Repository}}:{{.Tag}}\"); do sanitized_image_name=$(echo $image | tr / _) docker save -o docker_image_backups/${sanitized_image_name}.tar $image done # Backup Docker containers for container in $(docker ps -a --format \"{{.Names}}\"); do docker export -o docker_container_backups/${container}.tar $container done # Backup Docker volumes for volume in $(docker volume ls --format \"{{.Name}}\"); do docker run --rm -v ${volume}:/volume -v $(pwd)/docker_volume_backups:/backup alpine sh -c \"cd /volume && tar czf /backup/${volume}.tar.gz .\" done # Create a single tarball containing all backups tar czf docker_backup_$(date +%Y%m%d).tar.gz docker_image_backups docker_container_backups docker_volume_backups echo \"Backup completed successfully!\" To run the script: Save the script to a file, e.g., backup_docker.sh . Make the script executable: sh chmod +x backup_docker.sh Run the script: sh ./backup_docker.sh This will create a full backup of all Docker images, containers, and volumes.","title":"Save Docker Containers, Images, and Volumes on Mac/Linux"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#save-docker-containers-images-and-volumes-on-windows","text":"To back up Docker containers, images, and volumes on Windows, follow these steps: Create a folder and save the following content as backup_docker.ps1 : ```powershell","title":"Save Docker Containers, Images, and Volumes on Windows"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#backup-docker-images","text":"docker images -q | ForEach-Object { docker save -o \"$($ ).tar\" $ }","title":"Backup Docker Images"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#backup-running-containers","text":"docker ps -q | ForEach-Object { docker export -o \"$($ ).tar\" $ }","title":"Backup Running Containers"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#backup-docker-volumes","text":"$volumes = docker volume ls -q foreach ($volume in $volumes) { docker run --rm -v ${volume}:/volume -v $(pwd):/backup ubuntu tar cvf /backup/${volume}_backup.tar /volume }","title":"Backup Docker Volumes"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#backup-docker-configurations","text":"Copy-Item -Path \"C:\\path\\to\\your\\docker\\configurations\" -Destination \"C:\\path\\to\\your\\backup\\location\" -Recurse ``` Open PowerShell with administrative privileges and navigate to the folder you created: powershell cd path\\to\\your\\folder Set the execution policy to allow the script to run temporarily: powershell Set-ExecutionPolicy -ExecutionPolicy Bypass -Scope Process Run the script: powershell .\\backup_docker.ps1 This way you can back up all your Docker containers, images, and volumes to the current folder.","title":"Backup Docker Configurations"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#docker-common-errors","text":"HTTP code 500) server error - Ports are not available: exposing port TCP 0.0.0.0:50070 -> 0.0.0.0:0: listen tcp 0.0.0.0:50070: bind: An attempt was made to access a socket in a way forbidden by its access permissions. Execute command net stop winnat docker start <full container name> net start winnat","title":"Docker common errors"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#orchestration-tools-docker-swarm-vs-kubernetes","text":"To manage complex applications, many developers use containers. Containers package all the necessary dependencies, making applications portable, fast, secure, scalable, and easy to manage. To handle multiple containers, you need an orchestration tool like Docker Swarm or Kubernetes. Both tools manage containers but have different strengths and weaknesses. This article will help you decide which one is right for you.","title":"Orchestration Tools - Docker Swarm vs Kubernetes"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#docker-swarm","text":"Docker Swarm, an open-source orchestration tool by Docker, turns multiple Docker instances into a single virtual host. Here are its key components: Component Description Nodes Individual Docker instances. Services and Tasks The applications you run. Load Balancers Distribute requests across nodes. Advantages of Docker Swarm: - Ease of Use: Simple installation and understanding. - Integration: Works seamlessly with Docker CLI. - Automated Load Balancing: Distributes traffic within the cluster automatically. Disadvantages of Docker Swarm: - Limited Functionality: Less powerful compared to Kubernetes. - Basic Automation: Not as robust as Kubernetes.","title":"Docker Swarm"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#kubernetes","text":"Kubernetes, developed by Google, offers a more complex structure with nodes, pods, namespaces, and more. Component Description Nodes Worker machines in the cluster. Pods Smallest deployable units, containing one or more containers. Namespaces Logical isolation for resources. Advantages of Kubernetes: - Community Support: Backed by Google, with a large open-source community. - Operating System Support: Works on all OS. - Scalability and Management: Handles large and complex workloads. - Automation and Self-Healing: Automatically scales and repairs itself. - Built-in Monitoring: Includes monitoring tools. - Cloud Support: Available on Google Cloud, Azure, and AWS. Disadvantages of Kubernetes: - Complexity: Difficult to install and learn. - Separate Tools: Requires learning new CLI tools. - Transition: Moving from Docker Swarm can be challenging. - Overhead: Can be overly complex for simple applications.","title":"Kubernetes"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#comparing-docker-swarm-and-kubernetes","text":"Aspect Docker Swarm Kubernetes Installation Easy to install and use. Works with Docker CLI. Complex installation, separate CLI tools. Application Deployment YAML or Docker Compose for services or microservices. More options like namespaces, pods, and deployments. Availability and Scaling High availability, but no automatic scaling. Highly available, fault-tolerant, self-healing, and automatic scaling. Monitoring Requires third-party tools. Built-in monitoring and third-party integrations. Security Uses TLS for security. Supports multiple security protocols (RBAC, SSL/TLS, secrets management). Load Balancing Automatic load balancing using DNS. Uses tools like Nginx Ingress for load balancing.","title":"Comparing Docker Swarm and Kubernetes"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#k3s-best-of-both-worlds","text":"K3s , a lightweight version of Kubernetes. It gives you the full Kubernetes API without complexity. It's easy to use and CNCF certified. Consider K3s if you want Kubernetes features with simpler management.","title":"K3s, best of both worlds"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#which-one-should-you-choose","text":"Docker Swarm: Best for beginners and small-scale applications due to its ease of use. Kubernetes: Ideal for complex and large-scale projects requiring robust features and automation. K3s: Suitable if you want Kubernetes features with less complexity.","title":"Which One Should You Choose?"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#summary","text":"Feature Docker Swarm Kubernetes K3s Component - Nodes: Individual Docker instances. - Services and Tasks: The applications you run. - Load Balancers: Distribute requests across nodes. - Nodes: Worker and master nodes. - Pods: Smallest deployable units. - Namespaces: Virtual clusters. - Config Maps: Manage configuration. - Nodes: Worker and master nodes. - Pods: Smallest deployable units. - Namespaces: Virtual clusters. - Config Maps: Manage configuration. Advantages - Ease of Use: Simple installation and understanding. - Integration: Works seamlessly with Docker CLI. - Automated Load Balancing: Distributes traffic within the cluster automatically. - Community Support: Backed by Google, large open-source community. - Operating System Support: Works on all OS. - Scalability and Management: Handles large and complex workloads. - Automation and Self-Healing: Automatically scales and repairs itself. - Built-in Monitoring: Comes with monitoring tools. - Cloud Support: Available on Google Cloud, Azure, and AWS. - Lightweight: Easier and faster to set up. - Complete Kubernetes API: Offers all features without extra complexity. - CNCF Certified: Ensures compatibility and support. Disadvantages - Limited Functionality: Less powerful compared to Kubernetes. - Basic Automation: Not as robust as Kubernetes. - Complexity: Difficult to install and learn. - Separate Tools: Requires learning new CLI tools. - Transition: Moving from Docker Swarm can be hard. - Overhead: Can be overly complex for simple applications. - Limited Community Support: Smaller user base compared to Kubernetes. - Fewer Integrations: May not support all third-party tools. Application Deployment Deploy services or microservices using YAML or Docker Compose. Offers more options like namespaces, pods, and deployments. Supports deployments using YAML files with simplified configurations. Availability and Scaling High availability, but no automatic scaling. Highly available, fault-tolerant, self-healing, and automatic scaling. Provides high availability and simple scaling mechanisms. Monitoring Requires third-party tools. Built-in monitoring and third-party integrations. Supports basic monitoring with options for third-party integrations. Security Uses TLS for security. Supports multiple security protocols like RBAC, SSL/TLS, secrets management. Provides essential security features with easier management. Load Balancing Automatic load balancing using DNS. Uses tools like Nginx Ingress for load balancing. Simplified load balancing with integrated tools.","title":"Summary"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#common-docker-commands","text":"Purpose Command \ud83c\udfc3 Run docker run <image> \u23f9\ufe0f Stop docker stop <container> \u25b6\ufe0f Start docker start <container> \ud83d\uddd1\ufe0f Remove Ctr docker rm <container> \ud83d\uddbc\ufe0f Remove Img docker rmi <image> \ud83d\udcc3 List Ctrs docker ps \ud83d\uddbc\ufe0f List Imgs docker images \u2b07\ufe0f Pull docker pull <image> \u2328\ufe0f Exec docker exec <container> \ud83c\udfd7\ufe0f Build docker build -t <tag> . \ud83d\udd0a Logs docker logs <container> \ud83d\udd0d Inspect docker inspect <container_or_image> \ud83d\udcca Stats docker stats <container> \ud83d\udcc1 Volume List docker volume ls \ud83c\udd95 Volume Create docker volume create <volume_name> \ud83d\udeae Volume Remove docker volume rm <volume_name> \ud83c\udf10 Network List docker network ls \ud83c\udf09 Network Create docker network create <network> \ud83d\udce1 Network Connect docker network connect <network> <container> \ud83d\udd0c Network Disconnect docker network disconnect <network> <container> \ud83d\udd04 Pull Latest docker pull <image>:latest \ud83d\udeab Build No Cache docker build --no-cache -t <tag> .","title":"Common docker commands"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#running-windows-os-as-a-container-in-docker","text":"When you think of containers, you usually picture a small Linux OS. And you\u2019re right! But did you know that containers can run a small Windows OS too?","title":"Running Windows OS as a Container in Docker"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#switching-docker-desktop-to-windows-mode","text":"For most of your tasks, you've likely been using Docker containers, which are typically Linux-based. Running a Windows container might seem unusual. By default, Docker on your Windows machine operates in 'Linux mode.' To run Windows containers, you'll need to switch from this default Linux mode to Windows mode. And you can switch back to Linux containers easily. Note: When you switch to Windows mode, you won\u2019t be able to see your Linux containers.","title":"Switching Docker Desktop to Windows Mode"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#switching-using-command-line","text":"You can also switch using the command line: & $Env:ProgramFiles\\Docker\\Docker\\DockerCli.exe -SwitchDaemon","title":"Switching Using Command Line"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#further-reading","text":"For more information, check out: Windows Containers Documentation","title":"Further Reading"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#microsoft-base-images","text":"Microsoft offers several base images to start building your own container images: Windows : Contains the full set of Windows APIs and system services (excluding server roles). Windows Server : Contains the full set of Windows APIs and system services. Windows Server Core : A smaller image with a subset of Windows Server APIs, including the full .NET framework and most server roles (excluding some like Fax Server). Nano Server : The smallest Windows Server image, supporting .NET Core APIs and some server roles.","title":"Microsoft Base Images"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#ready-made-bundles","text":"Microsoft offers ready-made bundles that combine Windows Server 2022 with apps like MongoDB. For example, if you need MongoDB on Windows Server 2022, you can use this Dockerfile .","title":"Ready-Made Bundles"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#other-combinations","text":"Other combinations you can create include: aspnet iis Django apache-http-php nginx dotnet35 golang nodejs python python-django rails ruby server-jre-8u51-windows-x64 mongodb mysql redis sqlite sqlserver-express PowerShellDSC_iis-10.0 Now you know that containers are not just small Linux OSs. They can be a mini Windows OS too!","title":"Other Combinations"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#types-of-kubernetes","text":"Kubernetes is a system for managing containerized applications, but there are many ways to set it up based on your needs. The table below summriazes the various available kubernetes 'brands':","title":"Types of Kubernetes"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#for-the-busy-people","text":"Development : Minikube , K3d , Docker Desktop Kubernetes , and Kind are geared toward development and testing. Production : Kubeadm , Rancher , K3s , and managed services like Amazon EKS , Google GKE , and Azure AKS are suited for production environments. Mixed Use : K3s and MicroK8s can be used both in development and production, especially in edge computing and IoT.","title":"For the busy people:"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#kubernetes-brands","text":"Here's the revised table with the \"Environment\" column removed: Kubernetes Tool Company/Provider Best For Single Machine Setup Multi-Machine Setup Used By Minikube Kubernetes Community (CNCF) Development Yes No Individual developers, small startups. Kubeadm Kubernetes Community (CNCF) Production Yes (single node) Yes Enterprises, cloud service providers. Rancher SUSE Production Yes (for management) Yes Enterprises, companies managing multi-cloud or hybrid environments. K3s Rancher (SUSE) Development/Production Yes Yes IoT companies, edge computing solutions, small and medium enterprises. K3d Rancher (SUSE) Development Yes No Developers, small companies for testing multi-node setups. Docker Desktop Kubernetes Docker, Inc. Development Yes No Developers using Docker, small teams. MicroK8s Canonical (Ubuntu) Development/Production Yes Yes IoT and edge computing companies, startups. Amazon EKS Amazon Web Services (AWS) Production No Yes Large enterprises, companies using AWS. Google GKE Google Cloud Production No Yes Large enterprises, companies using Google Cloud. Azure AKS Microsoft Azure Production No Yes Enterprises, companies using Azure services. OpenShift Red Hat (IBM) Production Yes (for single node) Yes Enterprises needing integrated CI/CD, large companies in regulated industries. Kind (Kubernetes in Docker) Kubernetes Community (CNCF) Development Yes No Developers, CI/CD pipelines in tech companies.","title":"Kubernetes Brands"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#how-i-push-customized-images-to-docker-hubwebsite","text":"Recently, I set up a Kafka environment using the base images from Confluent. After tweaking and customizing these images to fit my specific needs, I realized that these modified images should be pushed to Docker Hub so I can easily reuse them later or share them with others. I had 9 images in total, so here\u2019s the process I followed.","title":"How I push customized Images to Docker Hub(Website)"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#step-1-log-in-to-docker-hub","text":"The first thing I did was log in to Docker Hub using my Docker Hub username, dwdas9 . docker login It asked for my Docker Hub username and password, and once I provided those, I was logged in.","title":"Step 1: Log In to Docker Hub"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#step-2-find-the-image-names","text":"To push the images, I needed to know their names. I used the following command to list all the Docker images on my local machine: docker images This command gave me a list of all the images, including their names, tags, and IDs. I picked out the relevant images that I had customized.","title":"Step 2: Find the Image Names"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#step-3-tag-the-images","text":"Before I could push the images, I had to tag them with my Docker Hub username and the repository name I wanted them to go into. Here\u2019s how I did it for each of the 9 images: docker tag confluentinc/cp-ksqldb-server:6.0.1 dwdas9/cp-ksqldb-server:v6 docker tag confluentinc/cp-kafka-rest:6.0.1 dwdas9/cp-kafka-rest:v6 docker tag confluentinc/cp-schema-registry:6.0.1 dwdas9/cp-schema-registry:v6 docker tag confluentinc/cp-enterprise-control-center:6.0.1 dwdas9/cp-enterprise-control-center:v6 docker tag confluentinc/cp-kafka-connect-base:6.0.1 dwdas9/cp-kafka-connect-base:v6 docker tag confluentinc/cp-server:6.0.1 dwdas9/cp-server:v6 docker tag confluentinc/cp-zookeeper:6.0.1 dwdas9/cp-zookeeper:v6 docker tag confluentinc/ksqldb-examples:6.0.1 dwdas9/ksqldb-examples:v6 docker tag confluentinc/cp-ksqldb-cli:6.0.1 dwdas9/cp-ksqldb-cli:v6 I replaced confluentinc with my username dwdas9 and added a custom tag v6 to each image.","title":"Step 3: Tag the Images"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#step-4-push-the-images-to-docker-hub","text":"With the images tagged, the next step was to push them to Docker Hub. I ran the following commands: docker push dwdas9/cp-ksqldb-server:v6 docker push dwdas9/cp-kafka-rest:v6 docker push dwdas9/cp-schema-registry:v6 docker push dwdas9/cp-enterprise-control-center:v6 docker push dwdas9/cp-kafka-connect-base:v6 docker push dwdas9/cp-server:v6 docker push dwdas9/cp-zookeeper:v6 docker push dwdas9/ksqldb-examples:v6 docker push dwdas9/cp-ksqldb-cli:v6 Docker started uploading each image to the repository. Once the upload was complete, I could see all my modified images on Docker Hub under my account.","title":"Step 4: Push the Images to Docker Hub"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#conclusion","text":"And that\u2019s it! Now, my customized Kafka environment images are safely stored on Docker Hub, ready to be pulled down whenever I need them. If you\u2019re working on something similar, these steps should help you push your images too.","title":"Conclusion"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#troubleshooting-docker-errors","text":"If your Docker container is showing up as orange (in Docker Desktop) or failed to start without giving specific details, it means the container likely encountered an error during startup. Docker doesn\u2019t always show detailed error messages in the UI, but you can retrieve more information using the following methods.","title":"Troubleshooting docker errors"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#steps-to-debug-the-issue","text":"","title":"Steps to Debug the Issue"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#1-check-the-container-logs","text":"You can check the logs for the failed container to see what went wrong. Run this command to inspect the container logs: docker logs your-container-name This will show you any errors or issues that occurred during the container\u2019s initialization. Look for specific errors related to: - Missing environment variables. - Errors in Spark or system configuration. - Issues with file mounting (e.g., mapping the conf directory).","title":"1. Check the Container Logs"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#2-inspect-the-container-status","text":"You can inspect the container to get more information about why it failed. Use the docker inspect command: docker inspect your-container-name This will provide detailed information about the container's configuration, including the exit code. Look for anything abnormal in the output, especially the State and ExitCode sections.","title":"2. Inspect the Container Status"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/#3-check-for-file-permissions-and-volume-issues","text":"Sometimes, volume mapping can cause issues, especially when the local directory being mounted doesn\u2019t have the correct permissions, or Docker has trouble accessing it. Make sure that the local mapped-folder folder has the correct permissions for Docker to access it. For example: Check if the directory exists and has read/write permissions: ls -ld mapped-folder If the directory is not accessible by Docker, try giving it the correct permissions: chmod -R 755 mapped-folder","title":"3. Check for File Permissions and Volume Issues"},{"location":"DockerAndKubernetes/SparkHiveHadoop/4_Spark-Hive-Hadoop/","text":"layout: default title: spark-hive-hadoop parent: Docker nav_order: 4 has_children: true","title":"4 Spark Hive Hadoop"},{"location":"M365/DocumentumToSharePoint/","text":"Case study: Documentum to SharePoint Online and Azure Blob For one of my clients, the Document Archving and ECM system used on-prem Documentum. This system was connected with a large number of field offices scanning/importing contet through OpenText Intelligent Capture. The total licensing and maintenace cost was exorbitant. There was non HA and DR. Moreover many departments used simple filesytem for archiving. When advising this client on enhancing their content management system, the focus was on overcoming the challenges posed by Documentum, particularly in terms of cost, integration difficulties, and user experience issues. The recommended strategy involved transitioning to SharePoint Online for active content management and Azure Blob Storage for archiving. This approach resulted in many benefits aligned with the client's need to reduce 'technical debt'. Challenges with Documentum High Costs : Licensing, hardware maintenance, and support costs were unnecessarily high. Integration and User Experience Issues : Difficulties in integrating with the Office 365 and Azure ecosystems, coupled with user experience challenges due to frequent maintenance issues with the Java applet in WebTop. The Strategic Shift To SharePoint Online for Live Content : For its cost-effectiveness, scalability, and seamless integration within the Office 365 ecosystem. To Azure Blob for Archiving : For its scalability and cost-efficiency in handling large volumes of archived(structured/Unstructured) data. There are many tiers, the hot(low latency) cool(infrequent access) Key Benefits of the Transition Enhanced Mapping Features : SharePoint Online supports advanced mapping of Documentum structures, including advanced library configuration. Improved Access Control : Superior ACL settings in SharePoint Online mirror Documentum's capabilities, offering more refined permission management. No Database Constraints : Unlike previous SharePoint versions, SharePoint Online has no SQL server database size limit, eliminating on-prem system capacity issues. Advanced Content Management : Supports the management of linked documents, renditions, and contentless objects, making the migration process straightforward. Direct Exports with OpenText : OpenText Intellinget Capture's Microsoft SharePoint Export - Exports documents and data directly to SharePoint. ADF abd AZCopy : Using these tools it was easily possible to move data to Azure blob storage. By moving to SharePoint Online and Azure Blob Storage, the client resolved their current issues and also used advanced features and future-proofed their system.","title":"Documentum-SharePoint Online"},{"location":"M365/DocumentumToSharePoint/#case-study-documentum-to-sharepoint-online-and-azure-blob","text":"For one of my clients, the Document Archving and ECM system used on-prem Documentum. This system was connected with a large number of field offices scanning/importing contet through OpenText Intelligent Capture. The total licensing and maintenace cost was exorbitant. There was non HA and DR. Moreover many departments used simple filesytem for archiving. When advising this client on enhancing their content management system, the focus was on overcoming the challenges posed by Documentum, particularly in terms of cost, integration difficulties, and user experience issues. The recommended strategy involved transitioning to SharePoint Online for active content management and Azure Blob Storage for archiving. This approach resulted in many benefits aligned with the client's need to reduce 'technical debt'.","title":"Case study: Documentum to SharePoint Online and Azure Blob"},{"location":"M365/DocumentumToSharePoint/#challenges-with-documentum","text":"High Costs : Licensing, hardware maintenance, and support costs were unnecessarily high. Integration and User Experience Issues : Difficulties in integrating with the Office 365 and Azure ecosystems, coupled with user experience challenges due to frequent maintenance issues with the Java applet in WebTop.","title":"Challenges with Documentum"},{"location":"M365/DocumentumToSharePoint/#the-strategic-shift","text":"To SharePoint Online for Live Content : For its cost-effectiveness, scalability, and seamless integration within the Office 365 ecosystem. To Azure Blob for Archiving : For its scalability and cost-efficiency in handling large volumes of archived(structured/Unstructured) data. There are many tiers, the hot(low latency) cool(infrequent access)","title":"The Strategic Shift"},{"location":"M365/DocumentumToSharePoint/#key-benefits-of-the-transition","text":"Enhanced Mapping Features : SharePoint Online supports advanced mapping of Documentum structures, including advanced library configuration. Improved Access Control : Superior ACL settings in SharePoint Online mirror Documentum's capabilities, offering more refined permission management. No Database Constraints : Unlike previous SharePoint versions, SharePoint Online has no SQL server database size limit, eliminating on-prem system capacity issues. Advanced Content Management : Supports the management of linked documents, renditions, and contentless objects, making the migration process straightforward. Direct Exports with OpenText : OpenText Intellinget Capture's Microsoft SharePoint Export - Exports documents and data directly to SharePoint. ADF abd AZCopy : Using these tools it was easily possible to move data to Azure blob storage. By moving to SharePoint Online and Azure Blob Storage, the client resolved their current issues and also used advanced features and future-proofed their system.","title":"Key Benefits of the Transition"},{"location":"M365/LicensingExamples/","text":"M365 Example licensing: I went to a shop, and the prices of the goods were so complicated that I just threw some cash down and told the shopkeeper to give me the best I could get for that amount. That's the story here: Microsoft licensing is so complex that these tables will give you some idea of what you would get if you shelled out $1,000 or more. Note : These tables are just here to give you some idea. Prices change, you might get discounts, or new features might come in. So, this is just to give you a basic idea. Don't take this table to the Microsoft sales guy and say that it says this. $10,000 Budget Product Annual Cost Per User Number of Users Features Power Automate $180 55 Unlimited flows, standard & premium connectors Power Apps $480 20 Unlimited apps per user, standard & premium connectors Power BI Pro $120 83 Self-service analytics, collaboration, and sharing Copilot (AI Builder) $500 (estimate) 20 AI capabilities in apps & workflows Teams (Microsoft 365 Business Basic) $60 166 Chat, file sharing, online meetings, collaboration Outlook (Microsoft 365 Business Basic) $60 166 Email, calendar, contacts, 1TB OneDrive storage $50,000 Budget Product Annual Cost Per User Number of Users Features Power Automate $180 277 Unlimited flows, standard & premium connectors Power Apps $480 104 Unlimited apps per user, standard & premium connectors Power BI Pro $120 416 Self-service analytics, collaboration, and sharing Copilot (AI Builder) $500 (estimate) 100 AI capabilities in apps & workflows Teams (Microsoft 365 Business Basic) $60 833 Chat, file sharing, online meetings, collaboration Outlook (Microsoft 365 Business Basic) $60 833 Email, calendar, contacts, 1TB OneDrive storage $100,000 Budget Product Annual Cost Per User Number of Users Features Power Automate $180 555 Unlimited flows, standard & premium connectors Power Apps $480 208 Unlimited apps per user, standard & premium connectors Power BI Pro $120 833 Self-service analytics, collaboration, and sharing Copilot (AI Builder) $500 (estimate) 200 AI capabilities in apps & workflows Teams (Microsoft 365 Business Basic) $60 1666 Chat, file sharing, online meetings, collaboration Outlook (Microsoft 365 Business Basic) $60 1666 Email, calendar, contacts, 1TB OneDrive storage M365 Features (Microsoft 365 Business Basic) Teams : Chat, file sharing, online meetings, collaboration. Outlook : Email, calendar, contacts, 1TB OneDrive storage. Summary of Inclusions for Each Budget $10,000 Budget : Smaller team with basic Power Automate, Power Apps, Power BI Pro, and M365 Business Basic features. $50,000 Budget : Medium-sized team with more extensive access to Power Platform tools and M365 Business Basic features. $100,000 Budget : Larger team with comprehensive access to Power Platform tools and M365 Business Basic features.","title":"M365 Licensing Examples"},{"location":"M365/LicensingExamples/#m365-example-licensing","text":"I went to a shop, and the prices of the goods were so complicated that I just threw some cash down and told the shopkeeper to give me the best I could get for that amount. That's the story here: Microsoft licensing is so complex that these tables will give you some idea of what you would get if you shelled out $1,000 or more. Note : These tables are just here to give you some idea. Prices change, you might get discounts, or new features might come in. So, this is just to give you a basic idea. Don't take this table to the Microsoft sales guy and say that it says this.","title":"M365 Example licensing:"},{"location":"M365/LicensingExamples/#10000-budget","text":"Product Annual Cost Per User Number of Users Features Power Automate $180 55 Unlimited flows, standard & premium connectors Power Apps $480 20 Unlimited apps per user, standard & premium connectors Power BI Pro $120 83 Self-service analytics, collaboration, and sharing Copilot (AI Builder) $500 (estimate) 20 AI capabilities in apps & workflows Teams (Microsoft 365 Business Basic) $60 166 Chat, file sharing, online meetings, collaboration Outlook (Microsoft 365 Business Basic) $60 166 Email, calendar, contacts, 1TB OneDrive storage","title":"$10,000 Budget"},{"location":"M365/LicensingExamples/#50000-budget","text":"Product Annual Cost Per User Number of Users Features Power Automate $180 277 Unlimited flows, standard & premium connectors Power Apps $480 104 Unlimited apps per user, standard & premium connectors Power BI Pro $120 416 Self-service analytics, collaboration, and sharing Copilot (AI Builder) $500 (estimate) 100 AI capabilities in apps & workflows Teams (Microsoft 365 Business Basic) $60 833 Chat, file sharing, online meetings, collaboration Outlook (Microsoft 365 Business Basic) $60 833 Email, calendar, contacts, 1TB OneDrive storage","title":"$50,000 Budget"},{"location":"M365/LicensingExamples/#100000-budget","text":"Product Annual Cost Per User Number of Users Features Power Automate $180 555 Unlimited flows, standard & premium connectors Power Apps $480 208 Unlimited apps per user, standard & premium connectors Power BI Pro $120 833 Self-service analytics, collaboration, and sharing Copilot (AI Builder) $500 (estimate) 200 AI capabilities in apps & workflows Teams (Microsoft 365 Business Basic) $60 1666 Chat, file sharing, online meetings, collaboration Outlook (Microsoft 365 Business Basic) $60 1666 Email, calendar, contacts, 1TB OneDrive storage","title":"$100,000 Budget"},{"location":"M365/LicensingExamples/#m365-features-microsoft-365-business-basic","text":"Teams : Chat, file sharing, online meetings, collaboration. Outlook : Email, calendar, contacts, 1TB OneDrive storage.","title":"M365 Features (Microsoft 365 Business Basic)"},{"location":"M365/LicensingExamples/#summary-of-inclusions-for-each-budget","text":"$10,000 Budget : Smaller team with basic Power Automate, Power Apps, Power BI Pro, and M365 Business Basic features. $50,000 Budget : Medium-sized team with more extensive access to Power Platform tools and M365 Business Basic features. $100,000 Budget : Larger team with comprehensive access to Power Platform tools and M365 Business Basic features.","title":"Summary of Inclusions for Each Budget"},{"location":"M365/SPMT/","text":"Migrating to Microsoft 365 with the SharePoint Migration Tool (SPMT) What Can Be Migrated? Let's get started Migrating to Microsoft 365 with the SharePoint Migration Tool (SPMT) If you're looking to move your content from on-site SharePoint locations to the cloud with Microsoft 365, there's a free and user-friendly tool just for you. The SharePoint Migration Tool (SPMT) is here to simplify the process of transferring your SharePoint Server sites and content. What Can Be Migrated? The table below contains the items which can be migrated using SPMT. Supported Feature Description Migration Sources On-premises fileshares Supports local and network fileshares migration. SharePoint Server versions Migrates from SharePoint Server 2010, 2013, 2016, and 2019. Content Types File, folder, list items Migrates files, folders, and lists. Pages Migrates pages in the site asset library. Permissions & Security Permissions Sets file share and SharePoint permissions separately. Site Features & Components Managed metadata and taxonomy Supports content types and term store migration; requires admin permissions. Navigation and icons Preserves and migrates site navigation for out-of-box sites. Site features Supports a wide range of site features. SharePoint web parts Supports SharePoint web parts migration. Site migration Migrates \"out-of-the-box\" SharePoint sites without coding or third-party tools. Site description Migrates site descriptions. Workflows & Automation OOTB Workflows to Power Automate Migrates SharePoint Server 2010 OOTB workflows to Power Automate. SPD Workflows to Power Automate Migrates SharePoint Server 2010 and 2013 Designer workflows to Power Automate. List, library, content-type workflows Migrates list, library, and content-type workflows (excluding site workflows). Workflow definitions and associations Migrates workflow definitions and associations, not history data. Additional Features Incremental Supports incremental migration by rerunning tasks later. Microsoft Teams Allows selection of Teams and channels for migration. Taxonomy migration Manages metadata and taxonomy in incremental updates; off by default. Bulk migration Allows bulk migration via JSON or CSV for numerous sources. Versions Lets you choose what file history to preserve. Let's get started","title":"SPMT - SharePoint Migration"},{"location":"M365/SPMT/#migrating-to-microsoft-365-with-the-sharepoint-migration-tool-spmt","text":"If you're looking to move your content from on-site SharePoint locations to the cloud with Microsoft 365, there's a free and user-friendly tool just for you. The SharePoint Migration Tool (SPMT) is here to simplify the process of transferring your SharePoint Server sites and content.","title":"Migrating to Microsoft 365 with the SharePoint Migration Tool (SPMT)"},{"location":"M365/SPMT/#what-can-be-migrated","text":"The table below contains the items which can be migrated using SPMT. Supported Feature Description Migration Sources On-premises fileshares Supports local and network fileshares migration. SharePoint Server versions Migrates from SharePoint Server 2010, 2013, 2016, and 2019. Content Types File, folder, list items Migrates files, folders, and lists. Pages Migrates pages in the site asset library. Permissions & Security Permissions Sets file share and SharePoint permissions separately. Site Features & Components Managed metadata and taxonomy Supports content types and term store migration; requires admin permissions. Navigation and icons Preserves and migrates site navigation for out-of-box sites. Site features Supports a wide range of site features. SharePoint web parts Supports SharePoint web parts migration. Site migration Migrates \"out-of-the-box\" SharePoint sites without coding or third-party tools. Site description Migrates site descriptions. Workflows & Automation OOTB Workflows to Power Automate Migrates SharePoint Server 2010 OOTB workflows to Power Automate. SPD Workflows to Power Automate Migrates SharePoint Server 2010 and 2013 Designer workflows to Power Automate. List, library, content-type workflows Migrates list, library, and content-type workflows (excluding site workflows). Workflow definitions and associations Migrates workflow definitions and associations, not history data. Additional Features Incremental Supports incremental migration by rerunning tasks later. Microsoft Teams Allows selection of Teams and channels for migration. Taxonomy migration Manages metadata and taxonomy in incremental updates; off by default. Bulk migration Allows bulk migration via JSON or CSV for numerous sources. Versions Lets you choose what file history to preserve.","title":"What Can Be Migrated?"},{"location":"M365/SPMT/#lets-get-started","text":"","title":"Let's get started"},{"location":"M365/SharePoint2007FarmUpgrade/","text":"Case Study: SharePoint Farm Design for a Media Company Background Project Overview \\& Understanding Current system study Current System study Current System Assessment Governance Infrastructure \\& Platform Information Management Usability SharePoint Farm Maintenance \\& Deployment Planning SharePoint Server Farm Capacity Planning Usage profile characteristics Topology recommendation by Capacity Planner Hardware recommendation by Capacity Planner Hardware profile for the servers Web Front End Servers Application Servers Database (SQL) Servers SAN Software Stack Browser Support About browser support Levels of browser support Level 1 Web browsers System Architecture \\& Design Recommendations Overall Design Goals System Landscape Farm Design Consideration High Availability for Web Front Ends NLB Application Servers Redundancy Index Role Query Role. Redundant Application Server Availability \\& Failover for Database Server Database Clustering Recommendation Scalability Storage Area Network Network Components Firewall DNS Configuration Active Directory Configuration Farm Security Considerations Communication in Server Farm Server-Server Communication Client-Server Communication Server Hardening Web \\& Application Server Hardening Database Server Hardening Secure Topology Design Checklists Server topology design Network topology design Planning for Extranet Topology Intranet \\& Extranet Zones Internet-facing Topology Split back-to-back topology Advantages Disadvantages Disaster Recovery Recommendations Disaster Recovery Disaster Recovery Strategy Disaster Recovery Configuration Process Asynchronous database mirroring Deployment Architecture Production DC \\& DR Non-Production Environments Development Environment QA Environment Virtualization of Environments Virtualize SharePoint Web Role Choose disk type for Query Role Consider using Index Role on physical server Do not virtualize Database role Do I need to virtualize Application role? Virtualized scenario sample Use proper number of CPU Use proper amount of RAM Plan to use physical drives Consider using hardware load balancing Be careful with snapshot feature on virtual servers Measure virtualized environment performance (only for Hyper-V) Case Study: SharePoint Farm Design for a Media Company I wokred as a SharePoint Architect for an farm deployment project for a prominent media company, this document captures the essence of our SharePoint Farm design project. Developed in collaboration with our networking and infrastructure teams, it outlines the deployment strategy employed and the considerations made to ensure a robust SharePoint environment. From SharePoint 2007 through to SharePoint 2016, the landscape of SharePoint has evolved, introducing several features and enhancements like miniRoles etc. However, the fundamental architecture focusing on high availability (HA) remains unchanged. Background Marxxx wants to redesign their existing Microsoft Office SharePoint based portal application which includes around 10 site collections and also wants to improve and simplify the ability to manage content. Objectives of redesign Deploy the best fit infrastructure and system architecture Using generic content types with optimum reusability Using consistent use of document management Using a collaboration platform with self-service a minimal learning curve Standardize enterprise content management utilizing built-in SharePoint tools Implement a MOSS Governance policy that will define Roles and Responsibilities, Policies, Processes, Deployment Strategies and Site Structure Design and deploy High Availability solution with no single point of failure Integration of plug-in and proprietary applications based on MOSS Project Overview & Understanding Marxxx has implemented the SharePoint to meet different organizational needs. However, the implementation lacks best practices and scope for future enhancements. On generic terms organizations adopting SharePoint face a variety of tasks \u2013 from planning, strategy, infrastructure and architecture design, UI Design, migration, and to development. All these tasks imply flexible infrastructural baseline before actual work starts. However, in reality we face the outdated environment and miss-configured farms that are not ready to implement new requirements. In such cases, baseline architecture becomes foundation stone of all SharePoint projects. Current system study Current System study HXX has detailed discussions with the Marxxx management team to understand the needs to revamp & redesign the current SharePoint implementation. Marxxx provided the VPN connection with site level permission to view the replica of the production sites without sensitive data. HXX provided a document to Marxxx on how to remove sensitive data and backup/restore the sites in development area for HCL\u2019s assessment. HXX followed the standard process of studying current system. The following section would highlight the key findings of the system study. Current System Assessment HXX's assessment findings based on current Marxxx SharePoint implementation are: Governance No defined SharePoint resource governance matrix. Seems to be no clear business owners of sites and accountability lies on only technical team. Governance document exist but that is incomplete (only 2 pages) and doesn\u2019t seem to serve the purpose of SharePoint governance. It is out-dated. It seems SharePoint administration process is not defined and there seems to be no dedicated administrator for ongoing maintenance and implement governance best practices. No application usage & customization policies exist Infrastructure & Platform Current SharePoint installation is not implemented for High Availability. No business continuity plans defined. Single Point of failure. No Disaster Recovery farm exists. Server boxes are out-dated for future upgrade of SharePoint and virtualization technologies. Sandbox environment is not yet configured. Development environment consists of sensitive data which ideally should NOT reside in any environments other than production environment. Minimal integration observed with other Enterprise Web Applications and Data Information Management Current taxonomy was developed without hierarchy \u2013 flat structure There are few lists/libraries implemented which are not used or obsolete in current sites and are eating up storage space. Contents scattered in File Systems as well \u2013 might be critical contents. No Expiration policies for contents. No Information Management documentation are available which narrates existing SharePoint implementation like taxonomy, design document etc,. Neither any user reference manuals. There is no ready reference on lists/libraries which contain sensitive information. Some custom ASPX pages are used without using SharePoint publishing feature. Content and document types are not being leveraged. Content publishing processes and policies do not exist. Usability Navigation is not structured in distinct, easily recognizable groups nor is it consistent in structure. MyMarvel Home page UI is good, however that the site menus and contents are not clearly identifiable in the glittery background. Quick links are not present in all sites to facilitate better navigation through different sites Broken Links present. SharePoint Farm Maintenance & Deployment Dedicated Web application created for each Site Collection in current environment without proper reason or Isolation needs All custom DLLs are placed in GAC should ideally be in \\BIN folder as per Microsoft Best Practices. Backup is done, but no archiving strategy and plan found. No true business process management or enterprise-level workflow automation solutions are in place. Might be present offline \u2013 not known. Planning SharePoint Server Farm Capacity Planning Assuming that 1000 users access the SharePoint web sites and the content size will grow to 2 TB in future, the following is the Capacity Planning based on Microsoft\u2019s System Center Capacity Planner 2007: Usage profile characteristics Figure 1: Usage profile of SharePoint sites. Topology recommendation by Capacity Planner Figure 2: Topology diagram as per MS System Center Capacity Planner 2007 Hardware recommendation by Capacity Planner Hardware profile for the servers The server configuration details are based on Microsoft\u2019s System Center Capacity Planner 2007: Total number of clients/users: 1000 Number of servers: 6 Number of SAN connections: 1 Web Front End Servers Server: Web Front End 1 Configuration Details Processor 2.20 GHz Quad Core Processor Minimum memory 8.0 GB Disk DiskArray 1\\Volume 1 (File System), 146 GB RAID 1 (2 x 146.00 GB SCSI 10,000 RPM) DiskArray 1\\Volume 2 (File System), 360 GB RAID 5 (6 x 72.00 GB SCSI 10,000 RPM) NIC 1 x 1,000 Mb/s Roles Web Front End; Query Server Server: Web Front End 2 Configuration Details Processor 2.20 GHz Quad Core Processor Minimum memory 8.0 GB Disk DiskArray 1\\Volume 1 (File System), 146 GB RAID 1 (2 x 146.00 GB SCSI 10,000 RPM) DiskArray 1\\Volume 2 (File System), 360 GB RAID 5 (6 x 72.00 GB SCSI 10,000 RPM) NIC 1 x 1,000 Mb/s Roles Web Front End Application Servers Server: Index Server & Query Server Configuration Details Processor 2.20 GHz Quad Core Processor Minimum memory 8.0 GB Disk DiskArray 1\\Volume 1 (File System), 1500 GB RAID 5 (6 x 300.00 GB SCSI 15,000 RPM) NIC 1 x 1,000 Mb/s Roles Index & Query Database (SQL) Servers Server: SQL Server Cluster (Failover) Configuration Details Processor 2.20 GHz Quad Core Processor Minimum memory 16.0 GB Disk No local disk devices NIC 1 x 1,000 Mb/s SAN connections 2 x 4 Gb/s SANs SAN Array\\Volume 3 (Log Files), 600 GB RAID 10 (4 x 300.00 GB SCSI 15,000 RPM) SAN Array\\Volume 4 (Data Files), 5400 GB RAID 10 (36 x 300.00 GB SCSI 15,000 RPM) Roles SQL Server (Clustered) Server: SQL Server Cluster (Primary) Configuration Details Processor 2.20 GHz Quad Core Processor Minimum memory 16.0 GB Disk No local disk devices NIC 1 x 1,000 Mb/s SAN connections 2 x 4 Gb/s SANs SAN Array\\Volume 1 (Log Files), 600 GB RAID 10 (4 x 300.00 GB SCSI 15,000 RPM) SAN Array\\Volume 2 (Data Files), 5400 GB RAID 10 (36 x 300.00 GB SCSI 15,000 RPM) Roles SQL Server SAN SAN: SAN Array Configuration Details Disk SAN Array\\Volume 1 (Log Files), 600 GB RAID 10 (4 x 300.00 GB SCSI 15,000 RPM) SAN Array\\Volume 2 (Data Files), 5400 GB RAID 10 (36 x 300.00 GB SCSI 15,000 RPM) SAN Array\\Volume 3 (Log Files), 600 GB RAID 10 (4 x 300.00 GB SCSI 15,000 RPM) SAN Array\\Volume 4 (Data Files), 5400 GB RAID 10 (36 x 300.00 GB SCSI 15,000 RPM) Software Stack Software / Tools - Internet Explorer 6.0 and above - Microsoft Office SharePoint Server 2007 - Microsoft Office SharePoint Designer 2007 - Microsoft Visual Studio 2008 - Microsoft Exchange Server - Microsoft SQL Server 2008 R2 - Microsoft Windows Server 2003 R2 - Microsoft ForeFront Browser Support About browser support Microsoft Office SharePoint Server 2007 supports several Web browsers that are commonly used. However, there are certain browsers that might cause some Office SharePoint Server 2007 functionality to be downgraded, limited, or available only through alternative steps. In some cases, functionality might be unavailable for noncritical administrative tasks. Levels of browser support Web browser support is divided into two levels: level 1 and level 2. Although administrative tasks on SharePoint sites are optimized for level 1 browser, Office SharePoint Server 2007 also provides support for other browsers that are commonly used. To ensure complete access to all the functionality, it is recommended to use level 1 browser. Level 1 Web browsers Level 1 Web browsers take advantage of advanced features provided by ActiveX controls and provide the most complete user experience. Level 1 browsers offer full functionality on all SharePoint sites, including the SharePoint Central Administration Web site. Level 1 browsers are: Microsoft Internet Explorer 6. x (32-bit) Windows Internet Explorer 7. x (32-bit) System Architecture & Design Recommendations Microsoft Office SharePoint Server 2007 provides the flexibility to meet many different deployment solution goals. This includes guidance that would help Marxxx in: Determine the number of server in farm required to meet the solution goals. Plan for the relationships between servers in the farm. Plan for Extranet -facing server farm. Design server-farm topologies to meet availability goals. Overall Design Goals The conceptualized design has been prepared considering the Marvel\u2019s need for SharePoint farm implementation. The key design goals are mentioned below: Use the minimum number of server farms to host various SharePoint web sites typically required by a Marxxx for their intranet and extranet, considering performance and high availability. Create a framework for designing a scalable environment. Design decisions for individual applications do not prevent the addition of other applications in future. For example, an initial deployment would include ten site collections that comprises of document center, document workspace, collaborative team sites or publishing sites that compose an intranet (team sites, My Sites, and published intranet content). By using a similar logical architecture design, Marxxx can add applications to the solution without affecting the design of the initial applications. In other words, the design does not incorporate design choices that limit the use of the environment. Provide access for several classes of users without compromising the security of the content within the disparate applications. Users from different network zones (both internal and external) with different authentication providers can participate in collaboration. Also, users can only access the content they are intended to access. By following a similar logical architecture design, Marxxx can create the opportunity to provide access to users in multiple locations and with different objectives. Ensure that the design can be used in an extranet environment. Deliberate design choices are made to ensure that the server farms can be securely deployed in a perimeter network (DMZ). The rest of this article discusses each of the physical components that appear in the landscape model in the following section and discusses the design choices that are applied to the model. The purpose of this approach is to demonstrate the different ways in which logical architecture components can be configured based on the Marvel\u2019s SharePoint Farm needs. ## System Landscape The Landscape has been divided in three zones. Extranet/Internet Perimeter Network (DMZ) Corporate Network (Intranet) Components Position Quantity Role Cisco Firewall Internet 1 (Optional) Hardware Firewall ISA 2006 Server Perimeter Network & Corporate Network 2 Software Firewall Web Front End Server Perimeter Network 2 WFE + Query Server Application Server Corporate Network 2 1 App Server + 1 Index Server SQL Server Corporate Network 2 Database Server SAN Array Corporate Network 1 SAN Storage ## Farm Design Consideration HXX has kept the following key design consideration at core while designing the framework of the proposed solution. To attain maximum redundancy with a minimum number of servers, deploy an additional application server to the middle tier for load balancing application server roles that are designed to be redundant. This server farm topology consists of six servers. The query role is installed to the front-end Web servers to achieve redundancy. This topology protects these server roles from direct user connections and optimizes the performance of the overall farm when compared to smaller farms. The SQL server would be a two node clustered installation. Office SharePoint Server 2007 supports scalable server farms for capacity, performance and availability. Typically, capacity is the first consideration in determining the number of server computers to start with. After factoring in performance, availability also plays a role in determining both the number of servers and the size or capacity of the server computers in a server farm. High Availability for Web Front Ends For high availability in a MOSS environment, HXX proposes to use a collection of servers supporting multiple SharePoint sites. A server farm militating against the effects of unexpected downtime in addition to downtime that is related to ongoing maintenance (such as operating system updates etc.). The HXX suggested server farm for Marxxx workloads that builds in availability will consists of six servers (please see the figure 3). There will be four servers for MOSS 2007, two dedicated to Web Server along with Query role, while the other two servers will share the Application services, and Indexing roles. All of the Web Server will be load balanced using NLB. SQL Server 2008 will be clustered across two servers in a primary-secondary cluster. #### NLB Network Load Balancer (NLB) is useful for ensuring that stateless applications, such as a Web server running Internet Information Services (IIS), are scalable by adding additional servers as the load increases. Windows NLB works in two different modes \u2013 Uni-cast and Multicast . When in unicast mode, NLB replaces the network card's original MAC address. When in multicast mode, NLB adds the new virtual MAC to the network card, but also keeps the card's original MAC address. Multiple network adapters in multicast mode for Marvel\u2019s new farm: This model is suitable for a cluster in which ordinary network communication among cluster hosts is necessary and in which there is heavy dedicated traffic from outside the cluster subnet to specific cluster hosts. Advantages Because there are at least two network adapters, overall network performance is typically enhanced. Ordinary network communication among cluster hosts is permitted. Disadvantages This model requires a second network adapter. Some routers might not support the use of a multicast media access control (MAC) address. This only affects the Network Load Balancing/MAC address (not all MAC addresses) and only when dynamic ARP replies are sent by the cluster to the router. not all MAC addresses Application Servers Redundancy The baseline server topology design depends on the requirements for redundancy of application server roles. This matrix below describes the application server roles relative to their redundancy options. Application server roles for Office SharePoint Server 2007 fall into two categories: Roles that can be redundant Roles that cannot be redundant Application server role Redundancy Allowed Query Yes Index No Windows SharePoint Services 3.0 search No Excel Calculation Services Yes Office Project Server 2007 Yes Index Role Microsoft has recommended as a best practice, having a dedicated index server available to crawl data and that should not be part of the potentially load-balanced front-end servers that actually serve up the web pages to end-users. The reason for this is indexing requires significant computing resources both on the indexing machine and the machines being crawled. Microsoft has also recommended having a dedicated Query server if the crawling content is more than 500 GB, this would alleviate the potential performance issues. Refer the proposed SharePoint System Landscape, the dedicated Index server will perform the index function of the farm in order to manage crawling of searchable content and will create index files. Query Role. The Query role would be combined with the web front end servers in a load balanced fashion. This will provide redundancy for Query server role. Redundant Application Server The Query role would be combined with the web front end servers in a load balanced fashion. This will provide redundancy for Query server role. Availability & Failover for Database Server The database server role affects the availability of the solution more than any other role. If a Web server or an application server fails, these roles can quickly be restored or redeployed. However, if a database server fails, the solution depends on restoring the database server. This can potentially include rebuilding the database server and then restoring data from the backup media. In this case, you can potentially lose any new or changed data dating back to the last backup job, depending on how SQL Server 2008 is configured. Additionally, the solution will be completely unavailable for the time it takes to restore the database server role. Microsoft enables SQL Server database availability with the SQL Server Always on Technologies program. This consists of Database mirroring Failover clustering Database Clustering Recommendation The proposed design depicts Failover Clustering among two SQL Server 2008 database nodes in the system landscape, which provides availability support for a complete instance of SQL Server. The following diagram shows the Database Failover cluster for SQL Server. The failover cluster is a combination of one or more nodes or servers, with two or more shared disks. It will appear as a single instance, but has functionality that provides failover from one node to another if the current node becomes unavailable. Office SharePoint Server 2007 references the cluster as a whole, so that failover is automatic and seamless from the perspective of SharePoint Products and Technologies. Scalability The architecture is inherently scalable as all product components and integration products are inherently distributed and support scalability by distribution in addition to scale-outs and scale-ups. This would help Marxxx in future to upkeep the system for better performance when the number of users increases. Scale Up Constraints: The SharePoint implementation in the new farm will be adaptable to version upgrade (SharePoint 2010) only when the software (O/S and Database) installed are x64. Microsoft recommends, as a part of implementation Best Practices, that all production environment have x64 hardware and software for performance benefits. Though there would be x64 hardware support in the new production farm, the design is based on x86 software stack as available with Marxxx at present. Storage Area Network Storage Area Network (SAN) would be used to handle the bulk data storage. All the SQL Server files such as .mdf(primary data file), .ndf(secondary data files), .ldf(log files) etc. would be stored in the SAN. Network Components Firewall A firewall is a software- or hardware-based barrier against unauthorized access to your network. Firewalls inspect all the traffic going through them. Configuration of a Firewall that Works Opt for a hardware firewall, such as Cisco. Hardware firewalls are more powerful, and give you more control over protecting your Web traffic than a software-based firewall. If Spam Protection is available on your firewall, enable it. Spam firewalls are proving popular in keeping the nastier emails out (viruses, botnets). Standardize all wireless connections on WPA. Not only does the WPA Protocol have better security than WEP, but this way you can recognize (and block) if someone else tries to break in using WEP or an unencrypted connection. Much of perimeter security is addressed by the firewall. However, with the increased risk from DoS attacks and malware scripts worming their ways past firewalls, additional security products are now needed for which Microsoft ForeFront is recommended. ForeFront is a suite of business security products, designed to cover every aspect of network infrastructure. Server operations, client PCs, gateways, network edge\u2014everything\u2019s protected. ForeFront includes: ForeFront Security for Exchange Server ForeFront Security for SharePoint ForeFront Client Security (FCS) \u2013 Malware protection for every client PC. Intelligent Application Gateway (IAG) \u2013 Remotely-connecting PCs must verify their compliance before gaining server access. Internet Security and Acceleration (ISA) Server 2006 \u2013 Manages security for VPN connections and remotely-accessed applications. Advantage of ForeFront Protect Exchange and MOSS 2007. Both servers deal with data coming and going. All day, every day. Cover them both with ForeFront Security for Exchange Server and ForeFront Security for SharePoint. Uniform Reporting. Instead of trying to piece together six reports from six different applications, you can read ForeFront reports. Collected from all components into one, so problems are quickly spotted. A wall against the next attack. Virus and malware attacks are becoming more creative. And sneakier. Since it\u2019s designed to watch the edges of your network, ForeFront will catch new break-in attempts before they find an exploit. Forefront goes beyond an antivirus on a desktop. It\u2019s an entire web of security spread out through all network systems, covering all access points in and out. DNS Configuration After setting up SharePoint Portal Server and creating the portal site, you must create a public DNS entry to map the public (external) FQDN to the IP address for the public (external) interface of the external ISA Server 2006 computer. The URL containing the FQDN is the URL that users will use to access the portal site across the extranet. Active Directory Configuration From a MOSS 2007 point of view the Microsoft Split back-to-back topology scenario has been followed; MOSS web front-end servers are located in the perimeter network while the application and database servers reside in the internal network. A one-way Active Directory trust exists between the perimeter domain and the internal domain. This is a non-transitive trust where the perimeter domain trusts the internal domain but not the inverse. This serves two main purposes; it allows windows authentication and delegation to occur between services in the perimeter domain and the internal domain and at the same time protects sensitive data such as payroll databases on the internal network. Forest with a single domain is proposed: perimeter.corp.local (perimeter production domain, contains accounts for all members and external collaborators) Authenticates users from both perimeter domain and internal domain via domain trust A one-way domain trust is configured between the corp.com domain and the perimeter.corp.local domain. This is done to allow internal accounts to be used in the perimeter domain which in turn enables windows authentication to be used when accessing backend resources such as SQL databases. This trust means that deploying MOSS web front end servers is as straight forward as adding the web front end role, all IIS configuration is completed by MOSS and remains valid in both the internal and perimeter domains. Farm Security Considerations SharePoint Server-farm security will include the following tasks: Secure communication planning \u2014 decide which methods of secure communication are most appropriate for Marxxx landscape. Server hardening \u2014 plan specific hardening settings for each of the server roles in the new server farm. Communication in Server Farm Secure communication between the components of Marxxx SharePoint Farm deployment is a vital element of in-depth security architecture to ensure business critical information is not intercepted and misappropriated during its flow from user request to Web Server to Application Server and Database Server. It\u2019s important to apply secure communication techniques for both inside and outside the firewall. Secure communication provides privacy and integrity of data. Privacy ensures that data remains private and confidential, and that it cannot be viewed by eavesdroppers armed with network-monitoring software. Privacy is usually provided by means of encryption. Integrity ensures that data is protected from accidental or deliberate (malicious) modification while in transit. Secure communication channels must provide integrity of data. Integrity is usually provided by using Message Authentication Codes (MACs). Server-Server Communication In this deployment scenario, the communications between servers is done using IPSec (Internet Protocol Security), which is an open standard framework that specifies encryption and packet-filtering techniques to enhance network security. IPSec is a transport-layer mechanism through which ensures the confidentiality and integrity of TCP/IP-based communications between servers. IPSec is completely transparent to applications because encryption, integrity, and authentication services are implemented at the transport level. Please refer MOSS Installation Manual \u2013 Part 2 Appendix B for instructions to setup IPSec. Client-Server Communication SSL is generally recommended to secure communications between users and servers when sensitive information must be secured. SSL can be configured to require server authentication or both server and client authentication. SSL can decrease the performance of the network. There are several common guidelines that can be used to optimize pages that use SSL. First, use SSL only for pages that require it. This includes pages that contain or capture sensitive data, such as passwords or other personal data. SSL should be used only if the following conditions are true: To encrypt the page data. To guarantee data is sent to the intended web server. Server Hardening The process of \u201cServer Hardening\u201d deals with activities to make servers in the SharePoint farm less vulnerable to attack. This section describes hardening characteristics for Web servers, application servers and database servers. Web & Application Server Hardening The following table summarizes the Microsoft Patterns and Practices guidance for securing Web Server Task Applies To Details Patches and Updates - WFE servers - Application servers Ports - WFE servers - Application servers - Limit Internet-facing ports to port 80 for HTTP and port 443 for HTTPS. - Use IPSec to encrypt or restrict intranet communication. \u2003( Refer the complete list below ) Sites and Virtual drives - WFE servers - Application servers hosting Central Admin - Move website to non-system drive. - Restrict Web permissions in the IIS meta-base. - Remove FrontPage Server extensions. Protocols - WFE servers - Application servers - Configure TCP/IP stack to mitigate the threat from network denial of service attacks. - Disable the NetBIOS and SMB protocols if not used. Shares - WFE servers - Application servers - Remove all unnecessary shared folders. - Restrict access to any required shares. IIS Lockdown - WFE servers - Application servers hosting Central Admin - Install URLScan to block requests that contain unsafe characters. It is installed automatically when IISLockdown is run. The following list depicts the important ports: TCP 80, TCP 443 (SSL) Direct-hosted SMB (TCP/UDP 445) \u2014 disable this port if not used NetBIOS over TCP/IP (NetBT) (TCP/UDP ports 137, 138, 139) \u2014 disable this port if not used Ports required for communication between Web servers and service applications (the default is HTTP): HTTP binding: 32843 HTTPS binding: 32844 net.tcp binding: 32845 (only if a third party has implemented this option for a service application) Ports required for synchronizing profiles between SharePoint and Active Directory on the server that runs the Forefront Identity Management agent: TCP/5725 TCP/UDP 389 (LDAP service) TCP/UDP 88 (Kerberos) TCP/UDP 53 (DNS) UDP 464 (Kerberos Change Password) UDP port 1434 and TCP port 1433 \u2014 default ports for SQL Server communication. If these ports are blocked on the SQL Server computer (recommended) and databases are installed on a named instance, configure a SQL Server client alias for connecting to the named instance. Block external access to the port that is used for the Central Administration site. TCP/25 (SMTP for e-mail integration) Database Server Hardening SharePoint Server 2007 has built-in support for protection against SQL Injection attacks; however, some steps must be taken to protect data from network eavesdropping, password cracking and unauthorized access. A default instance of SQL Server listens for connection on TCP port 1433. If a client computer is unable to connect to the database on TCP port 1433, it queries SQL Server Resolution Service on UDP port 1434. These standard ports are well known and the use of standard configuration enables the SQL Server Resolution Service to be targeted by malicious software. To protect for these the following steps need to be followed: Blocking of TCP port 1433 Blocking of UDP port 1434 Configuring SQL Server to listen to non-standard port Configure SQL Server client aliases on all Web Server and application servers Stop SQL Server Browser service to further secure Secure Topology Design Checklists Server topology design Review the following checklist to ensure that your plans meet the criteria for a secure server topology design. The topology incorporates dedicated front-end Web servers. Servers that host application server roles and database server roles are protected from direct user access. The SharePoint Central Administration site is hosted on a dedicated application server, such as the index server. ### Network topology design Review the following checklist to ensure that your plans meet the criteria for a secure networking topology design. All servers within the farm reside within a single data center and on the same vLAN. Access is allowed through a single point of entry, which is a firewall. For a more secure environment, the farm is separated into three tiers (front-end Web, application, and database), which are separated by routers or firewalls at each vLAN boundary. Planning for Extranet Topology Intranet & Extranet Zones An extranet environment is a private network that is securely extended to share part of an organization's information or processes with remote employees, external partners, or customers. The following table describes the benefits that the extranet provides for each group. Group Benefits Remote employees Remote employees can access corporate information and electronic resources anywhere, anytime, and any place, without requiring a virtual private network (VPN). Remote employees include: Traveling sales employees. Employees working from home offices or customer sites. Geographically dispersed virtual teams. External partners External partners can participate in business processes and collaborate with employees of your organization. You can use an extranet to help enhance the security of data in the following ways: Apply appropriate security and user-interface components to isolate partners and to segregate internal data. Authorize partners to use only sites and data that are necessary for their contributions. Restrict partners from viewing other partners\u2019 data. You can optimize processes and sites for partner collaboration in the following ways: Enable employees of your organization and partner employees to view, change, add, and delete content to promote successful results for both companies. Configure alerts to notify users when content changes or to start a workflow. Customers Publish branded, targeted content to partners and customers in the following ways: Target content based on product line or by customer profile. Segment content by implementing separate site collections within a farm. Limit content access and search results based on audience. Internet-facing Topology You can plan to host extranet content inside your corporate network and make it available through an edge firewall, or you can isolate the server farm inside a perimeter network. Together, these are the content and collaboration sites that employees will use on a day-to-day basis. Individually, each of these applications represents a distinct type of content. Each type of content: - Emphasizes different features of Office SharePoint Server 2007. - Hosts data with different data characteristics. - Is subject to a different usage profile. - Requires a different permissions management strategy. Consequently, design choices for each of these applications are intended to optimize the performance and security for each application. The use of a single Shared Services Provider (SSP) brings these three applications together to provide: - Navigation across the applications. - Enterprise-wide search. - Shared profile data. The following figure shows the three applications that make up the corporate intranet. Split back-to-back topology This topology splits the farm between the perimeter and corporate networks. The computers running Microsoft SQL Server database software are hosted inside the corporate network. Web servers are located in the perimeter network. The application server computers can be hosted in either the perimeter network or the corporate network. If the server farm is split between the perimeter network and the corporate network with the database servers located inside the corporate network, a domain trust relationship is required if Windows accounts are used to access SQL Server. In this scenario, the perimeter domain must trust the corporate domain. If SQL authentication is used, a domain trust relationship is not required. To optimize search performance and crawling, the application servers has been kept inside the corporate network with the database servers. Do not add the query role to the index server if the query role also is located on other servers in the farm. If you place Web servers in the perimeter network and application servers inside the corporate network, you must configure a one-way trust relationship in which the perimeter network domain trusts the corporate network domain. This one-way trust relationship is required in this scenario to support inter-server communication within the farm, regardless of whether you are using Windows authentication or SQL authentication to access SQL Server. Advantages Computers running SQL Server are not hosted inside the perimeter network. Farm components both within the corporate network and the perimeter network can share the same databases. Content can be isolated to a single farm inside the corporate network, which simplifies sharing and maintaining content across the corporate network and the perimeter network. With a separate Active Directory infrastructure, external user accounts can be created without affecting the internal corporate directory. Disadvantages Complexity of the solution is greatly increased. Inter-farm communication is typically split across two domains. Disaster Recovery Recommendations Disaster Recovery As part of the requirement, some kind replication needs to be established between the two sites so as to ensure the availability of data at all the sites. In case of failure of any one of the site the other must be able to take over the entire operations. So as to cater to these requirements, a site replication methodology is designed for Marxxx as shown in the below diagram. The proposed solution comprises of two sites Primary Site and DR Site. Between the Primary Site and the DR Site, data will be replicated asynchronously over the Dark FC/WAN IP network Connectivity. Disaster Recovery Strategy Figure 5: DC-DR Before Two logical farms: Only content databases can be successfully mirrored to failover farm. A separate configuration database must be maintained on the failover farm. All customizations must be done on both farms. Patches must be individually applied to both farms. SSP databases can be backed up and restored to the failover farm. Consult with your SAN vendor to determine whether you can use SAN replication or another supported mechanism to provide availability across data centers. Figure 6: DC-DR After On failover, traffic is redirected to the secondary farm. On failover, you must attach mirrored content databases to the Web applications running in the failover farm. Over time, you can either fail back or turn the primary farm into the failover farm. Disaster Recovery Configuration Process Disaster recovery solution for web content: Use SharePoint QA server for Disaster Recovery, first export the specific site from backup to a blank site and then extract the specific content that needs to be restored. Do not use production server for disaster recovery because the recovery process will overwrite the existing contents. Rationale: SharePoint\u2019s built-in recovery solution does not allow granular file/item level recovery. Entire site needs to be restored to recover a single file/item Technical Details: Use STSADM to restore the site to QA web server from an existing backup and then recover the file/item Disaster recovery solution for index servers: Solution1: The only clean and recommended way to recover from an index corruption is to completely rebuild the index on all the servers in the farm Solution 2: Build a new index server Disable and stop ossearch service on the index server Check if the propagation status is idle Copy the Portal_Content catalog from the query server to the indexer in the same location Asynchronous database mirroring There are two mirroring operating modes Synchronous mode - when a session starts, the mirror server synchronizes the mirror database together with the principal database as quickly as possible. As soon as the databases are synchronized, a transaction is committed on both partners, at the cost of increased transaction latency. Asynchronous mode - as soon as the principal server sends a log record to the mirror server, the principal server sends a confirmation to the client. It does not wait for an acknowledgement from the mirror server. This means that transactions commit without waiting for the mirror server to write the log to disk. Such asynchronous operation enables the principal server to run with minimum transaction latency, at the potential risk of some data loss. All database mirroring sessions support only one principal server and one mirror server. This configuration is shown in the following illustration. Deployment Architecture Production DC & DR Figure 5: Production Environment Non-Production Environments Development Environment SharePoint development environment configuration depends on the processes, type of engagements and type of work. The most popular solution that addresses the development scenarios is using local SharePoint farm, separated from the production servers, with the single installations of the SharePoint server on the development boxes. This provides isolation for builds, tests, and debugging across different teams, projects and production environments. The local environment is mostly isolated by development box and is installed on the host server or on virtual server. The following procedure is an overview of the steps that are required to create a typical SharePoint development environment. Development Box installation Use Windows Server, Visual Studio, and SQL Server. Windows Server 2008, VS 2008 and .NET 3.5, SQL 2008, with TFS 2008 is officially supported environment for SharePoint development. Advantage of Windows 2008 is that it is fast in virtualized environments. Install SharePoint on development boxes and prefer not to connect to existed farm instances used on other stages. Development environment should stay apart, to develop and tests in isolated environment. QA Environment The following diagram depicts the most common development environment, which is recommended by \u201cSharePoint Guidance patterns & practices\u201d team. Stand-alone SharePoint environment for development, unit testing and debugging of SharePoint project. Runs continuous integration and builds verification tests before deploying the SharePoint solutions to the test environment. Source Control/Build Server to build SharePoint packages (WSP) and to deploy solution to test environment. The test environment performs user acceptance testing, manual functional testing, automated functional testing, system testing, security testing, and performance testing. After the solution meets production requirements, the SharePoint solutions are deployed to the staging environment. The Staging server uses to test the \u201cproduction-ready\u201d solution in an environment that closely resembles the production environment. The purpose of this environment is to identify any potential deployment issues. Although the Staging environment is optional for smaller applications where deployment failures are not critical The staging environment represents the target production environment as closely as possible from the perspective of topology (for example, the server farm and database structure) and components (for example, the inclusion of the Microsoft Active Directory service and load balancing, where applicable). Virtualization of Environments Virtualization in SharePoint farm is one of the key design factors that simplify server availability by providing number of additional servers that might not be available over physical server models, or solution become very expensive. Microsoft officially supports SharePoint farm in virtualized environment since mid 2007. The following virtualizations technologies are supported: Hyper-V and 3rd party providers like VMware. One of the key factors for virtualization is that performance of virtualized farm is competitive to the physical farm. Microsoft tests shows: 7.2% less throughput on virtual Web roles with 8GB of RAM than a physical Web role server with 32GB of RAM; 4.4% slower in the page response time on the Hyper-V Web front-end than the physical server; Figure 6: Virtual Development Environment Virtualize SharePoint Web Role Web front-end servers are responsible for the content rendering and have comparatively lower memory requirements and disk activity than other roles, what makes them is an ideal candidate for virtualization. Choose disk type for Query Role The query role is responsible for a search performed by users is a good candidate for virtualization. The disk type choice for this role depends on the size of propagated index and the rate of index propagation. The recommendation for the large indexes and the farm with the high rate of the updated information to use a physical disk volume that is dedicated for the individual query server, rather than a virtual disk file. Consider using Index Role on physical server The Index server role in a SharePoint farm is often the most memory-intensive role, what makes it less ideal candidate for virtualization. Virtualized Index server role might be appropriate for development environment, small farm or farm with small content usage. Take into account, that index can vary from 10% to 30% of the total size of the documents being indexed. For the large indexes (above 200 GB) consider using physical disk volume that is dedicated to the individual query server, rather than virtual disk. For large farms with big amount of crawled data use physical Index server role due to large memory requirements and high disk I/O activity. Do not virtualize Database role SharePoint database role is the least appropriate role for virtualization in production scenarios, mainly due to the highest amount of disk I/O activity and very high memory and processor requirements. However, it is very common to see the SQL Server virtualized in test farms, quality assurance (QA) farms, and smaller SharePoint environments. Do I need to virtualize Application role? The decision of virtualizations the application roles, such Excel Services and InfoPath Services, depends on the roles usage. Those roles can be easily virtualized, because they are similar to Web Roles and mostly CPU intensive. When necessary, those servers can be easily moved to dedicated physical servers. Virtualized scenario sample The following picture demonstrates the common virtualized scenario of SharePoint Farm. Common deployment scenarios for the SQL role in a SharePoint farm may have multiple farms, both physical and virtual, use a single database server or database cluster, further increasing the amount of resources consumed by the role. For example, in the picture above, the sample SharePoint environment illustrated maintains a two-server SQL cluster that is used by several virtual farms and one production farm. Use proper number of CPU Do not use more virtual CPUs than physical CPUs on the virtual host computer \u2013 this will cause performance issues, because the hypervisor software has to swap out CPU contexts. The best performance can be realized if the number of virtual processors allocated to running guests does not exceed the number of logical processors (physical processors multiplied by the number of cores) on the host. For example, a four processor quad-core server will be able to allocate up to 16 virtual processors across its running sessions without any significant performance impact. Note that this only applies to sessions that are physically running simultaneously. Use proper amount of RAM Plan to allocate the memory on virtual sessions according the next rule \u2013 divide the total amount of RAM in the server by the number of logical processors (physical processors multiplied by number of cores) in the host server. This will align allocated memory along with NUMA sessions. Otherwise it will provide performance issues. In some testing, a virtual SharePoint Web server role with an allocation of 32GB of RAM actually performed worse than a virtual server with an allocation of 8GB of RAM. Plan to use physical drives In virtual scenarios front-end Web servers or Query servers disk performance is not as important as it would be physicals servers of the Index role or a SQL Server database. A fixed-size virtual disk typical provides better performance than a dynamically-sized disk. If disk speed is a high priority, consider adding physical drives to the host computer. Add new virtual hard drive and map it to an unused physical drive on the host. This configuration, called a \u201cpass-through disk\u201d, is likely to give the best overall disk throughput. Consider using hardware load balancing Hardware load balancing provides the best performance, comparing with the software load balancing. It offloads CPU and I/O pressure from the WFE\u2019s to hardware layer thereby improving availability of resources to SharePoint. Examples of Hardware: F5 BIG IP, Citrix Netscaler, Cisco Content Switch. Software load balancing examples are Windows Network load balancing, Round Robin load balancing with DNS. It is a trade-off between cost and performance. Be careful with snapshot feature on virtual servers Using snapshots for the backup might cause you troubles, because SharePoint timer service might be unsynchronized during the snapshot process, and once the snapshot is finished, errors or inconsistencies can arise. So, consider backups over the snapshots for the production environments. Measure virtualized environment performance (only for Hyper-V) After completing your virtualized environment installation and configuration, it's crucial to measure how fast your environment operates and optimize it for the best performance. Here are the key parameters to measure: Processor Performance Counter: \\HYPER-V HYPERVISOR LOGICAL Processor(_Total)% Total Run Time Results: <60% Utilization is fine. 60%-89% indicates caution. 90% signals significant performance degradation. Memory Performance Counter: \\MEMORY\\AVAILABLE MBytes This measures the physical memory available to processes running on the computer, as a percentage of the total physical memory installed on the computer. Results: 50% utilization is fine. 10% and below is critical. Disk Performance Counters: Read Latency: \\LOGICAL DISK(*)\\AVG. DISK SEC/READ Write Latency: \\LOGICAL DISK(*)\\AVG. DISK SEC/WRITE These measure disk latency on the Hyper-V host operating system. Results: Up to 15ms is fine. 15ms-25ms indicates a warning. Greater than 26ms is critical.","title":"SharePoint Farm Upgrade"},{"location":"M365/SharePoint2007FarmUpgrade/#case-study-sharepoint-farm-design-for-a-media-company","text":"I wokred as a SharePoint Architect for an farm deployment project for a prominent media company, this document captures the essence of our SharePoint Farm design project. Developed in collaboration with our networking and infrastructure teams, it outlines the deployment strategy employed and the considerations made to ensure a robust SharePoint environment. From SharePoint 2007 through to SharePoint 2016, the landscape of SharePoint has evolved, introducing several features and enhancements like miniRoles etc. However, the fundamental architecture focusing on high availability (HA) remains unchanged.","title":"Case Study: SharePoint Farm Design for a Media Company"},{"location":"M365/SharePoint2007FarmUpgrade/#background","text":"Marxxx wants to redesign their existing Microsoft Office SharePoint based portal application which includes around 10 site collections and also wants to improve and simplify the ability to manage content. Objectives of redesign Deploy the best fit infrastructure and system architecture Using generic content types with optimum reusability Using consistent use of document management Using a collaboration platform with self-service a minimal learning curve Standardize enterprise content management utilizing built-in SharePoint tools Implement a MOSS Governance policy that will define Roles and Responsibilities, Policies, Processes, Deployment Strategies and Site Structure Design and deploy High Availability solution with no single point of failure Integration of plug-in and proprietary applications based on MOSS","title":"Background"},{"location":"M365/SharePoint2007FarmUpgrade/#project-overview-understanding","text":"Marxxx has implemented the SharePoint to meet different organizational needs. However, the implementation lacks best practices and scope for future enhancements. On generic terms organizations adopting SharePoint face a variety of tasks \u2013 from planning, strategy, infrastructure and architecture design, UI Design, migration, and to development. All these tasks imply flexible infrastructural baseline before actual work starts. However, in reality we face the outdated environment and miss-configured farms that are not ready to implement new requirements. In such cases, baseline architecture becomes foundation stone of all SharePoint projects.","title":"Project Overview &amp; Understanding"},{"location":"M365/SharePoint2007FarmUpgrade/#current-system-study","text":"","title":"Current system study"},{"location":"M365/SharePoint2007FarmUpgrade/#current-system-study_1","text":"HXX has detailed discussions with the Marxxx management team to understand the needs to revamp & redesign the current SharePoint implementation. Marxxx provided the VPN connection with site level permission to view the replica of the production sites without sensitive data. HXX provided a document to Marxxx on how to remove sensitive data and backup/restore the sites in development area for HCL\u2019s assessment. HXX followed the standard process of studying current system. The following section would highlight the key findings of the system study.","title":"Current System study"},{"location":"M365/SharePoint2007FarmUpgrade/#current-system-assessment","text":"HXX's assessment findings based on current Marxxx SharePoint implementation are:","title":"Current System Assessment"},{"location":"M365/SharePoint2007FarmUpgrade/#governance","text":"No defined SharePoint resource governance matrix. Seems to be no clear business owners of sites and accountability lies on only technical team. Governance document exist but that is incomplete (only 2 pages) and doesn\u2019t seem to serve the purpose of SharePoint governance. It is out-dated. It seems SharePoint administration process is not defined and there seems to be no dedicated administrator for ongoing maintenance and implement governance best practices. No application usage & customization policies exist","title":"Governance"},{"location":"M365/SharePoint2007FarmUpgrade/#infrastructure-platform","text":"Current SharePoint installation is not implemented for High Availability. No business continuity plans defined. Single Point of failure. No Disaster Recovery farm exists. Server boxes are out-dated for future upgrade of SharePoint and virtualization technologies. Sandbox environment is not yet configured. Development environment consists of sensitive data which ideally should NOT reside in any environments other than production environment. Minimal integration observed with other Enterprise Web Applications and Data","title":"Infrastructure &amp; Platform"},{"location":"M365/SharePoint2007FarmUpgrade/#information-management","text":"Current taxonomy was developed without hierarchy \u2013 flat structure There are few lists/libraries implemented which are not used or obsolete in current sites and are eating up storage space. Contents scattered in File Systems as well \u2013 might be critical contents. No Expiration policies for contents. No Information Management documentation are available which narrates existing SharePoint implementation like taxonomy, design document etc,. Neither any user reference manuals. There is no ready reference on lists/libraries which contain sensitive information. Some custom ASPX pages are used without using SharePoint publishing feature. Content and document types are not being leveraged. Content publishing processes and policies do not exist.","title":"Information Management"},{"location":"M365/SharePoint2007FarmUpgrade/#usability","text":"Navigation is not structured in distinct, easily recognizable groups nor is it consistent in structure. MyMarvel Home page UI is good, however that the site menus and contents are not clearly identifiable in the glittery background. Quick links are not present in all sites to facilitate better navigation through different sites Broken Links present.","title":"Usability"},{"location":"M365/SharePoint2007FarmUpgrade/#sharepoint-farm-maintenance-deployment","text":"Dedicated Web application created for each Site Collection in current environment without proper reason or Isolation needs All custom DLLs are placed in GAC should ideally be in \\BIN folder as per Microsoft Best Practices. Backup is done, but no archiving strategy and plan found. No true business process management or enterprise-level workflow automation solutions are in place. Might be present offline \u2013 not known.","title":"SharePoint Farm Maintenance &amp; Deployment"},{"location":"M365/SharePoint2007FarmUpgrade/#planning-sharepoint-server-farm","text":"","title":"Planning SharePoint Server Farm"},{"location":"M365/SharePoint2007FarmUpgrade/#capacity-planning","text":"Assuming that 1000 users access the SharePoint web sites and the content size will grow to 2 TB in future, the following is the Capacity Planning based on Microsoft\u2019s System Center Capacity Planner 2007:","title":"Capacity Planning"},{"location":"M365/SharePoint2007FarmUpgrade/#usage-profile-characteristics","text":"Figure 1: Usage profile of SharePoint sites.","title":"Usage profile characteristics"},{"location":"M365/SharePoint2007FarmUpgrade/#topology-recommendation-by-capacity-planner","text":"Figure 2: Topology diagram as per MS System Center Capacity Planner 2007","title":"Topology recommendation by Capacity Planner"},{"location":"M365/SharePoint2007FarmUpgrade/#hardware-recommendation-by-capacity-planner","text":"","title":"Hardware recommendation by Capacity Planner"},{"location":"M365/SharePoint2007FarmUpgrade/#hardware-profile-for-the-servers","text":"The server configuration details are based on Microsoft\u2019s System Center Capacity Planner 2007: Total number of clients/users: 1000 Number of servers: 6 Number of SAN connections: 1","title":"Hardware profile for the servers"},{"location":"M365/SharePoint2007FarmUpgrade/#web-front-end-servers","text":"Server: Web Front End 1 Configuration Details Processor 2.20 GHz Quad Core Processor Minimum memory 8.0 GB Disk DiskArray 1\\Volume 1 (File System), 146 GB RAID 1 (2 x 146.00 GB SCSI 10,000 RPM) DiskArray 1\\Volume 2 (File System), 360 GB RAID 5 (6 x 72.00 GB SCSI 10,000 RPM) NIC 1 x 1,000 Mb/s Roles Web Front End; Query Server Server: Web Front End 2 Configuration Details Processor 2.20 GHz Quad Core Processor Minimum memory 8.0 GB Disk DiskArray 1\\Volume 1 (File System), 146 GB RAID 1 (2 x 146.00 GB SCSI 10,000 RPM) DiskArray 1\\Volume 2 (File System), 360 GB RAID 5 (6 x 72.00 GB SCSI 10,000 RPM) NIC 1 x 1,000 Mb/s Roles Web Front End","title":"Web Front End Servers"},{"location":"M365/SharePoint2007FarmUpgrade/#application-servers","text":"Server: Index Server & Query Server Configuration Details Processor 2.20 GHz Quad Core Processor Minimum memory 8.0 GB Disk DiskArray 1\\Volume 1 (File System), 1500 GB RAID 5 (6 x 300.00 GB SCSI 15,000 RPM) NIC 1 x 1,000 Mb/s Roles Index & Query","title":"Application Servers"},{"location":"M365/SharePoint2007FarmUpgrade/#database-sql-servers","text":"Server: SQL Server Cluster (Failover) Configuration Details Processor 2.20 GHz Quad Core Processor Minimum memory 16.0 GB Disk No local disk devices NIC 1 x 1,000 Mb/s SAN connections 2 x 4 Gb/s SANs SAN Array\\Volume 3 (Log Files), 600 GB RAID 10 (4 x 300.00 GB SCSI 15,000 RPM) SAN Array\\Volume 4 (Data Files), 5400 GB RAID 10 (36 x 300.00 GB SCSI 15,000 RPM) Roles SQL Server (Clustered) Server: SQL Server Cluster (Primary) Configuration Details Processor 2.20 GHz Quad Core Processor Minimum memory 16.0 GB Disk No local disk devices NIC 1 x 1,000 Mb/s SAN connections 2 x 4 Gb/s SANs SAN Array\\Volume 1 (Log Files), 600 GB RAID 10 (4 x 300.00 GB SCSI 15,000 RPM) SAN Array\\Volume 2 (Data Files), 5400 GB RAID 10 (36 x 300.00 GB SCSI 15,000 RPM) Roles SQL Server","title":"Database (SQL) Servers"},{"location":"M365/SharePoint2007FarmUpgrade/#san","text":"SAN: SAN Array Configuration Details Disk SAN Array\\Volume 1 (Log Files), 600 GB RAID 10 (4 x 300.00 GB SCSI 15,000 RPM) SAN Array\\Volume 2 (Data Files), 5400 GB RAID 10 (36 x 300.00 GB SCSI 15,000 RPM) SAN Array\\Volume 3 (Log Files), 600 GB RAID 10 (4 x 300.00 GB SCSI 15,000 RPM) SAN Array\\Volume 4 (Data Files), 5400 GB RAID 10 (36 x 300.00 GB SCSI 15,000 RPM)","title":"SAN"},{"location":"M365/SharePoint2007FarmUpgrade/#software-stack","text":"Software / Tools - Internet Explorer 6.0 and above - Microsoft Office SharePoint Server 2007 - Microsoft Office SharePoint Designer 2007 - Microsoft Visual Studio 2008 - Microsoft Exchange Server - Microsoft SQL Server 2008 R2 - Microsoft Windows Server 2003 R2 - Microsoft ForeFront","title":"Software Stack"},{"location":"M365/SharePoint2007FarmUpgrade/#browser-support","text":"","title":"Browser Support"},{"location":"M365/SharePoint2007FarmUpgrade/#about-browser-support","text":"Microsoft Office SharePoint Server 2007 supports several Web browsers that are commonly used. However, there are certain browsers that might cause some Office SharePoint Server 2007 functionality to be downgraded, limited, or available only through alternative steps. In some cases, functionality might be unavailable for noncritical administrative tasks.","title":"About browser support"},{"location":"M365/SharePoint2007FarmUpgrade/#levels-of-browser-support","text":"Web browser support is divided into two levels: level 1 and level 2. Although administrative tasks on SharePoint sites are optimized for level 1 browser, Office SharePoint Server 2007 also provides support for other browsers that are commonly used. To ensure complete access to all the functionality, it is recommended to use level 1 browser.","title":"Levels of browser support"},{"location":"M365/SharePoint2007FarmUpgrade/#level-1-web-browsers","text":"Level 1 Web browsers take advantage of advanced features provided by ActiveX controls and provide the most complete user experience. Level 1 browsers offer full functionality on all SharePoint sites, including the SharePoint Central Administration Web site. Level 1 browsers are: Microsoft Internet Explorer 6. x (32-bit) Windows Internet Explorer 7. x (32-bit)","title":"Level 1 Web browsers"},{"location":"M365/SharePoint2007FarmUpgrade/#system-architecture-design-recommendations","text":"Microsoft Office SharePoint Server 2007 provides the flexibility to meet many different deployment solution goals. This includes guidance that would help Marxxx in: Determine the number of server in farm required to meet the solution goals. Plan for the relationships between servers in the farm. Plan for Extranet -facing server farm. Design server-farm topologies to meet availability goals.","title":"System Architecture &amp; Design Recommendations"},{"location":"M365/SharePoint2007FarmUpgrade/#overall-design-goals","text":"The conceptualized design has been prepared considering the Marvel\u2019s need for SharePoint farm implementation. The key design goals are mentioned below: Use the minimum number of server farms to host various SharePoint web sites typically required by a Marxxx for their intranet and extranet, considering performance and high availability. Create a framework for designing a scalable environment. Design decisions for individual applications do not prevent the addition of other applications in future. For example, an initial deployment would include ten site collections that comprises of document center, document workspace, collaborative team sites or publishing sites that compose an intranet (team sites, My Sites, and published intranet content). By using a similar logical architecture design, Marxxx can add applications to the solution without affecting the design of the initial applications. In other words, the design does not incorporate design choices that limit the use of the environment. Provide access for several classes of users without compromising the security of the content within the disparate applications. Users from different network zones (both internal and external) with different authentication providers can participate in collaboration. Also, users can only access the content they are intended to access. By following a similar logical architecture design, Marxxx can create the opportunity to provide access to users in multiple locations and with different objectives. Ensure that the design can be used in an extranet environment. Deliberate design choices are made to ensure that the server farms can be securely deployed in a perimeter network (DMZ). The rest of this article discusses each of the physical components that appear in the landscape model in the following section and discusses the design choices that are applied to the model. The purpose of this approach is to demonstrate the different ways in which logical architecture components can be configured based on the Marvel\u2019s SharePoint Farm needs. ## System Landscape The Landscape has been divided in three zones. Extranet/Internet Perimeter Network (DMZ) Corporate Network (Intranet) Components Position Quantity Role Cisco Firewall Internet 1 (Optional) Hardware Firewall ISA 2006 Server Perimeter Network & Corporate Network 2 Software Firewall Web Front End Server Perimeter Network 2 WFE + Query Server Application Server Corporate Network 2 1 App Server + 1 Index Server SQL Server Corporate Network 2 Database Server SAN Array Corporate Network 1 SAN Storage ## Farm Design Consideration HXX has kept the following key design consideration at core while designing the framework of the proposed solution. To attain maximum redundancy with a minimum number of servers, deploy an additional application server to the middle tier for load balancing application server roles that are designed to be redundant. This server farm topology consists of six servers. The query role is installed to the front-end Web servers to achieve redundancy. This topology protects these server roles from direct user connections and optimizes the performance of the overall farm when compared to smaller farms. The SQL server would be a two node clustered installation. Office SharePoint Server 2007 supports scalable server farms for capacity, performance and availability. Typically, capacity is the first consideration in determining the number of server computers to start with. After factoring in performance, availability also plays a role in determining both the number of servers and the size or capacity of the server computers in a server farm.","title":"Overall Design Goals"},{"location":"M365/SharePoint2007FarmUpgrade/#high-availability-for-web-front-ends","text":"For high availability in a MOSS environment, HXX proposes to use a collection of servers supporting multiple SharePoint sites. A server farm militating against the effects of unexpected downtime in addition to downtime that is related to ongoing maintenance (such as operating system updates etc.). The HXX suggested server farm for Marxxx workloads that builds in availability will consists of six servers (please see the figure 3). There will be four servers for MOSS 2007, two dedicated to Web Server along with Query role, while the other two servers will share the Application services, and Indexing roles. All of the Web Server will be load balanced using NLB. SQL Server 2008 will be clustered across two servers in a primary-secondary cluster. #### NLB Network Load Balancer (NLB) is useful for ensuring that stateless applications, such as a Web server running Internet Information Services (IIS), are scalable by adding additional servers as the load increases. Windows NLB works in two different modes \u2013 Uni-cast and Multicast . When in unicast mode, NLB replaces the network card's original MAC address. When in multicast mode, NLB adds the new virtual MAC to the network card, but also keeps the card's original MAC address. Multiple network adapters in multicast mode for Marvel\u2019s new farm: This model is suitable for a cluster in which ordinary network communication among cluster hosts is necessary and in which there is heavy dedicated traffic from outside the cluster subnet to specific cluster hosts. Advantages Because there are at least two network adapters, overall network performance is typically enhanced. Ordinary network communication among cluster hosts is permitted. Disadvantages This model requires a second network adapter. Some routers might not support the use of a multicast media access control (MAC) address. This only affects the Network Load Balancing/MAC address (not all MAC addresses) and only when dynamic ARP replies are sent by the cluster to the router. not all MAC addresses","title":"High Availability for Web Front Ends"},{"location":"M365/SharePoint2007FarmUpgrade/#application-servers-redundancy","text":"The baseline server topology design depends on the requirements for redundancy of application server roles. This matrix below describes the application server roles relative to their redundancy options. Application server roles for Office SharePoint Server 2007 fall into two categories: Roles that can be redundant Roles that cannot be redundant Application server role Redundancy Allowed Query Yes Index No Windows SharePoint Services 3.0 search No Excel Calculation Services Yes Office Project Server 2007 Yes","title":"Application Servers Redundancy"},{"location":"M365/SharePoint2007FarmUpgrade/#index-role","text":"Microsoft has recommended as a best practice, having a dedicated index server available to crawl data and that should not be part of the potentially load-balanced front-end servers that actually serve up the web pages to end-users. The reason for this is indexing requires significant computing resources both on the indexing machine and the machines being crawled. Microsoft has also recommended having a dedicated Query server if the crawling content is more than 500 GB, this would alleviate the potential performance issues. Refer the proposed SharePoint System Landscape, the dedicated Index server will perform the index function of the farm in order to manage crawling of searchable content and will create index files.","title":"Index Role"},{"location":"M365/SharePoint2007FarmUpgrade/#query-role","text":"The Query role would be combined with the web front end servers in a load balanced fashion. This will provide redundancy for Query server role.","title":"Query Role."},{"location":"M365/SharePoint2007FarmUpgrade/#redundant-application-server","text":"The Query role would be combined with the web front end servers in a load balanced fashion. This will provide redundancy for Query server role.","title":"Redundant Application Server"},{"location":"M365/SharePoint2007FarmUpgrade/#availability-failover-for-database-server","text":"The database server role affects the availability of the solution more than any other role. If a Web server or an application server fails, these roles can quickly be restored or redeployed. However, if a database server fails, the solution depends on restoring the database server. This can potentially include rebuilding the database server and then restoring data from the backup media. In this case, you can potentially lose any new or changed data dating back to the last backup job, depending on how SQL Server 2008 is configured. Additionally, the solution will be completely unavailable for the time it takes to restore the database server role. Microsoft enables SQL Server database availability with the SQL Server Always on Technologies program. This consists of Database mirroring Failover clustering","title":"Availability &amp; Failover for Database Server"},{"location":"M365/SharePoint2007FarmUpgrade/#database-clustering-recommendation","text":"The proposed design depicts Failover Clustering among two SQL Server 2008 database nodes in the system landscape, which provides availability support for a complete instance of SQL Server. The following diagram shows the Database Failover cluster for SQL Server. The failover cluster is a combination of one or more nodes or servers, with two or more shared disks. It will appear as a single instance, but has functionality that provides failover from one node to another if the current node becomes unavailable. Office SharePoint Server 2007 references the cluster as a whole, so that failover is automatic and seamless from the perspective of SharePoint Products and Technologies.","title":"Database Clustering Recommendation"},{"location":"M365/SharePoint2007FarmUpgrade/#scalability","text":"The architecture is inherently scalable as all product components and integration products are inherently distributed and support scalability by distribution in addition to scale-outs and scale-ups. This would help Marxxx in future to upkeep the system for better performance when the number of users increases. Scale Up Constraints: The SharePoint implementation in the new farm will be adaptable to version upgrade (SharePoint 2010) only when the software (O/S and Database) installed are x64. Microsoft recommends, as a part of implementation Best Practices, that all production environment have x64 hardware and software for performance benefits. Though there would be x64 hardware support in the new production farm, the design is based on x86 software stack as available with Marxxx at present.","title":"Scalability"},{"location":"M365/SharePoint2007FarmUpgrade/#storage-area-network","text":"Storage Area Network (SAN) would be used to handle the bulk data storage. All the SQL Server files such as .mdf(primary data file), .ndf(secondary data files), .ldf(log files) etc. would be stored in the SAN.","title":"Storage Area Network"},{"location":"M365/SharePoint2007FarmUpgrade/#network-components","text":"","title":"Network Components"},{"location":"M365/SharePoint2007FarmUpgrade/#firewall","text":"A firewall is a software- or hardware-based barrier against unauthorized access to your network. Firewalls inspect all the traffic going through them. Configuration of a Firewall that Works Opt for a hardware firewall, such as Cisco. Hardware firewalls are more powerful, and give you more control over protecting your Web traffic than a software-based firewall. If Spam Protection is available on your firewall, enable it. Spam firewalls are proving popular in keeping the nastier emails out (viruses, botnets). Standardize all wireless connections on WPA. Not only does the WPA Protocol have better security than WEP, but this way you can recognize (and block) if someone else tries to break in using WEP or an unencrypted connection. Much of perimeter security is addressed by the firewall. However, with the increased risk from DoS attacks and malware scripts worming their ways past firewalls, additional security products are now needed for which Microsoft ForeFront is recommended. ForeFront is a suite of business security products, designed to cover every aspect of network infrastructure. Server operations, client PCs, gateways, network edge\u2014everything\u2019s protected. ForeFront includes: ForeFront Security for Exchange Server ForeFront Security for SharePoint ForeFront Client Security (FCS) \u2013 Malware protection for every client PC. Intelligent Application Gateway (IAG) \u2013 Remotely-connecting PCs must verify their compliance before gaining server access. Internet Security and Acceleration (ISA) Server 2006 \u2013 Manages security for VPN connections and remotely-accessed applications. Advantage of ForeFront Protect Exchange and MOSS 2007. Both servers deal with data coming and going. All day, every day. Cover them both with ForeFront Security for Exchange Server and ForeFront Security for SharePoint. Uniform Reporting. Instead of trying to piece together six reports from six different applications, you can read ForeFront reports. Collected from all components into one, so problems are quickly spotted. A wall against the next attack. Virus and malware attacks are becoming more creative. And sneakier. Since it\u2019s designed to watch the edges of your network, ForeFront will catch new break-in attempts before they find an exploit. Forefront goes beyond an antivirus on a desktop. It\u2019s an entire web of security spread out through all network systems, covering all access points in and out.","title":"Firewall"},{"location":"M365/SharePoint2007FarmUpgrade/#dns-configuration","text":"After setting up SharePoint Portal Server and creating the portal site, you must create a public DNS entry to map the public (external) FQDN to the IP address for the public (external) interface of the external ISA Server 2006 computer. The URL containing the FQDN is the URL that users will use to access the portal site across the extranet.","title":"DNS Configuration"},{"location":"M365/SharePoint2007FarmUpgrade/#active-directory-configuration","text":"From a MOSS 2007 point of view the Microsoft Split back-to-back topology scenario has been followed; MOSS web front-end servers are located in the perimeter network while the application and database servers reside in the internal network. A one-way Active Directory trust exists between the perimeter domain and the internal domain. This is a non-transitive trust where the perimeter domain trusts the internal domain but not the inverse. This serves two main purposes; it allows windows authentication and delegation to occur between services in the perimeter domain and the internal domain and at the same time protects sensitive data such as payroll databases on the internal network. Forest with a single domain is proposed: perimeter.corp.local (perimeter production domain, contains accounts for all members and external collaborators) Authenticates users from both perimeter domain and internal domain via domain trust A one-way domain trust is configured between the corp.com domain and the perimeter.corp.local domain. This is done to allow internal accounts to be used in the perimeter domain which in turn enables windows authentication to be used when accessing backend resources such as SQL databases. This trust means that deploying MOSS web front end servers is as straight forward as adding the web front end role, all IIS configuration is completed by MOSS and remains valid in both the internal and perimeter domains.","title":"Active Directory Configuration"},{"location":"M365/SharePoint2007FarmUpgrade/#farm-security-considerations","text":"SharePoint Server-farm security will include the following tasks: Secure communication planning \u2014 decide which methods of secure communication are most appropriate for Marxxx landscape. Server hardening \u2014 plan specific hardening settings for each of the server roles in the new server farm.","title":"Farm Security Considerations"},{"location":"M365/SharePoint2007FarmUpgrade/#communication-in-server-farm","text":"Secure communication between the components of Marxxx SharePoint Farm deployment is a vital element of in-depth security architecture to ensure business critical information is not intercepted and misappropriated during its flow from user request to Web Server to Application Server and Database Server. It\u2019s important to apply secure communication techniques for both inside and outside the firewall. Secure communication provides privacy and integrity of data. Privacy ensures that data remains private and confidential, and that it cannot be viewed by eavesdroppers armed with network-monitoring software. Privacy is usually provided by means of encryption. Integrity ensures that data is protected from accidental or deliberate (malicious) modification while in transit. Secure communication channels must provide integrity of data. Integrity is usually provided by using Message Authentication Codes (MACs).","title":"Communication in Server Farm"},{"location":"M365/SharePoint2007FarmUpgrade/#server-server-communication","text":"In this deployment scenario, the communications between servers is done using IPSec (Internet Protocol Security), which is an open standard framework that specifies encryption and packet-filtering techniques to enhance network security. IPSec is a transport-layer mechanism through which ensures the confidentiality and integrity of TCP/IP-based communications between servers. IPSec is completely transparent to applications because encryption, integrity, and authentication services are implemented at the transport level. Please refer MOSS Installation Manual \u2013 Part 2 Appendix B for instructions to setup IPSec.","title":"Server-Server Communication"},{"location":"M365/SharePoint2007FarmUpgrade/#client-server-communication","text":"SSL is generally recommended to secure communications between users and servers when sensitive information must be secured. SSL can be configured to require server authentication or both server and client authentication. SSL can decrease the performance of the network. There are several common guidelines that can be used to optimize pages that use SSL. First, use SSL only for pages that require it. This includes pages that contain or capture sensitive data, such as passwords or other personal data. SSL should be used only if the following conditions are true: To encrypt the page data. To guarantee data is sent to the intended web server.","title":"Client-Server Communication"},{"location":"M365/SharePoint2007FarmUpgrade/#server-hardening","text":"The process of \u201cServer Hardening\u201d deals with activities to make servers in the SharePoint farm less vulnerable to attack. This section describes hardening characteristics for Web servers, application servers and database servers.","title":"Server Hardening"},{"location":"M365/SharePoint2007FarmUpgrade/#web-application-server-hardening","text":"The following table summarizes the Microsoft Patterns and Practices guidance for securing Web Server Task Applies To Details Patches and Updates - WFE servers - Application servers Ports - WFE servers - Application servers - Limit Internet-facing ports to port 80 for HTTP and port 443 for HTTPS. - Use IPSec to encrypt or restrict intranet communication. \u2003( Refer the complete list below ) Sites and Virtual drives - WFE servers - Application servers hosting Central Admin - Move website to non-system drive. - Restrict Web permissions in the IIS meta-base. - Remove FrontPage Server extensions. Protocols - WFE servers - Application servers - Configure TCP/IP stack to mitigate the threat from network denial of service attacks. - Disable the NetBIOS and SMB protocols if not used. Shares - WFE servers - Application servers - Remove all unnecessary shared folders. - Restrict access to any required shares. IIS Lockdown - WFE servers - Application servers hosting Central Admin - Install URLScan to block requests that contain unsafe characters. It is installed automatically when IISLockdown is run. The following list depicts the important ports: TCP 80, TCP 443 (SSL) Direct-hosted SMB (TCP/UDP 445) \u2014 disable this port if not used NetBIOS over TCP/IP (NetBT) (TCP/UDP ports 137, 138, 139) \u2014 disable this port if not used Ports required for communication between Web servers and service applications (the default is HTTP): HTTP binding: 32843 HTTPS binding: 32844 net.tcp binding: 32845 (only if a third party has implemented this option for a service application) Ports required for synchronizing profiles between SharePoint and Active Directory on the server that runs the Forefront Identity Management agent: TCP/5725 TCP/UDP 389 (LDAP service) TCP/UDP 88 (Kerberos) TCP/UDP 53 (DNS) UDP 464 (Kerberos Change Password) UDP port 1434 and TCP port 1433 \u2014 default ports for SQL Server communication. If these ports are blocked on the SQL Server computer (recommended) and databases are installed on a named instance, configure a SQL Server client alias for connecting to the named instance. Block external access to the port that is used for the Central Administration site. TCP/25 (SMTP for e-mail integration)","title":"Web &amp; Application Server Hardening"},{"location":"M365/SharePoint2007FarmUpgrade/#database-server-hardening","text":"SharePoint Server 2007 has built-in support for protection against SQL Injection attacks; however, some steps must be taken to protect data from network eavesdropping, password cracking and unauthorized access. A default instance of SQL Server listens for connection on TCP port 1433. If a client computer is unable to connect to the database on TCP port 1433, it queries SQL Server Resolution Service on UDP port 1434. These standard ports are well known and the use of standard configuration enables the SQL Server Resolution Service to be targeted by malicious software. To protect for these the following steps need to be followed: Blocking of TCP port 1433 Blocking of UDP port 1434 Configuring SQL Server to listen to non-standard port Configure SQL Server client aliases on all Web Server and application servers Stop SQL Server Browser service to further secure","title":"Database Server Hardening"},{"location":"M365/SharePoint2007FarmUpgrade/#secure-topology-design-checklists","text":"","title":"Secure Topology Design Checklists"},{"location":"M365/SharePoint2007FarmUpgrade/#server-topology-design","text":"Review the following checklist to ensure that your plans meet the criteria for a secure server topology design. The topology incorporates dedicated front-end Web servers. Servers that host application server roles and database server roles are protected from direct user access. The SharePoint Central Administration site is hosted on a dedicated application server, such as the index server. ### Network topology design Review the following checklist to ensure that your plans meet the criteria for a secure networking topology design. All servers within the farm reside within a single data center and on the same vLAN. Access is allowed through a single point of entry, which is a firewall. For a more secure environment, the farm is separated into three tiers (front-end Web, application, and database), which are separated by routers or firewalls at each vLAN boundary.","title":"Server topology design"},{"location":"M365/SharePoint2007FarmUpgrade/#planning-for-extranet-topology","text":"","title":"Planning for Extranet Topology"},{"location":"M365/SharePoint2007FarmUpgrade/#intranet-extranet-zones","text":"An extranet environment is a private network that is securely extended to share part of an organization's information or processes with remote employees, external partners, or customers. The following table describes the benefits that the extranet provides for each group. Group Benefits Remote employees Remote employees can access corporate information and electronic resources anywhere, anytime, and any place, without requiring a virtual private network (VPN). Remote employees include: Traveling sales employees. Employees working from home offices or customer sites. Geographically dispersed virtual teams. External partners External partners can participate in business processes and collaborate with employees of your organization. You can use an extranet to help enhance the security of data in the following ways: Apply appropriate security and user-interface components to isolate partners and to segregate internal data. Authorize partners to use only sites and data that are necessary for their contributions. Restrict partners from viewing other partners\u2019 data. You can optimize processes and sites for partner collaboration in the following ways: Enable employees of your organization and partner employees to view, change, add, and delete content to promote successful results for both companies. Configure alerts to notify users when content changes or to start a workflow. Customers Publish branded, targeted content to partners and customers in the following ways: Target content based on product line or by customer profile. Segment content by implementing separate site collections within a farm. Limit content access and search results based on audience.","title":"Intranet &amp; Extranet Zones"},{"location":"M365/SharePoint2007FarmUpgrade/#internet-facing-topology","text":"You can plan to host extranet content inside your corporate network and make it available through an edge firewall, or you can isolate the server farm inside a perimeter network. Together, these are the content and collaboration sites that employees will use on a day-to-day basis. Individually, each of these applications represents a distinct type of content. Each type of content: - Emphasizes different features of Office SharePoint Server 2007. - Hosts data with different data characteristics. - Is subject to a different usage profile. - Requires a different permissions management strategy. Consequently, design choices for each of these applications are intended to optimize the performance and security for each application. The use of a single Shared Services Provider (SSP) brings these three applications together to provide: - Navigation across the applications. - Enterprise-wide search. - Shared profile data. The following figure shows the three applications that make up the corporate intranet.","title":"Internet-facing Topology"},{"location":"M365/SharePoint2007FarmUpgrade/#split-back-to-back-topology","text":"This topology splits the farm between the perimeter and corporate networks. The computers running Microsoft SQL Server database software are hosted inside the corporate network. Web servers are located in the perimeter network. The application server computers can be hosted in either the perimeter network or the corporate network. If the server farm is split between the perimeter network and the corporate network with the database servers located inside the corporate network, a domain trust relationship is required if Windows accounts are used to access SQL Server. In this scenario, the perimeter domain must trust the corporate domain. If SQL authentication is used, a domain trust relationship is not required. To optimize search performance and crawling, the application servers has been kept inside the corporate network with the database servers. Do not add the query role to the index server if the query role also is located on other servers in the farm. If you place Web servers in the perimeter network and application servers inside the corporate network, you must configure a one-way trust relationship in which the perimeter network domain trusts the corporate network domain. This one-way trust relationship is required in this scenario to support inter-server communication within the farm, regardless of whether you are using Windows authentication or SQL authentication to access SQL Server.","title":"Split back-to-back topology"},{"location":"M365/SharePoint2007FarmUpgrade/#advantages","text":"Computers running SQL Server are not hosted inside the perimeter network. Farm components both within the corporate network and the perimeter network can share the same databases. Content can be isolated to a single farm inside the corporate network, which simplifies sharing and maintaining content across the corporate network and the perimeter network. With a separate Active Directory infrastructure, external user accounts can be created without affecting the internal corporate directory.","title":"Advantages"},{"location":"M365/SharePoint2007FarmUpgrade/#disadvantages","text":"Complexity of the solution is greatly increased. Inter-farm communication is typically split across two domains.","title":"Disadvantages"},{"location":"M365/SharePoint2007FarmUpgrade/#disaster-recovery-recommendations","text":"","title":"Disaster Recovery Recommendations"},{"location":"M365/SharePoint2007FarmUpgrade/#disaster-recovery","text":"As part of the requirement, some kind replication needs to be established between the two sites so as to ensure the availability of data at all the sites. In case of failure of any one of the site the other must be able to take over the entire operations. So as to cater to these requirements, a site replication methodology is designed for Marxxx as shown in the below diagram. The proposed solution comprises of two sites Primary Site and DR Site. Between the Primary Site and the DR Site, data will be replicated asynchronously over the Dark FC/WAN IP network Connectivity.","title":"Disaster Recovery"},{"location":"M365/SharePoint2007FarmUpgrade/#disaster-recovery-strategy","text":"Figure 5: DC-DR Before Two logical farms: Only content databases can be successfully mirrored to failover farm. A separate configuration database must be maintained on the failover farm. All customizations must be done on both farms. Patches must be individually applied to both farms. SSP databases can be backed up and restored to the failover farm. Consult with your SAN vendor to determine whether you can use SAN replication or another supported mechanism to provide availability across data centers. Figure 6: DC-DR After On failover, traffic is redirected to the secondary farm. On failover, you must attach mirrored content databases to the Web applications running in the failover farm. Over time, you can either fail back or turn the primary farm into the failover farm.","title":"Disaster Recovery Strategy"},{"location":"M365/SharePoint2007FarmUpgrade/#disaster-recovery-configuration-process","text":"Disaster recovery solution for web content: Use SharePoint QA server for Disaster Recovery, first export the specific site from backup to a blank site and then extract the specific content that needs to be restored. Do not use production server for disaster recovery because the recovery process will overwrite the existing contents. Rationale: SharePoint\u2019s built-in recovery solution does not allow granular file/item level recovery. Entire site needs to be restored to recover a single file/item Technical Details: Use STSADM to restore the site to QA web server from an existing backup and then recover the file/item Disaster recovery solution for index servers: Solution1: The only clean and recommended way to recover from an index corruption is to completely rebuild the index on all the servers in the farm Solution 2: Build a new index server Disable and stop ossearch service on the index server Check if the propagation status is idle Copy the Portal_Content catalog from the query server to the indexer in the same location","title":"Disaster Recovery Configuration Process"},{"location":"M365/SharePoint2007FarmUpgrade/#asynchronous-database-mirroring","text":"There are two mirroring operating modes Synchronous mode - when a session starts, the mirror server synchronizes the mirror database together with the principal database as quickly as possible. As soon as the databases are synchronized, a transaction is committed on both partners, at the cost of increased transaction latency. Asynchronous mode - as soon as the principal server sends a log record to the mirror server, the principal server sends a confirmation to the client. It does not wait for an acknowledgement from the mirror server. This means that transactions commit without waiting for the mirror server to write the log to disk. Such asynchronous operation enables the principal server to run with minimum transaction latency, at the potential risk of some data loss. All database mirroring sessions support only one principal server and one mirror server. This configuration is shown in the following illustration.","title":"Asynchronous database mirroring"},{"location":"M365/SharePoint2007FarmUpgrade/#deployment-architecture","text":"","title":"Deployment Architecture"},{"location":"M365/SharePoint2007FarmUpgrade/#production-dc-dr","text":"Figure 5: Production Environment","title":"Production DC &amp; DR"},{"location":"M365/SharePoint2007FarmUpgrade/#non-production-environments","text":"","title":"Non-Production Environments"},{"location":"M365/SharePoint2007FarmUpgrade/#development-environment","text":"SharePoint development environment configuration depends on the processes, type of engagements and type of work. The most popular solution that addresses the development scenarios is using local SharePoint farm, separated from the production servers, with the single installations of the SharePoint server on the development boxes. This provides isolation for builds, tests, and debugging across different teams, projects and production environments. The local environment is mostly isolated by development box and is installed on the host server or on virtual server. The following procedure is an overview of the steps that are required to create a typical SharePoint development environment. Development Box installation Use Windows Server, Visual Studio, and SQL Server. Windows Server 2008, VS 2008 and .NET 3.5, SQL 2008, with TFS 2008 is officially supported environment for SharePoint development. Advantage of Windows 2008 is that it is fast in virtualized environments. Install SharePoint on development boxes and prefer not to connect to existed farm instances used on other stages. Development environment should stay apart, to develop and tests in isolated environment.","title":"Development Environment"},{"location":"M365/SharePoint2007FarmUpgrade/#qa-environment","text":"The following diagram depicts the most common development environment, which is recommended by \u201cSharePoint Guidance patterns & practices\u201d team. Stand-alone SharePoint environment for development, unit testing and debugging of SharePoint project. Runs continuous integration and builds verification tests before deploying the SharePoint solutions to the test environment. Source Control/Build Server to build SharePoint packages (WSP) and to deploy solution to test environment. The test environment performs user acceptance testing, manual functional testing, automated functional testing, system testing, security testing, and performance testing. After the solution meets production requirements, the SharePoint solutions are deployed to the staging environment. The Staging server uses to test the \u201cproduction-ready\u201d solution in an environment that closely resembles the production environment. The purpose of this environment is to identify any potential deployment issues. Although the Staging environment is optional for smaller applications where deployment failures are not critical The staging environment represents the target production environment as closely as possible from the perspective of topology (for example, the server farm and database structure) and components (for example, the inclusion of the Microsoft Active Directory service and load balancing, where applicable).","title":"QA Environment"},{"location":"M365/SharePoint2007FarmUpgrade/#virtualization-of-environments","text":"Virtualization in SharePoint farm is one of the key design factors that simplify server availability by providing number of additional servers that might not be available over physical server models, or solution become very expensive. Microsoft officially supports SharePoint farm in virtualized environment since mid 2007. The following virtualizations technologies are supported: Hyper-V and 3rd party providers like VMware. One of the key factors for virtualization is that performance of virtualized farm is competitive to the physical farm. Microsoft tests shows: 7.2% less throughput on virtual Web roles with 8GB of RAM than a physical Web role server with 32GB of RAM; 4.4% slower in the page response time on the Hyper-V Web front-end than the physical server; Figure 6: Virtual Development Environment","title":"Virtualization of Environments"},{"location":"M365/SharePoint2007FarmUpgrade/#virtualize-sharepoint-web-role","text":"Web front-end servers are responsible for the content rendering and have comparatively lower memory requirements and disk activity than other roles, what makes them is an ideal candidate for virtualization.","title":"Virtualize SharePoint Web Role"},{"location":"M365/SharePoint2007FarmUpgrade/#choose-disk-type-for-query-role","text":"The query role is responsible for a search performed by users is a good candidate for virtualization. The disk type choice for this role depends on the size of propagated index and the rate of index propagation. The recommendation for the large indexes and the farm with the high rate of the updated information to use a physical disk volume that is dedicated for the individual query server, rather than a virtual disk file.","title":"Choose disk type for Query Role"},{"location":"M365/SharePoint2007FarmUpgrade/#consider-using-index-role-on-physical-server","text":"The Index server role in a SharePoint farm is often the most memory-intensive role, what makes it less ideal candidate for virtualization. Virtualized Index server role might be appropriate for development environment, small farm or farm with small content usage. Take into account, that index can vary from 10% to 30% of the total size of the documents being indexed. For the large indexes (above 200 GB) consider using physical disk volume that is dedicated to the individual query server, rather than virtual disk. For large farms with big amount of crawled data use physical Index server role due to large memory requirements and high disk I/O activity.","title":"Consider using Index Role on physical server"},{"location":"M365/SharePoint2007FarmUpgrade/#do-not-virtualize-database-role","text":"SharePoint database role is the least appropriate role for virtualization in production scenarios, mainly due to the highest amount of disk I/O activity and very high memory and processor requirements. However, it is very common to see the SQL Server virtualized in test farms, quality assurance (QA) farms, and smaller SharePoint environments.","title":"Do not virtualize Database role"},{"location":"M365/SharePoint2007FarmUpgrade/#do-i-need-to-virtualize-application-role","text":"The decision of virtualizations the application roles, such Excel Services and InfoPath Services, depends on the roles usage. Those roles can be easily virtualized, because they are similar to Web Roles and mostly CPU intensive. When necessary, those servers can be easily moved to dedicated physical servers.","title":"Do I need to virtualize Application role?"},{"location":"M365/SharePoint2007FarmUpgrade/#virtualized-scenario-sample","text":"The following picture demonstrates the common virtualized scenario of SharePoint Farm. Common deployment scenarios for the SQL role in a SharePoint farm may have multiple farms, both physical and virtual, use a single database server or database cluster, further increasing the amount of resources consumed by the role. For example, in the picture above, the sample SharePoint environment illustrated maintains a two-server SQL cluster that is used by several virtual farms and one production farm.","title":"Virtualized scenario sample"},{"location":"M365/SharePoint2007FarmUpgrade/#use-proper-number-of-cpu","text":"Do not use more virtual CPUs than physical CPUs on the virtual host computer \u2013 this will cause performance issues, because the hypervisor software has to swap out CPU contexts. The best performance can be realized if the number of virtual processors allocated to running guests does not exceed the number of logical processors (physical processors multiplied by the number of cores) on the host. For example, a four processor quad-core server will be able to allocate up to 16 virtual processors across its running sessions without any significant performance impact. Note that this only applies to sessions that are physically running simultaneously.","title":"Use proper number of CPU"},{"location":"M365/SharePoint2007FarmUpgrade/#use-proper-amount-of-ram","text":"Plan to allocate the memory on virtual sessions according the next rule \u2013 divide the total amount of RAM in the server by the number of logical processors (physical processors multiplied by number of cores) in the host server. This will align allocated memory along with NUMA sessions. Otherwise it will provide performance issues. In some testing, a virtual SharePoint Web server role with an allocation of 32GB of RAM actually performed worse than a virtual server with an allocation of 8GB of RAM.","title":"Use proper amount of RAM"},{"location":"M365/SharePoint2007FarmUpgrade/#plan-to-use-physical-drives","text":"In virtual scenarios front-end Web servers or Query servers disk performance is not as important as it would be physicals servers of the Index role or a SQL Server database. A fixed-size virtual disk typical provides better performance than a dynamically-sized disk. If disk speed is a high priority, consider adding physical drives to the host computer. Add new virtual hard drive and map it to an unused physical drive on the host. This configuration, called a \u201cpass-through disk\u201d, is likely to give the best overall disk throughput.","title":"Plan to use physical drives"},{"location":"M365/SharePoint2007FarmUpgrade/#consider-using-hardware-load-balancing","text":"Hardware load balancing provides the best performance, comparing with the software load balancing. It offloads CPU and I/O pressure from the WFE\u2019s to hardware layer thereby improving availability of resources to SharePoint. Examples of Hardware: F5 BIG IP, Citrix Netscaler, Cisco Content Switch. Software load balancing examples are Windows Network load balancing, Round Robin load balancing with DNS. It is a trade-off between cost and performance.","title":"Consider using hardware load balancing"},{"location":"M365/SharePoint2007FarmUpgrade/#be-careful-with-snapshot-feature-on-virtual-servers","text":"Using snapshots for the backup might cause you troubles, because SharePoint timer service might be unsynchronized during the snapshot process, and once the snapshot is finished, errors or inconsistencies can arise. So, consider backups over the snapshots for the production environments.","title":"Be careful with snapshot feature on virtual servers"},{"location":"M365/SharePoint2007FarmUpgrade/#measure-virtualized-environment-performance-only-for-hyper-v","text":"After completing your virtualized environment installation and configuration, it's crucial to measure how fast your environment operates and optimize it for the best performance. Here are the key parameters to measure: Processor Performance Counter: \\HYPER-V HYPERVISOR LOGICAL Processor(_Total)% Total Run Time Results: <60% Utilization is fine. 60%-89% indicates caution. 90% signals significant performance degradation. Memory Performance Counter: \\MEMORY\\AVAILABLE MBytes This measures the physical memory available to processes running on the computer, as a percentage of the total physical memory installed on the computer. Results: 50% utilization is fine. 10% and below is critical. Disk Performance Counters: Read Latency: \\LOGICAL DISK(*)\\AVG. DISK SEC/READ Write Latency: \\LOGICAL DISK(*)\\AVG. DISK SEC/WRITE These measure disk latency on the Hyper-V host operating system. Results: Up to 15ms is fine. 15ms-25ms indicates a warning. Greater than 26ms is critical.","title":"Measure virtualized environment performance (only for Hyper-V)"},{"location":"M365/SharePoint2016FarmUpgrade/","text":"Background Project Overview \\& Understanding Objectives of Redesign Current System Study Current System Assessment Governance Infrastructure \\& Platform Information Management Usability SharePoint Farm Maintenance \\& Deployment System Architecture \\& Design Recommendations Available architectural models License models Topology Recommendation Services Installed: High Availability Explanation: Disaster recovery Recommendations Background Hello, readers. In this article I will share my experience architecting a farm topology for a SharePoint 2016 on-prem, in-country farm deployment project. The project's goal was to revamp the client's existing Microsoft Office SharePoint setup. Our preliminary assessment revealed a stark absence of best practice implementations in their topology design. Furthermore, we encountered issues with their CMS system and various other elements of their SharePoint portals. Without further delay, let's dive into the details. Project Overview & Understanding The client has deployed SharePoint to satisfy various organizational needs. Despite this, the implementation is lacking in best practices and future enhancement opportunities. Common challenges for organizations adopting SharePoint include planning, strategy, infrastructure, architecture design, UI Design, migration, and development. These tasks demand a flexible infrastructure as a prerequisite. Yet, often, organizations encounter outdated and improperly configured environments that hinder new implementations. Thus, the baseline architecture is critical for the success of all SharePoint projects. Objectives of Redesign Deploy the best fit infrastructure and system architecture. Utilize generic content types with optimum reusability. Ensure consistent use of document management. Leverage a collaboration platform that minimizes the learning curve through self-service. Standardize enterprise content management using built-in SharePoint tools. Implement a MOSS Governance policy to define Roles and Responsibilities, Policies, Processes, Deployment Strategies, and Site Structure. Design and deploy a High Availability solution without any single point of failure. Integrate plug-in and proprietary applications based on MOSS. Current System Study The main team working on this project engaged in thorough discussions with the The Client management team to comprehend the requirements for revamping and redesigning the current SharePoint setup. The Client facilitated this by providing VPN access with site-level permissions for us to review the production sites' replicas, minus any sensitive data. We has documented how to remove sensitive data and manage the backup/restore process for site assessments in the development environment, adhering to standard practices during the current system study. Current System Assessment Governance Absence of a defined SharePoint resource governance matrix. Lack of clear business ownership for sites, with accountability resting solely on the technical team. Existence of an incomplete and outdated governance document. Undefined SharePoint administration process and absence of a dedicated administrator for maintenance and governance best practices implementation. Lack of application usage and customization policies. Infrastructure & Platform Non-implementation of SharePoint for High Availability and lack of business continuity plans. Presence of single points of failure and absence of a Disaster Recovery farm. Outdated server boxes unsuitable for future SharePoint upgrades and virtualization technologies. Non-configured sandbox environment. Presence of sensitive data in the development environment, which should only reside in the production environment. Minimal integration with other Enterprise Web Applications and Data. Information Management Development of a flat structure taxonomy without hierarchy. Presence of unused or obsolete lists/libraries consuming storage space. Scattered contents in File Systems potentially holding critical contents without expiration policies. Absence of Information Management documentation describing existing SharePoint implementation. Lack of reference for lists/libraries containing sensitive information. Use of custom ASPX pages outside of the SharePoint publishing feature. Underutilization of content and document types. Nonexistence of content publishing processes and policies. Usability Unstructured navigation lacking distinct, easily recognizable groups and consistency. The UI of Home page is appealing, but site menus and contents are not easily identifiable against a glittery background. Absence of quick links across all sites for better navigation. Presence of broken links. SharePoint Farm Maintenance & Deployment Creation of dedicated web applications for each Site Collection without clear rationale or isolation needs. Placement of all custom DLLs in GAC, contrary to Microsoft Best Practices recommending the \\BIN folder. Execution of backups without an archiving strategy and plan. Absence of true business process management or enterprise-level workflow automation solutions, possibly available offline. System Architecture & Design Recommendations Microsoft Office SharePoint Server 2016 provides the flexibility to meet many different deployment solution goals. This includes guidance that would help the client in: - Determine the number of server in farm required to meet the solution goals. - Plan for the relationships between servers in the farm. - Plan for Extranet -facing server farm. - Design server-farm topologies to meet availability goals. Available architectural models The table below summarizes the different approaches to deploying and managing SharePoint 2016, ranging from fully cloud-based SaaS solutions to traditional on-premises deployments. Model Description SharePoint Online/SaaS You consume SharePoint through a Software as a Service (SaaS) with an Office 365 subscription. SharePoint is always up to date, but you are responsible for managing SharePoint itself. SharePoint Hybrid Combines SharePoint Online with a SharePoint Server 2016 farm, deployed either in Azure or on-premises. Incorporates SharePoint Online services into your overall SharePoint offering, starts building SaaS management skills in your organization, and moves your SharePoint Server 2016 sites and apps to the cloud at your own pace. SharePoint in Azure/IaaS Extends your on-premises environment into Azure Infrastructure as a Service (IaaS) for production, disaster recovery, or dev/test SharePoint Server 2016 farms. SharePoint on-premises Plans, deploys, maintains, and customizes your SharePoint Server 2016 farm in a datacenter that you maintain. License models Deployment Model Licensing Requirements SharePoint Online Assign licenses to Azure AD user accounts from your Office 365 subscription, no additional licenses needed SharePoint Hybrid - Office 365: Subscription model, no additional licenses needed - On-premises: Windows Server 2012 R2 or Windows Server 2016 - On-premises: SQL Server 2016 or SQL Server 2014 SP1 or later - On-premises: SharePoint Server 2016 License - On-premises: SharePoint Server 2016 Client Access License SharePoint Server 2016 in Azure (IaaS) - Azure subscription - SharePoint Server 2016 License - SharePoint Server 2016 Client Access License SharePoint Server 2016 On-premises - Windows Server 2016 or Windows Server 2012 R2 - SQL Server 2016 or SQL Server 2014 SP1 or later - SharePoint Server 2016 License - SharePoint Server 2016 Client Access License Topology Recommendation As the client wants to have an in-country farm due to data regulations. We recommend the following farm topology for a high-availability, on-premises SharePoint 2016 farm with six servers, using the latest features such as MinRole Front-End with Distributed Cache Servers: - Server 1 & 2 : These servers will handle all the user requests and serve the web pages. They will also host the Distributed Cache service, which is crucial for speeding up the retrieval of data and improving performance by caching frequently accessed information. Application with Search Servers: - Server 3 & 4 : These servers will run backend service applications and host the Search service. They handle the processing tasks that support the front-end servers, such as the Search service, which indexes content and processes search queries. Database Servers: - Server 5 & 6 : These will be the SQL Server databases configured in a high-availability cluster using SQL Server AlwaysOn Availability Groups. They store all the content and configurations for the SharePoint farm. Services Installed: Front-End with Distributed Cache Servers (Server 1 & 2): - Access Services - Business Data Connectivity Service - Managed Metadata Web Service - User Profile Service - Distributed Cache - Microsoft SharePoint Foundation Web Application - And other front-end related services... Application with Search Servers (Server 3 & 4): - App Management Service - Business Data Connectivity Service - Machine Translation Service - Managed Metadata Web Service - Search Host Controller Service - Search Query and Site Settings Service - Secure Store Service - User Profile Service - And other application services... Database Servers (Server 5 & 6): - SQL Server with AlwaysOn Availability Groups configured for all SharePoint databases. High Availability Explanation: Front-End Servers: - Having two front-end servers ensures that if one goes down, the other can continue to serve user requests without interruption. Network Load Balancer (NLB) would be used to distribute the requests evenly between the two servers. Application with Search Servers: - Two servers with application and search services offer redundancy for these critical components of the SharePoint infrastructure. If one server fails, the other can take over the services without impacting the availability of the SharePoint farm. Database Servers: - SQL Server AlwaysOn Availability Groups provide high availability for the databases. In the event of a database server failure, the other node in the AlwaysOn group will take over, ensuring the SharePoint farm's data remains accessible. This setup provides both high availability and disaster recovery. The suggested SharePoint 2016 farm topology is designed to minimize single points of failure, ensuring that user access is uninterrupted, search functionality remains operational, and data is consistently available even in the event of server outages. The use of MinRole ensures that each server is optimized for its role, improving performance and reliability. The distribution of roles across multiple servers, along with the redundancy built into each layer (front-end, application, and database), achieves a highly available environment that aligns with SharePoint 2016's infrastructure advancements. Disaster recovery Recommendations Implement a \"stretched farm\" where there will be two data centres configured as a single farm. This is possible if two data centres are in close proximity, connected with higg bandwidth fiber optiks link. The bandwidth requiremnt, recommended by Microsoft for such topology is: Intra-farm latency of <1ms (one way), 99.9% of the time over a period of ten minutes. The bandwidth speed must be at least 1 gigabit per second. The recommended topology from Microsoft would look something like as shown below:","title":"SharePoint Farm 2016 Farm Setup"},{"location":"M365/SharePoint2016FarmUpgrade/#background","text":"Hello, readers. In this article I will share my experience architecting a farm topology for a SharePoint 2016 on-prem, in-country farm deployment project. The project's goal was to revamp the client's existing Microsoft Office SharePoint setup. Our preliminary assessment revealed a stark absence of best practice implementations in their topology design. Furthermore, we encountered issues with their CMS system and various other elements of their SharePoint portals. Without further delay, let's dive into the details.","title":"Background"},{"location":"M365/SharePoint2016FarmUpgrade/#project-overview-understanding","text":"The client has deployed SharePoint to satisfy various organizational needs. Despite this, the implementation is lacking in best practices and future enhancement opportunities. Common challenges for organizations adopting SharePoint include planning, strategy, infrastructure, architecture design, UI Design, migration, and development. These tasks demand a flexible infrastructure as a prerequisite. Yet, often, organizations encounter outdated and improperly configured environments that hinder new implementations. Thus, the baseline architecture is critical for the success of all SharePoint projects.","title":"Project Overview &amp; Understanding"},{"location":"M365/SharePoint2016FarmUpgrade/#objectives-of-redesign","text":"Deploy the best fit infrastructure and system architecture. Utilize generic content types with optimum reusability. Ensure consistent use of document management. Leverage a collaboration platform that minimizes the learning curve through self-service. Standardize enterprise content management using built-in SharePoint tools. Implement a MOSS Governance policy to define Roles and Responsibilities, Policies, Processes, Deployment Strategies, and Site Structure. Design and deploy a High Availability solution without any single point of failure. Integrate plug-in and proprietary applications based on MOSS.","title":"Objectives of Redesign"},{"location":"M365/SharePoint2016FarmUpgrade/#current-system-study","text":"The main team working on this project engaged in thorough discussions with the The Client management team to comprehend the requirements for revamping and redesigning the current SharePoint setup. The Client facilitated this by providing VPN access with site-level permissions for us to review the production sites' replicas, minus any sensitive data. We has documented how to remove sensitive data and manage the backup/restore process for site assessments in the development environment, adhering to standard practices during the current system study.","title":"Current System Study"},{"location":"M365/SharePoint2016FarmUpgrade/#current-system-assessment","text":"","title":"Current System Assessment"},{"location":"M365/SharePoint2016FarmUpgrade/#governance","text":"Absence of a defined SharePoint resource governance matrix. Lack of clear business ownership for sites, with accountability resting solely on the technical team. Existence of an incomplete and outdated governance document. Undefined SharePoint administration process and absence of a dedicated administrator for maintenance and governance best practices implementation. Lack of application usage and customization policies.","title":"Governance"},{"location":"M365/SharePoint2016FarmUpgrade/#infrastructure-platform","text":"Non-implementation of SharePoint for High Availability and lack of business continuity plans. Presence of single points of failure and absence of a Disaster Recovery farm. Outdated server boxes unsuitable for future SharePoint upgrades and virtualization technologies. Non-configured sandbox environment. Presence of sensitive data in the development environment, which should only reside in the production environment. Minimal integration with other Enterprise Web Applications and Data.","title":"Infrastructure &amp; Platform"},{"location":"M365/SharePoint2016FarmUpgrade/#information-management","text":"Development of a flat structure taxonomy without hierarchy. Presence of unused or obsolete lists/libraries consuming storage space. Scattered contents in File Systems potentially holding critical contents without expiration policies. Absence of Information Management documentation describing existing SharePoint implementation. Lack of reference for lists/libraries containing sensitive information. Use of custom ASPX pages outside of the SharePoint publishing feature. Underutilization of content and document types. Nonexistence of content publishing processes and policies.","title":"Information Management"},{"location":"M365/SharePoint2016FarmUpgrade/#usability","text":"Unstructured navigation lacking distinct, easily recognizable groups and consistency. The UI of Home page is appealing, but site menus and contents are not easily identifiable against a glittery background. Absence of quick links across all sites for better navigation. Presence of broken links.","title":"Usability"},{"location":"M365/SharePoint2016FarmUpgrade/#sharepoint-farm-maintenance-deployment","text":"Creation of dedicated web applications for each Site Collection without clear rationale or isolation needs. Placement of all custom DLLs in GAC, contrary to Microsoft Best Practices recommending the \\BIN folder. Execution of backups without an archiving strategy and plan. Absence of true business process management or enterprise-level workflow automation solutions, possibly available offline.","title":"SharePoint Farm Maintenance &amp; Deployment"},{"location":"M365/SharePoint2016FarmUpgrade/#system-architecture-design-recommendations","text":"Microsoft Office SharePoint Server 2016 provides the flexibility to meet many different deployment solution goals. This includes guidance that would help the client in: - Determine the number of server in farm required to meet the solution goals. - Plan for the relationships between servers in the farm. - Plan for Extranet -facing server farm. - Design server-farm topologies to meet availability goals.","title":"System Architecture &amp; Design Recommendations"},{"location":"M365/SharePoint2016FarmUpgrade/#available-architectural-models","text":"The table below summarizes the different approaches to deploying and managing SharePoint 2016, ranging from fully cloud-based SaaS solutions to traditional on-premises deployments. Model Description SharePoint Online/SaaS You consume SharePoint through a Software as a Service (SaaS) with an Office 365 subscription. SharePoint is always up to date, but you are responsible for managing SharePoint itself. SharePoint Hybrid Combines SharePoint Online with a SharePoint Server 2016 farm, deployed either in Azure or on-premises. Incorporates SharePoint Online services into your overall SharePoint offering, starts building SaaS management skills in your organization, and moves your SharePoint Server 2016 sites and apps to the cloud at your own pace. SharePoint in Azure/IaaS Extends your on-premises environment into Azure Infrastructure as a Service (IaaS) for production, disaster recovery, or dev/test SharePoint Server 2016 farms. SharePoint on-premises Plans, deploys, maintains, and customizes your SharePoint Server 2016 farm in a datacenter that you maintain.","title":"Available architectural models"},{"location":"M365/SharePoint2016FarmUpgrade/#license-models","text":"Deployment Model Licensing Requirements SharePoint Online Assign licenses to Azure AD user accounts from your Office 365 subscription, no additional licenses needed SharePoint Hybrid - Office 365: Subscription model, no additional licenses needed - On-premises: Windows Server 2012 R2 or Windows Server 2016 - On-premises: SQL Server 2016 or SQL Server 2014 SP1 or later - On-premises: SharePoint Server 2016 License - On-premises: SharePoint Server 2016 Client Access License SharePoint Server 2016 in Azure (IaaS) - Azure subscription - SharePoint Server 2016 License - SharePoint Server 2016 Client Access License SharePoint Server 2016 On-premises - Windows Server 2016 or Windows Server 2012 R2 - SQL Server 2016 or SQL Server 2014 SP1 or later - SharePoint Server 2016 License - SharePoint Server 2016 Client Access License","title":"License models"},{"location":"M365/SharePoint2016FarmUpgrade/#topology-recommendation","text":"As the client wants to have an in-country farm due to data regulations. We recommend the following farm topology for a high-availability, on-premises SharePoint 2016 farm with six servers, using the latest features such as MinRole Front-End with Distributed Cache Servers: - Server 1 & 2 : These servers will handle all the user requests and serve the web pages. They will also host the Distributed Cache service, which is crucial for speeding up the retrieval of data and improving performance by caching frequently accessed information. Application with Search Servers: - Server 3 & 4 : These servers will run backend service applications and host the Search service. They handle the processing tasks that support the front-end servers, such as the Search service, which indexes content and processes search queries. Database Servers: - Server 5 & 6 : These will be the SQL Server databases configured in a high-availability cluster using SQL Server AlwaysOn Availability Groups. They store all the content and configurations for the SharePoint farm.","title":"Topology Recommendation"},{"location":"M365/SharePoint2016FarmUpgrade/#services-installed","text":"Front-End with Distributed Cache Servers (Server 1 & 2): - Access Services - Business Data Connectivity Service - Managed Metadata Web Service - User Profile Service - Distributed Cache - Microsoft SharePoint Foundation Web Application - And other front-end related services... Application with Search Servers (Server 3 & 4): - App Management Service - Business Data Connectivity Service - Machine Translation Service - Managed Metadata Web Service - Search Host Controller Service - Search Query and Site Settings Service - Secure Store Service - User Profile Service - And other application services... Database Servers (Server 5 & 6): - SQL Server with AlwaysOn Availability Groups configured for all SharePoint databases.","title":"Services Installed:"},{"location":"M365/SharePoint2016FarmUpgrade/#high-availability-explanation","text":"Front-End Servers: - Having two front-end servers ensures that if one goes down, the other can continue to serve user requests without interruption. Network Load Balancer (NLB) would be used to distribute the requests evenly between the two servers. Application with Search Servers: - Two servers with application and search services offer redundancy for these critical components of the SharePoint infrastructure. If one server fails, the other can take over the services without impacting the availability of the SharePoint farm. Database Servers: - SQL Server AlwaysOn Availability Groups provide high availability for the databases. In the event of a database server failure, the other node in the AlwaysOn group will take over, ensuring the SharePoint farm's data remains accessible. This setup provides both high availability and disaster recovery. The suggested SharePoint 2016 farm topology is designed to minimize single points of failure, ensuring that user access is uninterrupted, search functionality remains operational, and data is consistently available even in the event of server outages. The use of MinRole ensures that each server is optimized for its role, improving performance and reliability. The distribution of roles across multiple servers, along with the redundancy built into each layer (front-end, application, and database), achieves a highly available environment that aligns with SharePoint 2016's infrastructure advancements.","title":"High Availability Explanation:"},{"location":"M365/SharePoint2016FarmUpgrade/#disaster-recovery-recommendations","text":"Implement a \"stretched farm\" where there will be two data centres configured as a single farm. This is possible if two data centres are in close proximity, connected with higg bandwidth fiber optiks link. The bandwidth requiremnt, recommended by Microsoft for such topology is: Intra-farm latency of <1ms (one way), 99.9% of the time over a period of ten minutes. The bandwidth speed must be at least 1 gigabit per second. The recommended topology from Microsoft would look something like as shown below:","title":"Disaster recovery Recommendations"},{"location":"M365/SharePointEvents/","text":"SharePoint 2007 Event Receiver Project In this article I will show you how to Develop an event receiver in C# to handle item added, updated, or deleted events in a specific SharePoint document library. This article can be used from SharePoint 2007 till SharePoint 2013 without much changes. Event Receivers Using Event Receiver we can write custom code(e.g. send a mail) whenever something is done to a list or library item. Note: Nowadays SharePoint is moving towards Webhooks as it is lightweight and more efficient. Event Receivers Types Synchronous event receivers: These receivers run before or after the event. E.g. Before SharePoint saves an updated item(Before Item Updated) Asynchronous event receivers: This work after an event has happened. E.g. Sene a mail after an event is added. Let's get started Step 1: Create SharePoint Document Library Open your SharePoint site using a web browser. Navigate to the desired site where you want to create the document library. Click on \"Site Actions\" -> \"View All Site Content\" -> \"Create\". Select \"Document Library\" from the list of available templates. Provide a name for the document library (e.g., \"Custom Documents\"). Click \"Create\" to create the document library. Step 2: Create Visual Studio Project Open Visual Studio 2008 or later. Create a new SharePoint project: File -> New -> Project. Select \"SharePoint\" from the installed templates. Choose \"Empty SharePoint Project\" and provide a name (e.g., \"SharePointEventReceiverProject\"). Click \"OK\" to create the project. Step 3: Add Event Receiver Class Right-click on the project in Solution Explorer. Select \"Add\" -> \"New Item\". Choose \"Event Receiver\" and provide a name for the class (e.g., \"CustomDocumentLibraryEventReceiver\"). Click \"Add\" to add the event receiver class to the project. Step 4: Implement Event Receiver Logic //Das, 2010 using Microsoft.SharePoint; namespace SharePointEventReceiverProject { public class CustomDocumentLibraryEventReceiver : SPItemEventReceiver { public override void ItemAdded(SPItemEventProperties properties) { base.ItemAdded(properties); // Write your custom code here // Example: Send email notification or update metadata } public override void ItemUpdated(SPItemEventProperties properties) { base.ItemUpdated(properties); // Write your custom code here // Example: Log changes or trigger workflows } public override void ItemDeleted(SPItemEventProperties properties) { base.ItemDeleted(properties); // Perform actions when an item is deleted from the document library // Example: Archive deleted items or update related records } } } Step 5: Deploy using Feature Right-click on the project in Solution Explorer. Select \"Add\" -> \"New Item\". Choose \"Feature\" and provide a name for the feature (e.g., \"CustomEventReceiverFeature\"). Open the feature XML file (Feature.xml) and add an EventReceiver element to specify the event receiver class. Set the ReceiverAssembly and ReceiverClass attributes to reference the event receiver assembly and class. Build the SharePoint project to generate the event receiver assembly (.dll). Deploy the event receiver assembly to the SharePoint server using the feature: Activate the feature at the site collection or site level where the document library is located. Verify that the event receiver is attached to the document library.","title":"SharePoint Event Receiver"},{"location":"M365/SharePointEvents/#sharepoint-2007-event-receiver-project","text":"In this article I will show you how to Develop an event receiver in C# to handle item added, updated, or deleted events in a specific SharePoint document library. This article can be used from SharePoint 2007 till SharePoint 2013 without much changes.","title":"SharePoint 2007 Event Receiver Project"},{"location":"M365/SharePointEvents/#event-receivers","text":"Using Event Receiver we can write custom code(e.g. send a mail) whenever something is done to a list or library item. Note: Nowadays SharePoint is moving towards Webhooks as it is lightweight and more efficient.","title":"Event Receivers"},{"location":"M365/SharePointEvents/#event-receivers-types","text":"Synchronous event receivers: These receivers run before or after the event. E.g. Before SharePoint saves an updated item(Before Item Updated) Asynchronous event receivers: This work after an event has happened. E.g. Sene a mail after an event is added.","title":"Event Receivers Types"},{"location":"M365/SharePointEvents/#lets-get-started","text":"","title":"Let's get started"},{"location":"M365/SharePointEvents/#step-1-create-sharepoint-document-library","text":"Open your SharePoint site using a web browser. Navigate to the desired site where you want to create the document library. Click on \"Site Actions\" -> \"View All Site Content\" -> \"Create\". Select \"Document Library\" from the list of available templates. Provide a name for the document library (e.g., \"Custom Documents\"). Click \"Create\" to create the document library.","title":"Step 1: Create SharePoint Document Library"},{"location":"M365/SharePointEvents/#step-2-create-visual-studio-project","text":"Open Visual Studio 2008 or later. Create a new SharePoint project: File -> New -> Project. Select \"SharePoint\" from the installed templates. Choose \"Empty SharePoint Project\" and provide a name (e.g., \"SharePointEventReceiverProject\"). Click \"OK\" to create the project.","title":"Step 2: Create Visual Studio Project"},{"location":"M365/SharePointEvents/#step-3-add-event-receiver-class","text":"Right-click on the project in Solution Explorer. Select \"Add\" -> \"New Item\". Choose \"Event Receiver\" and provide a name for the class (e.g., \"CustomDocumentLibraryEventReceiver\"). Click \"Add\" to add the event receiver class to the project.","title":"Step 3: Add Event Receiver Class"},{"location":"M365/SharePointEvents/#step-4-implement-event-receiver-logic","text":"//Das, 2010 using Microsoft.SharePoint; namespace SharePointEventReceiverProject { public class CustomDocumentLibraryEventReceiver : SPItemEventReceiver { public override void ItemAdded(SPItemEventProperties properties) { base.ItemAdded(properties); // Write your custom code here // Example: Send email notification or update metadata } public override void ItemUpdated(SPItemEventProperties properties) { base.ItemUpdated(properties); // Write your custom code here // Example: Log changes or trigger workflows } public override void ItemDeleted(SPItemEventProperties properties) { base.ItemDeleted(properties); // Perform actions when an item is deleted from the document library // Example: Archive deleted items or update related records } } }","title":"Step 4: Implement Event Receiver Logic"},{"location":"M365/SharePointEvents/#step-5-deploy-using-feature","text":"Right-click on the project in Solution Explorer. Select \"Add\" -> \"New Item\". Choose \"Feature\" and provide a name for the feature (e.g., \"CustomEventReceiverFeature\"). Open the feature XML file (Feature.xml) and add an EventReceiver element to specify the event receiver class. Set the ReceiverAssembly and ReceiverClass attributes to reference the event receiver assembly and class. Build the SharePoint project to generate the event receiver assembly (.dll). Deploy the event receiver assembly to the SharePoint server using the feature: Activate the feature at the site collection or site level where the document library is located. Verify that the event receiver is attached to the document library.","title":"Step 5: Deploy using Feature"},{"location":"M365/SharePointFarmConsolidation/","text":"Introduction Purpose Existing System Specification Current System study Farm Scenario Current System Assessment General Farm Requirements High Availability (HA) and Disaster Recovery (DR) Requirements Assumptions \\& Dependencies Assumptions Scope High-level infrastructure layout Introduction Here, I am sharing about a project which I was part of acting as a consultant for architecture. Purpose The client wants to setup a new consolidated SharePoint farm in its European Data Center (EDC) for its intranet and extranet SharePoint applications in an efficient and cost effective manner. This new farm should be implemented in such a way so that it provides high availability (HA), Scalability and Disaster Recovery (DR). Existing System Specification Current System study Our team had detailed discussions with the client's management team to understand the needs to consolidate the existing SharePoint environments to new SharePoint environments in a more (cost) efficient and more stable manner. We followed the standard process of studying current system. The following section would highlight the key findings of the system study: Farm Scenario Particular Ceva portal Ceva NET Extranet Farm Topology One SharePoint 2007 farm in Florida consisting of 2 servers (WFE and 2 App roles combined), using a SQL Server cluster One SharePoint 2007 farm in Texasconsisting of 2 WFE and 2 App servers, using SQL Server 2005 cluster. One WSS 3.0 server; located in Texas containing few 100 sites. Sites 200 team sites, 15 apps (from Notes). 1000 sites, multilingual, custom apps (30% custom, 70% standard). Few hundred sites. Storage 50 gb 100 gb unspecified Users 500 \u2013 2000 24000 unspecified Average Requests 400 45000 unspecified NLB No Hardware NLB used. Uses Windows (software) NLB. No Hardware NLB used. Uses Windows (software) NLB. No Hardware NLB used. Uses Windows (software) NLB. Business Data Catalogue(BDC) Not configured Not configured Not configured. Current System Assessment Our assessment findings based on current client's SharePoint implementation are: Current SharePoint installation is not implemented for High Availability. No business continuity plans defined. Single Point of failure. No Disaster Recovery farm exists. Absence of redundancy for some of the server roles. Absence of scalability for some of the server roles. General Farm Requirements To design, build, test and deploy the new SharePoint 2007 farm in the EDC meant for intranet/extranet MOSS 2007 applications. To Perform English-only out of the box installation. To configure search, user profile sync etc. To incorporate scalability, High availability and Disaster recovery. To implement a supported full backup/restore solution, preferably with NetBackup (VERITAS/Symantec). To provide a migration plan. To help with test migrations. High Availability (HA) and Disaster Recovery (DR) Requirements The following parameters are to be met in for High availability and Disaster Recovery: Parameter CEVANet Gold --- --- Monitoring 24x7 on desk Availability 99.35% \u201cAllowed\u201d downtime per month Approximately 4.5 hours Recovery time (in the event of a complete failure this is the RTO) 8 hours Recovery Point Objective (RPO) 120 minutes Successful failover testing Once every 12 months Successful Disaster Recovery Testing Once every 12 months Successful Testing of a Restore from a Backup Once every 12 months Disaster Recovery Capacity 80% Assumptions & Dependencies Assumptions Development and (user) acceptance environments are already available. SharePoint 2007 SP2 with SQL Server 2008 R2 setup is already available and is clustered and mirrored across two data centers. Hardware ordering/installation and OS installation will be done by Ceva and are available within 4 weeks after ordering. SQL Server storage and resources are available and high available with disaster recovery. The reverse proxy will support SharePoint (possible impact on authentication and/or requires additional configuration) Scope Below activities will be performed to accomplish the task. Discovery Analyze the existing SharePoint 2007 farms to find the application size, no. of site collections, database size, and user base. Design Prepare Hardware BOM. Prepare Software BOM. Prepare Technical Architecture Specifications. Prepare Execution Environment Design. Prepare DR Plan. Prepare Backup/Restore Plan. Build MOSS Installation and Farm Setup. Backup/Restore Implementation. DR Implementation. Prepare the install and configuration guide. Prepare the Technology Policies and Procedures. Prepare the Scalability Matrix. Migration Support Provide any Farm related support during application migration. Test & Deploy Prepare the Test Plan to test the SharePoint Farm. Execute the Tests. Provide any Farm related support during the application Go Live. High-level infrastructure layout Below image shows the proposed high-level structure of the Farm","title":"SharePoint Farm Consolidation"},{"location":"M365/SharePointFarmConsolidation/#introduction","text":"Here, I am sharing about a project which I was part of acting as a consultant for architecture.","title":"Introduction"},{"location":"M365/SharePointFarmConsolidation/#purpose","text":"The client wants to setup a new consolidated SharePoint farm in its European Data Center (EDC) for its intranet and extranet SharePoint applications in an efficient and cost effective manner. This new farm should be implemented in such a way so that it provides high availability (HA), Scalability and Disaster Recovery (DR).","title":"Purpose"},{"location":"M365/SharePointFarmConsolidation/#existing-system-specification","text":"","title":"Existing System Specification"},{"location":"M365/SharePointFarmConsolidation/#current-system-study","text":"Our team had detailed discussions with the client's management team to understand the needs to consolidate the existing SharePoint environments to new SharePoint environments in a more (cost) efficient and more stable manner. We followed the standard process of studying current system. The following section would highlight the key findings of the system study:","title":"Current System study"},{"location":"M365/SharePointFarmConsolidation/#farm-scenario","text":"Particular Ceva portal Ceva NET Extranet Farm Topology One SharePoint 2007 farm in Florida consisting of 2 servers (WFE and 2 App roles combined), using a SQL Server cluster One SharePoint 2007 farm in Texasconsisting of 2 WFE and 2 App servers, using SQL Server 2005 cluster. One WSS 3.0 server; located in Texas containing few 100 sites. Sites 200 team sites, 15 apps (from Notes). 1000 sites, multilingual, custom apps (30% custom, 70% standard). Few hundred sites. Storage 50 gb 100 gb unspecified Users 500 \u2013 2000 24000 unspecified Average Requests 400 45000 unspecified NLB No Hardware NLB used. Uses Windows (software) NLB. No Hardware NLB used. Uses Windows (software) NLB. No Hardware NLB used. Uses Windows (software) NLB. Business Data Catalogue(BDC) Not configured Not configured Not configured.","title":"Farm Scenario"},{"location":"M365/SharePointFarmConsolidation/#current-system-assessment","text":"Our assessment findings based on current client's SharePoint implementation are: Current SharePoint installation is not implemented for High Availability. No business continuity plans defined. Single Point of failure. No Disaster Recovery farm exists. Absence of redundancy for some of the server roles. Absence of scalability for some of the server roles.","title":"Current System Assessment"},{"location":"M365/SharePointFarmConsolidation/#general-farm-requirements","text":"To design, build, test and deploy the new SharePoint 2007 farm in the EDC meant for intranet/extranet MOSS 2007 applications. To Perform English-only out of the box installation. To configure search, user profile sync etc. To incorporate scalability, High availability and Disaster recovery. To implement a supported full backup/restore solution, preferably with NetBackup (VERITAS/Symantec). To provide a migration plan. To help with test migrations.","title":"General Farm Requirements"},{"location":"M365/SharePointFarmConsolidation/#high-availability-ha-and-disaster-recovery-dr-requirements","text":"The following parameters are to be met in for High availability and Disaster Recovery: Parameter CEVANet Gold --- --- Monitoring 24x7 on desk Availability 99.35% \u201cAllowed\u201d downtime per month Approximately 4.5 hours Recovery time (in the event of a complete failure this is the RTO) 8 hours Recovery Point Objective (RPO) 120 minutes Successful failover testing Once every 12 months Successful Disaster Recovery Testing Once every 12 months Successful Testing of a Restore from a Backup Once every 12 months Disaster Recovery Capacity 80%","title":"High Availability (HA) and Disaster Recovery (DR) Requirements"},{"location":"M365/SharePointFarmConsolidation/#assumptions-dependencies","text":"","title":"Assumptions &amp; Dependencies"},{"location":"M365/SharePointFarmConsolidation/#assumptions","text":"Development and (user) acceptance environments are already available. SharePoint 2007 SP2 with SQL Server 2008 R2 setup is already available and is clustered and mirrored across two data centers. Hardware ordering/installation and OS installation will be done by Ceva and are available within 4 weeks after ordering. SQL Server storage and resources are available and high available with disaster recovery. The reverse proxy will support SharePoint (possible impact on authentication and/or requires additional configuration)","title":"Assumptions"},{"location":"M365/SharePointFarmConsolidation/#scope","text":"Below activities will be performed to accomplish the task. Discovery Analyze the existing SharePoint 2007 farms to find the application size, no. of site collections, database size, and user base. Design Prepare Hardware BOM. Prepare Software BOM. Prepare Technical Architecture Specifications. Prepare Execution Environment Design. Prepare DR Plan. Prepare Backup/Restore Plan. Build MOSS Installation and Farm Setup. Backup/Restore Implementation. DR Implementation. Prepare the install and configuration guide. Prepare the Technology Policies and Procedures. Prepare the Scalability Matrix. Migration Support Provide any Farm related support during application migration. Test & Deploy Prepare the Test Plan to test the SharePoint Farm. Execute the Tests. Provide any Farm related support during the application Go Live.","title":"Scope"},{"location":"M365/SharePointFarmConsolidation/#high-level-infrastructure-layout","text":"Below image shows the proposed high-level structure of the Farm","title":"High-level infrastructure layout"},{"location":"M365/SharePointFormsOrPowerApps/","text":"Choosing Power Apps Forms for SharePoint: When and Why? The SharePoint Context Enter Power Apps When to Use Power Apps for SharePoint Forms Considerations Before Transitioning Conclusion Choosing Power Apps Forms for SharePoint: When and Why? In the dynamic world of digital transformation, SharePoint has long been a staple for organizations seeking efficient content management and collaboration. However, with the advent of Power Apps, Microsoft introduced a powerful tool that further expands SharePoint's capabilities, especially in custom form development. Understanding when to use Power Apps-based forms for SharePoint can significantly impact how your organization manages data, automates processes, and enhances user interaction. The SharePoint Context SharePoint lists and libraries are fundamental components that store and organize data, ranging from simple contact lists to complex project management trackers. The default forms provided by SharePoint for adding, viewing, or editing items in these lists are straightforward but often lack the flexibility for customization or business logic implementation needed by many organizations. Enter Power Apps Power Apps is a part of Microsoft's Power Platform, designed to build custom apps and forms without the need for deep programming knowledge. When integrated with SharePoint, Power Apps elevates the customization level of forms, enabling tailored user experiences, complex data validations, and dynamic content presentation. When to Use Power Apps for SharePoint Forms 1. Enhanced Customization Requirements: If your organization needs more than simple field additions\u2014such as conditional visibility, customized layouts, or integration with external data sources\u2014Power Apps is your go-to solution. 2. Improved User Experience: For scenarios where user interaction with SharePoint data requires a more intuitive and visually appealing interface, Power Apps forms can provide a significantly enhanced user experience. 3. Complex Business Logic: When you need to incorporate complex business logic into your forms\u2014like dynamic dropdowns based on previous selections or data validation that goes beyond the basic\u2014Power Apps allows for these sophisticated scenarios. 4. Mobile Accessibility: If accessing SharePoint forms on mobile devices is a priority, Power Apps offers a responsive design that can adjust to various screen sizes, making it an ideal choice for a mobile-friendly interface. 5. Integration with Other Services: For forms that require pulling in data from or pushing data to other services and applications within the Microsoft ecosystem or external APIs, Power Apps provides a robust set of connectors and the ability to create custom connections. Considerations Before Transitioning While Power Apps offers impressive capabilities, it's essential to consider the learning curve, licensing requirements, and the potential need for ongoing maintenance and updates. Ensure your team has the skills\u2014or the willingness to learn\u2014before fully committing to Power Apps for your SharePoint forms. Conclusion The decision to use Power Apps-based forms for SharePoint hinges on the need for customization, the desire for a better user experience, and the requirement to integrate complex business logic. By leveraging Power Apps, organizations can transform their SharePoint sites into more dynamic, interactive, and efficient digital workspaces. Always weigh the benefits against the complexity and resources required to maintain these solutions to ensure they align with your organization's goals and capabilities. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"SharePoint Forms or PowerApps"},{"location":"M365/SharePointFormsOrPowerApps/#choosing-power-apps-forms-for-sharepoint-when-and-why","text":"In the dynamic world of digital transformation, SharePoint has long been a staple for organizations seeking efficient content management and collaboration. However, with the advent of Power Apps, Microsoft introduced a powerful tool that further expands SharePoint's capabilities, especially in custom form development. Understanding when to use Power Apps-based forms for SharePoint can significantly impact how your organization manages data, automates processes, and enhances user interaction.","title":"Choosing Power Apps Forms for SharePoint: When and Why?"},{"location":"M365/SharePointFormsOrPowerApps/#the-sharepoint-context","text":"SharePoint lists and libraries are fundamental components that store and organize data, ranging from simple contact lists to complex project management trackers. The default forms provided by SharePoint for adding, viewing, or editing items in these lists are straightforward but often lack the flexibility for customization or business logic implementation needed by many organizations.","title":"The SharePoint Context"},{"location":"M365/SharePointFormsOrPowerApps/#enter-power-apps","text":"Power Apps is a part of Microsoft's Power Platform, designed to build custom apps and forms without the need for deep programming knowledge. When integrated with SharePoint, Power Apps elevates the customization level of forms, enabling tailored user experiences, complex data validations, and dynamic content presentation.","title":"Enter Power Apps"},{"location":"M365/SharePointFormsOrPowerApps/#when-to-use-power-apps-for-sharepoint-forms","text":"1. Enhanced Customization Requirements: If your organization needs more than simple field additions\u2014such as conditional visibility, customized layouts, or integration with external data sources\u2014Power Apps is your go-to solution. 2. Improved User Experience: For scenarios where user interaction with SharePoint data requires a more intuitive and visually appealing interface, Power Apps forms can provide a significantly enhanced user experience. 3. Complex Business Logic: When you need to incorporate complex business logic into your forms\u2014like dynamic dropdowns based on previous selections or data validation that goes beyond the basic\u2014Power Apps allows for these sophisticated scenarios. 4. Mobile Accessibility: If accessing SharePoint forms on mobile devices is a priority, Power Apps offers a responsive design that can adjust to various screen sizes, making it an ideal choice for a mobile-friendly interface. 5. Integration with Other Services: For forms that require pulling in data from or pushing data to other services and applications within the Microsoft ecosystem or external APIs, Power Apps provides a robust set of connectors and the ability to create custom connections.","title":"When to Use Power Apps for SharePoint Forms"},{"location":"M365/SharePointFormsOrPowerApps/#considerations-before-transitioning","text":"While Power Apps offers impressive capabilities, it's essential to consider the learning curve, licensing requirements, and the potential need for ongoing maintenance and updates. Ensure your team has the skills\u2014or the willingness to learn\u2014before fully committing to Power Apps for your SharePoint forms.","title":"Considerations Before Transitioning"},{"location":"M365/SharePointFormsOrPowerApps/#conclusion","text":"The decision to use Power Apps-based forms for SharePoint hinges on the need for customization, the desire for a better user experience, and the requirement to integrate complex business logic. By leveraging Power Apps, organizations can transform their SharePoint sites into more dynamic, interactive, and efficient digital workspaces. Always weigh the benefits against the complexity and resources required to maintain these solutions to ensure they align with your organization's goals and capabilities. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Conclusion"},{"location":"M365/SharePointMiniRole/","text":"SharePoint 2016 MinRole Before 2016: SharePoint 2016/2019: Dedicated MinRole: Special MiniRole: Shared MiniRoles: SharePoint 2016 MinRole Before 2016: Say, you had two servers. You could install any service on either of them, in any combination . E,g,: Server 1 : Web Front-End + Search + Central Administration Server 2 : Indexing + Application Services This freedom, and lack of guidance, gave a lot of flexibility. But, you had to be careful about performance issues. SharePoint 2016/2019: MinRole is just a best combination of services. Now adminstrator know beforehand, what mix is best. It's not just advice, its shown as a radio button during installation. Also, like before, you can still install 'anything anywhere' by clicking 'Custom' MiniRole optioin. This is how the new installer shows them: Dedicated MinRole: Front-End Application Distributed Cache Search From Microsoft Site Special MiniRole: Single-Farm : All-in-one. Before called, Standalone Install mode. Used for Dev. Custom : Anything-anywhere. The-way-you-like-it From Microsoft Site Shared MiniRoles: Front-End + Distributed Cache Application + Search From Microsoft Site Long story short: MinRole is just 'best mix of SharePoint services' as per Microsoft.","title":"SharePoint Mini Role"},{"location":"M365/SharePointMiniRole/#sharepoint-2016-minrole","text":"","title":"SharePoint 2016 MinRole"},{"location":"M365/SharePointMiniRole/#before-2016","text":"Say, you had two servers. You could install any service on either of them, in any combination . E,g,: Server 1 : Web Front-End + Search + Central Administration Server 2 : Indexing + Application Services This freedom, and lack of guidance, gave a lot of flexibility. But, you had to be careful about performance issues.","title":"Before 2016:"},{"location":"M365/SharePointMiniRole/#sharepoint-20162019","text":"MinRole is just a best combination of services. Now adminstrator know beforehand, what mix is best. It's not just advice, its shown as a radio button during installation. Also, like before, you can still install 'anything anywhere' by clicking 'Custom' MiniRole optioin. This is how the new installer shows them:","title":"SharePoint 2016/2019:"},{"location":"M365/SharePointMiniRole/#dedicated-minrole","text":"Front-End Application Distributed Cache Search From Microsoft Site","title":"Dedicated MinRole:"},{"location":"M365/SharePointMiniRole/#special-minirole","text":"Single-Farm : All-in-one. Before called, Standalone Install mode. Used for Dev. Custom : Anything-anywhere. The-way-you-like-it From Microsoft Site","title":"Special MiniRole:"},{"location":"M365/SharePointMiniRole/#shared-miniroles","text":"Front-End + Distributed Cache Application + Search From Microsoft Site Long story short: MinRole is just 'best mix of SharePoint services' as per Microsoft.","title":"Shared MiniRoles:"},{"location":"M365/SharePointVersionEvolution/","text":"SharePoint Version Evolution SharePoint has evolved significantly since its inception, transforming from a simple document management tool to a comprehensive platform for collaboration, content management, and much more. Let's dive straight into the technical journey of SharePoint's evolution, highlighting key versions and features. SharePoint Portal Server 2001 Introduction: Launched as a document management and team collaboration tool. Key Features: Document libraries, web portals for information sharing. SharePoint Portal Server 2003 Enhancements: Improved integration with Microsoft Office, introduction of site collections, and better content management capabilities. Key Features: Personal sites, audience targeting. Microsoft Office SharePoint Server 2007 (MOSS 2007) Major Overhaul: Introduced the modern SharePoint architecture, including features like workflows, web parts, and enterprise content management. Key Features: Enhanced search, content types, and integration with Windows Workflow Foundation. SharePoint 2010 Focus on User Experience: Introduced the Ribbon interface, enhancing usability. Added features like sandboxed solutions and service applications. Key Features: SharePoint Designer enhancements, Visual Studio integration, and the introduction of Business Connectivity Services. SharePoint 2013 Social and Search: Emphasized social features and improved search functionality. Introduced the app model for custom development. Key Features: Community site, improved content search, and cross-site publishing. SharePoint 2016 Hybrid Cloud Integration: Focused on hybrid capabilities, allowing seamless integration between SharePoint Online (Office 365) and on-premises SharePoint. Key Features: MinRole architecture for optimized server roles, durable links, and improved file storage. SharePoint 2019 Modern Experience: Continued the shift towards a more modern user experience introduced in SharePoint Online, with modern sites, lists, and libraries. Key Features: Modern search experience, SharePoint Home page, and mobile-friendly design. SharePoint Online (Part of Office 365/Microsoft 365) Cloud-First: Continuously updated, providing the latest features and integrations with the Microsoft ecosystem. Key Features: Modern team sites, communication sites, integration with Power Platform (PowerApps, Power Automate), and continuous updates based on user feedback and technological advancements. Now, lets put what we learnt so far in tabular format Feature SharePoint 2007 SharePoint 2010 SharePoint 2013 SharePoint 2016 SharePoint 2019 User Interface Classic UI Ribbon UI Ribbon UI, Metro UI Ribbon UI, Metro UI Modern UI, Communication Sites Social Features Limited social features My Sites, Activity Feeds Community Sites, Microblogging Community Sites, Microblogging Yammer Integration, News Web Parts Mobile Compatibility Limited mobile support Improved mobile support Responsive design, Touch-friendly UI Responsive design, Touch-friendly UI Responsive design, Mobile App Workflow Capabilities Basic workflows with SharePoint Designer Improved workflows with SharePoint Designer Improved workflows with SharePoint Designer Improved workflows with SharePoint Designer Improved workflows with SharePoint Designer Search Functionality Basic search functionality Enterprise search capabilities Improved search relevance, refinement Hybrid search, Improved relevance Hybrid search, Intelligent search Cloud Integration Limited integration with Office 365 Improved integration with Office 365 Hybrid deployment options Hybrid deployment options Hybrid deployment options Business Intelligence Basic BI features PerformancePoint Services, Excel Services Power View, PowerPivot, Power Map Power View, PowerPivot, Power Map Power BI Integration App Model N/A Sandboxed Solutions, SharePoint App Store SharePoint App Model, App Catalog SharePoint App Model, App Catalog SharePoint App Model, App Catalog Site Templates Limited site templates Enhanced site templates Improved site templates Improved site templates Improved site templates Compliance and Records Management Basic compliance features Records Center, Document Sets Improved compliance features Improved compliance features Improved compliance features Hybrid Deployment N/A N/A Hybrid deployment options Hybrid deployment options Hybrid deployment options Customization Options Features, Web Parts, Custom Code Features, Web Parts, Custom Code Features, Apps, Client-Side Solutions Features, Apps, Client-Side Solutions Features, Apps, Client-Side Solutions Development Models Server-Side Solutions (C#, ASP.NET) Server-Side Solutions (C#, ASP.NET) Server-Side and Client-Side Solutions Server-Side and Client-Side Solutions Server-Side and Client-Side Solutions Windows Server Versions Windows Server 2003, Windows Server 2008 Windows Server 2008 R2, Windows Server 2012 Windows Server 2012, Windows Server 2012 R2 Windows Server 2012 R2, Windows Server 2016 Windows Server 2016, Windows Server 2019 SQL Server Versions SQL Server 2005, SQL Server 2008 SQL Server 2008 R2, SQL Server 2012 SQL Server 2012, SQL Server 2014 SQL Server 2014, SQL Server 2016 SQL Server 2016, SQL Server 2017 SharePoint Designer Versions SharePoint Designer 2007 SharePoint Designer 2010 SharePoint Designer 2013 SharePoint Designer 2013 SharePoint Designer 2013 \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"SharePoint Evolution"},{"location":"M365/SharePointVersionEvolution/#sharepoint-version-evolution","text":"SharePoint has evolved significantly since its inception, transforming from a simple document management tool to a comprehensive platform for collaboration, content management, and much more. Let's dive straight into the technical journey of SharePoint's evolution, highlighting key versions and features.","title":"SharePoint Version Evolution"},{"location":"M365/SharePointVersionEvolution/#sharepoint-portal-server-2001","text":"Introduction: Launched as a document management and team collaboration tool. Key Features: Document libraries, web portals for information sharing.","title":"SharePoint Portal Server 2001"},{"location":"M365/SharePointVersionEvolution/#sharepoint-portal-server-2003","text":"Enhancements: Improved integration with Microsoft Office, introduction of site collections, and better content management capabilities. Key Features: Personal sites, audience targeting.","title":"SharePoint Portal Server 2003"},{"location":"M365/SharePointVersionEvolution/#microsoft-office-sharepoint-server-2007-moss-2007","text":"Major Overhaul: Introduced the modern SharePoint architecture, including features like workflows, web parts, and enterprise content management. Key Features: Enhanced search, content types, and integration with Windows Workflow Foundation.","title":"Microsoft Office SharePoint Server 2007 (MOSS 2007)"},{"location":"M365/SharePointVersionEvolution/#sharepoint-2010","text":"Focus on User Experience: Introduced the Ribbon interface, enhancing usability. Added features like sandboxed solutions and service applications. Key Features: SharePoint Designer enhancements, Visual Studio integration, and the introduction of Business Connectivity Services.","title":"SharePoint 2010"},{"location":"M365/SharePointVersionEvolution/#sharepoint-2013","text":"Social and Search: Emphasized social features and improved search functionality. Introduced the app model for custom development. Key Features: Community site, improved content search, and cross-site publishing.","title":"SharePoint 2013"},{"location":"M365/SharePointVersionEvolution/#sharepoint-2016","text":"Hybrid Cloud Integration: Focused on hybrid capabilities, allowing seamless integration between SharePoint Online (Office 365) and on-premises SharePoint. Key Features: MinRole architecture for optimized server roles, durable links, and improved file storage.","title":"SharePoint 2016"},{"location":"M365/SharePointVersionEvolution/#sharepoint-2019","text":"Modern Experience: Continued the shift towards a more modern user experience introduced in SharePoint Online, with modern sites, lists, and libraries. Key Features: Modern search experience, SharePoint Home page, and mobile-friendly design.","title":"SharePoint 2019"},{"location":"M365/SharePointVersionEvolution/#sharepoint-online-part-of-office-365microsoft-365","text":"Cloud-First: Continuously updated, providing the latest features and integrations with the Microsoft ecosystem. Key Features: Modern team sites, communication sites, integration with Power Platform (PowerApps, Power Automate), and continuous updates based on user feedback and technological advancements. Now, lets put what we learnt so far in tabular format Feature SharePoint 2007 SharePoint 2010 SharePoint 2013 SharePoint 2016 SharePoint 2019 User Interface Classic UI Ribbon UI Ribbon UI, Metro UI Ribbon UI, Metro UI Modern UI, Communication Sites Social Features Limited social features My Sites, Activity Feeds Community Sites, Microblogging Community Sites, Microblogging Yammer Integration, News Web Parts Mobile Compatibility Limited mobile support Improved mobile support Responsive design, Touch-friendly UI Responsive design, Touch-friendly UI Responsive design, Mobile App Workflow Capabilities Basic workflows with SharePoint Designer Improved workflows with SharePoint Designer Improved workflows with SharePoint Designer Improved workflows with SharePoint Designer Improved workflows with SharePoint Designer Search Functionality Basic search functionality Enterprise search capabilities Improved search relevance, refinement Hybrid search, Improved relevance Hybrid search, Intelligent search Cloud Integration Limited integration with Office 365 Improved integration with Office 365 Hybrid deployment options Hybrid deployment options Hybrid deployment options Business Intelligence Basic BI features PerformancePoint Services, Excel Services Power View, PowerPivot, Power Map Power View, PowerPivot, Power Map Power BI Integration App Model N/A Sandboxed Solutions, SharePoint App Store SharePoint App Model, App Catalog SharePoint App Model, App Catalog SharePoint App Model, App Catalog Site Templates Limited site templates Enhanced site templates Improved site templates Improved site templates Improved site templates Compliance and Records Management Basic compliance features Records Center, Document Sets Improved compliance features Improved compliance features Improved compliance features Hybrid Deployment N/A N/A Hybrid deployment options Hybrid deployment options Hybrid deployment options Customization Options Features, Web Parts, Custom Code Features, Web Parts, Custom Code Features, Apps, Client-Side Solutions Features, Apps, Client-Side Solutions Features, Apps, Client-Side Solutions Development Models Server-Side Solutions (C#, ASP.NET) Server-Side Solutions (C#, ASP.NET) Server-Side and Client-Side Solutions Server-Side and Client-Side Solutions Server-Side and Client-Side Solutions Windows Server Versions Windows Server 2003, Windows Server 2008 Windows Server 2008 R2, Windows Server 2012 Windows Server 2012, Windows Server 2012 R2 Windows Server 2012 R2, Windows Server 2016 Windows Server 2016, Windows Server 2019 SQL Server Versions SQL Server 2005, SQL Server 2008 SQL Server 2008 R2, SQL Server 2012 SQL Server 2012, SQL Server 2014 SQL Server 2014, SQL Server 2016 SQL Server 2016, SQL Server 2017 SharePoint Designer Versions SharePoint Designer 2007 SharePoint Designer 2010 SharePoint Designer 2013 SharePoint Designer 2013 SharePoint Designer 2013 \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"SharePoint Online (Part of Office 365/Microsoft 365)"},{"location":"M365/SharePointVsOtherECM/","text":"SharePoint vs OpenText Documentum, and IBM FileNet for ECM When selecting an Enterprise Content Management (ECM) platform, SharePoint, OpenText Documentum, and IBM FileNet emerge as notable choices, each with unique strengths. Integration & Scalability : SharePoint excels in the Microsoft ecosystem, offering seamless integration with Office 365. Documentum and FileNet provide strong integration with enterprise systems but might need more customization. Customization & Flexibility : All three platforms allow significant customization. SharePoint is user-friendly with numerous templates; Documentum and FileNet offer deep customization but require more technical expertise. Security & Compliance : SharePoint utilizes Microsoft's security framework, making it secure for sensitive data. Documentum and FileNet are preferred in regulated industries for their robust security and compliance features. Collaboration : SharePoint shines in collaboration, integrating well with Microsoft Teams. Documentum and FileNet may need third-party tools for similar functionality. Cost & ROI : SharePoint could offer better ROI for Microsoft users. Documentum and FileNet may have higher initial costs but excel in managing complex workflows in regulated fields. User Experience : SharePoint is aiming for user-friendliness, while Documentum and FileNet, though powerful, might demand more training. Use Cases : SharePoint : Suitable for integrated ECM with strong collaboration, especially in Microsoft-centric environments. Documentum : Ideal for regulated industries needing stringent content management. FileNet : Fits businesses focusing on advanced workflow and content lifecycle management, like in finance and insurance. Conclusion : The choice between SharePoint, Documentum, and FileNet hinges on your organization's specific needs, compliance requirements, and tech ecosystem. SharePoint is versatile and collaboration-focused, while Documentum and FileNet excel in complex management needs within regulated sectors. Consider your organizational goals and budget to select the ECM that best supports your strategic objectives. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"SharePoint vs Other ECMS"},{"location":"M365/SharePointVsOtherECM/#sharepoint-vs-opentext-documentum-and-ibm-filenet-for-ecm","text":"When selecting an Enterprise Content Management (ECM) platform, SharePoint, OpenText Documentum, and IBM FileNet emerge as notable choices, each with unique strengths. Integration & Scalability : SharePoint excels in the Microsoft ecosystem, offering seamless integration with Office 365. Documentum and FileNet provide strong integration with enterprise systems but might need more customization. Customization & Flexibility : All three platforms allow significant customization. SharePoint is user-friendly with numerous templates; Documentum and FileNet offer deep customization but require more technical expertise. Security & Compliance : SharePoint utilizes Microsoft's security framework, making it secure for sensitive data. Documentum and FileNet are preferred in regulated industries for their robust security and compliance features. Collaboration : SharePoint shines in collaboration, integrating well with Microsoft Teams. Documentum and FileNet may need third-party tools for similar functionality. Cost & ROI : SharePoint could offer better ROI for Microsoft users. Documentum and FileNet may have higher initial costs but excel in managing complex workflows in regulated fields. User Experience : SharePoint is aiming for user-friendliness, while Documentum and FileNet, though powerful, might demand more training. Use Cases : SharePoint : Suitable for integrated ECM with strong collaboration, especially in Microsoft-centric environments. Documentum : Ideal for regulated industries needing stringent content management. FileNet : Fits businesses focusing on advanced workflow and content lifecycle management, like in finance and insurance. Conclusion : The choice between SharePoint, Documentum, and FileNet hinges on your organization's specific needs, compliance requirements, and tech ecosystem. SharePoint is versatile and collaboration-focused, while Documentum and FileNet excel in complex management needs within regulated sectors. Consider your organizational goals and budget to select the ECM that best supports your strategic objectives. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"SharePoint vs OpenText Documentum, and IBM FileNet for ECM"},{"location":"M365/WSS3DocumentUpload/","text":"Document Migration to SharePoint WSS 3.0 Using C#.NET Our client had a large number of documents saved on their network drives and wanted to move them to SharePoint WSS 3.0. They were excited about using the new features of WSS 3.0 to make document management easier and more efficient. The Challenge However, the migration task came with several technical challenges. The large volume of documents made the transfer process time-consuming and complex. Important metadata like creation and modification dates, along with author information, was getting lost during the migration, disrupting record-keeping and compliance. The need to rename files and folders for better organization and consistency was not easy to handle. Additionally, SharePoint's restrictions on file and folder names caused issues with invalid characters, needing manual intervention. A Custom Solution To solve these problems, we developed a custom C# solution using the SharePoint Object Model. This approach allowed us to automate the upload process, ensure metadata integrity, provide flexibility in renaming, and handle invalid characters efficiently. Key Features of the Solution The solution included automated uploads that allowed bulk document uploads directly from network drives to SharePoint. We also developed custom code to preserve important metadata like the original creation date, modification date, and author information. The solution provided functionality to rename files and folders as per the client's needs, and it included automated detection and correction of invalid characters in filenames. The C# Code Snippet Below is a simplified version of the C# code snippet that was used: using System; using Microsoft.SharePoint; namespace SharePointMigrationHelper { class Program { static void Main(string[] args) { string siteUrl = \"http://sharepointserver/sites/mrvl\"; string libraryName = \"Documents\"; string filePath = @\"C:\\networkdrive\\document.docx\"; string fileName = \"document.docx\"; // Example metadata DateTime creationDate = new DateTime(2020, 1, 1); string createdBy = \"CreatorName\"; using (SPSite site = new SPSite(siteUrl)) { using (SPWeb web = site.OpenWeb()) { SPFolder libraryFolder = web.Folders[libraryName]; byte[] fileContent = System.IO.File.ReadAllBytes(filePath); SPFile uploadedFile = libraryFolder.Files.Add(fileName, fileContent, true); SPListItem item = uploadedFile.Item; item[\"Created\"] = creationDate; item[\"Author\"] = web.EnsureUser(createdBy); item.Update(); } } Console.WriteLine(\"Document uploaded successfully with metadata.\"); } } } Conclusion The migration of documents to SharePoint WSS 3.0 allowed the client to use the robust features of WSS 3.0, making it easier for them to move to MOSS 2007 later. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Project - C# - WSS 3 Bulk ingestion"},{"location":"M365/WSS3DocumentUpload/#document-migration-to-sharepoint-wss-30-using-cnet","text":"Our client had a large number of documents saved on their network drives and wanted to move them to SharePoint WSS 3.0. They were excited about using the new features of WSS 3.0 to make document management easier and more efficient.","title":"Document Migration to SharePoint WSS 3.0 Using C#.NET"},{"location":"M365/WSS3DocumentUpload/#the-challenge","text":"However, the migration task came with several technical challenges. The large volume of documents made the transfer process time-consuming and complex. Important metadata like creation and modification dates, along with author information, was getting lost during the migration, disrupting record-keeping and compliance. The need to rename files and folders for better organization and consistency was not easy to handle. Additionally, SharePoint's restrictions on file and folder names caused issues with invalid characters, needing manual intervention.","title":"The Challenge"},{"location":"M365/WSS3DocumentUpload/#a-custom-solution","text":"To solve these problems, we developed a custom C# solution using the SharePoint Object Model. This approach allowed us to automate the upload process, ensure metadata integrity, provide flexibility in renaming, and handle invalid characters efficiently.","title":"A Custom Solution"},{"location":"M365/WSS3DocumentUpload/#key-features-of-the-solution","text":"The solution included automated uploads that allowed bulk document uploads directly from network drives to SharePoint. We also developed custom code to preserve important metadata like the original creation date, modification date, and author information. The solution provided functionality to rename files and folders as per the client's needs, and it included automated detection and correction of invalid characters in filenames.","title":"Key Features of the Solution"},{"location":"M365/WSS3DocumentUpload/#the-c-code-snippet","text":"Below is a simplified version of the C# code snippet that was used: using System; using Microsoft.SharePoint; namespace SharePointMigrationHelper { class Program { static void Main(string[] args) { string siteUrl = \"http://sharepointserver/sites/mrvl\"; string libraryName = \"Documents\"; string filePath = @\"C:\\networkdrive\\document.docx\"; string fileName = \"document.docx\"; // Example metadata DateTime creationDate = new DateTime(2020, 1, 1); string createdBy = \"CreatorName\"; using (SPSite site = new SPSite(siteUrl)) { using (SPWeb web = site.OpenWeb()) { SPFolder libraryFolder = web.Folders[libraryName]; byte[] fileContent = System.IO.File.ReadAllBytes(filePath); SPFile uploadedFile = libraryFolder.Files.Add(fileName, fileContent, true); SPListItem item = uploadedFile.Item; item[\"Created\"] = creationDate; item[\"Author\"] = web.EnsureUser(createdBy); item.Update(); } } Console.WriteLine(\"Document uploaded successfully with metadata.\"); } } }","title":"The C# Code Snippet"},{"location":"M365/WSS3DocumentUpload/#conclusion","text":"The migration of documents to SharePoint WSS 3.0 allowed the client to use the robust features of WSS 3.0, making it easier for them to move to MOSS 2007 later. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Conclusion"},{"location":"M365/oAuthSharePointPython/","text":"How to authenticate from your local python setup with SharePoint online Authenticating to SharePoint Online with Username and Password in Python: Challenges with MFA Connecting to SharePoint Online from Python Using OAuth as a solution Prerequisites Step 1: Register an Application in Azure AD Step 2: Grant Permissions to the App Step 3: Install Required Python Libraries Step 4: Authenticate Using OAuth Step 5: Interact with SharePoint Online Takeaways How to authenticate from your local python setup with SharePoint online The ability to integrate your local python enviornment with SharePoint online can have lot of advantages. Imagine you have a bunch of JSON files and want to put the information from those files into a SharePoint document library. With Python, you can easily write a program to do that. This article will show you a few ways to connect your Python environment to SharePoint Online. Authenticating to SharePoint Online with Username and Password in Python: Challenges with MFA Authenticating to SharePoint Online directly using a username and password from a Python script can be straightforward for basic scripts and automation tasks. However, this approach faces significant challenges, especially in environments where Modern Authentication policies, including Multi-Factor Authentication (MFA), are enforced. For example, I had MFA enabled and I tried to connect with SharePoint onlien from my local VS Code using python. Install the Office365-REST-Python-Client Library pip install Office365-REST-Python-Client Then, I tried to authanticate from office365.runtime.auth.authentication_context import AuthenticationContext from office365.sharepoint.client_context import ClientContext # SharePoint site URL site_url = \"https://yourdomain.sharepoint.com/sites/yoursite\" # Your Office 365 credentials username = \"yourusername@yourdomain.com\" password = \"yourpassword\" ctx_auth = AuthenticationContext(site_url) if ctx_auth.acquire_token_for_user(username, password): ctx = ClientContext(site_url, ctx_auth) web = ctx.web ctx.load(web) ctx.execute_query() print(f\"Authenticated as: {web.properties['Title']}\") else: print(\"Authentication failed.\") This failed: Conclusion Nowadays, MFA is everywhere, most organizations enforce MFA, hence while direct username/password authentication is theoritically possible, chances are it will fail 99% of the time. Connecting to SharePoint Online from Python Using OAuth as a solution We saw how our basic authentication method using username/password failed. Hence, Modern authentication via OAuth is recommended for secure access. Follow the steps below to connect with SharePoint online using oAuth authantication. oAuth is also used to connect to a lot of other Offie 365 and Azure services. Its very popular. Prerequisites Python : Ensure Python is installed on your system. The examples that I show you will need Python 3.6 or higher Azure AD App Registration : You will also need access to Azure with sufficient permission to register an application in Azure Active Directory (Azure AD) for OAuth authentication. Step 1: Register an Application in Azure AD Sign in to the Azure portal and navigate to Azure Active Directory > App registrations > New registration. Register your app by providing a name and selecting supported account types. The redirect URI (Web) can be http://localhost for testing purposes. Capture the Application (client) ID and Directory (tenant) ID from the Overview page after registration. Under Certificates & secrets , generate a new client secret and note it down. Step 2: Grant Permissions to the App Navigate to API permissions > Add a permission > APIs my organization uses > SharePoint. Choose Delegated permissions and add permissions like Sites.Read.All (adjust based on your needs). Click \"Add permissions\" and ensure an administrator grants consent if required. Step 3: Install Required Python Libraries Install the requests library to make HTTP requests and msal for Microsoft Authentication Library support. pip install requests msal Step 4: Authenticate Using OAuth from msal import ConfidentialClientApplication # Replace these with your Azure AD app registration details client_id = 'YOUR_APP_CLIENT_ID' client_secret = 'YOUR_APP_CLIENT_SECRET' tenant_id = 'YOUR_TENANT_ID' authority_url = f'https://login.microsoftonline.com/{tenant_id}' resource_url = 'https://graph.microsoft.com' redirect_uri = 'http://localhost' # Initialize the MSAL confidential client app = ConfidentialClientApplication( client_id, authority=authority_url, client_credential=client_secret, ) # Acquire token for SharePoint token_response = app.acquire_token_for_client(scopes=[f'{resource_url}/.default']) # Extract the access token access_token = token_response.get('access_token', None) if not access_token: raise Exception(\"Failed to acquire token. Check your credentials and permissions.\") print(\"Successfully authenticated.\") Step 5: Interact with SharePoint Online With the access token, you can now make authenticated requests to SharePoint Online. Here is a small example that shows how to list the titles of all SharePoint sites in your organization using the Microsoft Graph API. import requests # The endpoint to list all sites url = f'{resource_url}/v1.0/sites' # Headers for the request headers = { 'Authorization': f'Bearer {access_token}', 'Accept': 'application/json', } response = requests.get(url, headers=headers) if response.status_code == 200: sites = response.json() for site in sites.get('value', []): print(site.get('displayName')) else: print(f\"Failed to retrieve sites: {response.status_code}\") Takeaways This article showed you how to connect to SharePoint Online from a local Python environment using OAuth for authentication. By following these steps, you've registered an application in Azure AD, granted it necessary permissions, authenticated using the Microsoft Authentication Library (MSAL), and interacted with SharePoint Online via the Microsoft Graph API. Remember, OAuth provides a secure and robust method for authenticating and interacting with Microsoft services. Knowing about oAuth can be a valuable skill if you want to work with microsoft resources. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Python-oAuth-SharePointOnline"},{"location":"M365/oAuthSharePointPython/#how-to-authenticate-from-your-local-python-setup-with-sharepoint-online","text":"The ability to integrate your local python enviornment with SharePoint online can have lot of advantages. Imagine you have a bunch of JSON files and want to put the information from those files into a SharePoint document library. With Python, you can easily write a program to do that. This article will show you a few ways to connect your Python environment to SharePoint Online.","title":"How to authenticate from your local python setup with SharePoint online"},{"location":"M365/oAuthSharePointPython/#authenticating-to-sharepoint-online-with-username-and-password-in-python-challenges-with-mfa","text":"Authenticating to SharePoint Online directly using a username and password from a Python script can be straightforward for basic scripts and automation tasks. However, this approach faces significant challenges, especially in environments where Modern Authentication policies, including Multi-Factor Authentication (MFA), are enforced. For example, I had MFA enabled and I tried to connect with SharePoint onlien from my local VS Code using python. Install the Office365-REST-Python-Client Library pip install Office365-REST-Python-Client Then, I tried to authanticate from office365.runtime.auth.authentication_context import AuthenticationContext from office365.sharepoint.client_context import ClientContext # SharePoint site URL site_url = \"https://yourdomain.sharepoint.com/sites/yoursite\" # Your Office 365 credentials username = \"yourusername@yourdomain.com\" password = \"yourpassword\" ctx_auth = AuthenticationContext(site_url) if ctx_auth.acquire_token_for_user(username, password): ctx = ClientContext(site_url, ctx_auth) web = ctx.web ctx.load(web) ctx.execute_query() print(f\"Authenticated as: {web.properties['Title']}\") else: print(\"Authentication failed.\") This failed: Conclusion Nowadays, MFA is everywhere, most organizations enforce MFA, hence while direct username/password authentication is theoritically possible, chances are it will fail 99% of the time.","title":"Authenticating to SharePoint Online with Username and Password in Python: Challenges with MFA"},{"location":"M365/oAuthSharePointPython/#connecting-to-sharepoint-online-from-python-using-oauth-as-a-solution","text":"We saw how our basic authentication method using username/password failed. Hence, Modern authentication via OAuth is recommended for secure access. Follow the steps below to connect with SharePoint online using oAuth authantication. oAuth is also used to connect to a lot of other Offie 365 and Azure services. Its very popular.","title":"Connecting to SharePoint Online from Python Using OAuth as a solution"},{"location":"M365/oAuthSharePointPython/#prerequisites","text":"Python : Ensure Python is installed on your system. The examples that I show you will need Python 3.6 or higher Azure AD App Registration : You will also need access to Azure with sufficient permission to register an application in Azure Active Directory (Azure AD) for OAuth authentication.","title":"Prerequisites"},{"location":"M365/oAuthSharePointPython/#step-1-register-an-application-in-azure-ad","text":"Sign in to the Azure portal and navigate to Azure Active Directory > App registrations > New registration. Register your app by providing a name and selecting supported account types. The redirect URI (Web) can be http://localhost for testing purposes. Capture the Application (client) ID and Directory (tenant) ID from the Overview page after registration. Under Certificates & secrets , generate a new client secret and note it down.","title":"Step 1: Register an Application in Azure AD"},{"location":"M365/oAuthSharePointPython/#step-2-grant-permissions-to-the-app","text":"Navigate to API permissions > Add a permission > APIs my organization uses > SharePoint. Choose Delegated permissions and add permissions like Sites.Read.All (adjust based on your needs). Click \"Add permissions\" and ensure an administrator grants consent if required.","title":"Step 2: Grant Permissions to the App"},{"location":"M365/oAuthSharePointPython/#step-3-install-required-python-libraries","text":"Install the requests library to make HTTP requests and msal for Microsoft Authentication Library support. pip install requests msal","title":"Step 3: Install Required Python Libraries"},{"location":"M365/oAuthSharePointPython/#step-4-authenticate-using-oauth","text":"from msal import ConfidentialClientApplication # Replace these with your Azure AD app registration details client_id = 'YOUR_APP_CLIENT_ID' client_secret = 'YOUR_APP_CLIENT_SECRET' tenant_id = 'YOUR_TENANT_ID' authority_url = f'https://login.microsoftonline.com/{tenant_id}' resource_url = 'https://graph.microsoft.com' redirect_uri = 'http://localhost' # Initialize the MSAL confidential client app = ConfidentialClientApplication( client_id, authority=authority_url, client_credential=client_secret, ) # Acquire token for SharePoint token_response = app.acquire_token_for_client(scopes=[f'{resource_url}/.default']) # Extract the access token access_token = token_response.get('access_token', None) if not access_token: raise Exception(\"Failed to acquire token. Check your credentials and permissions.\") print(\"Successfully authenticated.\")","title":"Step 4: Authenticate Using OAuth"},{"location":"M365/oAuthSharePointPython/#step-5-interact-with-sharepoint-online","text":"With the access token, you can now make authenticated requests to SharePoint Online. Here is a small example that shows how to list the titles of all SharePoint sites in your organization using the Microsoft Graph API. import requests # The endpoint to list all sites url = f'{resource_url}/v1.0/sites' # Headers for the request headers = { 'Authorization': f'Bearer {access_token}', 'Accept': 'application/json', } response = requests.get(url, headers=headers) if response.status_code == 200: sites = response.json() for site in sites.get('value', []): print(site.get('displayName')) else: print(f\"Failed to retrieve sites: {response.status_code}\")","title":"Step 5: Interact with SharePoint Online"},{"location":"M365/oAuthSharePointPython/#takeaways","text":"This article showed you how to connect to SharePoint Online from a local Python environment using OAuth for authentication. By following these steps, you've registered an application in Azure AD, granted it necessary permissions, authenticated using the Microsoft Authentication Library (MSAL), and interacted with SharePoint Online via the Microsoft Graph API. Remember, OAuth provides a secure and robust method for authenticating and interacting with Microsoft services. Knowing about oAuth can be a valuable skill if you want to work with microsoft resources. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Takeaways"},{"location":"Microsoft-Fabric/DataFactory/","text":"Data Factory in Microsoft Fabric Background Pipelines in Microsoft Fabric Understand Pipelines Core Pipeline Concepts Activities : Executable tasks in a sequence. Two types: Parameters Pipeline Runs Canvas for desinign piplines The Copy Data Activity The Copy Data Tool Pipeline Templates Run and monitor pipelines Dataflows When you choose DataFlows Pipeline Copy Vs DataFlows Vs Spark Data Factory in Microsoft Fabric Background ADF is a very important commponent of Fabric. It is 100% the same old ADF in the new Fabric Platform. Pipelines in Microsoft Fabric Understand Pipelines A Pipeline is like a workflow for ingesting and transforming data. Using the GUI we can build complex pipelines with very less coding. Core Pipeline Concepts Activities : Executable tasks in a sequence. Two types: Data Transformation : Transfers and transforms data (e.g., Copy Data, Data Flow, Notebook, Stored Procedure). Control Flow : Implements loops, conditional branching, and manages variables. Parameters Enable specific values for each run, increasing reusability. Pipeline Runs Executed on-demand or scheduled. Unique run ID for tracking and reviewing each execution. Canvas for desinign piplines Fabric offers a Canvas where you can build complex pipeliens without much coding: The Copy Data Activity The Copy Data is the most important activity in data pipelines. Some pipelines only contain one Copy Data activity, thats all! When to use? Use the Copy Data activity to move data without transformations or to import raw data. For transformations or merging data, use a Data Flow (Gen2) activity with Power Query to define and include multiple transformation steps in a pipeline. The Copy Data Tool Pipeline Templates To create a pipeline based on a template on the start page choose Templates You will see templates like this: Run and monitor pipelines You can run a pipeline, schedule it and view the run history from the GUI Dataflows A way to import and transform data with Power Query Online. When you choose DataFlows You need to connect to and transform data to be loaded into a Fabric lakehouse. You aren't comfortable using Spark notebooks, so decide to use Dataflows Gen2. How would you complete this task? Answer: Create a Dataflow Gen2 to transform data > add your lakehouse as the data destination. You can either use Dataflows by iteself or add dataflows in pipelines. Pipeline Copy Vs DataFlows Vs Spark Property Pipeline Copy Activity Dataflow Gen 2 Spark Use Case Data lake and data warehouse migration, data ingestion, lightweight transformation Data ingestion, data transformation, data wrangling, data profiling Data ingestion, data transformation, data processing, data profiling Code Written No code, low code No code, low code Code Data Volume Low to high Low to high Low to high Development Interface Wizard, canvas Power Query Notebook, Spark job definition Sources 30+ connectors 150+ connectors Hundreds of Spark libraries Destinations 18+ connectors (Lakehouse, Azure SQL database, Azure Data explorer, Azure Synapse analytics) Hundreds of Spark libraries Transformation Complexity Low: lightweight (type conversion, column mapping, merge/split files, flatten hierarchy) Low to high: 300+ transformation functions Low to high: support for native Spark and open-source libraries","title":"Pipelines&DataFlows"},{"location":"Microsoft-Fabric/DataFactory/#data-factory-in-microsoft-fabric","text":"","title":"Data Factory in Microsoft Fabric"},{"location":"Microsoft-Fabric/DataFactory/#background","text":"ADF is a very important commponent of Fabric. It is 100% the same old ADF in the new Fabric Platform.","title":"Background"},{"location":"Microsoft-Fabric/DataFactory/#pipelines-in-microsoft-fabric","text":"","title":"Pipelines in Microsoft Fabric"},{"location":"Microsoft-Fabric/DataFactory/#understand-pipelines","text":"A Pipeline is like a workflow for ingesting and transforming data. Using the GUI we can build complex pipelines with very less coding.","title":"Understand Pipelines"},{"location":"Microsoft-Fabric/DataFactory/#core-pipeline-concepts","text":"","title":"Core Pipeline Concepts"},{"location":"Microsoft-Fabric/DataFactory/#activities-executable-tasks-in-a-sequence-two-types","text":"Data Transformation : Transfers and transforms data (e.g., Copy Data, Data Flow, Notebook, Stored Procedure). Control Flow : Implements loops, conditional branching, and manages variables.","title":"Activities: Executable tasks in a sequence. Two types:"},{"location":"Microsoft-Fabric/DataFactory/#parameters","text":"Enable specific values for each run, increasing reusability.","title":"Parameters"},{"location":"Microsoft-Fabric/DataFactory/#pipeline-runs","text":"Executed on-demand or scheduled. Unique run ID for tracking and reviewing each execution.","title":"Pipeline Runs"},{"location":"Microsoft-Fabric/DataFactory/#canvas-for-desinign-piplines","text":"Fabric offers a Canvas where you can build complex pipeliens without much coding:","title":"Canvas for desinign piplines"},{"location":"Microsoft-Fabric/DataFactory/#the-copy-data-activity","text":"The Copy Data is the most important activity in data pipelines. Some pipelines only contain one Copy Data activity, thats all! When to use? Use the Copy Data activity to move data without transformations or to import raw data. For transformations or merging data, use a Data Flow (Gen2) activity with Power Query to define and include multiple transformation steps in a pipeline.","title":"The Copy Data Activity"},{"location":"Microsoft-Fabric/DataFactory/#the-copy-data-tool","text":"","title":"The Copy Data Tool"},{"location":"Microsoft-Fabric/DataFactory/#pipeline-templates","text":"To create a pipeline based on a template on the start page choose Templates You will see templates like this:","title":"Pipeline Templates"},{"location":"Microsoft-Fabric/DataFactory/#run-and-monitor-pipelines","text":"You can run a pipeline, schedule it and view the run history from the GUI","title":"Run and monitor pipelines"},{"location":"Microsoft-Fabric/DataFactory/#dataflows","text":"A way to import and transform data with Power Query Online.","title":"Dataflows"},{"location":"Microsoft-Fabric/DataFactory/#when-you-choose-dataflows","text":"You need to connect to and transform data to be loaded into a Fabric lakehouse. You aren't comfortable using Spark notebooks, so decide to use Dataflows Gen2. How would you complete this task? Answer: Create a Dataflow Gen2 to transform data > add your lakehouse as the data destination. You can either use Dataflows by iteself or add dataflows in pipelines.","title":"When you choose DataFlows"},{"location":"Microsoft-Fabric/DataFactory/#pipeline-copy-vs-dataflows-vs-spark","text":"Property Pipeline Copy Activity Dataflow Gen 2 Spark Use Case Data lake and data warehouse migration, data ingestion, lightweight transformation Data ingestion, data transformation, data wrangling, data profiling Data ingestion, data transformation, data processing, data profiling Code Written No code, low code No code, low code Code Data Volume Low to high Low to high Low to high Development Interface Wizard, canvas Power Query Notebook, Spark job definition Sources 30+ connectors 150+ connectors Hundreds of Spark libraries Destinations 18+ connectors (Lakehouse, Azure SQL database, Azure Data explorer, Azure Synapse analytics) Hundreds of Spark libraries Transformation Complexity Low: lightweight (type conversion, column mapping, merge/split files, flatten hierarchy) Low to high: 300+ transformation functions Low to high: support for native Spark and open-source libraries","title":"Pipeline Copy Vs DataFlows Vs Spark"},{"location":"Microsoft-Fabric/DataScience/","text":"Common machine learning models Classification : Will the customer stay or leave? Regression : What will the product cost? Clustering : Group similar customers together. Forecasting : What will sales be next month? Let's get started Go to your workspace and click on Data Science on the left corner icon Now from the landing page click on Notebook Now your notebook will open, You can add your lakehouse from the left side pane: In the notebook, paste this read-made microsoft code: # Azure storage access info for open dataset diabetes blob_account_name = \"azureopendatastorage\" blob_container_name = \"mlsamples\" blob_relative_path = \"diabetes\" blob_sas_token = r\"\" # Blank since container is Anonymous access # Set Spark config to access blob storage wasbs_path = f\"wasbs://%s@%s.blob.core.windows.net/%s\" % (blob_container_name, blob_account_name, blob_relative_path) spark.conf.set(\"fs.azure.sas.%s.%s.blob.core.windows.net\" % (blob_container_name, blob_account_name), blob_sas_token) print(\"Remote blob path: \" + wasbs_path) # Spark read parquet, note that it won't load any data yet by now df = spark.read.parquet(wasbs_path) Fabric automatically creates the spark session and the df is created. Now lets see the df Enter python display(df) in the next cell to see the output With display(df) the output will show two options Table and Chart. Select Chart Then you can customize the chart as required Data Wrangler Tool The Data Wrangler tool is available at the top of your notebook. First, create a dataframe, then use Data Wrangler to clean it and generate PySpark or Pandas code. It's a useful tool that saves you from writing a lot of code. To open Data Wrangler, click on its icon and select the dataframe you created. Make sure your session is active. For example, to create a new column, choose Create Column from Formula and provide the details. This formula is only for pandas DF. The corresponding code will be generated automatically! Finally after cleaning Fabric will create a function and add it to the main notebook: Further steps Train Machine Learning Models Train a Regression Model Split Data : ```python from sklearn.model_selection import train_test_split X, y = df_clean[['AGE','SEX','BMI','BP','S1','S2','S3','S4','S5','S6']].values, df_clean['Y'].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0) 2. **Create a new experiment `diabetes-regression`**: python import mlflow experiment_name = \"diabetes-regression\" mlflow.set_experiment(experiment_name) 3. **Train Model**: python from sklearn.linear_model import LinearRegression with mlflow.start_run(): mlflow.autolog() model = LinearRegression() model.fit(X_train, y_train) ``` Train a Classification Model Split Data : ```python from sklearn.model_selection import train_test_split X, y = df_clean[['AGE','SEX','BMI','BP','S1','S2','S3','S4','S5','S6']].values, df_clean['Risk'].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0) 2. **Create a new experiment `diabetes-classification`**: python import mlflow experiment_name = \"diabetes-classification\" mlflow.set_experiment(experiment_name) 3. **Train Model**: python from sklearn.linear_model import LogisticRegression with mlflow.start_run(): mlflow.sklearn.autolog() model = LogisticRegression(C=1/0.1, solver=\"liblinear\").fit(X_train, y_train) ``` Explore Your Experiments Navigate to your workspace. Open the \"diabetes-regression\" experiment. Review Run metrics. Click Save run as ML Model to save it","title":"Data Science on Fabric Overview"},{"location":"Microsoft-Fabric/DataScience/#common-machine-learning-models","text":"Classification : Will the customer stay or leave? Regression : What will the product cost? Clustering : Group similar customers together. Forecasting : What will sales be next month?","title":"Common machine learning models"},{"location":"Microsoft-Fabric/DataScience/#lets-get-started","text":"Go to your workspace and click on Data Science on the left corner icon Now from the landing page click on Notebook Now your notebook will open, You can add your lakehouse from the left side pane: In the notebook, paste this read-made microsoft code: # Azure storage access info for open dataset diabetes blob_account_name = \"azureopendatastorage\" blob_container_name = \"mlsamples\" blob_relative_path = \"diabetes\" blob_sas_token = r\"\" # Blank since container is Anonymous access # Set Spark config to access blob storage wasbs_path = f\"wasbs://%s@%s.blob.core.windows.net/%s\" % (blob_container_name, blob_account_name, blob_relative_path) spark.conf.set(\"fs.azure.sas.%s.%s.blob.core.windows.net\" % (blob_container_name, blob_account_name), blob_sas_token) print(\"Remote blob path: \" + wasbs_path) # Spark read parquet, note that it won't load any data yet by now df = spark.read.parquet(wasbs_path) Fabric automatically creates the spark session and the df is created. Now lets see the df Enter python display(df) in the next cell to see the output With display(df) the output will show two options Table and Chart. Select Chart Then you can customize the chart as required","title":"Let's get started"},{"location":"Microsoft-Fabric/DataScience/#data-wrangler-tool","text":"The Data Wrangler tool is available at the top of your notebook. First, create a dataframe, then use Data Wrangler to clean it and generate PySpark or Pandas code. It's a useful tool that saves you from writing a lot of code. To open Data Wrangler, click on its icon and select the dataframe you created. Make sure your session is active. For example, to create a new column, choose Create Column from Formula and provide the details. This formula is only for pandas DF. The corresponding code will be generated automatically! Finally after cleaning Fabric will create a function and add it to the main notebook:","title":"Data Wrangler Tool"},{"location":"Microsoft-Fabric/DataScience/#further-steps","text":"","title":"Further steps"},{"location":"Microsoft-Fabric/DataScience/#train-machine-learning-models","text":"","title":"Train Machine Learning Models"},{"location":"Microsoft-Fabric/DataScience/#train-a-regression-model","text":"Split Data : ```python from sklearn.model_selection import train_test_split X, y = df_clean[['AGE','SEX','BMI','BP','S1','S2','S3','S4','S5','S6']].values, df_clean['Y'].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0) 2. **Create a new experiment `diabetes-regression`**: python import mlflow experiment_name = \"diabetes-regression\" mlflow.set_experiment(experiment_name) 3. **Train Model**: python from sklearn.linear_model import LinearRegression with mlflow.start_run(): mlflow.autolog() model = LinearRegression() model.fit(X_train, y_train) ```","title":"Train a Regression Model"},{"location":"Microsoft-Fabric/DataScience/#train-a-classification-model","text":"Split Data : ```python from sklearn.model_selection import train_test_split X, y = df_clean[['AGE','SEX','BMI','BP','S1','S2','S3','S4','S5','S6']].values, df_clean['Risk'].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0) 2. **Create a new experiment `diabetes-classification`**: python import mlflow experiment_name = \"diabetes-classification\" mlflow.set_experiment(experiment_name) 3. **Train Model**: python from sklearn.linear_model import LogisticRegression with mlflow.start_run(): mlflow.sklearn.autolog() model = LogisticRegression(C=1/0.1, solver=\"liblinear\").fit(X_train, y_train) ```","title":"Train a Classification Model"},{"location":"Microsoft-Fabric/DataScience/#explore-your-experiments","text":"Navigate to your workspace. Open the \"diabetes-regression\" experiment. Review Run metrics. Click Save run as ML Model to save it","title":"Explore Your Experiments"},{"location":"Microsoft-Fabric/DataWareHouse/","text":"Data Warehouse in Microsoft Fabric Background Two types of Warehouse available in Microsoft Fabric Fact and Dimension Table Concepts Surrogate keys and alternte keys How data is ingested into a warehouse in Fabric? COPY into syntax Fabric Datawarehouse interface Visual Query in Fabric Appendix Special Types of Dimension Tables Time Dimensions Slowly Changing Dimensions (SCD) Real-Life Impact Data Warehouse in Microsoft Fabric Background If earth is a database then Sun is a warehouse. Fabric's warehouse is unique - built on the Lakehouse (Delta format). You can use Full T-SQL. Two types of Warehouse available in Microsoft Fabric In Microsoft Fabric, there are two types of warehouses SQL Endpoint : The SQL Endpoint is auto-generated when you create a Lakehouse in Fabric. It serves as an analytics endpoint, allowing you to query data within the Lakehouse using T-SQL (Transact-SQL) language and the TDS (Tabular Data Stream) protocol. Each Lakehouse has one SQL analytics endpoint, and a workspace can have multiple Lakehouses. The SQL analytics endpoint exposes Delta tables from the Lakehouse as SQL tables, making them accessible via T-SQL queries. It also automatically creates a default Power BI semantic model based on the Lakehouse objects' naming conventions. No user action is required to create a SQL analytics endpoint; it is generated automatically during Lakehouse creation. Behind the scenes, the SQL analytics endpoint leverages the same engine as the Warehouse, ensuring high performance and low latency for SQL queries. Automatic metadata discovery keeps SQL metadata up to date without user intervention. Synapse Data Warehouse : The Synapse Data Warehouse is a SQL engine designed specifically for querying and transforming data within the Data Lake (OneLake) . It provides full transactional support , including DDL (Data Definition Language) and DML (Data Manipulation Language) queries. With the Synapse Data Warehouse, you can perform cross-database queries and seamlessly transition from read-only operations to building business logic on your OneLake data. It complements the Lakehouse by enabling more complex analytics scenarios. Fact and Dimension Table Concepts Fact tables have the numbers you want to look at, like a receipt. They have a lot of rows and are the main source of data for analysis. They're like the \"what\" you are measuring. Dimension tables have details about those numbers, like a restaurant menu. They have fewer rows and give context to the data in the fact tables. Examples: Column Fact Table Dimension Table Sales Amount \u2714 Order Quantity \u2714 Product ID \u2714 Customer ID \u2714 Transaction Date \u2714 Product Name \u2714 Customer Name \u2714 Supplier ID \u2714 Supplier Name \u2714 Discount Rate \u2714 Revenue \u2714 Store Location \u2714 Category \u2714 Time (Hour, Day, Month, Year) \u2714 Payment Method \u2714 Surrogate keys and alternte keys Surrogate key : A unique key for each row. Like a cop's badge number. Its unique in the police department. Alternate key : Its like a key that identifies the person in the whole ecosystem. Like a passport number of the cop - unique in the nation. How data is ingested into a warehouse in Fabric? Data is ingested using: Pipelines , Dataflows , cross-database querying , and the COPY INTO command. COPY into syntax COPY INTO dbo.apple FROM 'https://abc/xxx.csv' WITH ( FILE_TYPE = 'CSV' ,CREDENTIAL = ( IDENTITY = 'Shared Access Signature' , SECRET = 'xxx' ) ,FIRSTROW = 2 ) GO Fabric Datawarehouse interface Visual Query in Fabric Here I will show you how easy it is to create a left-outer join of Two tables - DimProduct & FactSalesOrder Just drag both the tables on to the canvas then perform the steps as shown in the image below Then select the required column. Here we selected ProductName. Then create a Power BI Reports quickly: Appendix Special Types of Dimension Tables Special types of dimensions provide additional context and enable more comprehensive data analysis. Let's explore this with an example from a popular online retail company, \"ShopEZ.\" Time Dimensions Time dimensions provide information about the time period in which an event occurred. This table enables data analysts to aggregate data over temporal intervals. For example, a time dimension might include columns for the year, quarter, month, and day in which a sales order was placed. Example: Sales Analysis at ShopEZ ShopEZ wants to analyze its sales performance to optimize inventory and marketing strategies. The time dimension table allows them to aggregate sales data over different periods. Year : 2023 Quarter : Q1 Month : January Day : 15 With these time dimensions, ShopEZ can easily aggregate sales data to see trends like: Increased sales during holiday seasons Monthly sales growth Quarterly performance comparison Slowly Changing Dimensions (SCD) Slowly changing dimensions track changes to dimension attributes over time, like changes to a customer's address or a product's price. They are crucial in a data warehouse because they allow users to analyze and understand changes to data over time. Example: Customer Loyalty Program at Wallmart ShopEZ runs a loyalty program where customers' membership tiers can change based on their purchase history. Tracking these changes accurately over time is essential for targeted marketing and personalized offers. Scenario: Change in Customer's Membership Tier Original Record (2022) : Customer ID: 456 Name: Sarah Lee Membership Tier: Silver Join Date: 2021-05-10 Updated Record (2023) : Customer ID: 456 Name: Sarah Lee Membership Tier: Gold Join Date: 2021-05-10 In a slowly changing dimension scenario, ShopEZ's data warehouse can handle this change using different SCD types: Type 1 (Overwrite) : The old membership tier is overwritten with the new tier. This approach is straightforward but loses historical data. Customer ID: 456 Name: Sarah Lee Membership Tier: Gold Join Date: 2021-05-10 Type 2 (Historical Tracking) : A new record is created for Sarah to preserve the history of changes. This method adds a new row for each change and typically includes an effective date range. Record 1 : Customer ID: 456 Name: Sarah Lee Membership Tier: Silver Join Date: 2021-05-10 End Date: 2023-01-14 Record 2 : Customer ID: 456 Name: Sarah Lee Membership Tier: Gold Join Date: 2021-05-10 Start Date: 2023-01-15 Type 3 (Limited History) : The old value is stored in additional columns, allowing some history tracking but limited to a predefined number of changes. Customer ID: 456 Name: Sarah Lee Current Membership Tier: Gold Previous Membership Tier: Silver Join Date: 2021-05-10 Real-Life Impact Using time dimensions, ShopEZ can identify that sales peak during certain times, such as Black Friday or Christmas. This insight helps them plan inventory, staffing, and marketing campaigns more effectively. With slowly changing dimensions, ShopEZ can track changes in customer behavior and preferences over time. For example, they can see that Sarah Lee upgraded her membership tier from Silver to Gold, indicating increased engagement and spending.","title":"DataWareHouse"},{"location":"Microsoft-Fabric/DataWareHouse/#data-warehouse-in-microsoft-fabric","text":"","title":"Data Warehouse in Microsoft Fabric"},{"location":"Microsoft-Fabric/DataWareHouse/#background","text":"If earth is a database then Sun is a warehouse. Fabric's warehouse is unique - built on the Lakehouse (Delta format). You can use Full T-SQL.","title":"Background"},{"location":"Microsoft-Fabric/DataWareHouse/#two-types-of-warehouse-available-in-microsoft-fabric","text":"In Microsoft Fabric, there are two types of warehouses SQL Endpoint : The SQL Endpoint is auto-generated when you create a Lakehouse in Fabric. It serves as an analytics endpoint, allowing you to query data within the Lakehouse using T-SQL (Transact-SQL) language and the TDS (Tabular Data Stream) protocol. Each Lakehouse has one SQL analytics endpoint, and a workspace can have multiple Lakehouses. The SQL analytics endpoint exposes Delta tables from the Lakehouse as SQL tables, making them accessible via T-SQL queries. It also automatically creates a default Power BI semantic model based on the Lakehouse objects' naming conventions. No user action is required to create a SQL analytics endpoint; it is generated automatically during Lakehouse creation. Behind the scenes, the SQL analytics endpoint leverages the same engine as the Warehouse, ensuring high performance and low latency for SQL queries. Automatic metadata discovery keeps SQL metadata up to date without user intervention. Synapse Data Warehouse : The Synapse Data Warehouse is a SQL engine designed specifically for querying and transforming data within the Data Lake (OneLake) . It provides full transactional support , including DDL (Data Definition Language) and DML (Data Manipulation Language) queries. With the Synapse Data Warehouse, you can perform cross-database queries and seamlessly transition from read-only operations to building business logic on your OneLake data. It complements the Lakehouse by enabling more complex analytics scenarios.","title":"Two types of Warehouse available in Microsoft Fabric"},{"location":"Microsoft-Fabric/DataWareHouse/#fact-and-dimension-table-concepts","text":"Fact tables have the numbers you want to look at, like a receipt. They have a lot of rows and are the main source of data for analysis. They're like the \"what\" you are measuring. Dimension tables have details about those numbers, like a restaurant menu. They have fewer rows and give context to the data in the fact tables. Examples: Column Fact Table Dimension Table Sales Amount \u2714 Order Quantity \u2714 Product ID \u2714 Customer ID \u2714 Transaction Date \u2714 Product Name \u2714 Customer Name \u2714 Supplier ID \u2714 Supplier Name \u2714 Discount Rate \u2714 Revenue \u2714 Store Location \u2714 Category \u2714 Time (Hour, Day, Month, Year) \u2714 Payment Method \u2714","title":"Fact and Dimension Table Concepts"},{"location":"Microsoft-Fabric/DataWareHouse/#surrogate-keys-and-alternte-keys","text":"Surrogate key : A unique key for each row. Like a cop's badge number. Its unique in the police department. Alternate key : Its like a key that identifies the person in the whole ecosystem. Like a passport number of the cop - unique in the nation.","title":"Surrogate keys and alternte keys"},{"location":"Microsoft-Fabric/DataWareHouse/#how-data-is-ingested-into-a-warehouse-in-fabric","text":"Data is ingested using: Pipelines , Dataflows , cross-database querying , and the COPY INTO command.","title":"How data is ingested into a warehouse in Fabric?"},{"location":"Microsoft-Fabric/DataWareHouse/#copy-into-syntax","text":"COPY INTO dbo.apple FROM 'https://abc/xxx.csv' WITH ( FILE_TYPE = 'CSV' ,CREDENTIAL = ( IDENTITY = 'Shared Access Signature' , SECRET = 'xxx' ) ,FIRSTROW = 2 ) GO","title":"COPY into syntax"},{"location":"Microsoft-Fabric/DataWareHouse/#fabric-datawarehouse-interface","text":"","title":"Fabric Datawarehouse interface"},{"location":"Microsoft-Fabric/DataWareHouse/#visual-query-in-fabric","text":"Here I will show you how easy it is to create a left-outer join of Two tables - DimProduct & FactSalesOrder Just drag both the tables on to the canvas then perform the steps as shown in the image below Then select the required column. Here we selected ProductName. Then create a Power BI Reports quickly:","title":"Visual Query in Fabric"},{"location":"Microsoft-Fabric/DataWareHouse/#appendix","text":"","title":"Appendix"},{"location":"Microsoft-Fabric/DataWareHouse/#special-types-of-dimension-tables","text":"Special types of dimensions provide additional context and enable more comprehensive data analysis. Let's explore this with an example from a popular online retail company, \"ShopEZ.\"","title":"Special Types of Dimension Tables"},{"location":"Microsoft-Fabric/DataWareHouse/#time-dimensions","text":"Time dimensions provide information about the time period in which an event occurred. This table enables data analysts to aggregate data over temporal intervals. For example, a time dimension might include columns for the year, quarter, month, and day in which a sales order was placed. Example: Sales Analysis at ShopEZ ShopEZ wants to analyze its sales performance to optimize inventory and marketing strategies. The time dimension table allows them to aggregate sales data over different periods. Year : 2023 Quarter : Q1 Month : January Day : 15 With these time dimensions, ShopEZ can easily aggregate sales data to see trends like: Increased sales during holiday seasons Monthly sales growth Quarterly performance comparison","title":"Time Dimensions"},{"location":"Microsoft-Fabric/DataWareHouse/#slowly-changing-dimensions-scd","text":"Slowly changing dimensions track changes to dimension attributes over time, like changes to a customer's address or a product's price. They are crucial in a data warehouse because they allow users to analyze and understand changes to data over time. Example: Customer Loyalty Program at Wallmart ShopEZ runs a loyalty program where customers' membership tiers can change based on their purchase history. Tracking these changes accurately over time is essential for targeted marketing and personalized offers. Scenario: Change in Customer's Membership Tier Original Record (2022) : Customer ID: 456 Name: Sarah Lee Membership Tier: Silver Join Date: 2021-05-10 Updated Record (2023) : Customer ID: 456 Name: Sarah Lee Membership Tier: Gold Join Date: 2021-05-10 In a slowly changing dimension scenario, ShopEZ's data warehouse can handle this change using different SCD types: Type 1 (Overwrite) : The old membership tier is overwritten with the new tier. This approach is straightforward but loses historical data. Customer ID: 456 Name: Sarah Lee Membership Tier: Gold Join Date: 2021-05-10 Type 2 (Historical Tracking) : A new record is created for Sarah to preserve the history of changes. This method adds a new row for each change and typically includes an effective date range. Record 1 : Customer ID: 456 Name: Sarah Lee Membership Tier: Silver Join Date: 2021-05-10 End Date: 2023-01-14 Record 2 : Customer ID: 456 Name: Sarah Lee Membership Tier: Gold Join Date: 2021-05-10 Start Date: 2023-01-15 Type 3 (Limited History) : The old value is stored in additional columns, allowing some history tracking but limited to a predefined number of changes. Customer ID: 456 Name: Sarah Lee Current Membership Tier: Gold Previous Membership Tier: Silver Join Date: 2021-05-10","title":"Slowly Changing Dimensions (SCD)"},{"location":"Microsoft-Fabric/DataWareHouse/#real-life-impact","text":"Using time dimensions, ShopEZ can identify that sales peak during certain times, such as Black Friday or Christmas. This insight helps them plan inventory, staffing, and marketing campaigns more effectively. With slowly changing dimensions, ShopEZ can track changes in customer behavior and preferences over time. For example, they can see that Sarah Lee upgraded her membership tier from Silver to Gold, indicating increased engagement and spending.","title":"Real-Life Impact"},{"location":"Microsoft-Fabric/DirectLake/","text":"DirectLake Mode in Fabric DirectLake is the third and newest data handling technique in Microsoft Fabric for Power BI. Before understanding DirectLake, let's first look at the other two modes: Let's say you have 10 GB of data stored in a SQL Server and you want to create a dashboard. Before DirectLake, you had two options to handle the data: Import Mode In Import mode, Power BI reads the data from your SQL Server, compresses it (e.g., from 10 GB to 1 GB), and stores it as columns in *.pbix files (Power BI Desktop) or *.idf files (Power BI service). When you create a report and apply filters, sums, etc., Power BI will smartly load only the required data into RAM, perform the calculations, and generate the visuals. However, everything happens on the Power BI side, cutting you off from your original SQL Server. If your SQL Server has new data, you have two options: reload all the data ( Full refresh ) or load only the new data ( Incremental refresh , available in Power BI Premium). DirectQuery Mode If you always want the freshest data on your dashboard, you can use DirectQuery mode. In DirectQuery mode, Power BI doesn\u2019t import the data but directly queries the SQL Server. Every Power BI report generates a DAX query, regardless of the mode. In DirectQuery mode, DAX queries are translated into SQL, sent to the SQL Server, executed there, and the results are returned. This ensures you always see the freshest data . However, network bandwidth, large data volumes, and frequent interactions might result in slowness or out-of-memory issues. To summarize: - Import Mode : Compresses and stores data locally, making reports fast and interactive, but requires periodic refreshes to get new data. - DirectQuery Mode : Provides always-fresh data by querying the live SQL Server directly, but may be slower and more resource-intensive. DirectLake Mode DirectLake mode is similar to Import mode, but it uses .parquet files directly instead of .idf files. The key requirement is that the data must be in OneLake as .parquet files. This avoids the need to save data from SQL Server into new .idf files, preventing data duplication. If you have 10GB of data for reporting, Import mode loads the entire dataset into memory (smartly), even if you don't need all the columns. In contrast, DirectLake mode only loads the data related to the columns used in your report, making it more efficient. If the data size exceeds a certain limit, DirectLake mode automatically switches to DirectQuery mode. Regarding refresh: Any changes in the Delta Lake files are automatically detected, and the report datasets are refreshed intelligently. Only the metadata is refreshed and synced, making DirectLake refreshes very quick, often taking less than 10 seconds. In Import mode, the refresh loads the data into memory, which takes longer. Pros and Cons of DirectLake - Personal Views DirectLake Mode DOES NOT replace any mode Fabric has Import Mode and DirectQuery Mode. DirectLake is just the third and newest option. It provides near-real-time reports like Import Mode but not real real-time like DirectQuery. It is not the best of both worlds This mode is unique, but it\u2019s not faster than Import mode and doesn\u2019t replace DirectQuery's special features. DirectLake Vs Import Mode DirectLake can be as good as Import mode but not better. It can sometimes be slower compared to Import mode. Import mode and DirectLake take the same amount of memory. DirectLake uses the same 'Import Required Columns in Memory' approach like Import mode. So it\u2019s not special. The only difference is in Import Mode, when the memory is less, it gives an out-of-memory exception, and in DirectLake, it switches to DirectQuery. This happens if your data is around 300 GB. Data Modeling has limitations in DirectLake You cannot use calculated columns, calculated tables, and MDX user hierarchies in DirectLake. This impacts Excel\u2019s user experience when consuming semantic models published on Power BI. Also, DirectLake models created in Fabric have case-sensitive collation. DirectLake requires real physical tables - no room for views When you use views in DirectLake, the mode changes to DirectQuery. To use 100% DirectLake, all your tables must be real. This might require duplicating your data. Note: We have option of creating shortcuts. Which comes as a relief. But, I haven't tested the performance of shortcut data. Hence, if you already use Import Mode or DirectQuery and everything works fine, there are no substantial benefits to moving to DirectLake. Practical Scenario (DP-600 Question) You have a Fabric tenant with a workspace named Workspace1, assigned to an F64 capacity, containing a lakehouse with one billion historical sales records, receiving up to 10,000 new or updated sales records every 15 minutes. You plan to build a custom Microsoft Power BI semantic model and reports from this data, requiring the best report performance and near-real-time data reporting. Which Power BI semantic model storage mode should you use? Answer: DirectLake How Microsoft Advertises It DirectLake storage mode offers near-real-time (NRT) access to data with performance close to Import storage mode, surpassing DirectQuery in terms of speed for large datasets. While DirectQuery provides NRT access, it can slow down with large datasets. Import Mode, though fast, requires data to be loaded into Power BI's memory, lacking NRT capabilities. Currently, DirectLake tables cannot be mixed with other table types (Import, DirectQuery, or Dual) within the same model, and composite models are not supported yet. For further details, you can refer to the Microsoft Fabric Direct Lake overview .","title":"DirectLake|Fabric|PowerBI"},{"location":"Microsoft-Fabric/DirectLake/#directlake-mode-in-fabric","text":"DirectLake is the third and newest data handling technique in Microsoft Fabric for Power BI. Before understanding DirectLake, let's first look at the other two modes: Let's say you have 10 GB of data stored in a SQL Server and you want to create a dashboard. Before DirectLake, you had two options to handle the data:","title":"DirectLake Mode in Fabric"},{"location":"Microsoft-Fabric/DirectLake/#import-mode","text":"In Import mode, Power BI reads the data from your SQL Server, compresses it (e.g., from 10 GB to 1 GB), and stores it as columns in *.pbix files (Power BI Desktop) or *.idf files (Power BI service). When you create a report and apply filters, sums, etc., Power BI will smartly load only the required data into RAM, perform the calculations, and generate the visuals. However, everything happens on the Power BI side, cutting you off from your original SQL Server. If your SQL Server has new data, you have two options: reload all the data ( Full refresh ) or load only the new data ( Incremental refresh , available in Power BI Premium).","title":"Import Mode"},{"location":"Microsoft-Fabric/DirectLake/#directquery-mode","text":"If you always want the freshest data on your dashboard, you can use DirectQuery mode. In DirectQuery mode, Power BI doesn\u2019t import the data but directly queries the SQL Server. Every Power BI report generates a DAX query, regardless of the mode. In DirectQuery mode, DAX queries are translated into SQL, sent to the SQL Server, executed there, and the results are returned. This ensures you always see the freshest data . However, network bandwidth, large data volumes, and frequent interactions might result in slowness or out-of-memory issues. To summarize: - Import Mode : Compresses and stores data locally, making reports fast and interactive, but requires periodic refreshes to get new data. - DirectQuery Mode : Provides always-fresh data by querying the live SQL Server directly, but may be slower and more resource-intensive.","title":"DirectQuery Mode"},{"location":"Microsoft-Fabric/DirectLake/#directlake-mode","text":"DirectLake mode is similar to Import mode, but it uses .parquet files directly instead of .idf files. The key requirement is that the data must be in OneLake as .parquet files. This avoids the need to save data from SQL Server into new .idf files, preventing data duplication. If you have 10GB of data for reporting, Import mode loads the entire dataset into memory (smartly), even if you don't need all the columns. In contrast, DirectLake mode only loads the data related to the columns used in your report, making it more efficient. If the data size exceeds a certain limit, DirectLake mode automatically switches to DirectQuery mode. Regarding refresh: Any changes in the Delta Lake files are automatically detected, and the report datasets are refreshed intelligently. Only the metadata is refreshed and synced, making DirectLake refreshes very quick, often taking less than 10 seconds. In Import mode, the refresh loads the data into memory, which takes longer.","title":"DirectLake Mode"},{"location":"Microsoft-Fabric/DirectLake/#pros-and-cons-of-directlake-personal-views","text":"","title":"Pros and Cons of DirectLake - Personal Views"},{"location":"Microsoft-Fabric/DirectLake/#directlake-mode-does-not-replace-any-mode","text":"Fabric has Import Mode and DirectQuery Mode. DirectLake is just the third and newest option. It provides near-real-time reports like Import Mode but not real real-time like DirectQuery.","title":"DirectLake Mode DOES NOT replace any mode"},{"location":"Microsoft-Fabric/DirectLake/#it-is-not-the-best-of-both-worlds","text":"This mode is unique, but it\u2019s not faster than Import mode and doesn\u2019t replace DirectQuery's special features.","title":"It is not the best of both worlds"},{"location":"Microsoft-Fabric/DirectLake/#directlake-vs-import-mode","text":"DirectLake can be as good as Import mode but not better. It can sometimes be slower compared to Import mode. Import mode and DirectLake take the same amount of memory. DirectLake uses the same 'Import Required Columns in Memory' approach like Import mode. So it\u2019s not special. The only difference is in Import Mode, when the memory is less, it gives an out-of-memory exception, and in DirectLake, it switches to DirectQuery. This happens if your data is around 300 GB.","title":"DirectLake Vs Import Mode"},{"location":"Microsoft-Fabric/DirectLake/#data-modeling-has-limitations-in-directlake","text":"You cannot use calculated columns, calculated tables, and MDX user hierarchies in DirectLake. This impacts Excel\u2019s user experience when consuming semantic models published on Power BI. Also, DirectLake models created in Fabric have case-sensitive collation.","title":"Data Modeling has limitations in DirectLake"},{"location":"Microsoft-Fabric/DirectLake/#directlake-requires-real-physical-tables-no-room-for-views","text":"When you use views in DirectLake, the mode changes to DirectQuery. To use 100% DirectLake, all your tables must be real. This might require duplicating your data. Note: We have option of creating shortcuts. Which comes as a relief. But, I haven't tested the performance of shortcut data. Hence, if you already use Import Mode or DirectQuery and everything works fine, there are no substantial benefits to moving to DirectLake.","title":"DirectLake requires real physical tables - no room for views"},{"location":"Microsoft-Fabric/DirectLake/#practical-scenario-dp-600-question","text":"You have a Fabric tenant with a workspace named Workspace1, assigned to an F64 capacity, containing a lakehouse with one billion historical sales records, receiving up to 10,000 new or updated sales records every 15 minutes. You plan to build a custom Microsoft Power BI semantic model and reports from this data, requiring the best report performance and near-real-time data reporting. Which Power BI semantic model storage mode should you use? Answer: DirectLake","title":"Practical Scenario (DP-600 Question)"},{"location":"Microsoft-Fabric/DirectLake/#how-microsoft-advertises-it","text":"DirectLake storage mode offers near-real-time (NRT) access to data with performance close to Import storage mode, surpassing DirectQuery in terms of speed for large datasets. While DirectQuery provides NRT access, it can slow down with large datasets. Import Mode, though fast, requires data to be loaded into Power BI's memory, lacking NRT capabilities. Currently, DirectLake tables cannot be mixed with other table types (Import, DirectQuery, or Dual) within the same model, and composite models are not supported yet. For further details, you can refer to the Microsoft Fabric Direct Lake overview .","title":"How Microsoft Advertises It"},{"location":"Microsoft-Fabric/E2EProject/","text":"Concepts we will learn Here you will understand two concepts of ETL: Medallion architecture for data management: Star Schema: Project summary Here we will first import data(few .csv files) into bronze layer(Simple folders). Then clean and transform it and load it into Silver Delta Table. Then create star schema and load the data from silver to Gold Delta Tables. You have successfully taken data from your bronze layer, transformed it, and loaded it into a silver Delta table. Now you\u2019ll use a new notebook to transform the data further, model it into a star schema, and load it into gold Delta tables. In data processing, you often hear about Bronze, Silver, and Gold layers, which are part of the Medallion Architecture. Here I will show the use of Medallion architecture in a simple straightforward ETL project: Import data files into the Bronze layer (Lakehouse folder). Define a schema and create a dataframe from the .csv files. Clean the data and add new columns as needed. Manually create a Silver Delta Table with the same schema as the dataframe. Perform an upsert operation on the Delta table, which means updating existing records based on certain conditions and adding new records if no match is found. Explore data in the silver layer using the SQL endpoint","title":"E2EProject"},{"location":"Microsoft-Fabric/E2EProject/#concepts-we-will-learn","text":"Here you will understand two concepts of ETL: Medallion architecture for data management: Star Schema:","title":"Concepts we will learn"},{"location":"Microsoft-Fabric/E2EProject/#project-summary","text":"Here we will first import data(few .csv files) into bronze layer(Simple folders). Then clean and transform it and load it into Silver Delta Table. Then create star schema and load the data from silver to Gold Delta Tables. You have successfully taken data from your bronze layer, transformed it, and loaded it into a silver Delta table. Now you\u2019ll use a new notebook to transform the data further, model it into a star schema, and load it into gold Delta tables. In data processing, you often hear about Bronze, Silver, and Gold layers, which are part of the Medallion Architecture. Here I will show the use of Medallion architecture in a simple straightforward ETL project: Import data files into the Bronze layer (Lakehouse folder). Define a schema and create a dataframe from the .csv files. Clean the data and add new columns as needed. Manually create a Silver Delta Table with the same schema as the dataframe. Perform an upsert operation on the Delta table, which means updating existing records based on certain conditions and adding new records if no match is found. Explore data in the silver layer using the SQL endpoint","title":"Project summary"},{"location":"Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse/","text":"Background Let's start Prepare the Local Files and On-Premises Gateway Create a Delta Lake Table to Store the JSON Data Set Up the Copy Data Process Run the Pipeline Check the Delta Lake Table Data Conclusion Background Here, I'll show you two things: How to connect Fabric or Azure Data Factory (ADF) to your local file system using an on-premises gateway. How to use a simple, no-code method to automatically copy JSON files into a Delta Lake table. Let's start Prepare the Local Files and On-Premises Gateway Download the sample JSON files from here and place them in a local folder on your computer. Install the on-premises gateway. It\u2019s straightforward and easy to install. You can find detailed instructions here . Note: Ensure that an admin account has all permissions in the security tab of the local folder. While not always practical, admin access simplifies the setup and reduces complications. Create a Delta Lake Table to Store the JSON Data Create a notebook and run the following SparkSQL code: %%sql Create table jsonDelta ( id string, name string, email string, age int, country string ) using delta Remember: using delta at the end. Set Up the Copy Data Process Source Setup : Follow the diagram to set up the source. Ensure the admin account has the right permissions for the source folder. Destination Setup : Follow the diagram for the destination setup. Mapping Setup : Follow the diagram to set up the mapping. Run the Pipeline Click \"Run.\" The pipeline will process all the JSON files and add the data to the Delta Lake table. That\u2019s it! Check the Delta Lake Table Data Go to the lakehouse, expand the table, and you will see that all the JSON data has been loaded into the Delta Lake table. Conclusion I hope I was able to show how nearly no-code, simple, and straightforward it is to load data from your local system into a Delta Lake in Fabric.","title":"JSON-DeltaLake-OPG"},{"location":"Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse/#background","text":"Here, I'll show you two things: How to connect Fabric or Azure Data Factory (ADF) to your local file system using an on-premises gateway. How to use a simple, no-code method to automatically copy JSON files into a Delta Lake table.","title":"Background"},{"location":"Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse/#lets-start","text":"","title":"Let's start"},{"location":"Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse/#prepare-the-local-files-and-on-premises-gateway","text":"Download the sample JSON files from here and place them in a local folder on your computer. Install the on-premises gateway. It\u2019s straightforward and easy to install. You can find detailed instructions here . Note: Ensure that an admin account has all permissions in the security tab of the local folder. While not always practical, admin access simplifies the setup and reduces complications.","title":"Prepare the Local Files and On-Premises Gateway"},{"location":"Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse/#create-a-delta-lake-table-to-store-the-json-data","text":"Create a notebook and run the following SparkSQL code: %%sql Create table jsonDelta ( id string, name string, email string, age int, country string ) using delta Remember: using delta at the end.","title":"Create a Delta Lake Table to Store the JSON Data"},{"location":"Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse/#set-up-the-copy-data-process","text":"Source Setup : Follow the diagram to set up the source. Ensure the admin account has the right permissions for the source folder. Destination Setup : Follow the diagram for the destination setup. Mapping Setup : Follow the diagram to set up the mapping.","title":"Set Up the Copy Data Process"},{"location":"Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse/#run-the-pipeline","text":"Click \"Run.\" The pipeline will process all the JSON files and add the data to the Delta Lake table. That\u2019s it!","title":"Run the Pipeline"},{"location":"Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse/#check-the-delta-lake-table-data","text":"Go to the lakehouse, expand the table, and you will see that all the JSON data has been loaded into the Delta Lake table.","title":"Check the Delta Lake Table Data"},{"location":"Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse/#conclusion","text":"I hope I was able to show how nearly no-code, simple, and straightforward it is to load data from your local system into a Delta Lake in Fabric.","title":"Conclusion"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse/","text":"Background Ways to Ingest Data into Lakehouse When to choose which method? Microsoft's recommendation. The entire project in just 8 pyspark lines THE code The explanation Appendix Connect to Azure blob storage with Spark from Fabric Connect to Azure SQL Database with a Service Principal Write data into a Lakehouse File Write data into a Lakehouse Delta Table Optimize[Fewer files] - V-Order \\& optimizeWrite Knowledge check Summary Background Here, I'll show you how to use a PySpark Notebook to build a complete ETL solution. We'll import parquet files from external sources into a Fabric Lakehouse folder, clean the data, and create Delta tables\u2014all using the PySpark Notebook. Ways to Ingest Data into Lakehouse Apart from using Pyspark in Notebooks there are other methods to Copy data into Lakehouse. Based on the the situation you will have to choose a method. ADF Data Pipelines : You can both ingest and transoform using ADF pipeline. Use the Copy data activity for ingestion( no transformation ) and a Notebook activity or Dataflow activity for transformation. If there is no transformation, blindly choose Copy data activity. {: .highlight } {: .fw-400 } The Copy data activity: Best performance, fastest, most-direct when copying data from large datasets or migrating data from one system to another. But, this activity can't do any transformation. Power BI Dataflow : Power BI Dataflows can handle both ingestion and transformation. They support ingestion from thousands of sources and use Power Query for transformation. Note: Fabric uses the same Power BI Dataflow. Manual Upload : You can always manually upload your files into a folder. Then you can use a Noteook or Dataflow for the transformation and Delta Lake Table creation :-) Additionally , there's an important T-SQL command called COPY INTO . This command copies data into tables and supports Parquet and CSV formats from Azure Data Lake Storage Gen2/Azure Blob. However, it only copies data into tables and not into Lakehouse folders from external systems. When to choose which method? Microsoft's recommendation. Scenario 1: You have a Fabric tenant that contains a lakehouse named Lakehouse1. You need to ingest data into Lakehouse1 from a large Azure SQL Database table that contains more than 500 million records. The data must be ingested without applying any additional transformations. The solution must minimize costs and administrative effort. What should you use to ingest the data? a pipeline with the Copy data activity a SQL stored procedure Dataflow Gen2 notebooks Answer: When ingesting a large data source without applying transformations, the recommended method is to use the Copy data activity in pipelines. Notebooks are recommended for complex data transformations, whereas Dataflow Gen2 is suitable for smaller data and/or specific connectors. Scenario 2: You have a Fabric tenant that contains a lakehouse.On a local computer, you have a CSV file that contains a static list of company office locations. You need to recommend a method to perform a one-time copy to ingest the CSV file into the lakehouse. The solution must minimize administrative effort. Which method should you recommend? - a Dataflow Gen2 query - a local file upload by using Lakehouse explorer - a pipeline with the Copy data activity - a Spark notebook Answer : For a one-time copy of small local files into a lakehouse, using Lakehouse explorer and a local file upload is recommended. Scenario 3: You need to ensure that the pipeline activity supports parameterization. Which two activities support parameterization in the data pipeline UI? - Dataflow Gen2 - KQL activity - notebooks - SQL stored procedures - user-defined functions Answer : Only notebooks and SQL stored procedures provide a possibility to define parameters in the data pipeline UI. Dataflow Gen2 and KQL activity only require connection details, but no parameters can be supplied. User-defined functions cannot be added as an activity to a pipeline. Scenario 4: You have an external Snowflake database that contains a table with 200 million rows. You need to use a data pipeline to migrate the database to Lakehouse1. What is the most performant (fastest) method for ingesting data this large (200 million rows) by using a data pipeline? Data Pipeline (Copy data) Data Pipeline (Dataflow Gen2) Data Pipeline (Lookup) Data Pipeline Spark (Notebook) Answer: Copy data is the fastest and most direct method for migrating data from one system to another, with no transformations applied. The entire project in just 8 pyspark lines THE code Here is the core code for the project. I've intentionally kept it short to highlight the key concept. # Enable V-Order for Parquet files to improve data skipping and query performance. # V-Order helps in reducing the amount of data read during queries by organizing the data for better compression and faster access. spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\") # Enable automatic Delta optimized write to enhance write performance. # This setting allows Delta Lake to optimize the way data is written, improving speed and efficiency. spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # wasbs path = cont_name@act_name.blob.core.windows.net/folder_path # Read parquet data from Azure Blob Storage path df = spark.read.parquet(f'wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow') # Right click ...(RawData) folder -> Copy ABFS Path. # ABFS_Path/yellow_taxi(New sub folder name) # Write the first 1000 rows as a Parquet file df.limit(1000).write.mode(\"overwrite\").parquet(f\"abfss://WorkSpaceA@onelake.dfs.fabric.microsoft.com/LakeHouseBhutu.Lakehouse/Files/RawData/yellow_taxi\") # Now read back from the folder where we copied the parquets files raw_df = spark.read.parquet(fabric_put_path) # Filter rows where column 'trip_distance' is greater than 0 and column 'fare_amount' is greater than 0 cleaned_df = raw_df.filter(raw_df.tripDistance > 0) # Now write the cleaned df into Delta Table in Lakehouse cleaned_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"delta_yellow_taxi\") # Display results display(cleaned_df.limit(10)) The explanation I have included screenshots of the working code in the notebook and added some comments to help you understand it. Appendix The code above is quite short. In the real world, it wouldn't be this simple. To better understand the concepts, review the following sections that provide more detailed explanations. Connect to Azure blob storage with Spark from Fabric # Azure Blob Storage access info blob_account_name = \"azureopendatastorage\" blob_container_name = \"nyctlc\" blob_relative_path = \"yellow\" blob_sas_token = \"sv=2022-11-02&ss=bfqt&srt=c&sp=rwdlacupiytfx&se=2023-09-08T23:50:02Z&st=2023-09-08T15:50:02Z&spr=https&sig=abcdefg123456\" # Construct the path for connection wasbs_path = f'wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net/{blob_relative_path}?{blob_sas_token}' # Read parquet data from Azure Blob Storage path blob_df = spark.read.parquet(wasbs_path) # Show the Azure Blob DataFrame blob_df.show() Connect to Azure SQL Database with a Service Principal # Placeholders for Azure SQL Database connection info server_name = \"your_server_name.database.windows.net\" port_number = 1433 # Default port number for SQL Server database_name = \"your_database_name\" table_name = \"YourTableName\" # Database table client_id = \"YOUR_CLIENT_ID\" # Service principal client ID client_secret = \"YOUR_CLIENT_SECRET\" # Service principal client secret tenant_id = \"YOUR_TENANT_ID\" # Azure Active Directory tenant ID # Build the Azure SQL Database JDBC URL with Service Principal (Active Directory Integrated) jdbc_url = f\"jdbc:sqlserver://{server_name}:{port_number};database={database_name};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;Authentication=ActiveDirectoryIntegrated\" # Properties for the JDBC connection properties = { \"user\": client_id, \"password\": client_secret, \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\", \"tenantId\": tenant_id } # Read entire table from Azure SQL Database using AAD Integrated authentication sql_df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=properties) # Show the Azure SQL DataFrame sql_df.show() Write data into a Lakehouse File # Write DataFrame to Parquet file format parquet_output_path = \"dbfs:/FileStore/your_folder/your_file_name\" df.write.mode(\"overwrite\").parquet(parquet_output_path) print(f\"DataFrame has been written to Parquet file: {parquet_output_path}\") # Write DataFrame to Delta table delta_table_name = \"your_delta_table_name\" df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(delta_table_name) print(f\"DataFrame has been written to Delta table: {delta_table_name}\") Write data into a Lakehouse Delta Table # Use format and save to load as a Delta table table_name = \"nyctaxi_raw\" filtered_df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\") # Confirm load as Delta table print(f\"Spark DataFrame saved to Delta table: {table_name}\") Optimize[Fewer files] - V-Order & optimizeWrite V-Order and OptimizeWrite sorts data and creates fewer , larger , more efficient Parquet files. Hence, the deta table is optimized. V-Order is enabled by default in Microsoft Fabric and in Apache Spark. Here is how you can configure them in Pyspark: # Enable V-Order spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\") # Enable automatic Delta optimized write spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") Knowledge check What are the four data ingestion options available in Microsoft Fabric for loading data into a data warehouse? Answer : COPY (Transact-SQL) statement, data pipelines, dataflows, and cross-warehouse are the four data ingestion options available in Microsoft Fabric for loading data into a data warehouse. What are the supported data sources and file formats for the COPY (Transact-SQL) statement in Warehouse? Answer : The COPY (Transact-SQL) statement currently supports the PARQUET and CSV file formats, and Azure Data Lake Storage (ADLS) Gen2 and Azure Blob Storage as data sources. What is the recommended minimum file size when working with external data on files in Microsoft Fabric? Answer : When working with external data on files, we recommend that files are at least 4 MB in size. Summary Pyspark notebook alone can completely create end-to-end robust ETL workflows ADF Pipeline with Copy Data + Notebook or Dataflow can do the same job Copy data activity in ADF pipeline can't do transformation. It is used for data ingestion only. Read, write and saveAsTable are the three important pyspark commands to learn. spark.read.parquet(\"path of external parquets\") df.limit(1000).write.mode(\"overwrite\").parquet(\"path of lakehouse folder\") cleaned_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"theDeltatableName\")","title":"ETL-Load data into Lakehouse - Pyspark Notebook"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse/#background","text":"Here, I'll show you how to use a PySpark Notebook to build a complete ETL solution. We'll import parquet files from external sources into a Fabric Lakehouse folder, clean the data, and create Delta tables\u2014all using the PySpark Notebook.","title":"Background"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse/#ways-to-ingest-data-into-lakehouse","text":"Apart from using Pyspark in Notebooks there are other methods to Copy data into Lakehouse. Based on the the situation you will have to choose a method. ADF Data Pipelines : You can both ingest and transoform using ADF pipeline. Use the Copy data activity for ingestion( no transformation ) and a Notebook activity or Dataflow activity for transformation. If there is no transformation, blindly choose Copy data activity. {: .highlight } {: .fw-400 } The Copy data activity: Best performance, fastest, most-direct when copying data from large datasets or migrating data from one system to another. But, this activity can't do any transformation. Power BI Dataflow : Power BI Dataflows can handle both ingestion and transformation. They support ingestion from thousands of sources and use Power Query for transformation. Note: Fabric uses the same Power BI Dataflow. Manual Upload : You can always manually upload your files into a folder. Then you can use a Noteook or Dataflow for the transformation and Delta Lake Table creation :-) Additionally , there's an important T-SQL command called COPY INTO . This command copies data into tables and supports Parquet and CSV formats from Azure Data Lake Storage Gen2/Azure Blob. However, it only copies data into tables and not into Lakehouse folders from external systems.","title":"Ways to Ingest Data into Lakehouse"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse/#when-to-choose-which-method-microsofts-recommendation","text":"Scenario 1: You have a Fabric tenant that contains a lakehouse named Lakehouse1. You need to ingest data into Lakehouse1 from a large Azure SQL Database table that contains more than 500 million records. The data must be ingested without applying any additional transformations. The solution must minimize costs and administrative effort. What should you use to ingest the data? a pipeline with the Copy data activity a SQL stored procedure Dataflow Gen2 notebooks Answer: When ingesting a large data source without applying transformations, the recommended method is to use the Copy data activity in pipelines. Notebooks are recommended for complex data transformations, whereas Dataflow Gen2 is suitable for smaller data and/or specific connectors. Scenario 2: You have a Fabric tenant that contains a lakehouse.On a local computer, you have a CSV file that contains a static list of company office locations. You need to recommend a method to perform a one-time copy to ingest the CSV file into the lakehouse. The solution must minimize administrative effort. Which method should you recommend? - a Dataflow Gen2 query - a local file upload by using Lakehouse explorer - a pipeline with the Copy data activity - a Spark notebook Answer : For a one-time copy of small local files into a lakehouse, using Lakehouse explorer and a local file upload is recommended. Scenario 3: You need to ensure that the pipeline activity supports parameterization. Which two activities support parameterization in the data pipeline UI? - Dataflow Gen2 - KQL activity - notebooks - SQL stored procedures - user-defined functions Answer : Only notebooks and SQL stored procedures provide a possibility to define parameters in the data pipeline UI. Dataflow Gen2 and KQL activity only require connection details, but no parameters can be supplied. User-defined functions cannot be added as an activity to a pipeline. Scenario 4: You have an external Snowflake database that contains a table with 200 million rows. You need to use a data pipeline to migrate the database to Lakehouse1. What is the most performant (fastest) method for ingesting data this large (200 million rows) by using a data pipeline? Data Pipeline (Copy data) Data Pipeline (Dataflow Gen2) Data Pipeline (Lookup) Data Pipeline Spark (Notebook) Answer: Copy data is the fastest and most direct method for migrating data from one system to another, with no transformations applied.","title":"When to choose which method? Microsoft's recommendation."},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse/#the-entire-project-in-just-8-pyspark-lines","text":"","title":"The entire project in just 8 pyspark lines"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse/#the-code","text":"Here is the core code for the project. I've intentionally kept it short to highlight the key concept. # Enable V-Order for Parquet files to improve data skipping and query performance. # V-Order helps in reducing the amount of data read during queries by organizing the data for better compression and faster access. spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\") # Enable automatic Delta optimized write to enhance write performance. # This setting allows Delta Lake to optimize the way data is written, improving speed and efficiency. spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # wasbs path = cont_name@act_name.blob.core.windows.net/folder_path # Read parquet data from Azure Blob Storage path df = spark.read.parquet(f'wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow') # Right click ...(RawData) folder -> Copy ABFS Path. # ABFS_Path/yellow_taxi(New sub folder name) # Write the first 1000 rows as a Parquet file df.limit(1000).write.mode(\"overwrite\").parquet(f\"abfss://WorkSpaceA@onelake.dfs.fabric.microsoft.com/LakeHouseBhutu.Lakehouse/Files/RawData/yellow_taxi\") # Now read back from the folder where we copied the parquets files raw_df = spark.read.parquet(fabric_put_path) # Filter rows where column 'trip_distance' is greater than 0 and column 'fare_amount' is greater than 0 cleaned_df = raw_df.filter(raw_df.tripDistance > 0) # Now write the cleaned df into Delta Table in Lakehouse cleaned_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"delta_yellow_taxi\") # Display results display(cleaned_df.limit(10))","title":"THE code"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse/#the-explanation","text":"I have included screenshots of the working code in the notebook and added some comments to help you understand it.","title":"The explanation"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse/#appendix","text":"The code above is quite short. In the real world, it wouldn't be this simple. To better understand the concepts, review the following sections that provide more detailed explanations.","title":"Appendix"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse/#connect-to-azure-blob-storage-with-spark-from-fabric","text":"# Azure Blob Storage access info blob_account_name = \"azureopendatastorage\" blob_container_name = \"nyctlc\" blob_relative_path = \"yellow\" blob_sas_token = \"sv=2022-11-02&ss=bfqt&srt=c&sp=rwdlacupiytfx&se=2023-09-08T23:50:02Z&st=2023-09-08T15:50:02Z&spr=https&sig=abcdefg123456\" # Construct the path for connection wasbs_path = f'wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net/{blob_relative_path}?{blob_sas_token}' # Read parquet data from Azure Blob Storage path blob_df = spark.read.parquet(wasbs_path) # Show the Azure Blob DataFrame blob_df.show()","title":"Connect to Azure blob storage with Spark from Fabric"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse/#connect-to-azure-sql-database-with-a-service-principal","text":"# Placeholders for Azure SQL Database connection info server_name = \"your_server_name.database.windows.net\" port_number = 1433 # Default port number for SQL Server database_name = \"your_database_name\" table_name = \"YourTableName\" # Database table client_id = \"YOUR_CLIENT_ID\" # Service principal client ID client_secret = \"YOUR_CLIENT_SECRET\" # Service principal client secret tenant_id = \"YOUR_TENANT_ID\" # Azure Active Directory tenant ID # Build the Azure SQL Database JDBC URL with Service Principal (Active Directory Integrated) jdbc_url = f\"jdbc:sqlserver://{server_name}:{port_number};database={database_name};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;Authentication=ActiveDirectoryIntegrated\" # Properties for the JDBC connection properties = { \"user\": client_id, \"password\": client_secret, \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\", \"tenantId\": tenant_id } # Read entire table from Azure SQL Database using AAD Integrated authentication sql_df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=properties) # Show the Azure SQL DataFrame sql_df.show()","title":"Connect to Azure SQL Database with a Service Principal"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse/#write-data-into-a-lakehouse-file","text":"# Write DataFrame to Parquet file format parquet_output_path = \"dbfs:/FileStore/your_folder/your_file_name\" df.write.mode(\"overwrite\").parquet(parquet_output_path) print(f\"DataFrame has been written to Parquet file: {parquet_output_path}\") # Write DataFrame to Delta table delta_table_name = \"your_delta_table_name\" df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(delta_table_name) print(f\"DataFrame has been written to Delta table: {delta_table_name}\")","title":"Write data into a Lakehouse File"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse/#write-data-into-a-lakehouse-delta-table","text":"# Use format and save to load as a Delta table table_name = \"nyctaxi_raw\" filtered_df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\") # Confirm load as Delta table print(f\"Spark DataFrame saved to Delta table: {table_name}\")","title":"Write data into a Lakehouse Delta Table"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse/#optimizefewer-files-v-order-optimizewrite","text":"V-Order and OptimizeWrite sorts data and creates fewer , larger , more efficient Parquet files. Hence, the deta table is optimized. V-Order is enabled by default in Microsoft Fabric and in Apache Spark. Here is how you can configure them in Pyspark: # Enable V-Order spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\") # Enable automatic Delta optimized write spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")","title":"Optimize[Fewer files] - V-Order &amp; optimizeWrite"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse/#knowledge-check","text":"What are the four data ingestion options available in Microsoft Fabric for loading data into a data warehouse? Answer : COPY (Transact-SQL) statement, data pipelines, dataflows, and cross-warehouse are the four data ingestion options available in Microsoft Fabric for loading data into a data warehouse. What are the supported data sources and file formats for the COPY (Transact-SQL) statement in Warehouse? Answer : The COPY (Transact-SQL) statement currently supports the PARQUET and CSV file formats, and Azure Data Lake Storage (ADLS) Gen2 and Azure Blob Storage as data sources. What is the recommended minimum file size when working with external data on files in Microsoft Fabric? Answer : When working with external data on files, we recommend that files are at least 4 MB in size.","title":"Knowledge check"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse/#summary","text":"Pyspark notebook alone can completely create end-to-end robust ETL workflows ADF Pipeline with Copy Data + Notebook or Dataflow can do the same job Copy data activity in ADF pipeline can't do transformation. It is used for data ingestion only. Read, write and saveAsTable are the three important pyspark commands to learn. spark.read.parquet(\"path of external parquets\") df.limit(1000).write.mode(\"overwrite\").parquet(\"path of lakehouse folder\") cleaned_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"theDeltatableName\")","title":"Summary"},{"location":"Microsoft-Fabric/FabricAdministration/","text":"Open Fabric Admin Portal Delete a Fabric Workspace In the Workspace settings pane, select Other > Remove this workspace. Find your Fabric home region To find your Fabric home region, follow these steps: Sign in to Fabric. Open the Help pane and choose About Microsoft Fabric. Look for the value next to Your data is stored in. The location shown is the default region where your data is stored. You may also be using capacities in different regions for your workspaces. Configure Spark In Microsoft Fabric, each workspace is assigned a Spark cluster. An administrator can manage settings for the Spark cluster in the Data Engineering/Science section of the workspace settings.","title":"Administration"},{"location":"Microsoft-Fabric/FabricAdministration/#open-fabric-admin-portal","text":"","title":"Open Fabric Admin Portal"},{"location":"Microsoft-Fabric/FabricAdministration/#delete-a-fabric-workspace","text":"In the Workspace settings pane, select Other > Remove this workspace.","title":"Delete a Fabric Workspace"},{"location":"Microsoft-Fabric/FabricAdministration/#find-your-fabric-home-region","text":"To find your Fabric home region, follow these steps: Sign in to Fabric. Open the Help pane and choose About Microsoft Fabric. Look for the value next to Your data is stored in. The location shown is the default region where your data is stored. You may also be using capacities in different regions for your workspaces.","title":"Find your Fabric home region"},{"location":"Microsoft-Fabric/FabricAdministration/#configure-spark","text":"In Microsoft Fabric, each workspace is assigned a Spark cluster. An administrator can manage settings for the Spark cluster in the Data Engineering/Science section of the workspace settings.","title":"Configure Spark"},{"location":"Microsoft-Fabric/FabricQ%26A/","text":"\ud83d\udccc Which of the following is a key benefit of using Microsoft Fabric in data projects? A. It allows data professionals to work on data projects independently, without the need for collaboration. B. It requires duplication of data across different systems and teams to ensure data availability. C. It provides a single, integrated environment for data professionals and the business to collaborate on data projects. Answer: C. Fabric's OneLake provides a single, integrated environment for data professionals and the business to collaborate on data projects. \ud83d\udccc What is the default storage format for Fabric's OneLake? A. Delta B. JSON C. CSV Answer: A. The default storage format for OneLake is Delta Parquet, an open-source storage layer that brings reliability to data lakes. \ud83d\udccc Which of the following Fabric workloads is used to move and transform data? A. Data Science B. Data Warehousing C. Data Factory Answer: C. The Data Factory workload combines Power Query with the scale of Azure Data Factory to move and transform data. \ud83d\udccc What is a Microsoft Fabric lakehouse? A. A relational database based on the Microsoft SQL Server database engine. B. A hierarchy of folders and files in Azure Data Lake Store Gen2. C. An analytical store that combines the file storage flexibility of a data lake with the SQL-based query capabilities of a data warehouse. Answer: C. Lakehouses combine data lake and data warehouse features. \ud83d\udccc You want to include data in an external Azure Data Lake Store Gen2 location in your lakehouse, without the requirement to copy the data. What should you do? A. Create a Data pipeline that uses a Copy Data activity to load the external data into a file. B. Create a shortcut. C. Create a Dataflow (Gen2) that extracts the data and loads it into a table. Answer: B. A shortcut enables you to include external data in the lakehouse without copying the data. \ud83d\udccc You want to use Apache Spark to interactively explore data in a file in the lakehouse. What should you do? A. Create a notebook. B. Switch to the SQL analytics endpoint mode. C. Create a Dataflow (Gen2). Answer: A. A notebook enables interactive Spark coding. \ud83d\udccc Which of the following descriptions best fits Delta Lake? A. A Spark API for exporting data from a relational database into CSV files. B. A relational storage layer for Spark that supports tables based on Parquet files. C. A synchronization solution that replicates data between SQL Server and Spark. Answer: B. Delta Lake provides a relational storage layer in which you can create tables based on Parquet files in a data lake. \ud83d\udccc You have a managed table based on a folder that contains data files in delta format. If you drop the table, what happens? A. The table metadata and data files are deleted. B. The table definition is removed from the metastore, but the data files remain intact. C. The table definition remains in the metastore, but the data files are deleted. Answer: A. The life-cycle of the metadata and data for a managed table are the same. \ud83d\udccc What is a data pipeline? A. A special folder in OneLake storage where data can be exported from a lakehouse B. A sequence of activities to orchestrate a data ingestion or transformation process C. A saved Power Query Answer: B. A pipeline consists of activities to ingest and transform data. \ud83d\udccc You want to use a pipeline to copy data to a folder with a specified name for each run. What should you do? A. Create multiple pipelines - one for each folder name B. Use a Dataflow (Gen2) C. Add a parameter to the pipeline and use it to specify the folder name for each run Answer: C. Using a parameter enables greater flexibility for your pipeline. \ud83d\udccc You have previously run a pipeline containing multiple activities. What's the best way to check how long each individual activity took to complete? A. Rerun the pipeline and observe the output, timing each activity. B. View the run details in the run history. C. View the Refreshed value for your lakehouse's default dataset Answer: B. The run history details show the time taken for each activity - optionally as a Gantt chart. \ud83d\udccc What is a Dataflow Gen2? A. A hybrid database that supports ACID transactions. B. A way to export data to Power BI Desktop. C. A way to import and transform data with Power Query Online. Answer: C. Dataflow Gen2 allows you to get and transform data, then optionally ingest to a lakehouse. \ud83d\udccc Which workload experience lets you create a Dataflow Gen2? A. Real-time analytics. B. Data warehouse. C. Data Factory. Answer: C. Data Factory and Power BI workloads allow Dataflow Gen2 creation. \ud83d\udccc You need to connect to and transform data to be loaded into a Fabric lakehouse. You aren't comfortable using Spark notebooks, so decide to use Dataflows Gen2. How would you complete this task? A. Connect to Data Factory workload > Create a Dataflow Gen2 to transform data > add your lakehouse as the data destination. B. Connect to Real-time Analytics workload > Create Pipeline to copy data > Transform data with an Eventstream. C. Connect to Data Factory workload > Create Pipeline to copy data and load to lakehouse > Transform directly in the lakehouse. Answer: A. Connect to Data Factory workload > Create a Dataflow Gen2 to transform data > add your lakehouse as the data destination. \ud83d\udccc Which type of table should an insurance company use to store supplier attribute details for aggregating claims? A. Fact table. B. Dimension table. C. Staging table. Answer: B. A dimension table stores attributes used to group numeric measures. \ud83d\udccc What is a semantic model in the data warehouse experience? A. A semantic model is a business-oriented data model that provides a consistent and reusable representation of data across the organization. B. A semantic model is a physical data model that describes the structure of the data stored in the data warehouse. C. A semantic model is a machine learning model that is used to make predictions based on data in the data warehouse. Answer: A. A semantic model in the data warehouse experience provides a way to organize and structure data in a way that is meaningful to business users, enabling them to easily access and analyze data. \ud83d\udccc What is the purpose of item permissions in a workspace? A. To grant access to all items within a workspace. B. To grant access to specific columns within a table. C. To grant access to individual warehouses for downstream consumption. Answer: C. By granting access to a single data warehouse using item permissions, you can enable downstream consumption of data. Sure, here's the revised version: \ud83d\udccc You have access to a historical dataset that contains the monthly expenses of the marketing department. You want to generate predictions of the expenses for the coming month. Which type of machine learning model is needed? - A. Classification - Incorrect. Classification is used when you want to predict a categorical value. - B. Regression - Incorrect. Regression is used when you want to predict numerical values. - C. Forecasting - Correct. Forecasting is used when you want to predict future numerical values based on time-series data. Answer: Correct. Forecasting is used when you want to predict future numerical values based on time-series data. \ud83d\udccc Which feature in Microsoft Fabric should you use to review the results of MLflow's tracking through a user interface? - A. Notebooks - Incorrect. You can use MLflow in a notebook to review tracked metrics. However, when working with notebooks you need to write code and don't use a user interface. - B. Experiments - Correct. Microsoft Fabric's experiments offer a visual user interface to explore the metrics. - C. Models - Incorrect. Metrics are only shown when a model is saved. Answer: Correct. Microsoft Fabric's experiments offer a visual user interface to explore the metrics. \ud83d\udccc Which feature in Microsoft Fabric should you use to accelerate data exploration and cleansing? - A. Dataflows - B. Data Wrangler - C. Lakehouse - Answer: Correct. Use Data Wrangler to visualize and clean your data. \ud83d\udccc Which of the following statements best describes the concept of capacity in Fabric? Capacity refers to a dedicated space for organizations to create, store, and manage Fabric items. Capacity defines the ability of a resource to perform an activity or to produce output. Capacity is a collection of items that are logically grouped together. Answer: Capacity defines the ability of a resource to perform an activity or to produce output. \ud83d\udccc Which of the following statements is true about the difference between promotion and certification in Fabric? Promotion and certification both allow any workspace member to endorse content. Promotion requires a higher level of permissions than certification. Certification must be enabled in the tenant by the admin, while promotion can be done by a workspace member. Answer: Certification must be enabled in the tenant by the admin, and only designated certifiers can perform the endorsement. In contrast, promotion can be done by any workspace member who has been granted the necessary permissions. \ud83d\udccc Which of the following sets of layers are typically associated with the Medallion Architecture for data management? Raw, Polished, Refined Bronze, Silver, Gold Initial, Intermediate, Final Answer: Bronze, silver, gold is the correct sequence of layers typically used in the medallion architecture. Data flows from the raw and unrefined state (bronze) to a curated and validated state (silver), and finally to an enriched and well-structured presentation state (gold). \ud83d\udccc Which tool is best suited for data transformation in Fabric when dealing with large-scale data that will continue to grow? Dataflows (Gen2) Pipelines Notebooks Answer: Notebooks are a more suitable tool for data transformation with big data in Fabric. \ud83d\udccc What is the benefit of storing different layers of your lakehouse in separate workspaces? It can enhance security, manage capacity use, and optimize cost-effectiveness. It makes it easier to share data with colleagues. There's no benefit of storing different layers of your lakehouse in separate workspaces. Answer: Storing different layers of your lakehouse in separate workspaces enhances security and optimizes cost-effectiveness. \ud83d\udccc What is the benefit of using Fabric notebooks over manual uploads for data ingestion? Notebooks provide an automated approach to ingestion and transformation. Notebooks can orchestrate the Copy Data activity and transformations. Notebooks offer a user-friendly, low-code experience for large semantic models. Answer: Notebooks provide an automated approach to ingestion and transformation. \ud83d\udccc What is the purpose of V-Order and Optimize Write in Delta tables? V-Order and Optimize Write sorts the Delta table when queried with PySpark in a Fabric Notebook. V-Order and Optimize Write enhance Delta tables by sorting data and creating fewer, larger Parquet files. V-Order and Optimize Write create many small csv files. Answer: V-Order and Optimize Write enhance Delta tables by sorting data and creating fewer, larger Parquet files. \ud83d\udccc Why consider basic data cleansing when loading data into Fabric lakehouse? To reduce data load size and processing time. To ensure data quality and consistency. To enforce data privacy and security measures. Answer: Basic cleaning is done to ensure data quality and consistency before moving on to transformation and modeling steps. \ud83d\udccc What is the purpose of creating a Reflex in Data Activator? To connect to data sources, monitor conditions, and initiate actions. To customize your Fabric experience to Data Activator. To navigate between data mode and design mode. Answer: A Reflex item contains all the necessary details to connect to data sources, monitor conditions, and initiate actions for each business segment or process being monitored. \ud83d\udccc What is Data Activator's capability in real-time data analysis? Data Activator can only analyze data in batches. Data Activator can quickly respond to and analyze data in real-time. Data Activator can only analyze data from a single source. Answer: Data Activator is tailored to handle real-time data streams and can distinguish itself through its capability to quickly respond to and analyze data in real-time. \ud83d\udccc What is one of Data Activator's strengths in terms of interoperability with other Fabric experiences? Data Activator can ingest data from EventStreams and Power BI reports. Data Activator can't ingest data from other Fabric experiences. Data Activator can only ingest data from Power BI reports. Answer: One of Data Activator's strengths is its integration capabilities with other Fabric experiences, such as ingesting data from EventStreams and Power BI reports. \ud83d\udccc You are developing a Microsoft Power BI semantic model. Two tables in the data model are not connected in a physical relationship. You need to establish a virtual relationship between the tables. Which DAX function should you use? CROSSFILTER() PATH() TREATAS() USERELATIONSHIP() Answer: TREATAS() applies the result of a table expression as filters to columns from an unrelated table. USERELATIONSHIP() activates different physical relationships between tables during a query execution. CROSSFILTER() defines the cross filtering direction of a physical relationship. PATH() returns a string of all the members in the column hierarchy. \ud83d\udccc You have a Fabric workspace that contains a lakehouse named Lakehouse1. A user named User1 plans to use Lakehouse explorer to read Lakehouse1 data. You need to assign a workspace role to User1. The solution must follow the principle of least privilege. Which workspace role should you assign to User1? Admin Contributor Member Viewer Answer: To read the data from a Fabric lakehouse by using Lakehouse explorer, users must be assigned roles of either Admin, Member, or Contributor. However, respecting the least privileged principle, a user must be assigned the Contributor role. The viewer role does not provide permission to read the lakehouse data through Lakehouse explorer. \ud83d\udccc You have a Fabric tenant that contains a workspace named Workspace1. Workspace1 contains a lakehouse, a data pipeline, a notebook, and several Microsoft Power BI reports. A user named User1 plans to use SQL to access the lakehouse to analyze data. User1 must have the following access: User1 must have read-only access to the lakehouse. User1 must NOT be able to access the rest of the items in Workspace1. User1 must NOT be able to use Spark to query the underlying files in the lakehouse. You need to configure access for User1. What should you do? Add User1 to the workspace as a member, share the lakehouse with User1, and select Read all SQL Endpoint data. Add User1 to the workspace as a viewer, share the lakehouse with User1, and select Read all SQL Endpoint data. Share the lakehouse with User1 directly and select Build reports on the default dataset. Share the lakehouse with User1 directly and select Read all SQL Endpoint data. Answer: Since the user only needs access to the lakehouse and not the other items in the workspace, you should share the lakehouse directly and select Read all SQL Endpoint data. The user should not be added as a member of the workspace. All members of the workspace, even viewers, will be able to open all Power BI reports in the workspace. The SQL analytics endpoint itself cannot be shared directly; the Share options only show for the lakehouse. \ud83d\udccc You use Microsoft Power BI Desktop to create a Power BI semantic model. You need to recommend a solution to collaborate with another Power BI modeler. The solution must ensure that you can both work on different parts of the model simultaneously. The solution must provide the most efficient and productive way to collaborate on the same model. What should you recommend? Save your work as a PBIX file and email the file to the other modeler. Save your work as a PBIX file and publish the file to a Fabric workspace. Add the other modeler as member to the workspace. Save your work as a PBIX file to Microsoft OneDrive and share the file with the other modeler. Save your work as a Power BI Project (PBIP). Initialize a Git repository with version control. Answer: Saving your Power BI work as a PBIP enables you to save the work as individual plain text files in a simple, intuitive folder structure, which can be checked into a source control system such as Git. This will enable multiple developers to work on different parts of the model simultaneously. Emailing a Power BI model back and forth is not efficient for collaboration. Saving a Power BI model as a PBIX file to OneDrive eases developers access, but only one developer can have the file open at time. Publishing a PBIX file to a shared workspace does not allow multiple developers to work on the model simultaneously. \ud83d\udccc **You have a semantic model that pulls data from an Azure SQL database and is synced via Fabric deployment pipelines to three workspaces named Development, Test, and Production. You need to reduce the size of the dataset. Which DAX function should you use to remove unused columns?** SELECTCOLUMNS() KEEPFILTERS() ADDCOLUMNS() REMOVECOLUMNS() Answer: Use the SELECTCOLUMNS() function to select columns from a table while preserving the table structure. REMOVECOLUMNS() also removes columns from a table, but should be used only for columns that are not referenced anywhere else in the semantic model. \ud83d\udccc What is the most cost-effective approach to move data from Azure Data Lake Storage to a lakehouse in Fabric? Use the built-in Data Factory in Fabric to move the data. Use a Dataflow Gen2 to move the data. Use a Power BI Dataflow to move the data. Use a Spark Notebook to move the data. Answer: Using a built-in Data Factory in Fabric can be a cost-effective approach to move the data, providing an integrated and seamless way to handle data transfer within the same platform. You are planning a Fabric analytics solution. You need to recommend a licensing strategy to support 10 Microsoft Power BI report authors and 600 report consumers. The solution must use Dataflow Gen2 for data ingestion and minimize costs. Which Fabric license type should you recommend? Select only one answer. F16 F32 F64 Premium Per User (PPU) Answer: While F32 and F16 license types will provide all the necessary set of features, these licenses are not cost-efficient because report consumers require a Pro or PPU license. Starting with the F64 license, report consumers can use a free per-user license. PPU is incorrect, because you cannot create non-Power BI items (in this case Dataflow Gen2) with PPU. You are planning a Fabric analytics solution for the following users: 2,000 Microsoft Power BI consumers without an individual Power BI Pro license. 32 Power BI modelers with an individual Power BI Pro license. 16 data scientists You need to recommend a Fabric capacity SKU. The solution must minimize costs. What should you recommend? Select only one answer. F2 F2048 F32 F64 Answer: F64 is the smallest Fabric capacity (equivalent to a P1 Power BI Premium capacity) that supports premium Fabric workspaces and does not require Power BI report consumers to have individual Power BI Pro licenses. F2 and F32 are incorrect since they require that the 2,000 employees have individual Power BI Pro licenses to consume Power BI content. F2048 is incorrect since it is not the smallest capacity that meets the stated requirements. You are planning the configuration of a new Fabric tenant. You need to recommend a solution to ensure that reports meet the following requirements: Require authentication for embedded reports. Allow only read-only (live) connections against Fabric capacity cloud semantic models. Which two actions should you recommend performing from the Fabric admin portal? Each correct answer presents part of the solution. Select all answers that apply. From Capacity settings, set XMLA Endpoint to Read Write. From Embed Codes, delete all existing codes. From Premium Per User, set XMLA Endpoint to Off. From Tenant settings, disable Allow XMLA endpoints and Analyze in Excel with on-premises semantic models. From Tenant settings, disable Publish to web. Answer: Disabling Publish to web disables the ability to publish any unsecured (no login required) reports to any embedded location. Disabling XMLA Endpoints ensures that semantic models can be connected to, but not edited directly in, workspaces. You have a new Fabric tenant. You need to recommend a workspace architecture to meet best practices for content distribution and data governance. Which two actions should you recommend? Each correct answer presents part of the solution. Select all answers that apply. Create a copy of each semantic model in each workspace. Create direct query semantic models in each workspace. Place semantic models and reports in separate workspaces. Place semantic models and reports in the same workspace. Reuse shared semantic models for multiple reports. Answer: Using shared semantic models for multiple reports enables the reusability of items, while placing semantic models and reports in separate workspaces ensures that the data governance recommended practices are in place. You use Microsoft Power BI Desktop to create a Power BI semantic model. You need to recommend a solution to collaborate with another Power BI modeler. The solution must ensure that you can both work on different parts of the model simultaneously. The solution must provide the most efficient and productive way to collaborate on the same model. What should you recommend? Save your work as a PBIX file and email the file to the other modeler. Save your work as a PBIX file and publish the file to a Fabric workspace. Add the other modeler as member to the workspace. Save your work as a PBIX file to Microsoft OneDrive and share the file with the other modeler. Save your work as a Power BI Project (PBIP). Initialize a Git repository with version control. Answer: Saving your Power BI work as a PBIP enables you to save the work as individual plain text files in a simple, intuitive folder structure, which can be checked into a source control system such as Git. This will enable multiple developers to work on different parts of the model simultaneously. Emailing a Power BI model back and forth is not efficient for collaboration. Saving a Power BI model as a PBIX file to OneDrive eases developers access, but only one developer can have the file open at time. Publishing a PBIX file to a shared workspace does not allow multiple developers to work on the model simultaneously. You have a Fabric tenant that has XMLA Endpoint set to Read Write. You need to use the XMLA endpoint to deploy changes to only one table from the data model. What is the main limitation of using XMLA endpoints for the Microsoft Power BI deployment process? Select only one answer. A PBIX file cannot be downloaded from the Power BI service. Only the user that deployed the report can make changes. Table partitioning is impossible. You cannot use parameters for incremental refresh. Answer: Whenever the semantic model is deployed/changed by using XMLA endpoints, there is no possibility to download the PBIX file from the Power BI service. This means that no one can download the PBIX file (even the user who deployed the report). Table partitioning, as well as using parameters, is still supported, thus doesn\u2019t represent a limitation. You have an Azure SQL database that contains a customer dimension table. The table contains two columns named CustomerID and CustomerCompositeKey. You have a Fabric workspace that contains a Dataflow Gen2 query that connects to the database. You need to use Dataflows Query Editor to identify which of the two columns contains non-duplicate values per customer. Which option should you use? Select only one answer. Column distribution \u2013 distinct values Column distribution \u2013 unique values Column profile \u2013 values count Column quality \u2013 valid values Answer: Only the distinct values displayed under Column distribution will show the true number of rows of values that are distinct (one row per value). The count of unique only shows the number of distinct values that are in the first 1,000 rows, and the other two options do not review uniqueness You have a Fabric workspace that contains a lakehouse named Lakehouse1. A user named User1 plans to use Lakehouse explorer to read Lakehouse1 data. You need to assign a workspace role to User1. The solution must follow the principle of least privilege. Which workspace role should you assign to User1? Select only one answer. Admin Contributor Member Viewer To read the data from a Fabric lakehouse by using Lakehouse explorer, users must be assigned roles of either Admin, Member, or Contributor. However, respecting the least privileged principle, a user must be assigned the Contributor role. The viewer role does not provide permission to read the lakehouse data through Lakehouse explorer. Question: You are developing a large semantic model. You have a fact table that contains 500 million rows. Most analytic queries will target aggregated data, but some users must still be able to view data on a detailed level. You plan to create a composite model and implement user-defined aggregations. Which three storage modes should you use for each type of table? Each correct answer presents part of the solution. Select all answers that apply. - Aggregated tables should use Dual mode. - Aggregated tables should use Import mode. - The detailed fact table should use DirectQuery mode. - The detailed fact table should use Import mode. - Dimension tables should use DirectQuery mode. - Dimension tables should use Dual mode. Answer: - Aggregated tables should use Import mode. - The detailed fact table should use DirectQuery mode. - Dimension tables should use Dual mode. Explanation: When using user-defined aggregations, the detailed fact table must be in DirectQuery mode. It is recommended to set the storage mode to Import for aggregated tables because of the performance, while dimension tables should be set to Dual mode to avoid the limitations of limited relationships Question : You have a Microsoft Power BI report that contains a bar chart visual. You need to ensure that users can change the y-axis category of the bar chart by using a slicer selection. Which Power BI feature should you add? calculation groups drillthrough field parameters WhatIf parameters Answer : field parameters Explanation: Field parameters allow users to change between columns that can be used on the categorical axis of visuals. All other options do not grant this ability","title":"Fabric Q&A"},{"location":"Microsoft-Fabric/FabricQ%26A/#answer-while-f32-and-f16-license-types-will-provide-all-the-necessary-set-of-features-these-licenses-are-not-cost-efficient-because-report-consumers-require-a-pro-or-ppu-license-starting-with-the-f64-license-report-consumers-can-use-a-free-per-user-license-ppu-is-incorrect-because-you-cannot-create-non-power-bi-items-in-this-case-dataflow-gen2-with-ppu","text":"You are planning a Fabric analytics solution for the following users: 2,000 Microsoft Power BI consumers without an individual Power BI Pro license. 32 Power BI modelers with an individual Power BI Pro license. 16 data scientists You need to recommend a Fabric capacity SKU. The solution must minimize costs. What should you recommend? Select only one answer. F2 F2048 F32 F64","title":"Answer: While F32 and F16 license types will provide all the necessary set of features, these licenses are not cost-efficient because report consumers require a Pro or PPU license. Starting with the F64 license, report consumers can use a free per-user license. PPU is incorrect, because you cannot create non-Power BI items (in this case Dataflow Gen2) with PPU."},{"location":"Microsoft-Fabric/FabricQ%26A/#answer-f64-is-the-smallest-fabric-capacity-equivalent-to-a-p1-power-bi-premium-capacity-that-supports-premium-fabric-workspaces-and-does-not-require-power-bi-report-consumers-to-have-individual-power-bi-pro-licenses-f2-and-f32-are-incorrect-since-they-require-that-the-2000-employees-have-individual-power-bi-pro-licenses-to-consume-power-bi-content-f2048-is-incorrect-since-it-is-not-the-smallest-capacity-that-meets-the-stated-requirements","text":"You are planning the configuration of a new Fabric tenant. You need to recommend a solution to ensure that reports meet the following requirements: Require authentication for embedded reports. Allow only read-only (live) connections against Fabric capacity cloud semantic models. Which two actions should you recommend performing from the Fabric admin portal? Each correct answer presents part of the solution. Select all answers that apply. From Capacity settings, set XMLA Endpoint to Read Write. From Embed Codes, delete all existing codes. From Premium Per User, set XMLA Endpoint to Off. From Tenant settings, disable Allow XMLA endpoints and Analyze in Excel with on-premises semantic models. From Tenant settings, disable Publish to web. Answer: Disabling Publish to web disables the ability to publish any unsecured (no login required) reports to any embedded location. Disabling XMLA Endpoints ensures that semantic models can be connected to, but not edited directly in, workspaces. You have a new Fabric tenant. You need to recommend a workspace architecture to meet best practices for content distribution and data governance. Which two actions should you recommend? Each correct answer presents part of the solution. Select all answers that apply. Create a copy of each semantic model in each workspace. Create direct query semantic models in each workspace. Place semantic models and reports in separate workspaces. Place semantic models and reports in the same workspace. Reuse shared semantic models for multiple reports. Answer: Using shared semantic models for multiple reports enables the reusability of items, while placing semantic models and reports in separate workspaces ensures that the data governance recommended practices are in place. You use Microsoft Power BI Desktop to create a Power BI semantic model. You need to recommend a solution to collaborate with another Power BI modeler. The solution must ensure that you can both work on different parts of the model simultaneously. The solution must provide the most efficient and productive way to collaborate on the same model. What should you recommend? Save your work as a PBIX file and email the file to the other modeler. Save your work as a PBIX file and publish the file to a Fabric workspace. Add the other modeler as member to the workspace. Save your work as a PBIX file to Microsoft OneDrive and share the file with the other modeler. Save your work as a Power BI Project (PBIP). Initialize a Git repository with version control. Answer: Saving your Power BI work as a PBIP enables you to save the work as individual plain text files in a simple, intuitive folder structure, which can be checked into a source control system such as Git. This will enable multiple developers to work on different parts of the model simultaneously. Emailing a Power BI model back and forth is not efficient for collaboration. Saving a Power BI model as a PBIX file to OneDrive eases developers access, but only one developer can have the file open at time. Publishing a PBIX file to a shared workspace does not allow multiple developers to work on the model simultaneously. You have a Fabric tenant that has XMLA Endpoint set to Read Write. You need to use the XMLA endpoint to deploy changes to only one table from the data model. What is the main limitation of using XMLA endpoints for the Microsoft Power BI deployment process? Select only one answer. A PBIX file cannot be downloaded from the Power BI service. Only the user that deployed the report can make changes. Table partitioning is impossible. You cannot use parameters for incremental refresh. Answer: Whenever the semantic model is deployed/changed by using XMLA endpoints, there is no possibility to download the PBIX file from the Power BI service. This means that no one can download the PBIX file (even the user who deployed the report). Table partitioning, as well as using parameters, is still supported, thus doesn\u2019t represent a limitation. You have an Azure SQL database that contains a customer dimension table. The table contains two columns named CustomerID and CustomerCompositeKey. You have a Fabric workspace that contains a Dataflow Gen2 query that connects to the database. You need to use Dataflows Query Editor to identify which of the two columns contains non-duplicate values per customer. Which option should you use? Select only one answer. Column distribution \u2013 distinct values Column distribution \u2013 unique values Column profile \u2013 values count Column quality \u2013 valid values Answer: Only the distinct values displayed under Column distribution will show the true number of rows of values that are distinct (one row per value). The count of unique only shows the number of distinct values that are in the first 1,000 rows, and the other two options do not review uniqueness You have a Fabric workspace that contains a lakehouse named Lakehouse1. A user named User1 plans to use Lakehouse explorer to read Lakehouse1 data. You need to assign a workspace role to User1. The solution must follow the principle of least privilege. Which workspace role should you assign to User1? Select only one answer. Admin Contributor Member Viewer To read the data from a Fabric lakehouse by using Lakehouse explorer, users must be assigned roles of either Admin, Member, or Contributor. However, respecting the least privileged principle, a user must be assigned the Contributor role. The viewer role does not provide permission to read the lakehouse data through Lakehouse explorer. Question: You are developing a large semantic model. You have a fact table that contains 500 million rows. Most analytic queries will target aggregated data, but some users must still be able to view data on a detailed level. You plan to create a composite model and implement user-defined aggregations. Which three storage modes should you use for each type of table? Each correct answer presents part of the solution. Select all answers that apply. - Aggregated tables should use Dual mode. - Aggregated tables should use Import mode. - The detailed fact table should use DirectQuery mode. - The detailed fact table should use Import mode. - Dimension tables should use DirectQuery mode. - Dimension tables should use Dual mode. Answer: - Aggregated tables should use Import mode. - The detailed fact table should use DirectQuery mode. - Dimension tables should use Dual mode. Explanation: When using user-defined aggregations, the detailed fact table must be in DirectQuery mode. It is recommended to set the storage mode to Import for aggregated tables because of the performance, while dimension tables should be set to Dual mode to avoid the limitations of limited relationships Question : You have a Microsoft Power BI report that contains a bar chart visual. You need to ensure that users can change the y-axis category of the bar chart by using a slicer selection. Which Power BI feature should you add? calculation groups drillthrough field parameters WhatIf parameters Answer : field parameters Explanation: Field parameters allow users to change between columns that can be used on the categorical axis of visuals. All other options do not grant this ability","title":"Answer: F64 is the smallest Fabric capacity (equivalent to a P1 Power BI Premium capacity) that supports premium Fabric workspaces and does not require Power BI report consumers to have individual Power BI Pro licenses. F2 and F32 are incorrect since they require that the 2,000 employees have individual Power BI Pro licenses to consume Power BI content. F2048 is incorrect since it is not the smallest capacity that meets the stated requirements."},{"location":"Microsoft-Fabric/FabricSparkStreaming/","text":"Microsoft Fabric - Delta Tables - Spark Streaming Delta Lake supports streaming data, allowing Delta tables to serve as both sinks and sources. In this example, I will demonstrate how to use a Delta Lake table in Microsoft Fabric for Spark streaming. Steps: Make a few copies of the sample source JSON file. Place one file inside the Files folder. Create a subfolder for aesthetics, such as Files/FolderA/ . Place the JSON file inside this subfolder. In a notebook, run the following code: from pyspark.sql.types import * from pyspark.sql.functions import * # Define the schema for the JSON data jsonSchema = StructType([ StructField(\"device\", StringType(), False), StructField(\"status\", StringType(), False) ]) # Create a stream that reads data from JSON files in the folder iotstream = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(\"Files/FolderA/SSF\") print(\"Source stream created...\") # Write the stream to a Delta table. The table will be created automatically deltastream = iotstream.writeStream.format(\"delta\").option(\"checkpointLocation\", \"Files/FolderA/ChkPT\").start(\"Tables/TableDLTS\") print(\"Streaming to delta sink...\") # Keep the stream running. Note: This will need to be stopped manually as it will continue to add new rows. deltastream.awaitTermination() Now, add more json files and see the Delta table grow! How It Works The source code continuously runs, first creating rows in the Delta Lake table from the JSON files present in the source folder. As new files are added, the code continues to append new rows to the Delta table.","title":"Spark Streaming"},{"location":"Microsoft-Fabric/FabricSparkStreaming/#microsoft-fabric-delta-tables-spark-streaming","text":"Delta Lake supports streaming data, allowing Delta tables to serve as both sinks and sources. In this example, I will demonstrate how to use a Delta Lake table in Microsoft Fabric for Spark streaming.","title":"Microsoft Fabric - Delta Tables - Spark Streaming"},{"location":"Microsoft-Fabric/FabricSparkStreaming/#steps","text":"Make a few copies of the sample source JSON file. Place one file inside the Files folder. Create a subfolder for aesthetics, such as Files/FolderA/ . Place the JSON file inside this subfolder. In a notebook, run the following code: from pyspark.sql.types import * from pyspark.sql.functions import * # Define the schema for the JSON data jsonSchema = StructType([ StructField(\"device\", StringType(), False), StructField(\"status\", StringType(), False) ]) # Create a stream that reads data from JSON files in the folder iotstream = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(\"Files/FolderA/SSF\") print(\"Source stream created...\") # Write the stream to a Delta table. The table will be created automatically deltastream = iotstream.writeStream.format(\"delta\").option(\"checkpointLocation\", \"Files/FolderA/ChkPT\").start(\"Tables/TableDLTS\") print(\"Streaming to delta sink...\") # Keep the stream running. Note: This will need to be stopped manually as it will continue to add new rows. deltastream.awaitTermination() Now, add more json files and see the Delta table grow!","title":"Steps:"},{"location":"Microsoft-Fabric/FabricSparkStreaming/#how-it-works","text":"The source code continuously runs, first creating rows in the Delta Lake table from the JSON files present in the source folder. As new files are added, the code continues to append new rows to the Delta table.","title":"How It Works"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/","text":"Microsoft Fabric What?! Let's learn some basics about LakeHouse Pre-requisites for a LakeHouse Shortcuts in Lakehouse Ways to ingest data into Lakehouse Ways to transform data in Fabric Lakehouse Ways to Visualize Let's create a Fabric Lakehouse Create a LakeHouse Workspace Create a LakeHouse Upload a simple excel to the LakeHouse Load the excel data into a table Query the excel file table using SQL Write a normal SQL query using Editor Write a Visual Query Create a Report Connect external data using shortcuts Apache Spark In Microsoft Fabric Run a Spark Notebook Run a simple Pyspark code in Notebook Create a Spark job definition Delta Lake in Microsoft Fabric Delta Lake Tables Using df.write Using API - DeltaTableBuilder Using Spark SQL No table - Just Delta Files Use time travel Delta Lake - Spark Streaming Delta table - streaming source Delta table - streaming sink Microsoft Fabric What?! One-stop low-to-no-code platform. To understand Fabric, first understand OneLake. OneLake is Fabric's Database. Each fabric tenant gets OneOneLake by default. OneLake ~ OneDrive . OneLake is cover over ADLS . Either importa data in OneLake or Create Shortcuts to external data. Default storage us Delta. Let's learn some basics about LakeHouse Lake house is Data Lake and Warehouse. The base of Fabric is Data lakehouse. Features of Lakehouse: Lakehous is Data Lake + Warehouse. Flexibility of storing data in lake and SQL query like warehouse! Pre-requisites for a LakeHouse Before you can create a lakehouse, you create a workspace in the Microsoft Fabric platform. You create and configure a new lakehouse in the Data Engineering workload. Each L produces three named items in the Fabric-enabled workspace: Lakehouse is the lakehouse storage and metadata, where you interact with files, folders, and table data. Semantic model (default) is an automatically created data model based on the tables in the lakehouse. Power BI reports can be built from the semantic model. SQL Endpoint is a read-only SQL endpoint through which you can connect and query data with Transact-SQL. You can work with the data in the lakehouse in two modes: Lakehouse enables you to add and interact with tables, files, and folders in the lakehouse. SQL analytics endpoint enables you to use SQL to query the tables in the lakehouse and manage its relational data model. Shortcuts in Lakehouse Shortcuts enable you to integrate data into your lakehouse while keeping it stored in external storage. Shortcuts can be created in both lakehouses and KQL databases, and appear as a folder in the lake. Spark, SQL, Real-Time Analytics, and Analysis Services can access data via shortcuts when querying data. Shortcuts have limited data source connectors, so when you can't use shortcuts, you can ingest data directly into your lakehouse. Source data permissions and credentials are all managed by OneLake. Ways to ingest data into Lakehouse Upload: Upload local files or folders to the lakehouse. You can then explore and process the file data, and load the results into tables. Dataflows (Gen2): Import and transform data from a range of sources using Power Query Online, and load it directly into a table in the lakehouse. Notebooks: Use notebooks in Fabric to ingest and transform data, and load it into tables or files in the lakehouse. Data Factory pipelines: Copy data and orchestrate data processing activities, loading the results into tables or files in the lakehouse. Ways to transform data in Fabric Lakehouse Apache Spark: Pyspark, SparkSQL, Notebooks, Spark job definitions SQL analytic endpoint: Transact SQL Dataflows (Gen2): Power Query Data pipelines: Ways to Visualize Create an end-to-end solution using Power BI and Fabric Lakehouse. Let's create a Fabric Lakehouse Create a LakeHouse Workspace Go to https://app.fabric.microsoft.com. Select Synapse Data Engineering Create a LakeHouse Click on Create then Lakehouse. Give it any name. Fabric will create everything automatically and you will have your Lakehouse Upload a simple excel to the LakeHouse Download and save sample excel file from here https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/sales.csv Go to Explorer, create a Data subfolder under Files, then upload the downloaded csv in it Load the excel data into a table Now, if you want to use SQL you need to import the excel into a table. It is pretty simple. Just click on the elipses next to the excel file and select Load to Tables Once loaded you can see your table in tabular format Query the excel file table using SQL Write a normal SQL query using Editor Write a Visual Query Create a Report At the bottom of the SQL Endpoint page, click on the Model tab to view the data model schema for the semantic model. Navigate to the Reporting tab in the menu ribbon and select New report to open a new browser tab for report design. In the Data pane, expand the sales table and select Item and Quantity . This will add a table visualization to your report. Hide the Data and Filters panes to create more space. Change the visualization to a Clustered bar chart and resize it. Save the report by selecting Save from the File menu and name it Item Sales Report . Close the report tab and return to the SQL endpoint page. In the hub menu, select your workspace to verify it contains your lakehouse, the SQL analytics endpoint, a default semantic model, and the Item Sales Report . Connect external data using shortcuts It is easy to import data into Fabric. But, what if the data wants to stay outside? Then we create shortcuts to that external data in Lakehouse. It appears like a folder. Lets create a shortcut to dataverse. Click on the cli Note: The region of Dataverse and Fabric should be same. Apache Spark In Microsoft Fabric Spark divides and conquers : It splits a large job across computers. SparkContext does the splitting etc. Spark can use many languages, but in Industry PySpark and Spark SQL are most used. In Fabric. One Workspace gets One Spark Cluster. Run a Spark Notebook Run a simple Pyspark code in Notebook Here, the spark session is already created. All you have to do is create dataframe and start coding! Create a Spark job definition Access the Spark Job Definition Page. Create a PySpark Job Definition: Develop a main definition file named anyname.py . The file should include the following code: ```python from pyspark.sql import SparkSession # This code executes only when the .py file is run directly. if name == \" main \": # Initialize a Spark session specifically for this job. spark = SparkSession.builder.appName(\"Sales Aggregation\").getOrCreate() # Read data from a CSV file into a DataFrame. df = spark.read.csv('Files/data/sales.csv', header=True, inferSchema=True) # Write the DataFrame to a Delta table, overwriting existing data. df.write.mode('overwrite').format('delta').save('Files/data/delta') ``` Upload and Schedule the File: Delta Lake in Microsoft Fabric Delta lake is just Data Lake with a SQL Cover . In Fabric, any table imported from .csv/excel etc. automatically becomes a Delta Lake table. For these tables, you'll find .parquet files and delta log folders when you view the files. Since every table is automatically a Delta Lake table, this is a very useful feature. There's no need to convert files into Delta tables separately. Delta Lake Tables Using df.write Managed table : df . write . format (\" delta \") . saveAsTable (\" tableName \") External table : df . write . format (\" delta \") . saveAsTable (\" tableName \", path=\" Files/folderX \") Using API - DeltaTableBuilder Managed table : from delta.tables import * DeltaTable.create(spark) \\ .tableName(\"Planet\") \\ .addColumn(\"Size\", \"INT\") \\ .addColumn(\"Name\", \"STRING\") \\ .execute() Using Spark SQL Create Managed table : CREATE TABLE salesorders ( Orderid INT NOT NULL, OrderDate TIMESTAMP NOT NULL, CustomerName STRING, SalesTotal FLOAT NOT NULL ) USING DELTA; Create External table : CREATE TABLE MyExternalTable USING DELTA LOCATION 'Files/mydata'; Insert rows: This is the most common way: spark.sql(\"INSERT INTO products VALUES (1, 'Widget', 'Accessories', 2.99)\") Insert rows - %%sql magic: %%sql UPDATE products SET Price = 2.6 WHERE ProductId = 1; No table - Just Delta Files To just save the dataframe as delta format. No table created. df.write.format(\"delta\").mode(\"overwrite / append\").save(\"Folder/Path\") After running the code you will see a folder with: parquet files _delta_log sub-folder: Later you can create an DeltaTable from the folder and modify it: from delta.tables import * from pyspark.sql.functions import * # Create a DeltaTable object delta_path = \"Files/mytable\" deltaTable = DeltaTable.forPath(spark, delta_path) # Update the table deltaTable.update( condition = \"Category == 'Accessories'\", set = { \"Price\": \"Price * 0.9\" }) Use time travel The command will show all the transactions to the table: %%sql DESCRIBE HISTORY products For a specifc version: df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path) For a specific date: df = spark.read.format(\"delta\").option(\"timestampAsOf\", '2022-01-01').load(delta_path) Delta Lake - Spark Streaming Delta table - streaming source Here a delta table stores internet sales data. When new data added a stream is created: from pyspark.sql.types import * from pyspark.sql.functions import * # Load a streaming dataframe from the Delta Table stream_df = spark.readStream.format(\"delta\") \\ .option(\"ignoreChanges\", \"true\") \\ .load(\"Files/delta/internetorders\") # Now you can process the streaming data in the dataframe # for example, show it: stream_df.show() Delta table - streaming sink Here a folder has JSON files. As new JSON files are added. The contents are added to a Delta Lake table: from pyspark.sql.types import * from pyspark.sql.functions import * # Create a stream that reads JSON data from a folder inputPath = 'Files/streamingdata/' jsonSchema = StructType([ StructField(\"device\", StringType(), False), StructField(\"status\", StringType(), False) ]) stream_df = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath) # Write the stream to a delta table table_path = 'Files/delta/devicetable' checkpoint_path = 'Files/delta/checkpoint' delta_stream = stream_df.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpoint_path).start(table_path) To stop writing to the delta lake table use stop: delta_stream.stop()","title":"Hello Fabric"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#microsoft-fabric-what","text":"One-stop low-to-no-code platform. To understand Fabric, first understand OneLake. OneLake is Fabric's Database. Each fabric tenant gets OneOneLake by default. OneLake ~ OneDrive . OneLake is cover over ADLS . Either importa data in OneLake or Create Shortcuts to external data. Default storage us Delta.","title":"Microsoft Fabric What?!"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#lets-learn-some-basics-about-lakehouse","text":"Lake house is Data Lake and Warehouse. The base of Fabric is Data lakehouse. Features of Lakehouse: Lakehous is Data Lake + Warehouse. Flexibility of storing data in lake and SQL query like warehouse!","title":"Let's learn some basics about LakeHouse"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#pre-requisites-for-a-lakehouse","text":"Before you can create a lakehouse, you create a workspace in the Microsoft Fabric platform. You create and configure a new lakehouse in the Data Engineering workload. Each L produces three named items in the Fabric-enabled workspace: Lakehouse is the lakehouse storage and metadata, where you interact with files, folders, and table data. Semantic model (default) is an automatically created data model based on the tables in the lakehouse. Power BI reports can be built from the semantic model. SQL Endpoint is a read-only SQL endpoint through which you can connect and query data with Transact-SQL. You can work with the data in the lakehouse in two modes: Lakehouse enables you to add and interact with tables, files, and folders in the lakehouse. SQL analytics endpoint enables you to use SQL to query the tables in the lakehouse and manage its relational data model.","title":"Pre-requisites for a LakeHouse"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#shortcuts-in-lakehouse","text":"Shortcuts enable you to integrate data into your lakehouse while keeping it stored in external storage. Shortcuts can be created in both lakehouses and KQL databases, and appear as a folder in the lake. Spark, SQL, Real-Time Analytics, and Analysis Services can access data via shortcuts when querying data. Shortcuts have limited data source connectors, so when you can't use shortcuts, you can ingest data directly into your lakehouse. Source data permissions and credentials are all managed by OneLake.","title":"Shortcuts in Lakehouse"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#ways-to-ingest-data-into-lakehouse","text":"Upload: Upload local files or folders to the lakehouse. You can then explore and process the file data, and load the results into tables. Dataflows (Gen2): Import and transform data from a range of sources using Power Query Online, and load it directly into a table in the lakehouse. Notebooks: Use notebooks in Fabric to ingest and transform data, and load it into tables or files in the lakehouse. Data Factory pipelines: Copy data and orchestrate data processing activities, loading the results into tables or files in the lakehouse.","title":"Ways to ingest data into Lakehouse"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#ways-to-transform-data-in-fabric-lakehouse","text":"Apache Spark: Pyspark, SparkSQL, Notebooks, Spark job definitions SQL analytic endpoint: Transact SQL Dataflows (Gen2): Power Query Data pipelines:","title":"Ways to transform data in Fabric Lakehouse"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#ways-to-visualize","text":"Create an end-to-end solution using Power BI and Fabric Lakehouse.","title":"Ways to Visualize"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#lets-create-a-fabric-lakehouse","text":"","title":"Let's create a Fabric Lakehouse"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#create-a-lakehouse-workspace","text":"Go to https://app.fabric.microsoft.com. Select Synapse Data Engineering","title":"Create a LakeHouse Workspace"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#create-a-lakehouse","text":"Click on Create then Lakehouse. Give it any name. Fabric will create everything automatically and you will have your Lakehouse","title":"Create a LakeHouse"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#upload-a-simple-excel-to-the-lakehouse","text":"Download and save sample excel file from here https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/sales.csv Go to Explorer, create a Data subfolder under Files, then upload the downloaded csv in it","title":"Upload a simple excel to the LakeHouse"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#load-the-excel-data-into-a-table","text":"Now, if you want to use SQL you need to import the excel into a table. It is pretty simple. Just click on the elipses next to the excel file and select Load to Tables Once loaded you can see your table in tabular format","title":"Load the excel data into a table"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#query-the-excel-file-table-using-sql","text":"","title":"Query the excel file table using SQL"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#write-a-normal-sql-query-using-editor","text":"","title":"Write a normal SQL query using Editor"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#write-a-visual-query","text":"","title":"Write a Visual Query"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#create-a-report","text":"At the bottom of the SQL Endpoint page, click on the Model tab to view the data model schema for the semantic model. Navigate to the Reporting tab in the menu ribbon and select New report to open a new browser tab for report design. In the Data pane, expand the sales table and select Item and Quantity . This will add a table visualization to your report. Hide the Data and Filters panes to create more space. Change the visualization to a Clustered bar chart and resize it. Save the report by selecting Save from the File menu and name it Item Sales Report . Close the report tab and return to the SQL endpoint page. In the hub menu, select your workspace to verify it contains your lakehouse, the SQL analytics endpoint, a default semantic model, and the Item Sales Report .","title":"Create a Report"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#connect-external-data-using-shortcuts","text":"It is easy to import data into Fabric. But, what if the data wants to stay outside? Then we create shortcuts to that external data in Lakehouse. It appears like a folder. Lets create a shortcut to dataverse. Click on the cli Note: The region of Dataverse and Fabric should be same.","title":"Connect external data using shortcuts"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#apache-spark-in-microsoft-fabric","text":"Spark divides and conquers : It splits a large job across computers. SparkContext does the splitting etc. Spark can use many languages, but in Industry PySpark and Spark SQL are most used. In Fabric. One Workspace gets One Spark Cluster.","title":"Apache Spark In Microsoft Fabric"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#run-a-spark-notebook","text":"","title":"Run a Spark Notebook"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#run-a-simple-pyspark-code-in-notebook","text":"Here, the spark session is already created. All you have to do is create dataframe and start coding!","title":"Run a simple Pyspark code in Notebook"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#create-a-spark-job-definition","text":"Access the Spark Job Definition Page. Create a PySpark Job Definition: Develop a main definition file named anyname.py . The file should include the following code: ```python from pyspark.sql import SparkSession # This code executes only when the .py file is run directly. if name == \" main \": # Initialize a Spark session specifically for this job. spark = SparkSession.builder.appName(\"Sales Aggregation\").getOrCreate() # Read data from a CSV file into a DataFrame. df = spark.read.csv('Files/data/sales.csv', header=True, inferSchema=True) # Write the DataFrame to a Delta table, overwriting existing data. df.write.mode('overwrite').format('delta').save('Files/data/delta') ``` Upload and Schedule the File:","title":"Create a Spark job definition"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#delta-lake-in-microsoft-fabric","text":"Delta lake is just Data Lake with a SQL Cover . In Fabric, any table imported from .csv/excel etc. automatically becomes a Delta Lake table. For these tables, you'll find .parquet files and delta log folders when you view the files. Since every table is automatically a Delta Lake table, this is a very useful feature. There's no need to convert files into Delta tables separately.","title":"Delta Lake in Microsoft Fabric"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#delta-lake-tables","text":"","title":"Delta Lake Tables"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#using-dfwrite","text":"Managed table : df . write . format (\" delta \") . saveAsTable (\" tableName \") External table : df . write . format (\" delta \") . saveAsTable (\" tableName \", path=\" Files/folderX \")","title":"Using df.write"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#using-api-deltatablebuilder","text":"Managed table : from delta.tables import * DeltaTable.create(spark) \\ .tableName(\"Planet\") \\ .addColumn(\"Size\", \"INT\") \\ .addColumn(\"Name\", \"STRING\") \\ .execute()","title":"Using API - DeltaTableBuilder"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#using-spark-sql","text":"Create Managed table : CREATE TABLE salesorders ( Orderid INT NOT NULL, OrderDate TIMESTAMP NOT NULL, CustomerName STRING, SalesTotal FLOAT NOT NULL ) USING DELTA; Create External table : CREATE TABLE MyExternalTable USING DELTA LOCATION 'Files/mydata'; Insert rows: This is the most common way: spark.sql(\"INSERT INTO products VALUES (1, 'Widget', 'Accessories', 2.99)\") Insert rows - %%sql magic: %%sql UPDATE products SET Price = 2.6 WHERE ProductId = 1;","title":"Using Spark SQL"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#no-table-just-delta-files","text":"To just save the dataframe as delta format. No table created. df.write.format(\"delta\").mode(\"overwrite / append\").save(\"Folder/Path\") After running the code you will see a folder with: parquet files _delta_log sub-folder: Later you can create an DeltaTable from the folder and modify it: from delta.tables import * from pyspark.sql.functions import * # Create a DeltaTable object delta_path = \"Files/mytable\" deltaTable = DeltaTable.forPath(spark, delta_path) # Update the table deltaTable.update( condition = \"Category == 'Accessories'\", set = { \"Price\": \"Price * 0.9\" })","title":"No table - Just Delta Files"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#use-time-travel","text":"The command will show all the transactions to the table: %%sql DESCRIBE HISTORY products For a specifc version: df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path) For a specific date: df = spark.read.format(\"delta\").option(\"timestampAsOf\", '2022-01-01').load(delta_path)","title":"Use time travel"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#delta-lake-spark-streaming","text":"","title":"Delta Lake - Spark Streaming"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#delta-table-streaming-source","text":"Here a delta table stores internet sales data. When new data added a stream is created: from pyspark.sql.types import * from pyspark.sql.functions import * # Load a streaming dataframe from the Delta Table stream_df = spark.readStream.format(\"delta\") \\ .option(\"ignoreChanges\", \"true\") \\ .load(\"Files/delta/internetorders\") # Now you can process the streaming data in the dataframe # for example, show it: stream_df.show()","title":"Delta table - streaming source"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric/#delta-table-streaming-sink","text":"Here a folder has JSON files. As new JSON files are added. The contents are added to a Delta Lake table: from pyspark.sql.types import * from pyspark.sql.functions import * # Create a stream that reads JSON data from a folder inputPath = 'Files/streamingdata/' jsonSchema = StructType([ StructField(\"device\", StringType(), False), StructField(\"status\", StringType(), False) ]) stream_df = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath) # Write the stream to a delta table table_path = 'Files/delta/devicetable' checkpoint_path = 'Files/delta/checkpoint' delta_stream = stream_df.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpoint_path).start(table_path) To stop writing to the delta lake table use stop: delta_stream.stop()","title":"Delta table - streaming sink"},{"location":"Microsoft-Fabric/InspectingDataframes/","text":"Whats in your df? Find out quick. Firs step - Use the display() Function Table View Chart View Creating a Power BI Report Microsoft Fabric DP-600 question Reference Whats in your df? Find out quick. When working with a dataframe, you often need to understand its structure, statistics, and null values - essentially, you want to 'profile' the dataset. Here's how you can do it quickly in Fabric notebook. Firs step - Use the display() Function The first step is to use the display() function. This will produce a rich table and chart view, displaying it directly in the output. display(df) Once you have displayed your df using df(). You can use the Table view Chart view and Inspect(Table view) option. Lets explore them one by one. Table View The table view allows you to view the data in a tabular format. Follow these steps to inspect your dataframe: View the Data: The data is displayed in a table format. Click on the Inspect Button: This is a powerful tool for a detailed inspection of your dataframe. Explore Further: Use the Inspect button to delve deeper into your dataframe. Chart View The chart view provides a visual representation of your data. Here's how to get started: Switch to Chart View: Click on the chart view icon. Automatic Key-Value Pair: Fabric will automatically generate a key-value pair to get you started. Aggregate Data: You can then aggregate and analyze the data as needed. Creating a Power BI Report You can also create a real-time Power BI report using your dataframe with the following code: # Create a Power BI report object from spark dataframe from powerbiclient import QuickVisualize, get_dataset_config PBI_visualize = QuickVisualize(get_dataset_config(df)) # Render new report PBI_visualize Microsoft Fabric DP-600 question In your fabric Notebook you have a dataframe. You want to profile the dataframe - want to find the the columns, which columsn are empty etc etc. What would the easiest way to do it? Options: Create a pandas DataFrame first, and then visualize the data in an embedded Microsoft Power BI report by running QuickVisualize method on the pandas DataFrame. Display the DataFrame by running display(df), and then clicking the Inspect button. Display the DataDrame by running display(df), and then switching to chart view. Visualize the data in an embedded Microsoft Power BI report by running the QuickVisualize method on the DataFrame. Answer: Use the Inspect (Table view) option Reference Microsoft Reference","title":"Whats in your df?"},{"location":"Microsoft-Fabric/InspectingDataframes/#whats-in-your-df-find-out-quick","text":"When working with a dataframe, you often need to understand its structure, statistics, and null values - essentially, you want to 'profile' the dataset. Here's how you can do it quickly in Fabric notebook.","title":"Whats in your df? Find out quick."},{"location":"Microsoft-Fabric/InspectingDataframes/#firs-step-use-the-display-function","text":"The first step is to use the display() function. This will produce a rich table and chart view, displaying it directly in the output. display(df) Once you have displayed your df using df(). You can use the Table view Chart view and Inspect(Table view) option. Lets explore them one by one.","title":"Firs step - Use the display() Function"},{"location":"Microsoft-Fabric/InspectingDataframes/#table-view","text":"The table view allows you to view the data in a tabular format. Follow these steps to inspect your dataframe: View the Data: The data is displayed in a table format. Click on the Inspect Button: This is a powerful tool for a detailed inspection of your dataframe. Explore Further: Use the Inspect button to delve deeper into your dataframe.","title":"Table View"},{"location":"Microsoft-Fabric/InspectingDataframes/#chart-view","text":"The chart view provides a visual representation of your data. Here's how to get started: Switch to Chart View: Click on the chart view icon. Automatic Key-Value Pair: Fabric will automatically generate a key-value pair to get you started. Aggregate Data: You can then aggregate and analyze the data as needed.","title":"Chart View"},{"location":"Microsoft-Fabric/InspectingDataframes/#creating-a-power-bi-report","text":"You can also create a real-time Power BI report using your dataframe with the following code: # Create a Power BI report object from spark dataframe from powerbiclient import QuickVisualize, get_dataset_config PBI_visualize = QuickVisualize(get_dataset_config(df)) # Render new report PBI_visualize","title":"Creating a Power BI Report"},{"location":"Microsoft-Fabric/InspectingDataframes/#microsoft-fabric-dp-600-question","text":"In your fabric Notebook you have a dataframe. You want to profile the dataframe - want to find the the columns, which columsn are empty etc etc. What would the easiest way to do it? Options: Create a pandas DataFrame first, and then visualize the data in an embedded Microsoft Power BI report by running QuickVisualize method on the pandas DataFrame. Display the DataFrame by running display(df), and then clicking the Inspect button. Display the DataDrame by running display(df), and then switching to chart view. Visualize the data in an embedded Microsoft Power BI report by running the QuickVisualize method on the DataFrame. Answer: Use the Inspect (Table view) option","title":"Microsoft Fabric DP-600 question"},{"location":"Microsoft-Fabric/InspectingDataframes/#reference","text":"Microsoft Reference","title":"Reference"},{"location":"Microsoft-Fabric/KQL/","text":"What is KQL? How to run KQL query in Fabric? KQL vs SQL Databases KQL Vs SQL Query KQL vs SQL - DQL KQL Vs SQL - DDLs, DMLs \\& DQLs KQL Q\\&A KQL Questions - Keywords KQL Quesitons - General What is KQL? A KQL (Kusto Query Language) Database handles large volumes of structured, semi-structured, and unstructured data for real-time analytics and ad-hoc querying. It is part of the Azure Data Explorer service. The data in a KQL database is stored in Azure Data Explorer . It uses a columnar storage format, for high-performance. How to run KQL query in Fabric? There is no magic command like %%KQL KQL vs SQL Databases Feature KQL Database Standard SQL Database Query Language Kusto Query Language (KQL) Structured Query Language (SQL) Storage Format Columnar Row-based Optimized For Real-time analytics, log and time-series data Transactional data, relational data Data Structure Tables, columns, materialized views, functions Tables, columns, views, stored procedures Scalability Highly scalable and distributed Varies by implementation (SQL Server, MySQL, etc.) Indexing Automatically indexed for fast query performance Manual and automatic indexing Data Ingestion Supports batch and streaming ingestion Primarily batch ingestion Use Cases Log analytics, telemetry data, IoT data OLTP, data warehousing, reporting Storage Location Azure Data Explorer service in the cloud Varies (on-premises, cloud-based) Performance Optimized for read-heavy and analytical workloads Balanced for read and write operations Schema Flexible schema with support for semi-structured data Rigid schema with well-defined data types KQL Vs SQL Query KQL vs SQL - DQL Operation SQL KQL Select and Count SELECT Name, Age, COUNT(*) FROM Employees WHERE Age > 30 GROUP BY Name, Age; Employees \\| where Age > 30 \\| summarize count() by Name, Age Group By and Order By SELECT Department, AVG(Salary) AS AverageSalary FROM Employees GROUP BY Department ORDER BY AverageSalary DESC; Employees \\| summarize AverageSalary=avg(Salary) by Department \\| sort by AverageSalary desc Join SELECT e.Name, d.DepartmentName FROM Employees e JOIN Departments d ON e.DepartmentID = d.ID; Employees \\| join kind=inner (Departments) on $left.DepartmentID == $right.ID \\| project Name, DepartmentName Subquery and Limit SELECT Name FROM (SELECT * FROM Employees WHERE Age > 30) AS SubQuery WHERE DepartmentID = 5 LIMIT 10; let SubQuery = Employees \\| where Age > 30; SubQuery \\| where DepartmentID == 5 \\| project Name \\| take 10 String Functions SELECT Name FROM Employees WHERE UPPER(FirstName) = 'JOHN'; Employees \\| where tolower(FirstName) == 'john' \\| project Name Date Functions SELECT Name FROM Employees WHERE YEAR(HireDate) = 2020; Employees \\| where datetime_part('year', HireDate) == 2020 \\| project Name Between SELECT * FROM Employees WHERE Age BETWEEN 25 AND 35; Employees \\| where Age between (25 .. 35) Date Range SELECT * FROM Sales WHERE SaleDate BETWEEN '2021-01-01' AND '2021-12-31'; Sales \\| where SaleDate between (datetime(2021-01-01) .. datetime(2021-12-31)) Distinct SELECT DISTINCT Department FROM Employees; Employees \\| summarize by Department Top N SELECT TOP 5 Name, Salary FROM Employees ORDER BY Salary DESC; Employees \\| top 5 by Salary desc \\| project Name, Salary Aggregation with Conditions SELECT Department, COUNT(*) FROM Employees WHERE Age > 30 GROUP BY Department; Employees \\| where Age > 30 \\| summarize count() by Department KQL Vs SQL - DDLs, DMLs & DQLs Description Example Category Tables Create a new table .create table MyTable (Column1: string, Column2: int) DDL Show the schema of a table .show table MyTable schema DQL Ingest data into a table .ingest into table MyTable <DataSource> DML Rename a table .rename table OldTableName to NewTableName DDL Drop a table .drop table TableName DDL List all tables .show tables DQL Columns Add a column .alter table TableName add column ColumnName: DataType DDL Drop a column .alter table TableName drop column ColumnName DDL Rename a column .rename column OldColumnName to NewColumnName in table TableName DDL Functions Create a new function .create function with (docstring = \"Description\", folder = \"FolderName\") MyFunction () { <KQLQuery> } DDL Show available functions .show functions DQL Materialized Views Create a new materialized view .create materialized-view MyView on table MyTable { <KQLQuery> } DDL Show available materialized views .show materialized-views DQL Indexes Create an index .create index IndexName on TableName (ColumnName) DDL Drop an index .drop index IndexName on TableName DDL Show indexes .show indexes DQL Ingest Ingest data into a table .ingest into table MyTable <DataSource> DML Ingest data from JSON .ingest into table TableName h@\"https://path/to/file.json\" DML Database Operations Create a database .create database DatabaseName DDL Drop a database .drop database DatabaseName DDL List all databases .show databases DQL Permissions Grant table permissions .grant select on table TableName to UserName DDL Revoke table permissions .revoke select on table TableName from UserName DDL Show permissions .show table TableName policy access DQL Views Create a view .create view ViewName as <KQLQuery> DDL Drop a view .drop view ViewName DDL Show views .show views DQL Diagnostics Show cluster diagnostics .show cluster diagnostics DQL Show table statistics .show table TableName stats DQL Data Export Export data to JSON .export to json at <FilePath> <KQLQuery> DML KQL Q&A Highlight the answers to reveal it! KQL Questions - Keywords Which KQL keyword is used to limit the results of a query to a specified number of rows? A. select B. take C. project Answer : Take limits the results to a specified number of rows. Which KQL keyword is used to group and aggregate data? A. group_by B. aggregate C. summarize Answer : Use the summarize keyword to group and aggregate data. Which KQL keyword is used to filter rows based on a condition? A. where B. filter C. select Answer : where is used to filter rows based on a condition. Which KQL keyword is used to create a new column or modify an existing column? A. create B. extend C. modify Answer : extend is used to create a new column or modify an existing column. Which KQL keyword is used to sort the results of a query? A. order B. arrange C. sort Answer : sort is used to order the results of a query. Which KQL keyword is used to rename a column in the results? A. rename B. project-rename C. alias Answer : project-rename is used to rename a column in the results. Which KQL keyword is used to join two tables on a common column? A. merge B. union C. join Answer : join is used to combine two tables on a common column. Which KQL keyword is used to calculate the total number of rows in the results? A. count B. total C. sum Answer : count is used to calculate the total number of rows in the results. Which KQL keyword is used to remove duplicates from the results? A. distinct B. unique C. remove-duplicates Answer : distinct is used to remove duplicate rows from the results. Which KQL keyword is used to extract a substring from a string column? A. substring B. extract C. substr Answer : substring is used to extract a part of a string column. Which KQL keyword is used to combine the results of two or more queries? A. combine B. union C. join Answer : union is used to combine the results of two or more queries. Which KQL keyword is used to convert a column to a different data type? A. convert B. cast C. toType Answer : cast is used to convert a column to a different data type. Which KQL keyword is used to filter rows with null values? A. isnull B. isnotnull C. isnonempty Answer : isnotnull is used to filter rows with null values. Which KQL keyword is used to calculate the average of a numeric column? A. average B. mean C. avg Answer : avg is used to calculate the average of a numeric column. Which KQL keyword is used to create a time series chart? A. timeseries B. render C. chart Answer : render is used to create a time series chart. Which KQL keyword is used to specify the columns to include in the results? A. include B. select C. project Answer : project is used to specify the columns to include in the results. Which KQL keyword is used to calculate the maximum value of a numeric column? A. max B. maximum C. highest Answer : max is used to calculate the maximum value of a numeric column. Which KQL keyword is used to calculate the minimum value of a numeric column? A. min B. minimum C. lowest Answer : min is used to calculate the minimum value of a numeric column. Which KQL keyword is used to convert a datetime column to a specific format? A. format B. convert C. format_datetime Answer : format_datetime is used to convert a datetime column to a specific format. Which KQL keyword is used to calculate the difference between two datetime columns? A. datetime_diff B. date_diff C. time_diff Answer : date_diff is used to calculate the difference between two datetime columns. Which KQL keyword is used to filter rows based on a regular expression? A. regex_match B. matches_regex C. search Answer : matches_regex is used to filter rows based on a regular expression. Which KQL keyword is used to calculate the sum of a numeric column? A. sum B. total C. aggregate_sum Answer : sum is used to calculate the sum of a numeric column. Which KQL keyword is used to create a new table with the results of a query? A. create_table B. into C. output Answer : into is used to create a new table with the results of a query. Which KQL keyword is used to parse a string into multiple columns? A. split B. parse C. dissect Answer : parse is used to parse a string into multiple columns. Which KQL keyword is used to join two tables and keep only the rows with matching keys? A. inner join B. equijoin C. join Answer : join is used to join two tables and keep only the rows with matching keys. Which KQL keyword is used to create an alias for a column in the results? A. alias B. as C. rename Answer : as is used to create an alias for a column in the results. Which KQL keyword is used to filter rows based on a range of values? A. between B. in_range C. within Answer : between is used to filter rows based on a range of values. Which KQL keyword is used to concatenate two or more strings? A. concat B. strcat C. joinstr Answer : strcat is used to concatenate two or more strings. Which KQL keyword is used to extract a portion of a datetime value? A. extract B. datetime_part C. datetime_extract Answer : extract is used to extract a portion of a datetime value. Which KQL keyword is used to find the median of a numeric column? A. median B. percentile C. mid Answer : percentile is used to find the median of a numeric column (percentile 50). Which KQL keyword is used to return a specified number of rows from the start of the results? A. top B. limit C. head Answer : head is used to return a specified number of rows from the start of the results. Which KQL keyword is used to combine multiple conditions in a query? A. combine B. and C. both Answer : and is used to combine multiple conditions in a query. Which KQL keyword is used to calculate the standard deviation of a numeric column? A. stddev B. stdev C. sd Answer : stdev is used to calculate the standard deviation of a numeric column. Which KQL keyword is used to return rows where a column value is within a list of values? A. in B. within C. includes Answer : in is used to return rows where a column value is within a list of values. Which KQL keyword is used to calculate the variance of a numeric column? A. variance B. var C. varp Answer : var is used to calculate the variance of a numeric column. Which KQL keyword is used to format a string column? A. format B. str_format C. tostring Answer : tostring is used to format a string column. Which KQL keyword is used to pivot a table? A. pivot B. transform C. make-series Answer : make-series is used to pivot a table. Which KQL keyword is used to calculate the cumulative sum of a numeric column? A. cumulative_sum B. sum C. running_sum Answer : cumulative_sum is used to calculate the cumulative sum of a numeric column. Which KQL keyword is used to create a histogram of a numeric column? A. histogram B. bin C. bucket Answer : bin is used to create a histogram of a numeric column. Which KQL keyword is used to evaluate a condition and return one of two values? A. if B. case C. switch Answer : if is used to evaluate a condition and return one of two values. KQL Quesitons - General You have a table Sales with columns ProductID , Quantity , and Price . How would you calculate the total revenue for each product? A. Sales | summarize TotalRevenue = sum(Quantity * Price) by ProductID B. Sales | summarize TotalRevenue = avg(Quantity * Price) by ProductID C. Sales | extend TotalRevenue = Quantity * Price | summarize Total = sum(TotalRevenue) by ProductID Answer : Sales | summarize TotalRevenue = sum(Quantity *</span> Price) by ProductID How would you find all records in the Logs table where the Message column contains the word \"error\"? A. Logs | where Message contains \"error\" B. Logs | where Message == \"error\" C. Logs | search \"error\" Answer : Logs | where Message contains \"error\" What function would you use to calculate the moving average of a column in KQL? A. moving_avg() B. series_fir() C. avg() Answer : series_fir() is used to calculate the moving average. How do you join two tables Table1 and Table2 on the ID column, keeping all records from Table1 ? A. Table1 | innerjoin (Table2) on ID B. Table1 | join kind=inner (Table2) on ID C. Table1 | join kind=leftouter (Table2) on ID Answer : Table1 | join kind=leftouter (Table2) on ID How can you create a histogram of the Age column in the Users table? A. Users | histogram Age by 10 B. Users | summarize count() by Age C. Users | summarize count() by bin(Age, 10) Answer : Users | summarize count() by bin(Age, 10) You need to extract the year from a datetime column called Timestamp in the Events table. Which function would you use? A. year(Timestamp) B. extract_year(Timestamp) C. datetime_part('year', Timestamp) Answer : year(Timestamp) How would you filter rows in the Sales table to only include those where the Date is within the last 30 days? A. Sales | where Date > ago(30d) B. Sales | where Date between (now() - 30d) and now() C. Sales | where Date > datetime(30 days ago) Answer : Sales | where Date > ago(30d) How do you rename the column OldName to NewName in a KQL query? A. | project-rename NewName = OldName B. | rename OldName to NewName C. | project OldName as NewName Answer : | project OldName as NewName Which KQL function would you use to concatenate the values of two columns FirstName and LastName in the Employees table? A. concat(FirstName, LastName) B. strcat(FirstName, \" \", LastName) C. combine(FirstName, LastName) Answer : strcat(FirstName, \" \", LastName) In the Orders table, how would you calculate the average order value? A. Orders | summarize avg(OrderValue) B. Orders | summarize AverageOrder = mean(OrderValue) C. Orders | summarize AverageOrder = avg(OrderValue) Answer : Orders | summarize AverageOrder = avg(OrderValue) How can you list the unique values of the Country column from the Customers table? A. Customers | distinct Country B. Customers | summarize by Country C. Customers | unique Country Answer : Customers | distinct Country What is the correct way to calculate the total number of orders in the Orders table? A. Orders | summarize count() B. Orders | count() C. Orders | summarize total_orders = count() Answer : Orders | summarize total_orders = count() How would you convert the Price column in the Products table from a string to a real number? A. Products | project Price = toreal(Price) B. Products | extend Price = todouble(Price) C. Products | cast(Price as real) Answer : Products | project Price = toreal(Price) You want to visualize the Sales table's total revenue over time using a line chart. Which render statement should you use? A. | render linechart B. | render timechart C. | render barchart Answer : | render linechart How can you combine the results of two queries in KQL? A. combine B. union C. join Answer : union In KQL, how do you create a new column that shows the length of the Description column in the Products table? A. Products | extend Length = len(Description) B. Products | project Length = strlen(Description) C. Products | project Length = length(Description) Answer : Products | project Length = strlen(Description) How would you find the earliest OrderDate in the Orders table? A. Orders | summarize EarliestDate = min(OrderDate) B. Orders | summarize EarliestDate = earliest(OrderDate) C. Orders | summarize EarliestDate = first(OrderDate) Answer : Orders | summarize EarliestDate = min(OrderDate) Which KQL keyword is used to combine two tables side by side, based on a common column? A. merge B. union C. join Answer : join How would you calculate the median value of the Income column in the Employees table? A. Employees | summarize median(Income) B. Employees | summarize Percentile_50 = percentile(Income, 50) C. Employees | summarize MedianIncome = median(Income) Answer : Employees | summarize Percentile_50 = percentile(Income, 50) How can you filter the Events table to show only records where the Status column is either \"Active\" or \"Pending\"? A. Events | where Status in (\"Active\", \"Pending\") B. Events | where Status == \"Active\" or Status == \"Pending\" C. Events | where Status matches (\"Active\", \"Pending\") Answer : Events | where Status in (\"Active\", \"Pending\") How would you list the top 5 products by total sales in the Sales table? A. Sales | top 5 by sum(TotalSales) B. Sales | summarize TotalSales = sum(SalesAmount) by ProductID | top 5 by TotalSales C. Sales | summarize TotalSales = sum(SalesAmount) by ProductID | limit 5 by TotalSales Answer : Sales | summarize TotalSales = sum(SalesAmount) by ProductID | top 5 by TotalSales Which function in KQL would you use to format a datetime value as a string? A. format_datetime() B. datetime_to_string() C. tostring() Answer : format_datetime() How can you find the number of unique customers in the Sales table? A. Sales | summarize UniqueCustomers = dcount(CustomerID) B. Sales | summarize UniqueCustomers = countdistinct(CustomerID) C. Sales | summarize UniqueCustomers = unique(CustomerID) Answer : Sales | summarize UniqueCustomers = dcount(CustomerID) Which KQL function would you use to convert a string to a datetime value? A. todatetime() B. string_to_datetime() C. datetime() Answer : todatetime() How can you calculate the standard deviation of the Price column in the Products table? A. Products | summarize StdDevPrice = stdev(Price) B. Products | summarize StdDevPrice = stddev(Price) C. Products | summarize StdDevPrice = variance(Price) Answer : Products | summarize StdDevPrice = stddev(Price) Which KQL function is used to round a numeric value to the nearest integer? A. round() B. truncate() C. ceil() Answer : round() How would you extract the day of the week from a datetime column OrderDate in the Orders table? A. Orders | extend DayOfWeek = dayofweek(OrderDate) B. Orders | extend DayOfWeek = extract('dow', OrderDate) C. Orders | extend DayOfWeek = day(OrderDate) Answer : Orders | extend DayOfWeek = dayofweek(OrderDate) Which function in KQL can be used to split a string into an array based on a delimiter? A. split() B. string_split() C. explode() Answer : split() How would you calculate the cumulative sum of the SalesAmount column in the Sales table? A. Sales | extend CumulativeSales = sum(SalesAmount) B. Sales | extend CumulativeSales = running_sum(SalesAmount) C. Sales | extend CumulativeSales = cumulative_sum(SalesAmount Answer : Sales | extend CumulativeSales = cumulative_sum(SalesAmount) How do you find all records in the Logs table where the Severity column is either \"Error\" or \"Warning\"? A. Logs | where Severity == \"Error\" or Severity == \"Warning\" B. Logs | where Severity in (\"Error\", \"Warning\") C. Logs | where Severity matches (\"Error\", \"Warning\") Answer : Logs | where Severity in (\"Error\", \"Warning\") Which KQL function would you use to get the current date and time? A. now() B. current_datetime() C. getdate() Answer : now() How would you calculate the number of days between two datetime columns StartDate and EndDate in the Projects table? A. Projects | extend DaysBetween = date_diff('day', EndDate, StartDate) B. Projects | extend DaysBetween = datetime_diff('day', EndDate, StartDate) C. Projects | extend DaysBetween = day_diff(EndDate, StartDate Answer : Projects | extend DaysBetween = date_diff('day', EndDate, StartDate) Which function would you use to get the maximum value of a column Price in the Products table? A. Products | summarize MaxPrice = max(Price) B. Products | summarize MaxPrice = maximum(Price) C. Products | summarize MaxPrice = greatest(Price) Answer : Products | summarize MaxPrice = max(Price) How can you filter the Events table to show records where the EventDate is in the current year? A. Events | where year(EventDate) == year(now()) B. Events | where EventDate >= startofyear(now()) C. Events | where EventDate between (startofyear(now()) .. endofyear(now())) Answer : Events | where year(EventDate) == year(now()) How would you rename the OldColumn to NewColumn in the Data table? A. Data | project NewColumn = OldColumn B. Data | rename OldColumn as NewColumn C. Data | project-rename NewColumn = OldColumn Answer : Data | project-rename NewColumn = OldColumn Which function in KQL is used to get the number of elements in an array? A. count() B. length() C. array_length() Answer : length() How would you calculate the average Salary for each department in the Employees table? A. Employees | summarize AvgSalary = avg(Salary) by Department B. Employees | summarize AvgSalary = mean(Salary) by Department C. Employees | summarize AvgSalary = average(Salary) by Department Answer : Employees | summarize AvgSalary = avg(Salary) by Department How can you convert the Price column in the Products table from a string to a real number? A. Products | project Price = toreal(Price) B. Products | extend Price = todouble(Price) C. Products | cast(Price as real) Answer : Products | project Price = toreal(Price) Which function would you use to extract the hour from a datetime column EventTime in the Events table? A. Events | extend Hour = hour(EventTime) B. Events | extend Hour = extract('hour', EventTime) C. Events | extend Hour = gethour(EventTime) Answer : Events | extend Hour = hour(EventTime) How can you create a column FullName by concatenating FirstName and LastName in the Employees table? A. Employees | extend FullName = FirstName + \" \" + LastName B. Employees | extend FullName = strcat(FirstName, \" \", LastName) C. Employees | extend FullName = concat(FirstName, \" \", LastName) Answer : Employees | extend FullName = strcat(FirstName, \" \", LastName) How would you calculate the 90th percentile of the ResponseTime column in the Requests table? A. Requests | summarize Percentile_90 = percentile(ResponseTime, 90) B. Requests | summarize Percentile_90 = p90(ResponseTime) C. Requests | summarize Percentile_90 = percentile_approx(ResponseTime, 0.90) Answer : Requests | summarize Percentile_90 = percentile(ResponseTime, 90) Which function in KQL would you use to replace all occurrences of a substring in a string column? A. replace() B. str_replace() C. substitute() Answer : replace() How can you find the maximum value of the Score column in the Results table? A. Results | summarize MaxScore = max(Score) B. Results | summarize MaxScore = maximum(Score) C. Results | summarize MaxScore = highest(Score) Answer : Results | summarize MaxScore = max(Score) How would you create a new column Month by extracting the month from the OrderDate column in the Sales table? A. Sales | extend Month = extract('month', OrderDate) B. Sales | extend Month = month(OrderDate) C. Sales | extend Month = getmonth(OrderDate) Answer : Sales | extend Month = month(OrderDate) How do you calculate the variance of the Duration column in the Sessions table? A. Sessions | summarize VarDuration = variance(Duration) B. Sessions | summarize VarDuration = var(Duration) C. Sessions | summarize VarDuration = varp(Duration) Answer : Sessions | summarize VarDuration = var(Duration) Which KQL function would you use to count the number of non-null values in a column? A. count() B. countif() C. count_not_null() Answer : countif() How can you filter the Orders table to show only records where the TotalAmount is greater than 100? A. Orders | where TotalAmount > 100 B. Orders | filter TotalAmount > 100 C. Orders | find TotalAmount > 100 Answer : Orders | where TotalAmount > 100 How would you create a new column Year by extracting the year from the PurchaseDate column in the Purchases table? A. Purchases | extend Year = year(PurchaseDate) B. Purchases | extend Year = extract('year', PurchaseDate) C. Purchases | extend Year = getyear(PurchaseDate) Answer : Purchases | extend Year = year(PurchaseDate) Which function in KQL would you use to get the number of elements in an array? A. count() B. length() C. array_length() Answer : length() How do you calculate the sum of the SalesAmount column in the Sales table? A. Sales | summarize TotalSales = sum(SalesAmount) B. Sales | summarize TotalSales = adds(SalesAmount) C. Sales | summarize TotalSales = cumulative_sum(SalesAmount) Answer : Sales | summarize TotalSales = sum(SalesAmount) How can you find the earliest StartDate in the Projects table? A. Projects | summarize EarliestStart = earliest(StartDate) B. Projects | summarize EarliestStart = min(StartDate) C. Projects | summarize EarliestStart = first(StartDate) Answer : Projects | summarize EarliestStart = min(StartDate) Which KQL function is used to get the current date and time? A. now() B. current_datetime() C. getdate() Answer : now() How would you calculate the difference in months between two datetime columns StartDate and EndDate in the Tasks table? A. Tasks | extend MonthsBetween = date_diff('month', EndDate, StartDate) B. Tasks | extend MonthsBetween = datetime_diff('month', EndDate, StartDate) C. Tasks | extend MonthsBetween = month_diff(EndDate, StartDate) Answer : Tasks | extend MonthsBetween = date_diff('month', EndDate, StartDate) Which function in KQL would you use to get the sum of a column Amount in the Transactions table? A. Transactions | summarize TotalAmount = sum(Amount) B. Transactions | summarize TotalAmount = sum_amount(Amount) C. Transactions | summarize TotalAmount = sum(Amount) Answer : Transactions | summarize TotalAmount = sum(Amount) How can you filter the Logs table to show only records where the Level column is \"Error\"? A. Logs | where Level == \"Error\" B. Logs | where Level equals \"Error\" C. Logs | filter Level == \"Error\" Answer : Logs | where Level == \"Error\" How would you rename the OldColumn to NewColumn in the Data table? A. Data | project NewColumn = OldColumn B. Data | rename OldColumn as NewColumn C. Data | project-rename NewColumn = OldColumn Answer : Data | project-rename NewColumn = OldColumn Which function in KQL is used to count the number of elements in an array? A. count() B. length() C. array_length() Answer : length() How would you calculate the average Price for each product in the Products table? A. Products | summarize AvgPrice = avg(Price) by ProductID B. Products | summarize AvgPrice = mean(Price) by ProductID C. Products | summarize AvgPrice = average(Price) by ProductID Answer : Products | summarize AvgPrice = avg(Price) by ProductID How can you convert the Revenue column in the Sales table from a string to a real number? A. Sales | project Revenue = toreal(Revenue) B. Sales | extend Revenue = todouble(Revenue) C. Sales | cast(Revenue as real) Answer : Sales | project Revenue = toreal(Revenue) Which function would you use to extract the minute from a datetime column EventTime in the Events table? A. Events | extend Minute = minute(EventTime) B. Events | extend Minute = extract('minute', EventTime) C. Events | extend Minute = getminute(EventTime) Answer : Events | extend Minute = minute(EventTime) How can you create a column FullAddress by concatenating Street , City , and ZipCode in the Addresses table? A. Addresses | extend FullAddress = strcat(Street, \", \", City, \", \", ZipCode) B. Addresses | extend FullAddress = concat(Street, \", \", City, \", \", ZipCode) C. Addresses | extend FullAddress = Street + \", \" + City + \", \" + ZipCode Answer : Addresses | extend FullAddress = strcat(Street, \", \", City, \", \", ZipCode) How would you calculate the 95th percentile of the LoadTime column in the WebRequests table? A. WebRequests | summarize Percentile_95 = percentile(LoadTime, 95) B. WebRequests | summarize Percentile_95 = p95(LoadTime) C. WebRequests | summarize Percentile_95 = percentile_approx(LoadTime, 0.95) Answer : WebRequests | summarize Percentile_95 = percentile(LoadTime, 95) Which function in KQL would you use to replace all occurrences of a substring in a string column? A. replace() B. str_replace() C. substitute() Answer : replace() How can you find the maximum value of the Salary column in the Employees table? A. Employees | summarize MaxSalary = max(Salary) B. Employees | summarize MaxSalary = maximum(Salary) C. Employees | summarize MaxSalary = highest(Salary) Answer : Employees | summarize MaxSalary = max(Salary) How would you create a new column Quarter by extracting the quarter from the OrderDate column in the Orders table? A. Orders | extend Quarter = extract('quarter', OrderDate) B. Orders | extend Quarter = quarter(OrderDate) C. Orders | extend Quarter = getquarter(OrderDate) Answer : Orders | extend Quarter = quarter(OrderDate) How do you calculate the variance of the ProcessingTime column in the Transactions table? A. Transactions | summarize VarProcessingTime = variance(ProcessingTime) B. Transactions | summarize VarProcessingTime = var(ProcessingTime) C. Transactions | summarize VarProcessingTime = varp(ProcessingTime) Answer : Transactions | summarize VarProcessingTime = var(ProcessingTime) Which KQL function would you use to count the number of non-null values in a column? A. count() B. countif() C. count_not_null() Answer : countif() How can you filter the Invoices table to show only records where the Total is greater than 500? A. Invoices | where Total > 500 B. Invoices | filter Total > 500 C. Invoices | find Total > 500 Answer : Invoices | where Total > 500 How would you create a new column Year by extracting the year from the Date column in the Events table? A. Events | extend Year = year(Date) B. Events | extend Year = extract('year', Date) C. Events | extend Year = getyear(Date) Answer : Events | extend Year = year(Date) Which function in KQL would you use to get the sum of a column Amount in the Payments table? A. Payments | summarize TotalAmount = sum(Amount) B. Payments | summarize TotalAmount = sum_amount(Amount) C. Payments | summarize TotalAmount = adds(Amount) Answer : Payments | summarize TotalAmount = sum(Amount) How can you find the earliest HireDate in the Employees table? A. Employees | summarize EarliestHire = earliest(HireDate) B. Employees | summarize EarliestHire = min(HireDate) C. Employees | summarize EarliestHire = first(HireDate) Answer : Employees | summarize EarliestHire = min(HireDate) How would you calculate the difference in days between two datetime columns StartDate and EndDate in the Projects table? A. Projects | extend DaysBetween = date_diff('day', EndDate, StartDate) B. Projects | extend DaysBetween = datetime_diff('day', EndDate, StartDate) C. Projects | extend DaysBetween = day_diff(EndDate, StartDate Answer : Projects | extend DaysBetween = date_diff('day', EndDate, StartDate) How can you find the latest EndDate in the Tasks table? A. Tasks | summarize LatestEnd = latest(EndDate) B. Tasks | summarize LatestEnd = max(EndDate) C. Tasks | summarize LatestEnd = last(EndDate) Answer : Tasks | summarize LatestEnd = max(EndDate) Which function in KQL would you use to concatenate multiple string columns in the Products table? A. Products | extend FullDescription = concat(Description, \" - \", Category) B. Products | extend FullDescription = strcat(Description, \" - \", Category) C. Products | extend FullDescription = joinstr(Description, \" - \", Category) Answer : Products | extend FullDescription = strcat(Description, \" - \", Category) How can you calculate the total number of orders in the Orders table? A. Orders | summarize TotalOrders = count() B. Orders | count() C. Orders | summarize TotalOrders = count(OrderID) Answer : Orders | summarize TotalOrders = count() How would you find all records in the Errors table where the Message column contains the word \"timeout\"? A. Errors | where Message contains \"timeout\" B. Errors | where Message == \"timeout\" C. Errors | search \"timeout\" Answer : Errors | where Message contains \"timeout\" How would you list the unique values of the Status column from the Tasks table? A. Tasks | distinct Status B. Tasks | summarize by Status C. Tasks | unique Status Answer : Tasks | distinct Status How can you calculate the average ResponseTime for each URL in the WebRequests table? A. WebRequests | summarize AvgResponseTime = avg(ResponseTime) by URL B. WebRequests | summarize AvgResponseTime = mean(ResponseTime) by URL C. WebRequests | summarize AvgResponseTime = average(ResponseTime) by URL Answer : WebRequests | summarize AvgResponseTime = avg(ResponseTime) by URL How would you calculate the cumulative sum of the Revenue column in the Sales table? A. Sales | extend CumulativeRevenue = sum(Revenue) B. Sales | extend CumulativeRevenue = running_sum(Revenue) C. Sales | extend CumulativeRevenue = cumulative_sum(Revenue) Answer : Sales | extend CumulativeRevenue = cumulative_sum(Revenue) Which function in KQL would you use to get the maximum value of a column Value in the Metrics table? A. Metrics | summarize MaxValue = max(Value) B. Metrics | summarize MaxValue = maximum(Value) C. Metrics | summarize MaxValue = greatest(Value) Answer : Metrics | summarize MaxValue = max(Value) How can you filter the Activities table to show only records where the ActivityDate is in the current month? A. Activities | where month(ActivityDate) == month(now()) B. Activities | where ActivityDate >= startofmonth(now()) C. Activities | where ActivityDate between (startofmonth(now()) .. endofmonth(now())) Answer : Activities | where month(ActivityDate) == month(now()) Which function in KQL is used to split a string into an array based on a delimiter? A. split() B. string_split() C. explode() Answer : split() How would you calculate the 75th percentile of the ProcessingTime column in the Operations table? A. Operations | summarize Percentile_75 = percentile(ProcessingTime, 75) B. Operations | summarize Percentile_75 = p75(ProcessingTime) C. Operations | summarize Percentile_75 = percentile_approx(ProcessingTime, 0.75) Answer : Operations | summarize Percentile_75 = percentile(ProcessingTime, 75) How would you calculate the median value of the Income column in the Employees table? A. Employees | summarize MedianIncome = median(Income) B. Employees | summarize MedianIncome = percentile(Income, 50) C. Employees | summarize MedianIncome = avg(Income) Answer : Employees | summarize MedianIncome = percentile(Income, 50) How can you list the unique values of the Category column from the Products table? A. Products | distinct Category B. Products | summarize by Category C. Products | unique Category Answer : Products | distinct Category Which function in KQL is used to format a datetime value as a string? A. format_datetime() B. datetime_to_string() C. tostring() Answer : format_datetime() How can you create a new column Month by extracting the month from the Timestamp column in the Events table? A. Events | extend Month = extract('month', Timestamp) B. Events | extend Month = month(Timestamp) C. Events | extend Month = getmonth(Timestamp) Answer : Events | extend Month = month(Timestamp) How do you filter the Orders table to include only those where the OrderDate is within the last 7 days? A. Orders | where OrderDate > ago(7d) B. Orders | where OrderDate >= startofweek(now()) C. Orders | where OrderDate between (now() - 7d) and now() Answer : Orders | where OrderDate > ago(7d) How would you extract the year from a datetime column called Timestamp in the Logs table? A. Logs | extend Year = year(Timestamp) B. Logs | extend Year = extract('year', Timestamp) C. Logs | extend Year = getyear(Timestamp) Answer : Logs | extend Year = year(Timestamp) How would you rename the column OldName to NewName in a KQL query? A. | project-rename NewName = OldName B. | rename OldName to NewName C. | project OldName as NewName Answer : | project OldName as NewName Which function in KQL would you use to get the current date and time? A. now() B. current_datetime() C. getdate() Answer : now() How can you create a column FullName by concatenating FirstName and LastName in the Employees table? A. Employees | extend FullName = FirstName + \" \" + LastName B. Employees | extend FullName = strcat(FirstName, \" \", LastName) C. Employees | extend FullName = concat(FirstName, \" \", LastName) Answer : Employees | extend FullName = strcat(FirstName, \" \", LastName) How would you find all records in the Logs table where the Message column contains the word \"error\"? A. Logs | where Message contains \"error\" B. Logs | where Message == \"error\" C. Logs | search \"error\" Answer : Logs | where Message contains \"error\" Which KQL function would you use to convert a string to a datetime value? A. todatetime() B. string_to_datetime() C. datetime() Answer : todatetime() How would you filter rows in the Sales table to only include those where the Date is within the last 30 days? A. Sales | where Date > ago(30d) B. Sales | where Date between (now() - 30d) and now() C. Sales | where Date > datetime(30 days ago) Answer : Sales | where Date > ago(30d) How do you join two tables Table1 and Table2 on the ID column, keeping all records from Table1 ? A. Table1 | innerjoin (Table2) on ID B. Table1 | join kind=inner (Table2) on ID C. Table1 | join kind=leftouter (Table2) on ID Answer : Table1 | join kind=leftouter (Table2) on ID How would you list the top 10 products by total sales in the Sales table? A. Sales | top 10 by sum(TotalSales) B. Sales | summarize TotalSales = sum(SalesAmount) by ProductID | top 10 by TotalSales C. Sales | summarize TotalSales = sum(SalesAmount) by ProductID | limit 10 by TotalSales Answer : Sales | summarize TotalSales = sum(SalesAmount) by ProductID | top 10 by TotalSales How would you create a time series chart for the Sales table's total revenue over time? A. Sales | summarize TotalRevenue = sum(Revenue) by bin(Time, 1h) | render timechart B. Sales | summarize TotalRevenue = sum(Revenue) by bin(Time, 1h) | render linechart C. Sales | summarize TotalRevenue = sum(Revenue) by bin(Time, 1h) | render barchart Answer : Sales | summarize TotalRevenue = sum(Revenue) by bin(Time, 1h) | render linechart How can you calculate the total number of distinct customers in the Sales table? A. Sales | summarize DistinctCustomers = dcount(CustomerID) B. Sales | summarize DistinctCustomers = countdistinct(CustomerID) C. Sales | summarize DistinctCustomers = unique(CustomerID) Answer : Sales | summarize DistinctCustomers = dcount(CustomerID) How can you concatenate the values of two columns FirstName and LastName in the Contacts table? A. Contacts | extend FullName = strcat(FirstName, \" \", LastName) B. Contacts | extend FullName = concat(FirstName, \" \", LastName) C. Contacts | extend FullName = joinstr(FirstName, \" \", LastName) Answer : Contacts | extend FullName = strcat(FirstName, \" \", LastName)","title":"KQL"},{"location":"Microsoft-Fabric/KQL/#what-is-kql","text":"A KQL (Kusto Query Language) Database handles large volumes of structured, semi-structured, and unstructured data for real-time analytics and ad-hoc querying. It is part of the Azure Data Explorer service. The data in a KQL database is stored in Azure Data Explorer . It uses a columnar storage format, for high-performance.","title":"What is KQL?"},{"location":"Microsoft-Fabric/KQL/#how-to-run-kql-query-in-fabric","text":"There is no magic command like %%KQL","title":"How to run KQL query in Fabric?"},{"location":"Microsoft-Fabric/KQL/#kql-vs-sql-databases","text":"Feature KQL Database Standard SQL Database Query Language Kusto Query Language (KQL) Structured Query Language (SQL) Storage Format Columnar Row-based Optimized For Real-time analytics, log and time-series data Transactional data, relational data Data Structure Tables, columns, materialized views, functions Tables, columns, views, stored procedures Scalability Highly scalable and distributed Varies by implementation (SQL Server, MySQL, etc.) Indexing Automatically indexed for fast query performance Manual and automatic indexing Data Ingestion Supports batch and streaming ingestion Primarily batch ingestion Use Cases Log analytics, telemetry data, IoT data OLTP, data warehousing, reporting Storage Location Azure Data Explorer service in the cloud Varies (on-premises, cloud-based) Performance Optimized for read-heavy and analytical workloads Balanced for read and write operations Schema Flexible schema with support for semi-structured data Rigid schema with well-defined data types","title":"KQL vs SQL Databases"},{"location":"Microsoft-Fabric/KQL/#kql-vs-sql-query","text":"","title":"KQL Vs SQL Query"},{"location":"Microsoft-Fabric/KQL/#kql-vs-sql-dql","text":"Operation SQL KQL Select and Count SELECT Name, Age, COUNT(*) FROM Employees WHERE Age > 30 GROUP BY Name, Age; Employees \\| where Age > 30 \\| summarize count() by Name, Age Group By and Order By SELECT Department, AVG(Salary) AS AverageSalary FROM Employees GROUP BY Department ORDER BY AverageSalary DESC; Employees \\| summarize AverageSalary=avg(Salary) by Department \\| sort by AverageSalary desc Join SELECT e.Name, d.DepartmentName FROM Employees e JOIN Departments d ON e.DepartmentID = d.ID; Employees \\| join kind=inner (Departments) on $left.DepartmentID == $right.ID \\| project Name, DepartmentName Subquery and Limit SELECT Name FROM (SELECT * FROM Employees WHERE Age > 30) AS SubQuery WHERE DepartmentID = 5 LIMIT 10; let SubQuery = Employees \\| where Age > 30; SubQuery \\| where DepartmentID == 5 \\| project Name \\| take 10 String Functions SELECT Name FROM Employees WHERE UPPER(FirstName) = 'JOHN'; Employees \\| where tolower(FirstName) == 'john' \\| project Name Date Functions SELECT Name FROM Employees WHERE YEAR(HireDate) = 2020; Employees \\| where datetime_part('year', HireDate) == 2020 \\| project Name Between SELECT * FROM Employees WHERE Age BETWEEN 25 AND 35; Employees \\| where Age between (25 .. 35) Date Range SELECT * FROM Sales WHERE SaleDate BETWEEN '2021-01-01' AND '2021-12-31'; Sales \\| where SaleDate between (datetime(2021-01-01) .. datetime(2021-12-31)) Distinct SELECT DISTINCT Department FROM Employees; Employees \\| summarize by Department Top N SELECT TOP 5 Name, Salary FROM Employees ORDER BY Salary DESC; Employees \\| top 5 by Salary desc \\| project Name, Salary Aggregation with Conditions SELECT Department, COUNT(*) FROM Employees WHERE Age > 30 GROUP BY Department; Employees \\| where Age > 30 \\| summarize count() by Department","title":"KQL vs SQL - DQL"},{"location":"Microsoft-Fabric/KQL/#kql-vs-sql-ddls-dmls-dqls","text":"Description Example Category Tables Create a new table .create table MyTable (Column1: string, Column2: int) DDL Show the schema of a table .show table MyTable schema DQL Ingest data into a table .ingest into table MyTable <DataSource> DML Rename a table .rename table OldTableName to NewTableName DDL Drop a table .drop table TableName DDL List all tables .show tables DQL Columns Add a column .alter table TableName add column ColumnName: DataType DDL Drop a column .alter table TableName drop column ColumnName DDL Rename a column .rename column OldColumnName to NewColumnName in table TableName DDL Functions Create a new function .create function with (docstring = \"Description\", folder = \"FolderName\") MyFunction () { <KQLQuery> } DDL Show available functions .show functions DQL Materialized Views Create a new materialized view .create materialized-view MyView on table MyTable { <KQLQuery> } DDL Show available materialized views .show materialized-views DQL Indexes Create an index .create index IndexName on TableName (ColumnName) DDL Drop an index .drop index IndexName on TableName DDL Show indexes .show indexes DQL Ingest Ingest data into a table .ingest into table MyTable <DataSource> DML Ingest data from JSON .ingest into table TableName h@\"https://path/to/file.json\" DML Database Operations Create a database .create database DatabaseName DDL Drop a database .drop database DatabaseName DDL List all databases .show databases DQL Permissions Grant table permissions .grant select on table TableName to UserName DDL Revoke table permissions .revoke select on table TableName from UserName DDL Show permissions .show table TableName policy access DQL Views Create a view .create view ViewName as <KQLQuery> DDL Drop a view .drop view ViewName DDL Show views .show views DQL Diagnostics Show cluster diagnostics .show cluster diagnostics DQL Show table statistics .show table TableName stats DQL Data Export Export data to JSON .export to json at <FilePath> <KQLQuery> DML","title":"KQL Vs SQL - DDLs, DMLs &amp; DQLs"},{"location":"Microsoft-Fabric/KQL/#kql-qa","text":"Highlight the answers to reveal it!","title":"KQL Q&amp;A"},{"location":"Microsoft-Fabric/KQL/#kql-questions-keywords","text":"Which KQL keyword is used to limit the results of a query to a specified number of rows? A. select B. take C. project Answer : Take limits the results to a specified number of rows. Which KQL keyword is used to group and aggregate data? A. group_by B. aggregate C. summarize Answer : Use the summarize keyword to group and aggregate data. Which KQL keyword is used to filter rows based on a condition? A. where B. filter C. select Answer : where is used to filter rows based on a condition. Which KQL keyword is used to create a new column or modify an existing column? A. create B. extend C. modify Answer : extend is used to create a new column or modify an existing column. Which KQL keyword is used to sort the results of a query? A. order B. arrange C. sort Answer : sort is used to order the results of a query. Which KQL keyword is used to rename a column in the results? A. rename B. project-rename C. alias Answer : project-rename is used to rename a column in the results. Which KQL keyword is used to join two tables on a common column? A. merge B. union C. join Answer : join is used to combine two tables on a common column. Which KQL keyword is used to calculate the total number of rows in the results? A. count B. total C. sum Answer : count is used to calculate the total number of rows in the results. Which KQL keyword is used to remove duplicates from the results? A. distinct B. unique C. remove-duplicates Answer : distinct is used to remove duplicate rows from the results. Which KQL keyword is used to extract a substring from a string column? A. substring B. extract C. substr Answer : substring is used to extract a part of a string column. Which KQL keyword is used to combine the results of two or more queries? A. combine B. union C. join Answer : union is used to combine the results of two or more queries. Which KQL keyword is used to convert a column to a different data type? A. convert B. cast C. toType Answer : cast is used to convert a column to a different data type. Which KQL keyword is used to filter rows with null values? A. isnull B. isnotnull C. isnonempty Answer : isnotnull is used to filter rows with null values. Which KQL keyword is used to calculate the average of a numeric column? A. average B. mean C. avg Answer : avg is used to calculate the average of a numeric column. Which KQL keyword is used to create a time series chart? A. timeseries B. render C. chart Answer : render is used to create a time series chart. Which KQL keyword is used to specify the columns to include in the results? A. include B. select C. project Answer : project is used to specify the columns to include in the results. Which KQL keyword is used to calculate the maximum value of a numeric column? A. max B. maximum C. highest Answer : max is used to calculate the maximum value of a numeric column. Which KQL keyword is used to calculate the minimum value of a numeric column? A. min B. minimum C. lowest Answer : min is used to calculate the minimum value of a numeric column. Which KQL keyword is used to convert a datetime column to a specific format? A. format B. convert C. format_datetime Answer : format_datetime is used to convert a datetime column to a specific format. Which KQL keyword is used to calculate the difference between two datetime columns? A. datetime_diff B. date_diff C. time_diff Answer : date_diff is used to calculate the difference between two datetime columns. Which KQL keyword is used to filter rows based on a regular expression? A. regex_match B. matches_regex C. search Answer : matches_regex is used to filter rows based on a regular expression. Which KQL keyword is used to calculate the sum of a numeric column? A. sum B. total C. aggregate_sum Answer : sum is used to calculate the sum of a numeric column. Which KQL keyword is used to create a new table with the results of a query? A. create_table B. into C. output Answer : into is used to create a new table with the results of a query. Which KQL keyword is used to parse a string into multiple columns? A. split B. parse C. dissect Answer : parse is used to parse a string into multiple columns. Which KQL keyword is used to join two tables and keep only the rows with matching keys? A. inner join B. equijoin C. join Answer : join is used to join two tables and keep only the rows with matching keys. Which KQL keyword is used to create an alias for a column in the results? A. alias B. as C. rename Answer : as is used to create an alias for a column in the results. Which KQL keyword is used to filter rows based on a range of values? A. between B. in_range C. within Answer : between is used to filter rows based on a range of values. Which KQL keyword is used to concatenate two or more strings? A. concat B. strcat C. joinstr Answer : strcat is used to concatenate two or more strings. Which KQL keyword is used to extract a portion of a datetime value? A. extract B. datetime_part C. datetime_extract Answer : extract is used to extract a portion of a datetime value. Which KQL keyword is used to find the median of a numeric column? A. median B. percentile C. mid Answer : percentile is used to find the median of a numeric column (percentile 50). Which KQL keyword is used to return a specified number of rows from the start of the results? A. top B. limit C. head Answer : head is used to return a specified number of rows from the start of the results. Which KQL keyword is used to combine multiple conditions in a query? A. combine B. and C. both Answer : and is used to combine multiple conditions in a query. Which KQL keyword is used to calculate the standard deviation of a numeric column? A. stddev B. stdev C. sd Answer : stdev is used to calculate the standard deviation of a numeric column. Which KQL keyword is used to return rows where a column value is within a list of values? A. in B. within C. includes Answer : in is used to return rows where a column value is within a list of values. Which KQL keyword is used to calculate the variance of a numeric column? A. variance B. var C. varp Answer : var is used to calculate the variance of a numeric column. Which KQL keyword is used to format a string column? A. format B. str_format C. tostring Answer : tostring is used to format a string column. Which KQL keyword is used to pivot a table? A. pivot B. transform C. make-series Answer : make-series is used to pivot a table. Which KQL keyword is used to calculate the cumulative sum of a numeric column? A. cumulative_sum B. sum C. running_sum Answer : cumulative_sum is used to calculate the cumulative sum of a numeric column. Which KQL keyword is used to create a histogram of a numeric column? A. histogram B. bin C. bucket Answer : bin is used to create a histogram of a numeric column. Which KQL keyword is used to evaluate a condition and return one of two values? A. if B. case C. switch Answer : if is used to evaluate a condition and return one of two values.","title":"KQL Questions - Keywords"},{"location":"Microsoft-Fabric/KQL/#kql-quesitons-general","text":"You have a table Sales with columns ProductID , Quantity , and Price . How would you calculate the total revenue for each product? A. Sales | summarize TotalRevenue = sum(Quantity * Price) by ProductID B. Sales | summarize TotalRevenue = avg(Quantity * Price) by ProductID C. Sales | extend TotalRevenue = Quantity * Price | summarize Total = sum(TotalRevenue) by ProductID Answer : Sales | summarize TotalRevenue = sum(Quantity *</span> Price) by ProductID How would you find all records in the Logs table where the Message column contains the word \"error\"? A. Logs | where Message contains \"error\" B. Logs | where Message == \"error\" C. Logs | search \"error\" Answer : Logs | where Message contains \"error\" What function would you use to calculate the moving average of a column in KQL? A. moving_avg() B. series_fir() C. avg() Answer : series_fir() is used to calculate the moving average. How do you join two tables Table1 and Table2 on the ID column, keeping all records from Table1 ? A. Table1 | innerjoin (Table2) on ID B. Table1 | join kind=inner (Table2) on ID C. Table1 | join kind=leftouter (Table2) on ID Answer : Table1 | join kind=leftouter (Table2) on ID How can you create a histogram of the Age column in the Users table? A. Users | histogram Age by 10 B. Users | summarize count() by Age C. Users | summarize count() by bin(Age, 10) Answer : Users | summarize count() by bin(Age, 10) You need to extract the year from a datetime column called Timestamp in the Events table. Which function would you use? A. year(Timestamp) B. extract_year(Timestamp) C. datetime_part('year', Timestamp) Answer : year(Timestamp) How would you filter rows in the Sales table to only include those where the Date is within the last 30 days? A. Sales | where Date > ago(30d) B. Sales | where Date between (now() - 30d) and now() C. Sales | where Date > datetime(30 days ago) Answer : Sales | where Date > ago(30d) How do you rename the column OldName to NewName in a KQL query? A. | project-rename NewName = OldName B. | rename OldName to NewName C. | project OldName as NewName Answer : | project OldName as NewName Which KQL function would you use to concatenate the values of two columns FirstName and LastName in the Employees table? A. concat(FirstName, LastName) B. strcat(FirstName, \" \", LastName) C. combine(FirstName, LastName) Answer : strcat(FirstName, \" \", LastName) In the Orders table, how would you calculate the average order value? A. Orders | summarize avg(OrderValue) B. Orders | summarize AverageOrder = mean(OrderValue) C. Orders | summarize AverageOrder = avg(OrderValue) Answer : Orders | summarize AverageOrder = avg(OrderValue) How can you list the unique values of the Country column from the Customers table? A. Customers | distinct Country B. Customers | summarize by Country C. Customers | unique Country Answer : Customers | distinct Country What is the correct way to calculate the total number of orders in the Orders table? A. Orders | summarize count() B. Orders | count() C. Orders | summarize total_orders = count() Answer : Orders | summarize total_orders = count() How would you convert the Price column in the Products table from a string to a real number? A. Products | project Price = toreal(Price) B. Products | extend Price = todouble(Price) C. Products | cast(Price as real) Answer : Products | project Price = toreal(Price) You want to visualize the Sales table's total revenue over time using a line chart. Which render statement should you use? A. | render linechart B. | render timechart C. | render barchart Answer : | render linechart How can you combine the results of two queries in KQL? A. combine B. union C. join Answer : union In KQL, how do you create a new column that shows the length of the Description column in the Products table? A. Products | extend Length = len(Description) B. Products | project Length = strlen(Description) C. Products | project Length = length(Description) Answer : Products | project Length = strlen(Description) How would you find the earliest OrderDate in the Orders table? A. Orders | summarize EarliestDate = min(OrderDate) B. Orders | summarize EarliestDate = earliest(OrderDate) C. Orders | summarize EarliestDate = first(OrderDate) Answer : Orders | summarize EarliestDate = min(OrderDate) Which KQL keyword is used to combine two tables side by side, based on a common column? A. merge B. union C. join Answer : join How would you calculate the median value of the Income column in the Employees table? A. Employees | summarize median(Income) B. Employees | summarize Percentile_50 = percentile(Income, 50) C. Employees | summarize MedianIncome = median(Income) Answer : Employees | summarize Percentile_50 = percentile(Income, 50) How can you filter the Events table to show only records where the Status column is either \"Active\" or \"Pending\"? A. Events | where Status in (\"Active\", \"Pending\") B. Events | where Status == \"Active\" or Status == \"Pending\" C. Events | where Status matches (\"Active\", \"Pending\") Answer : Events | where Status in (\"Active\", \"Pending\") How would you list the top 5 products by total sales in the Sales table? A. Sales | top 5 by sum(TotalSales) B. Sales | summarize TotalSales = sum(SalesAmount) by ProductID | top 5 by TotalSales C. Sales | summarize TotalSales = sum(SalesAmount) by ProductID | limit 5 by TotalSales Answer : Sales | summarize TotalSales = sum(SalesAmount) by ProductID | top 5 by TotalSales Which function in KQL would you use to format a datetime value as a string? A. format_datetime() B. datetime_to_string() C. tostring() Answer : format_datetime() How can you find the number of unique customers in the Sales table? A. Sales | summarize UniqueCustomers = dcount(CustomerID) B. Sales | summarize UniqueCustomers = countdistinct(CustomerID) C. Sales | summarize UniqueCustomers = unique(CustomerID) Answer : Sales | summarize UniqueCustomers = dcount(CustomerID) Which KQL function would you use to convert a string to a datetime value? A. todatetime() B. string_to_datetime() C. datetime() Answer : todatetime() How can you calculate the standard deviation of the Price column in the Products table? A. Products | summarize StdDevPrice = stdev(Price) B. Products | summarize StdDevPrice = stddev(Price) C. Products | summarize StdDevPrice = variance(Price) Answer : Products | summarize StdDevPrice = stddev(Price) Which KQL function is used to round a numeric value to the nearest integer? A. round() B. truncate() C. ceil() Answer : round() How would you extract the day of the week from a datetime column OrderDate in the Orders table? A. Orders | extend DayOfWeek = dayofweek(OrderDate) B. Orders | extend DayOfWeek = extract('dow', OrderDate) C. Orders | extend DayOfWeek = day(OrderDate) Answer : Orders | extend DayOfWeek = dayofweek(OrderDate) Which function in KQL can be used to split a string into an array based on a delimiter? A. split() B. string_split() C. explode() Answer : split() How would you calculate the cumulative sum of the SalesAmount column in the Sales table? A. Sales | extend CumulativeSales = sum(SalesAmount) B. Sales | extend CumulativeSales = running_sum(SalesAmount) C. Sales | extend CumulativeSales = cumulative_sum(SalesAmount Answer : Sales | extend CumulativeSales = cumulative_sum(SalesAmount) How do you find all records in the Logs table where the Severity column is either \"Error\" or \"Warning\"? A. Logs | where Severity == \"Error\" or Severity == \"Warning\" B. Logs | where Severity in (\"Error\", \"Warning\") C. Logs | where Severity matches (\"Error\", \"Warning\") Answer : Logs | where Severity in (\"Error\", \"Warning\") Which KQL function would you use to get the current date and time? A. now() B. current_datetime() C. getdate() Answer : now() How would you calculate the number of days between two datetime columns StartDate and EndDate in the Projects table? A. Projects | extend DaysBetween = date_diff('day', EndDate, StartDate) B. Projects | extend DaysBetween = datetime_diff('day', EndDate, StartDate) C. Projects | extend DaysBetween = day_diff(EndDate, StartDate Answer : Projects | extend DaysBetween = date_diff('day', EndDate, StartDate) Which function would you use to get the maximum value of a column Price in the Products table? A. Products | summarize MaxPrice = max(Price) B. Products | summarize MaxPrice = maximum(Price) C. Products | summarize MaxPrice = greatest(Price) Answer : Products | summarize MaxPrice = max(Price) How can you filter the Events table to show records where the EventDate is in the current year? A. Events | where year(EventDate) == year(now()) B. Events | where EventDate >= startofyear(now()) C. Events | where EventDate between (startofyear(now()) .. endofyear(now())) Answer : Events | where year(EventDate) == year(now()) How would you rename the OldColumn to NewColumn in the Data table? A. Data | project NewColumn = OldColumn B. Data | rename OldColumn as NewColumn C. Data | project-rename NewColumn = OldColumn Answer : Data | project-rename NewColumn = OldColumn Which function in KQL is used to get the number of elements in an array? A. count() B. length() C. array_length() Answer : length() How would you calculate the average Salary for each department in the Employees table? A. Employees | summarize AvgSalary = avg(Salary) by Department B. Employees | summarize AvgSalary = mean(Salary) by Department C. Employees | summarize AvgSalary = average(Salary) by Department Answer : Employees | summarize AvgSalary = avg(Salary) by Department How can you convert the Price column in the Products table from a string to a real number? A. Products | project Price = toreal(Price) B. Products | extend Price = todouble(Price) C. Products | cast(Price as real) Answer : Products | project Price = toreal(Price) Which function would you use to extract the hour from a datetime column EventTime in the Events table? A. Events | extend Hour = hour(EventTime) B. Events | extend Hour = extract('hour', EventTime) C. Events | extend Hour = gethour(EventTime) Answer : Events | extend Hour = hour(EventTime) How can you create a column FullName by concatenating FirstName and LastName in the Employees table? A. Employees | extend FullName = FirstName + \" \" + LastName B. Employees | extend FullName = strcat(FirstName, \" \", LastName) C. Employees | extend FullName = concat(FirstName, \" \", LastName) Answer : Employees | extend FullName = strcat(FirstName, \" \", LastName) How would you calculate the 90th percentile of the ResponseTime column in the Requests table? A. Requests | summarize Percentile_90 = percentile(ResponseTime, 90) B. Requests | summarize Percentile_90 = p90(ResponseTime) C. Requests | summarize Percentile_90 = percentile_approx(ResponseTime, 0.90) Answer : Requests | summarize Percentile_90 = percentile(ResponseTime, 90) Which function in KQL would you use to replace all occurrences of a substring in a string column? A. replace() B. str_replace() C. substitute() Answer : replace() How can you find the maximum value of the Score column in the Results table? A. Results | summarize MaxScore = max(Score) B. Results | summarize MaxScore = maximum(Score) C. Results | summarize MaxScore = highest(Score) Answer : Results | summarize MaxScore = max(Score) How would you create a new column Month by extracting the month from the OrderDate column in the Sales table? A. Sales | extend Month = extract('month', OrderDate) B. Sales | extend Month = month(OrderDate) C. Sales | extend Month = getmonth(OrderDate) Answer : Sales | extend Month = month(OrderDate) How do you calculate the variance of the Duration column in the Sessions table? A. Sessions | summarize VarDuration = variance(Duration) B. Sessions | summarize VarDuration = var(Duration) C. Sessions | summarize VarDuration = varp(Duration) Answer : Sessions | summarize VarDuration = var(Duration) Which KQL function would you use to count the number of non-null values in a column? A. count() B. countif() C. count_not_null() Answer : countif() How can you filter the Orders table to show only records where the TotalAmount is greater than 100? A. Orders | where TotalAmount > 100 B. Orders | filter TotalAmount > 100 C. Orders | find TotalAmount > 100 Answer : Orders | where TotalAmount > 100 How would you create a new column Year by extracting the year from the PurchaseDate column in the Purchases table? A. Purchases | extend Year = year(PurchaseDate) B. Purchases | extend Year = extract('year', PurchaseDate) C. Purchases | extend Year = getyear(PurchaseDate) Answer : Purchases | extend Year = year(PurchaseDate) Which function in KQL would you use to get the number of elements in an array? A. count() B. length() C. array_length() Answer : length() How do you calculate the sum of the SalesAmount column in the Sales table? A. Sales | summarize TotalSales = sum(SalesAmount) B. Sales | summarize TotalSales = adds(SalesAmount) C. Sales | summarize TotalSales = cumulative_sum(SalesAmount) Answer : Sales | summarize TotalSales = sum(SalesAmount) How can you find the earliest StartDate in the Projects table? A. Projects | summarize EarliestStart = earliest(StartDate) B. Projects | summarize EarliestStart = min(StartDate) C. Projects | summarize EarliestStart = first(StartDate) Answer : Projects | summarize EarliestStart = min(StartDate) Which KQL function is used to get the current date and time? A. now() B. current_datetime() C. getdate() Answer : now() How would you calculate the difference in months between two datetime columns StartDate and EndDate in the Tasks table? A. Tasks | extend MonthsBetween = date_diff('month', EndDate, StartDate) B. Tasks | extend MonthsBetween = datetime_diff('month', EndDate, StartDate) C. Tasks | extend MonthsBetween = month_diff(EndDate, StartDate) Answer : Tasks | extend MonthsBetween = date_diff('month', EndDate, StartDate) Which function in KQL would you use to get the sum of a column Amount in the Transactions table? A. Transactions | summarize TotalAmount = sum(Amount) B. Transactions | summarize TotalAmount = sum_amount(Amount) C. Transactions | summarize TotalAmount = sum(Amount) Answer : Transactions | summarize TotalAmount = sum(Amount) How can you filter the Logs table to show only records where the Level column is \"Error\"? A. Logs | where Level == \"Error\" B. Logs | where Level equals \"Error\" C. Logs | filter Level == \"Error\" Answer : Logs | where Level == \"Error\" How would you rename the OldColumn to NewColumn in the Data table? A. Data | project NewColumn = OldColumn B. Data | rename OldColumn as NewColumn C. Data | project-rename NewColumn = OldColumn Answer : Data | project-rename NewColumn = OldColumn Which function in KQL is used to count the number of elements in an array? A. count() B. length() C. array_length() Answer : length() How would you calculate the average Price for each product in the Products table? A. Products | summarize AvgPrice = avg(Price) by ProductID B. Products | summarize AvgPrice = mean(Price) by ProductID C. Products | summarize AvgPrice = average(Price) by ProductID Answer : Products | summarize AvgPrice = avg(Price) by ProductID How can you convert the Revenue column in the Sales table from a string to a real number? A. Sales | project Revenue = toreal(Revenue) B. Sales | extend Revenue = todouble(Revenue) C. Sales | cast(Revenue as real) Answer : Sales | project Revenue = toreal(Revenue) Which function would you use to extract the minute from a datetime column EventTime in the Events table? A. Events | extend Minute = minute(EventTime) B. Events | extend Minute = extract('minute', EventTime) C. Events | extend Minute = getminute(EventTime) Answer : Events | extend Minute = minute(EventTime) How can you create a column FullAddress by concatenating Street , City , and ZipCode in the Addresses table? A. Addresses | extend FullAddress = strcat(Street, \", \", City, \", \", ZipCode) B. Addresses | extend FullAddress = concat(Street, \", \", City, \", \", ZipCode) C. Addresses | extend FullAddress = Street + \", \" + City + \", \" + ZipCode Answer : Addresses | extend FullAddress = strcat(Street, \", \", City, \", \", ZipCode) How would you calculate the 95th percentile of the LoadTime column in the WebRequests table? A. WebRequests | summarize Percentile_95 = percentile(LoadTime, 95) B. WebRequests | summarize Percentile_95 = p95(LoadTime) C. WebRequests | summarize Percentile_95 = percentile_approx(LoadTime, 0.95) Answer : WebRequests | summarize Percentile_95 = percentile(LoadTime, 95) Which function in KQL would you use to replace all occurrences of a substring in a string column? A. replace() B. str_replace() C. substitute() Answer : replace() How can you find the maximum value of the Salary column in the Employees table? A. Employees | summarize MaxSalary = max(Salary) B. Employees | summarize MaxSalary = maximum(Salary) C. Employees | summarize MaxSalary = highest(Salary) Answer : Employees | summarize MaxSalary = max(Salary) How would you create a new column Quarter by extracting the quarter from the OrderDate column in the Orders table? A. Orders | extend Quarter = extract('quarter', OrderDate) B. Orders | extend Quarter = quarter(OrderDate) C. Orders | extend Quarter = getquarter(OrderDate) Answer : Orders | extend Quarter = quarter(OrderDate) How do you calculate the variance of the ProcessingTime column in the Transactions table? A. Transactions | summarize VarProcessingTime = variance(ProcessingTime) B. Transactions | summarize VarProcessingTime = var(ProcessingTime) C. Transactions | summarize VarProcessingTime = varp(ProcessingTime) Answer : Transactions | summarize VarProcessingTime = var(ProcessingTime) Which KQL function would you use to count the number of non-null values in a column? A. count() B. countif() C. count_not_null() Answer : countif() How can you filter the Invoices table to show only records where the Total is greater than 500? A. Invoices | where Total > 500 B. Invoices | filter Total > 500 C. Invoices | find Total > 500 Answer : Invoices | where Total > 500 How would you create a new column Year by extracting the year from the Date column in the Events table? A. Events | extend Year = year(Date) B. Events | extend Year = extract('year', Date) C. Events | extend Year = getyear(Date) Answer : Events | extend Year = year(Date) Which function in KQL would you use to get the sum of a column Amount in the Payments table? A. Payments | summarize TotalAmount = sum(Amount) B. Payments | summarize TotalAmount = sum_amount(Amount) C. Payments | summarize TotalAmount = adds(Amount) Answer : Payments | summarize TotalAmount = sum(Amount) How can you find the earliest HireDate in the Employees table? A. Employees | summarize EarliestHire = earliest(HireDate) B. Employees | summarize EarliestHire = min(HireDate) C. Employees | summarize EarliestHire = first(HireDate) Answer : Employees | summarize EarliestHire = min(HireDate) How would you calculate the difference in days between two datetime columns StartDate and EndDate in the Projects table? A. Projects | extend DaysBetween = date_diff('day', EndDate, StartDate) B. Projects | extend DaysBetween = datetime_diff('day', EndDate, StartDate) C. Projects | extend DaysBetween = day_diff(EndDate, StartDate Answer : Projects | extend DaysBetween = date_diff('day', EndDate, StartDate) How can you find the latest EndDate in the Tasks table? A. Tasks | summarize LatestEnd = latest(EndDate) B. Tasks | summarize LatestEnd = max(EndDate) C. Tasks | summarize LatestEnd = last(EndDate) Answer : Tasks | summarize LatestEnd = max(EndDate) Which function in KQL would you use to concatenate multiple string columns in the Products table? A. Products | extend FullDescription = concat(Description, \" - \", Category) B. Products | extend FullDescription = strcat(Description, \" - \", Category) C. Products | extend FullDescription = joinstr(Description, \" - \", Category) Answer : Products | extend FullDescription = strcat(Description, \" - \", Category) How can you calculate the total number of orders in the Orders table? A. Orders | summarize TotalOrders = count() B. Orders | count() C. Orders | summarize TotalOrders = count(OrderID) Answer : Orders | summarize TotalOrders = count() How would you find all records in the Errors table where the Message column contains the word \"timeout\"? A. Errors | where Message contains \"timeout\" B. Errors | where Message == \"timeout\" C. Errors | search \"timeout\" Answer : Errors | where Message contains \"timeout\" How would you list the unique values of the Status column from the Tasks table? A. Tasks | distinct Status B. Tasks | summarize by Status C. Tasks | unique Status Answer : Tasks | distinct Status How can you calculate the average ResponseTime for each URL in the WebRequests table? A. WebRequests | summarize AvgResponseTime = avg(ResponseTime) by URL B. WebRequests | summarize AvgResponseTime = mean(ResponseTime) by URL C. WebRequests | summarize AvgResponseTime = average(ResponseTime) by URL Answer : WebRequests | summarize AvgResponseTime = avg(ResponseTime) by URL How would you calculate the cumulative sum of the Revenue column in the Sales table? A. Sales | extend CumulativeRevenue = sum(Revenue) B. Sales | extend CumulativeRevenue = running_sum(Revenue) C. Sales | extend CumulativeRevenue = cumulative_sum(Revenue) Answer : Sales | extend CumulativeRevenue = cumulative_sum(Revenue) Which function in KQL would you use to get the maximum value of a column Value in the Metrics table? A. Metrics | summarize MaxValue = max(Value) B. Metrics | summarize MaxValue = maximum(Value) C. Metrics | summarize MaxValue = greatest(Value) Answer : Metrics | summarize MaxValue = max(Value) How can you filter the Activities table to show only records where the ActivityDate is in the current month? A. Activities | where month(ActivityDate) == month(now()) B. Activities | where ActivityDate >= startofmonth(now()) C. Activities | where ActivityDate between (startofmonth(now()) .. endofmonth(now())) Answer : Activities | where month(ActivityDate) == month(now()) Which function in KQL is used to split a string into an array based on a delimiter? A. split() B. string_split() C. explode() Answer : split() How would you calculate the 75th percentile of the ProcessingTime column in the Operations table? A. Operations | summarize Percentile_75 = percentile(ProcessingTime, 75) B. Operations | summarize Percentile_75 = p75(ProcessingTime) C. Operations | summarize Percentile_75 = percentile_approx(ProcessingTime, 0.75) Answer : Operations | summarize Percentile_75 = percentile(ProcessingTime, 75) How would you calculate the median value of the Income column in the Employees table? A. Employees | summarize MedianIncome = median(Income) B. Employees | summarize MedianIncome = percentile(Income, 50) C. Employees | summarize MedianIncome = avg(Income) Answer : Employees | summarize MedianIncome = percentile(Income, 50) How can you list the unique values of the Category column from the Products table? A. Products | distinct Category B. Products | summarize by Category C. Products | unique Category Answer : Products | distinct Category Which function in KQL is used to format a datetime value as a string? A. format_datetime() B. datetime_to_string() C. tostring() Answer : format_datetime() How can you create a new column Month by extracting the month from the Timestamp column in the Events table? A. Events | extend Month = extract('month', Timestamp) B. Events | extend Month = month(Timestamp) C. Events | extend Month = getmonth(Timestamp) Answer : Events | extend Month = month(Timestamp) How do you filter the Orders table to include only those where the OrderDate is within the last 7 days? A. Orders | where OrderDate > ago(7d) B. Orders | where OrderDate >= startofweek(now()) C. Orders | where OrderDate between (now() - 7d) and now() Answer : Orders | where OrderDate > ago(7d) How would you extract the year from a datetime column called Timestamp in the Logs table? A. Logs | extend Year = year(Timestamp) B. Logs | extend Year = extract('year', Timestamp) C. Logs | extend Year = getyear(Timestamp) Answer : Logs | extend Year = year(Timestamp) How would you rename the column OldName to NewName in a KQL query? A. | project-rename NewName = OldName B. | rename OldName to NewName C. | project OldName as NewName Answer : | project OldName as NewName Which function in KQL would you use to get the current date and time? A. now() B. current_datetime() C. getdate() Answer : now() How can you create a column FullName by concatenating FirstName and LastName in the Employees table? A. Employees | extend FullName = FirstName + \" \" + LastName B. Employees | extend FullName = strcat(FirstName, \" \", LastName) C. Employees | extend FullName = concat(FirstName, \" \", LastName) Answer : Employees | extend FullName = strcat(FirstName, \" \", LastName) How would you find all records in the Logs table where the Message column contains the word \"error\"? A. Logs | where Message contains \"error\" B. Logs | where Message == \"error\" C. Logs | search \"error\" Answer : Logs | where Message contains \"error\" Which KQL function would you use to convert a string to a datetime value? A. todatetime() B. string_to_datetime() C. datetime() Answer : todatetime() How would you filter rows in the Sales table to only include those where the Date is within the last 30 days? A. Sales | where Date > ago(30d) B. Sales | where Date between (now() - 30d) and now() C. Sales | where Date > datetime(30 days ago) Answer : Sales | where Date > ago(30d) How do you join two tables Table1 and Table2 on the ID column, keeping all records from Table1 ? A. Table1 | innerjoin (Table2) on ID B. Table1 | join kind=inner (Table2) on ID C. Table1 | join kind=leftouter (Table2) on ID Answer : Table1 | join kind=leftouter (Table2) on ID How would you list the top 10 products by total sales in the Sales table? A. Sales | top 10 by sum(TotalSales) B. Sales | summarize TotalSales = sum(SalesAmount) by ProductID | top 10 by TotalSales C. Sales | summarize TotalSales = sum(SalesAmount) by ProductID | limit 10 by TotalSales Answer : Sales | summarize TotalSales = sum(SalesAmount) by ProductID | top 10 by TotalSales How would you create a time series chart for the Sales table's total revenue over time? A. Sales | summarize TotalRevenue = sum(Revenue) by bin(Time, 1h) | render timechart B. Sales | summarize TotalRevenue = sum(Revenue) by bin(Time, 1h) | render linechart C. Sales | summarize TotalRevenue = sum(Revenue) by bin(Time, 1h) | render barchart Answer : Sales | summarize TotalRevenue = sum(Revenue) by bin(Time, 1h) | render linechart How can you calculate the total number of distinct customers in the Sales table? A. Sales | summarize DistinctCustomers = dcount(CustomerID) B. Sales | summarize DistinctCustomers = countdistinct(CustomerID) C. Sales | summarize DistinctCustomers = unique(CustomerID) Answer : Sales | summarize DistinctCustomers = dcount(CustomerID) How can you concatenate the values of two columns FirstName and LastName in the Contacts table? A. Contacts | extend FullName = strcat(FirstName, \" \", LastName) B. Contacts | extend FullName = concat(FirstName, \" \", LastName) C. Contacts | extend FullName = joinstr(FirstName, \" \", LastName) Answer : Contacts | extend FullName = strcat(FirstName, \" \", LastName)","title":"KQL Quesitons - General"},{"location":"Microsoft-Fabric/PandasVsSparkDf/","text":"Here is a question from Microsoft Fabric Certificaiton exam: You have a Parquet file named Customers.parquet uploaded to the Files section of a Fabric lakehouse. You plan to use Data Wrangler to view basic summary statistics for the data before you load it to a Delta table. You open a notebook in the lakehouse. You need to load the data to a pandas DataFrame. Which PySpark code should you run to complete the task? Select only one answer. df = pandas.read_parquet(\"/lakehouse/default/Files/Customers.parquet\") df = pandas.read_parquet(\"/lakehouse/Files/Customers.parquet\") This answer is incorrect. import pandas as pd df = pd.read_parquet(\"/lakehouse/default/Files/Customers.parquet\") This answer is correct. import pandas as pd df = pd.read_parquet(\"/lakehouse/Files/Customers.parquet\") To load data to a pandas DataFrame, you must first import the pandas library by running import pandas as pd. Pandas DataFrames use the File API Path vs. the File relative path that Spark uses. The File API Path has the format of lakehouse/default/Files/Customers.parquet.","title":"PandasVsSparkDf"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL/","text":"Describe Formatted TableName DataFrame from csv Write Delta Table Creating a SparkSession python from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"xxx\").getOrCreate() df = spark.read.csv(\"abc.csv\", header=True, inferSchema=True) Creating a DataFrame Showing dataframe, rows describing tables df.show() python df.show() df.show(n=10, truncate=False) display(df) pyrhon display(df.limit(3)) df.describe() describe is used to generate descriptive statistics of the DataFrame. For numeric data, results include COUNT, MEAN, STD, MIN, and MAX, while for object data it will also include TOP, UNIQUE, and FREQ. python df.describe() df.describe().show() python df.printSchema() df.columns DESCRIBE FORMATTED tableName python spark.sql(\"DESCRIBE FORMATTED tableName\") SQL CREATE OR REPLACE VIEW doesn't work in Fabric/AzureSynapse/ADF etc. Instead use this: If Exists (Select * From sys.sysobjects where name = 'apple') DROP TABLE dbo.apple; GO Dropping a table Small dataframe df.limit(100) Selecting Columns python df.select(\"column1\", \"column2\").show() Filtering Data python df.filter(df[\"column\"] > value).show() df.filter(df[\"column\"] == \"value\").show() Adding Columns python df.withColumn(\"new_column\", df[\"existing_column\"] * 2).show() Renaming Columns python df.withColumnRenamed(\"old_name\", \"new_name\") Dropping Columns python df.drop(\"column_name\") Grouping and Aggregating python df.groupBy(\"column\").count().show() df.groupBy(\"column\").agg({\"column2\": \"avg\", \"column3\": \"sum\"}).show() Sorting Data python df.orderBy(\"column\").show() df.orderBy(df[\"column\"].desc()).show() RDD Operations Creating an RDD python rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5]) Transformations python rdd2 = rdd.map(lambda x: x * 2) rdd3 = rdd.filter(lambda x: x % 2 == 0) Actions python rdd.collect() rdd.count() rdd.first() rdd.take(3) SQL Operations Creating Temp View python df.createOrReplaceTempView(\"table_name\") Running SQL Queries python spark.sql(\"SELECT * FROM table_name\").show() Saving Data Saving as CSV python df.write.csv(\"path/to/save.csv\") Saving as Parquet python df.write.parquet(\"path/to/save.parquet\") Saving to Hive python df.write.saveAsTable(\"table_name\") Miscellaneous Caching and Unpersisting DataFrames python df.cache() df.unpersist() Explain Plan python df.explain() Repartitioning Data python df.repartition(10) df.coalesce(5) Pyspark when(condition).otherwise(default) from pyspark.sql.functions import col, when result = when(col(\"Age\") > 16, True).otherwise(False) Remember The GroupBY columns must match the columns used in the SELECT statement. DENSE_RANK() function returns the rank of each row within the result set partition, with no gaps in the ranking values. The RANK() function includes gaps in the ranking.","title":"Pyspark|SparkSQL CheatSheet"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL/#creating-a-sparksession","text":"python from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"xxx\").getOrCreate() df = spark.read.csv(\"abc.csv\", header=True, inferSchema=True)","title":"Creating a SparkSession"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL/#creating-a-dataframe","text":"","title":"Creating a DataFrame"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL/#showing-dataframe-rows-describing-tables","text":"","title":"Showing dataframe, rows describing tables"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL/#dfshow","text":"python df.show() df.show(n=10, truncate=False)","title":"df.show()"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL/#displaydf","text":"pyrhon display(df.limit(3))","title":"display(df)"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL/#dfdescribe","text":"describe is used to generate descriptive statistics of the DataFrame. For numeric data, results include COUNT, MEAN, STD, MIN, and MAX, while for object data it will also include TOP, UNIQUE, and FREQ. python df.describe() df.describe().show() python df.printSchema() df.columns","title":"df.describe()"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL/#describe-formatted-tablename","text":"python spark.sql(\"DESCRIBE FORMATTED tableName\")","title":"DESCRIBE FORMATTED tableName"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL/#sql","text":"CREATE OR REPLACE VIEW doesn't work in Fabric/AzureSynapse/ADF etc. Instead use this: If Exists (Select * From sys.sysobjects where name = 'apple') DROP TABLE dbo.apple; GO","title":"SQL"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL/#dropping-a-table","text":"Small dataframe df.limit(100) Selecting Columns python df.select(\"column1\", \"column2\").show() Filtering Data python df.filter(df[\"column\"] > value).show() df.filter(df[\"column\"] == \"value\").show() Adding Columns python df.withColumn(\"new_column\", df[\"existing_column\"] * 2).show() Renaming Columns python df.withColumnRenamed(\"old_name\", \"new_name\") Dropping Columns python df.drop(\"column_name\") Grouping and Aggregating python df.groupBy(\"column\").count().show() df.groupBy(\"column\").agg({\"column2\": \"avg\", \"column3\": \"sum\"}).show() Sorting Data python df.orderBy(\"column\").show() df.orderBy(df[\"column\"].desc()).show()","title":"Dropping a table"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL/#rdd-operations","text":"Creating an RDD python rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5]) Transformations python rdd2 = rdd.map(lambda x: x * 2) rdd3 = rdd.filter(lambda x: x % 2 == 0) Actions python rdd.collect() rdd.count() rdd.first() rdd.take(3)","title":"RDD Operations"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL/#sql-operations","text":"Creating Temp View python df.createOrReplaceTempView(\"table_name\") Running SQL Queries python spark.sql(\"SELECT * FROM table_name\").show()","title":"SQL Operations"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL/#saving-data","text":"Saving as CSV python df.write.csv(\"path/to/save.csv\") Saving as Parquet python df.write.parquet(\"path/to/save.parquet\") Saving to Hive python df.write.saveAsTable(\"table_name\")","title":"Saving Data"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL/#miscellaneous","text":"Caching and Unpersisting DataFrames python df.cache() df.unpersist() Explain Plan python df.explain() Repartitioning Data python df.repartition(10) df.coalesce(5)","title":"Miscellaneous"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL/#pyspark-whenconditionotherwisedefault","text":"from pyspark.sql.functions import col, when result = when(col(\"Age\") > 16, True).otherwise(False)","title":"Pyspark when(condition).otherwise(default)"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL/#remember","text":"The GroupBY columns must match the columns used in the SELECT statement. DENSE_RANK() function returns the rank of each row within the result set partition, with no gaps in the ranking values. The RANK() function includes gaps in the ranking.","title":"Remember"},{"location":"Microsoft-Fabric/RealTimeAnalytics/","text":"Real-time Intelligence - Microsoft Fabric Core elements of Real-Time Intelligence in Microsoft Fabric Eventhouse KQL Database KQL Queryset Real-Time Dashboards Eventstream Let's Get Started Create a Power BI Report from KQL Queryset Real-time Intelligence - Microsoft Fabric Core elements of Real-Time Intelligence in Microsoft Fabric Eventhouse Central workspace/hub - has multiple KQL databases Use an Eventhouse for event-based scenarios Automatically index and partition data based on ingestion time When you create an EventHouse, it initializes a KQL database with the same name inside it KQL databses can be standalone or part of an EventHouse Can ingest data from multiple sources KQL Database A KQL (Kusto Query Language) Database handles large volumes of structured, semi-structured, and unstructured data for real-time analytics and ad-hoc querying. It is part of the Azure Data Explorer service. The data in a KQL database is stored in Azure Data Explorer . It uses a columnar storage format, for high-performance. KQL Queryset It is a just a query written in KQL. Let's not make it more complex than that! Real-Time Dashboards Customizable control panels for displaying specific data. Tiles for different data views, organized on various pages. Export KQL queries into visual tiles. Enhances data exploration and visualization. Eventstream Handles live data without coding. Automates data collection, transformation, and distribution. Processes real-time data for immediate insights. Let's Get Started Open Real-Time Intelligence: Select it from the bottom left-hand corner of the screen: Start Screen: Here is how it looks: Create an Eventhouse: Eventhouses are groups of databases. When you create an Eventhouse, Fabric creates a KQL Database with the same name inside it KQL Database Structure: This is the standard structure: Data Ingestion: Importing data from files like .csv is a childs play using the GUI, which creates a fully structured database with the correct columns. Create a Power BI Report from KQL Queryset Click the three dots next to the table. Select \"Show any 100 records\" to open the KQL editor. Create your custom KQL query. Save the query or create the Power BI report directly from the editor.","title":"Real-time Intelligence"},{"location":"Microsoft-Fabric/RealTimeAnalytics/#real-time-intelligence-microsoft-fabric","text":"","title":"Real-time Intelligence - Microsoft Fabric"},{"location":"Microsoft-Fabric/RealTimeAnalytics/#core-elements-of-real-time-intelligence-in-microsoft-fabric","text":"","title":"Core elements of Real-Time Intelligence in Microsoft Fabric"},{"location":"Microsoft-Fabric/RealTimeAnalytics/#eventhouse","text":"Central workspace/hub - has multiple KQL databases Use an Eventhouse for event-based scenarios Automatically index and partition data based on ingestion time When you create an EventHouse, it initializes a KQL database with the same name inside it KQL databses can be standalone or part of an EventHouse Can ingest data from multiple sources","title":"Eventhouse"},{"location":"Microsoft-Fabric/RealTimeAnalytics/#kql-database","text":"A KQL (Kusto Query Language) Database handles large volumes of structured, semi-structured, and unstructured data for real-time analytics and ad-hoc querying. It is part of the Azure Data Explorer service. The data in a KQL database is stored in Azure Data Explorer . It uses a columnar storage format, for high-performance.","title":"KQL Database"},{"location":"Microsoft-Fabric/RealTimeAnalytics/#kql-queryset","text":"It is a just a query written in KQL. Let's not make it more complex than that!","title":"KQL Queryset"},{"location":"Microsoft-Fabric/RealTimeAnalytics/#real-time-dashboards","text":"Customizable control panels for displaying specific data. Tiles for different data views, organized on various pages. Export KQL queries into visual tiles. Enhances data exploration and visualization.","title":"Real-Time Dashboards"},{"location":"Microsoft-Fabric/RealTimeAnalytics/#eventstream","text":"Handles live data without coding. Automates data collection, transformation, and distribution. Processes real-time data for immediate insights.","title":"Eventstream"},{"location":"Microsoft-Fabric/RealTimeAnalytics/#lets-get-started","text":"Open Real-Time Intelligence: Select it from the bottom left-hand corner of the screen: Start Screen: Here is how it looks: Create an Eventhouse: Eventhouses are groups of databases. When you create an Eventhouse, Fabric creates a KQL Database with the same name inside it KQL Database Structure: This is the standard structure: Data Ingestion: Importing data from files like .csv is a childs play using the GUI, which creates a fully structured database with the correct columns.","title":"Let's Get Started"},{"location":"Microsoft-Fabric/RealTimeAnalytics/#create-a-power-bi-report-from-kql-queryset","text":"Click the three dots next to the table. Select \"Show any 100 records\" to open the KQL editor. Create your custom KQL query. Save the query or create the Power BI report directly from the editor.","title":"Create a Power BI Report from KQL Queryset"},{"location":"Misc/10_Fact_vs_Dimension_tables/","text":"Fact Table vs. Dimension Table (Simple Explanation) In simple terms, a fact table is where you store numbers or quantities , and a dimension table is where you store descriptions . Just remember these: Fact tables = verbs(e.g. Orders). Dimension tables = nowns(products, customers). Fact tables are heavy. Dimension tables are light. The fact table has the measurements (numbers) and has the primary keys to link to the descriptive information in dimension tables.","title":"Fact Vs Dimension Table"},{"location":"Misc/10_Fact_vs_Dimension_tables/#fact-table-vs-dimension-table-simple-explanation","text":"In simple terms, a fact table is where you store numbers or quantities , and a dimension table is where you store descriptions . Just remember these: Fact tables = verbs(e.g. Orders). Dimension tables = nowns(products, customers). Fact tables are heavy. Dimension tables are light. The fact table has the measurements (numbers) and has the primary keys to link to the descriptive information in dimension tables.","title":"Fact Table vs. Dimension Table (Simple Explanation)"},{"location":"Misc/1_GoogleCloudSeeUsage/","text":"Background Worried about what resources are running and causing costs? You can check them directly from the cloud console. Here are some of the useful commands to check it. Single command to see it all Copy and paste this command in your cloud console to get an overview. gcloud compute instances list && \\ gcloud container clusters list && \\ gcloud storage buckets list && \\ gcloud sql instances list && \\ gcloud functions list && \\ gcloud app services list Separate commands You can run the following commands in your cloud console to see each service if they are running or not. Task Command Description List All Compute Engine VM Instances gcloud compute instances list Shows all the virtual machines running in your project, with details like zone, machine type, and status. List Kubernetes Clusters gcloud container clusters list Shows all the Kubernetes Engine clusters in your project. List Cloud Storage Buckets gcloud storage buckets list Lists all the storage buckets in your project. List Cloud SQL Instances gcloud sql instances list Shows all the Cloud SQL instances in your project. List App Engine Services gcloud app services list Shows all the services you have deployed in App Engine. List Cloud Functions gcloud functions list Lists all the Cloud Functions running in your project. List BigQuery Datasets bq ls Lists all the BigQuery datasets in your project. List All Resources in Your Project gcloud projects describe $(gcloud config get-value project) Gives an overview of your project, but doesn\u2019t list every resource. Use specific commands for detailed info. Check Billing Information gcloud alpha billing accounts list Shows billing accounts, but might not give a detailed cost breakdown. List Active Firewall Rules (Networking) gcloud compute firewall-rules list Lists all the firewall rules, helping you see network settings that might be in use. List All Active Services gcloud services list --enabled Shows all the enabled APIs and services in your Google Cloud project.","title":"Check Google Usage"},{"location":"Misc/1_GoogleCloudSeeUsage/#background","text":"Worried about what resources are running and causing costs? You can check them directly from the cloud console. Here are some of the useful commands to check it.","title":"Background"},{"location":"Misc/1_GoogleCloudSeeUsage/#single-command-to-see-it-all","text":"Copy and paste this command in your cloud console to get an overview. gcloud compute instances list && \\ gcloud container clusters list && \\ gcloud storage buckets list && \\ gcloud sql instances list && \\ gcloud functions list && \\ gcloud app services list","title":"Single command to see it all"},{"location":"Misc/1_GoogleCloudSeeUsage/#separate-commands","text":"You can run the following commands in your cloud console to see each service if they are running or not. Task Command Description List All Compute Engine VM Instances gcloud compute instances list Shows all the virtual machines running in your project, with details like zone, machine type, and status. List Kubernetes Clusters gcloud container clusters list Shows all the Kubernetes Engine clusters in your project. List Cloud Storage Buckets gcloud storage buckets list Lists all the storage buckets in your project. List Cloud SQL Instances gcloud sql instances list Shows all the Cloud SQL instances in your project. List App Engine Services gcloud app services list Shows all the services you have deployed in App Engine. List Cloud Functions gcloud functions list Lists all the Cloud Functions running in your project. List BigQuery Datasets bq ls Lists all the BigQuery datasets in your project. List All Resources in Your Project gcloud projects describe $(gcloud config get-value project) Gives an overview of your project, but doesn\u2019t list every resource. Use specific commands for detailed info. Check Billing Information gcloud alpha billing accounts list Shows billing accounts, but might not give a detailed cost breakdown. List Active Firewall Rules (Networking) gcloud compute firewall-rules list Lists all the firewall rules, helping you see network settings that might be in use. List All Active Services gcloud services list --enabled Shows all the enabled APIs and services in your Google Cloud project.","title":"Separate commands"},{"location":"Misc/2_InstallVMWareFree/","text":"How to Download and Install VMware Workstation Pro from Broadcom And Use for Free! Visit Broadcom's Website : Go to broadcom.com . Access the Support Portal : In the top right corner, click on 'Support Portal'. If you have an account, click 'Go To Portal' to log in. If not, click 'Register' to create a basic Broadcom account. Go to VMware Cloud Foundation : After logging in, if you're not automatically redirected, visit support.broadcom.com . From the dropdown, select the 'VMware Cloud Foundation' division. Find Your Downloads : On the left side, click on 'My Downloads'. Search for 'Fusion' or Workstation . Download VMware Workstation Pro : Click on 'VMware Workstation Pro for Windows'. You\u2019ll see a dropdown for the Personal Use edition (same as the Commercial version). Select the version you want (17.5.2 or 13.5.2). Install and Set Up : Download and install the software. When asked for a key, just skip it. After installation, open the software, select 'personal use', and you're all set to create VM images! Install and use Linux OS Download Ubuntu Linux. Its free on their website. It will be dowloaded as ubuntu-24.04-desktop-amd64.iso or some other version. Then Use the VMWare workstation to create the image","title":"VMWare For Free"},{"location":"Misc/2_InstallVMWareFree/#how-to-download-and-install-vmware-workstation-pro-from-broadcom-and-use-for-free","text":"Visit Broadcom's Website : Go to broadcom.com . Access the Support Portal : In the top right corner, click on 'Support Portal'. If you have an account, click 'Go To Portal' to log in. If not, click 'Register' to create a basic Broadcom account. Go to VMware Cloud Foundation : After logging in, if you're not automatically redirected, visit support.broadcom.com . From the dropdown, select the 'VMware Cloud Foundation' division. Find Your Downloads : On the left side, click on 'My Downloads'. Search for 'Fusion' or Workstation . Download VMware Workstation Pro : Click on 'VMware Workstation Pro for Windows'. You\u2019ll see a dropdown for the Personal Use edition (same as the Commercial version). Select the version you want (17.5.2 or 13.5.2). Install and Set Up : Download and install the software. When asked for a key, just skip it. After installation, open the software, select 'personal use', and you're all set to create VM images!","title":"How to Download and Install VMware Workstation Pro from Broadcom And Use for Free!"},{"location":"Misc/2_InstallVMWareFree/#install-and-use-linux-os","text":"Download Ubuntu Linux. Its free on their website. It will be dowloaded as ubuntu-24.04-desktop-amd64.iso or some other version. Then Use the VMWare workstation to create the image","title":"Install and use Linux OS"},{"location":"Misc/3_Markdown/","text":"Fonts These fonts should be already present in your system. You can download it from internet and install them. h1 in LovelexieHandwritten Architects Daughter h1 in Gunny Rewritten h1 in Permanent Marker h1 Comic Sans MS # Escape PIPE character In markdown using | will create confusion. Use | # HTML Colors This guide helps you quickly find and use HTML colors. You can style your text by using the following format: **\\ The text\\ **. This changes the color of \"The text\" to blue. Basic Colors Extended Colors Named Colors Red Aqua AliceBlue Green Fuchsia AntiqueWhite Blue Lime Aquamarine Yellow Maroon Azure Cyan Navy Beige Magenta Olive Bisque Black Purple BlanchedAlmond White Silver BlueViolet Gray Teal Brown BurlyWood CadetBlue Chartreuse Chocolate Coral CornflowerBlue Cornsilk Crimson DarkBlue DarkCyan DarkGoldenRod ## Almost all colors | Color Name | Hex Code | Example | |------------|----------|---------| | Black | #000000 | This is a sample text\u25a0 | | White | #FFFFFF | \u25a0 | | Red | #FF0000 | This is a sample text\u25a0 | | Lime | #00FF00 | This is a sample text\u25a0 | | Blue | #0000FF | This is a sample text\u25a0 | | Yellow | #FFFF00 | This is a sample text\u25a0 | | Cyan | #00FFFF | This is a sample text\u25a0 | | Magenta | #FF00FF | This is a sample text\u25a0 | | Silver | #C0C0C0 | This is a sample text\u25a0 | | Gray | #808080 | This is a sample text\u25a0 | | Maroon | #800000 | This is a sample text\u25a0 | | Olive | #808000 | This is a sample text\u25a0 | | Green | #008000 | This is a sample text\u25a0 | | Purple | #800080 | This is a sample text\u25a0 | | Teal | #008080 | This is a sample text\u25a0 | | Navy | #000080 | This is a sample text\u25a0 | | Orange | #FFA500 | This is a sample text\u25a0 | | AliceBlue | #F0F8FF | This is a sample text\u25a0 | | AntiqueWhite | #FAEBD7 | This is a sample text\u25a0 | | Aqua | #00FFFF | This is a sample text\u25a0 | | Aquamarine | #7FFFD4 | This is a sample text\u25a0 | | Azure | #F0FFFF | This is a sample text\u25a0 | | Beige | #F5F5DC | This is a sample text\u25a0 | | Bisque | #FFE4C4 | This is a sample text\u25a0 | | BlanchedAlmond | #FFEBCD | This is a sample text\u25a0 | | BlueViolet | #8A2BE2 | This is a sample text\u25a0 | | Brown | #A52A2A | This is a sample text\u25a0 | | BurlyWood | #DEB887 | This is a sample text\u25a0 | | CadetBlue | #5F9EA0 | This is a sample text\u25a0 | | Chartreuse | #7FFF00 | This is a sample text\u25a0 | | Chocolate | #D2691E | This is a sample text\u25a0 | | Coral | #FF7F50 | This is a sample text\u25a0 | | CornflowerBlue | #6495ED | This is a sample text\u25a0 | | Cornsilk | #FFF8DC | This is a sample text\u25a0 | | Crimson | #DC143C | This is a sample text\u25a0 | | DarkBlue | #00008B | This is a sample text\u25a0 | | DarkCyan | #008B8B | This is a sample text\u25a0 | | DarkGoldenRod | #B8860B | This is a sample text\u25a0 | | DarkGray | #A9A9A9 | This is a sample text\u25a0 | | DarkGreen | #006400 | This is a sample text\u25a0 | | DarkKhaki | #BDB76B | This is a sample text\u25a0 | | DarkMagenta | #8B008B | This is a sample text\u25a0 | | DarkOliveGreen | #556B2F | This is a sample text\u25a0 | | Darkorange | #FF8C00 | This is a sample text\u25a0 | | DarkOrchid | #9932CC | This is a sample text\u25a0 | | DarkRed | #8B0000 | This is a sample text\u25a0 | | DarkSalmon | #E9967A | This is a sample text\u25a0 | | DarkSeaGreen | #8FBC8F | This is a sample text\u25a0 | | DarkSlateBlue | #483D8B | This is a sample text\u25a0 | | DarkSlateGray | #2F4F4F | This is a sample text\u25a0 | | DarkTurquoise | #00CED1 | This is a sample text\u25a0 | | DarkViolet | #9400D3 | This is a sample text\u25a0 | | DeepPink | #FF1493 | This is a sample text\u25a0 | | DeepSkyBlue | #00BFFF | This is a sample text\u25a0 | | DimGray | #696969 | This is a sample text\u25a0 | | DodgerBlue | #1E90FF | This is a sample text\u25a0 | | FireBrick | #B22222 | This is a sample text\u25a0 | | FloralWhite | #FFFAF0 | This is a sample text\u25a0 | | ForestGreen | #228B22 | This is a sample text\u25a0 | | Fuchsia | #FF00FF | This is a sample text\u25a0 | | Gainsboro | #DCDCDC | This is a sample text\u25a0 | | GhostWhite | #F8F8FF | This is a sample text\u25a0 | | Gold | #FFD700 | This is a sample text\u25a0 | | GoldenRod | #DAA520 | This is a sample text\u25a0 | | GreenYellow | #ADFF2F | This is a sample text\u25a0 | | HoneyDew | #F0FFF0 | This is a sample text\u25a0 | | HotPink | #FF69B4 | This is a sample text\u25a0 | | IndianRed | #CD5C5C | This is a sample text\u25a0 | | Indigo | #4B0082 | This is a sample text\u25a0 | | Ivory | #FFFFF0 | This is a sample text\u25a0 | | Khaki | #F0E68C | This is a sample text\u25a0 | | Lavender | #E6E6FA | This is a sample text\u25a0 | | LavenderBlush | #FFF0F5 | This is a sample text\u25a0 | | LawnGreen | #7CFC00 | This is a sample text\u25a0 | | LemonChiffon | #FFFACD | This is a sample text\u25a0 | | LightBlue | #ADD8E6 | This is a sample text\u25a0 | | LightCoral | #F08080 | This is a sample text\u25a0 | | LightCyan | #E0FFFF | This is a sample text\u25a0 | | LightGoldenRodYellow | #FAFAD2 | This is a sample text\u25a0 | | LightGray | #D3D3D3 | This is a sample text\u25a0 | | LightGreen | #90EE90 | This is a sample text\u25a0 | | LightPink | #FFB6C1 | This is a sample text\u25a0 | | LightSalmon | #FFA07A | This is a sample text\u25a0 | | LightSeaGreen | #20B2AA | This is a sample text\u25a0 | | LightSkyBlue | #87CEFA | This is a sample text\u25a0 | | LightSlateGray | #778899 | This is a sample text\u25a0 | | LightSteelBlue | #B0C4DE | This is a sample text\u25a0 | | LightYellow | #FFFFE0 | This is a sample text\u25a0 | | LimeGreen | #32CD32 | This is a sample text\u25a0 | | Linen | #FAF0E6 | This is a sample text\u25a0 | | MediumAquaMarine | #66CDAA | This is a sample text\u25a0 | | MediumBlue | #0000CD | This is a sample text\u25a0 | | MediumOrchid | #BA55D3 | This is a sample text\u25a0 | | MediumPurple | #9370DB | This is a sample text\u25a0 | | MediumSeaGreen | #3CB371 | This is a sample text\u25a0 | | MediumSlateBlue | #7B68EE | This is a sample text\u25a0 | | MediumSpringGreen | #00FA9A | This is a sample text\u25a0 | | MediumTurquoise | #48D1CC | This is a sample text\u25a0 | | MediumVioletRed | #C71585 | This is a sample text\u25a0 | | MidnightBlue | #191970 | This is a sample text\u25a0 | | MintCream | #F5FFFA | This is a sample text\u25a0 | | MistyRose | #FFE4E1 | This is a sample text\u25a0 | | Moccasin | #FFE4B5 | This is a sample text\u25a0 | | NavajoWhite | #FFDEAD | This is a sample text\u25a0 | | OldLace | #FDF5E6 | This is a sample text\u25a0 | | OliveDrab | #6B8E23 | This is a sample text\u25a0 | | OrangeRed | #FF4500 | This is a sample text\u25a0 | | Orchid | #DA70D6 | This is a sample text\u25a0 | | PaleGoldenRod | #EEE8AA | This is a sample text\u25a0 | | PaleGreen | #98FB98 | This is a sample text\u25a0 | | PaleTurquoise | #AFEEEE | This is a sample text\u25a0 | | PaleVioletRed | #DB7093 | This is a sample text\u25a0 | | PapayaWhip | #FFEFD5 | This is a sample text\u25a0 | | PeachPuff | #FFDAB9 | This is a sample text\u25a0 | | Peru | #CD853F | This is a sample text\u25a0 | | Pink | #FFC0CB | This is a sample text\u25a0 | | Plum | #DDA0DD | This is a sample text\u25a0 | | PowderBlue | #B0E0E6 | This is a sample text\u25a0 | | RosyBrown | #BC8F8F | This is a sample text\u25a0 | | RoyalBlue | #4169E1 | This is a sample text\u25a0 | | SaddleBrown | #8B4513 | This is a sample text\u25a0 | | Salmon | #FA8072 | This is a sample text\u25a0 | | SandyBrown | #F4A460 | This is a sample text\u25a0 | | SeaGreen | #2E8B57 | This is a sample text\u25a0 | | SeaShell | #FFF5EE | This is a sample text\u25a0 | | Sienna | #A0522D | This is a sample text\u25a0 | | SkyBlue | #87CEEB | This is a sample text\u25a0 | | SlateBlue | #6A5ACD | This is a sample text\u25a0 | | SlateGray | #708090 | This is a sample text\u25a0 | | Snow | #FFFAFA | This is a sample text\u25a0 | | SpringGreen | #00FF7F | This is a sample text\u25a0 | | SteelBlue | #4682B4 | This is a sample text\u25a0 | | Tan | #D2B48C | This is a sample text\u25a0 | | Thistle | #D8BFD8 | This is a sample text\u25a0 | | Tomato | #FF6347 | This is a sample text\u25a0 | | Turquoise | #40E0D0 | This is a sample text\u25a0 | | Violet | #EE82EE | This is a sample text\u25a0 | | Wheat | #F5DEB3 | This is a sample text\u25a0 | | WhiteSmoke | #F5F5F5 | This is a sample text\u25a0 | | YellowGreen | #9ACD32 | This is a sample text\u25a0 | # Sample Div This is how I create divs. I create the block once, and reuse it. Note: Changeable items (also called mutable) are items that can be modified after they are created. For example: Lists : You can add, remove, or change elements. Dictionaries : You can add, remove, or change key-value pairs. These items cannot be added to a set because sets need items that do not change. Unchangeable items (also called immutable) are items that cannot be modified after they are created. For example: Numbers : Once created, their values cannot be changed. Strings : Any modification creates a new string. Tuples : Their elements cannot be changed once created. These items can be added to a set because their values stay the same. Note 1 Note 2 Note 3 Note 4 Note 5 Note 6 Dashboard Sidebar Home Profile Settings Logout Content Block 1 This is some content inside the first content block. Content Block 2 This is some content inside the second content block. Content Block 3 This is some content inside the third content block. \u00a9 2024 Dass creation. All rights reserved. range(start, stop, step) Start optional Stop Must Step Optional Stop Must print(*(i for i in range(5))) 0 1 2 3 4 This text will wrap itself. In Synapse or Azure Data Factory (ADF), a pipeline is simply the end-to-end workflow. This text will wrap itself. print(*(fruits[i] for i in range(len(fruits)))) This will not wrap. print(*(fruits[i] for i in range(len(fruits)))) print ( * ( fruits [ i ] for i in range ( len ( fruits )))) Creating connection strings is pretty easy with the icons in Synapse Linked Service. # <span style=\"color: blueviolet;Font-family: Comic Sans MS, sans-serif;\">Synapse Analytics Core Concepts</span> # <span style=\"color: DarkCyan;Font-family: Comic Sans MS, sans-serif;\">Synapse Workspace</span> # <span style=\"color: DodgerBlue\">Serverless SQL Pool</span> # <span style=\"color: FireBrick\">Synapse Spark Pool</span> # <span style=\"color: CadetBlue\">What is Integration Runtime?</span> # <span style=\"color: BlueViolet\">1. Azure Integration Runtime:</span> # <span style=\"color: PaleVioletRed;Font-family: Segoe UI, sans-serif;\">Background</span> # <span style=\"color: blueviolet;Font-family: Segoe UI, sans-serif;\">Azure Synapse analytics</span> # <span style=\"color: DarkSeaGreen; font-family: 'Comic Sans MS', sans-serif;\">Accessing Items in a Tuple: Indexing</span> <p style=\"color: #804000; font-family: 'Trebuchet MS', Helvetica, sans-serif; background-color: BurlyWood; padding: 15px; border-left: 5px solid #b35900;\"> A service to to run SQL Server Integration Services (SSIS) packages in the Azure cloud. </p> <p style=\"color: #006600; font-family: 'Trebuchet MS', Helvetica, sans-serif; background-color: #e6ffe6; padding: 15px; border-left: 5px solid #00cc66;\"> A cloud-based compute resource for running data integration and transformation tasks in Azure Data Factory and Azure Synapse Analytics. </p> <p style=\"color: #804000; font-family: 'Trebuchet MS', Helvetica, sans-serif; background-color: #fff5e6; padding: 15px; border-left: 5px solid #b35900;\"> A tool for connecting and moving data between on-premises sources and Azure cloud services. </p> <div style=\"display: flex; justify-content: center; align-items: center; margin: 5px;\"> <div style=\"padding: 5px; border: 1px solid #ddd; box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.3); border-radius: 5px; background-color: #000000; margin: 3px;font-family: 'Comic Sans MS', sans-serif;\"> <span style=\"font-size: 1.2em;color: #fff; font-weight: bold;font-family: 'Courier New', Courier, monospace;\">print(*(fruits[i] for i in range(len(fruits))))</span> </div> </div> <div style=\"display: flex; justify-content: center; align-items: center; margin: 5px;\"> <div style=\"padding: 5px; border: 1px solid #ddd; box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.3); border-radius: 5px; background-color: #ffeb3b; margin: 3px;font-family: 'Comic Sans MS', sans-serif;\"> <span style=\"font-size: 1.2em;color: #000; font-weight: bold;font-family: 'Courier New', Courier, monospace;\">This text will wrap itself. In Synapse or Azure Data Factory (ADF), a pipeline is simply the end-to-end workflow.</span> </div> </div> # My progressively darkening color themes for many colors--- ### **1. BlueViolet to Darker Shades:** # <span style=\"color: BlueViolet; font-family: Comic Sans MS, sans-serif;\">\"BlueViolet; Font-family: Comic Sans MS, sans-serif;\"</span> ## <span style=\"color: #7A3DAA; font-family: Comic Sans MS, sans-serif;\">\"#7A3DAA; Font-family: Comic Sans MS, sans-serif;\"</span> ### <span style=\"color: #653090; font-family: Comic Sans MS, sans-serif;\">\"#653090; Font-family: Comic Sans MS, sans-serif;\"</span> #### <span style=\"color: #502276; font-family: Comic Sans MS, sans-serif;\">\"#502276; Font-family: Comic Sans MS, sans-serif;\"</span> ##### <span style=\"color: #3B165C; font-family: Comic Sans MS, sans-serif;\">\"#3B165C; Font-family: Comic Sans MS, sans-serif;\"</span> ###### <span style=\"color: #260A42; font-family: Comic Sans MS, sans-serif;\">\"#260A42; Font-family: Comic Sans MS, sans-serif;\"</span> ### **2. MediumOrchid to Darker Shades:** # <span style=\"color: MediumOrchid; font-family: Segoe UI, sans-serif;\">\"MediumOrchid; Font-family: Segoe UI, sans-serif;\"</span> ## <span style=\"color: #AD49B3; font-family: Segoe UI, sans-serif;\">\"#AD49B3; Font-family: Segoe UI, sans-serif;\"</span> ### <span style=\"color: #963F9C; font-family: Segoe UI, sans-serif;\">\"#963F9C; Font-family: Segoe UI, sans-serif;\"</span> #### <span style=\"color: #7F3585; font-family: Segoe UI, sans-serif;\">\"#7F3585; Font-family: Segoe UI, sans-serif;\"</span> ##### <span style=\"color: #682A6E; font-family: Segoe UI, sans-serif;\">\"#682A6E; Font-family: Segoe UI, sans-serif;\"</span> ###### <span style=\"color: #511F57; font-family: Segoe UI, sans-serif;\">\"#511F57; Font-family: Segoe UI, sans-serif;\"</span> ### **3. PaleVioletRed to Darker Shades:** # <span style=\"color: PaleVioletRed; font-family: Segoe UI, sans-serif;\">\"PaleVioletRed; Font-family: Segoe UI, sans-serif;\"</span> ## <span style=\"color: #D86487; font-family: Segoe UI, sans-serif;\">\"#D86487; Font-family: Segoe UI, sans-serif;\"</span> ### <span style=\"color: #C05075; font-family: Segoe UI, sans-serif;\">\"#C05075; Font-family: Segoe UI, sans-serif;\"</span> #### <span style=\"color: #A93C63; font-family: Segoe UI, sans-serif;\">\"#A93C63; Font-family: Segoe UI, sans-serif;\"</span> ##### <span style=\"color: #932851; font-family: Segoe UI, sans-serif;\">\"#932851; Font-family: Segoe UI, sans-serif;\"</span> ###### <span style=\"color: #7C143F; font-family: Segoe UI, sans-serif;\">\"#7C143F; Font-family: Segoe UI, sans-serif;\"</span> ### **4. FireBrick to Darker Shades:** # <span style=\"color: FireBrick;\">\"FireBrick;\"</span> ## <span style=\"color: #C22E2E;\">\"#C22E2E;\"</span> ### <span style=\"color: #A12828;\">\"#A12828;\"</span> #### <span style=\"color: #801F1F;\">\"#801F1F;\"</span> ##### <span style=\"color: #601515;\">\"#601515;\"</span> ###### <span style=\"color: #400B0B;\">\"#400B0B;\"</span> ### **5. MediumSlateBlue to Darker Shades:** # <span style=\"color: MediumSlateBlue;\">\"MediumSlateBlue;\"</span> ## <span style=\"color: #695ED6;\">\"#695ED6;\"</span> ### <span style=\"color: #574BB3;\">\"#574BB3;\"</span> #### <span style=\"color: #463890;\">\"#463890;\"</span> ##### <span style=\"color: #35256D;\">\"#35256D;\"</span> ###### <span style=\"color: #24124A;\">\"#24124A;\"</span> ### **6. DodgerBlue to Darker Shades:** # <span style=\"color: DodgerBlue;\">\"DodgerBlue;\"</span> ## <span style=\"color: #1E7FCC;\">\"#1E7FCC;\"</span> ### <span style=\"color: #1B6FB2;\">\"#1B6FB2;\"</span> #### <span style=\"color: #185F99;\">\"#185F99;\"</span> ##### <span style=\"color: #144F7F;\">\"#144F7F;\"</span> ###### <span style=\"color: #103F66;\">\"#103F66;\"</span> ### **7. CadetBlue to Darker Shades:** # <span style=\"color: CadetBlue;\">\"CadetBlue;\"</span> ## <span style=\"color: #4D8C8C;\">\"#4D8C8C;\"</span> ### <span style=\"color: #437A7A;\">\"#437A7A;\"</span> #### <span style=\"color: #396868;\">\"#396868;\"</span> ##### <span style=\"color: #2F5656;\">\"#2F5656;\"</span> ###### <span style=\"color: #253B3B;\">\"#253B3B;\"</span> ### **8. DarkSeaGreen to Darker Shades:** # <span style=\"color: DarkSeaGreen; font-family: 'Comic Sans MS', sans-serif;\">\"DarkSeaGreen; Font-family: 'Comic Sans MS', sans-serif;\"</span> ## <span style=\"color: #89A989; font-family: 'Comic Sans MS', sans-serif;\">\"#89A989; Font-family: 'Comic Sans MS', sans-serif;\"</span> ### <span style=\"color: #768E76; font-family: 'Comic Sans MS', sans-serif;\">\"#768E76; Font-family: 'Comic Sans MS', sans-serif;\"</span> #### <span style=\"color: #627362; font-family: 'Comic Sans MS', sans-serif;\">\"#627362; Font-family: 'Comic Sans MS', sans-serif;\"</span> ##### <span style=\"color: #4F584F; font-family: 'Comic Sans MS', sans-serif;\">\"#4F584F; Font-family: 'Comic Sans MS', sans-serif;\"</span> ###### <span style=\"color: #3B3D3B; font-family: 'Comic Sans MS', sans-serif;\">\"#3B3D3B; Font-family: 'Comic Sans MS', sans-serif;\"</span> ### **9. DarkCyan to Darker Shades:** # <span style=\"color: DarkCyan; font-family: Comic Sans MS, sans-serif;\">\"DarkCyan; Font-family: Comic Sans MS, sans-serif;\"</span> ## <span style=\"color: #007A7A; font-family: Comic Sans MS, sans-serif;\">\"#007A7A; Font-family: Comic Sans MS, sans-serif;\"</span> ### <span style=\"color: #006767; font-family: Comic Sans MS, sans-serif;\">\"#006767; Font-family: Comic Sans MS, sans-serif;\"</span> #### <span style=\"color: #005454; font-family: Comic Sans MS, sans-serif;\">\"#005454; Font-family: Comic Sans MS, sans-serif;\"</span> ##### <span style=\"color: #004141; font-family: Comic Sans MS, sans-serif;\">\"#004141; Font-family: Comic Sans MS, sans-serif;\"</span> ###### <span style=\"color: #002E2E; font-family: Comic Sans MS, sans-serif;\">\"#002E2E; Font-family: Comic Sans MS, sans-serif;\"</span> # How to use headers in markdown and still apply some style ### Teal Style <h2 style=\"text-align: center; background-color: #008080; color: white; font-family: 'Segoe UI', sans-serif; padding: 15px 20px; border-radius: 10px; box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.3); border: 2px solid #005F5F; letter-spacing: 1px;\"> Single-Node Bitnami Spark With Master and Worker </h2> Some text here bla bla bla ### Light Blue Style <h2 style=\"text-align: center; background-color: #4682B4; color: white; font-family: 'Segoe UI', sans-serif; padding: 15px 20px; border-radius: 10px; box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.3); border: 2px solid #365f82; letter-spacing: 1px;\"> Single-Node Bitnami Spark With Master and Worker </h2> - **Background**: Steel Blue (`#4682B4`) - **Border**: Dark Steel Blue (`#365f82`) - **Text**: White ### Green Style <h2 style=\"text-align: center; background-color: #228B22; color: white; font-family: 'Segoe UI', sans-serif; padding: 15px 20px; border-radius: 10px; box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.3); border: 2px solid #176317; letter-spacing: 1px;\"> Single-Node Bitnami Spark With Master and Worker </h2> - **Background**: Forest Green (`#228B22`) - **Border**: Darker Forest Green (`#176317`) - **Text**: White ### Navy Blue Style <h2 style=\"text-align: center; background-color: #000080; color: white; font-family: 'Segoe UI', sans-serif; padding: 15px 20px; border-radius: 10px; box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.3); border: 2px solid #00004f; letter-spacing: 1px;\"> Single-Node Bitnami Spark With Master and Worker </h2> - **Background**: Navy (`#000080`) - **Border**: Dark Navy (`#00004f`) - **Text**: White ### Olive Green Style <h2 style=\"text-align: center; background-color: #808000; color: white; font-family: 'Segoe UI', sans-serif; padding: 15px 20px; border-radius: 10px; box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.3); border: 2px solid #5f5f00; letter-spacing: 1px;\"> Single-Node Bitnami Spark With Master and Worker </h2> - **Background**: Olive (`#808000`) - **Border**: Dark Olive (`#5f5f00`) - **Text**: White ### Cyan Style <h2 style=\"text-align: center; background-color: #00CED1; color: white; font-family: 'Segoe UI', sans-serif; padding: 15px 20px; border-radius: 10px; box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.3); border: 2px solid #009ca0; letter-spacing: 1px;\"> Single-Node Bitnami Spark With Master and Worker </h2> - **Background**: Dark Turquoise (`#00CED1`) - **Border**: Darker Cyan (`#009ca0`) - **Text**: White","title":"Markdown"},{"location":"Misc/3_Markdown/#fonts","text":"These fonts should be already present in your system. You can download it from internet and install them.","title":"Fonts"},{"location":"Misc/4_VSTrics/","text":"Replace all .png files with images.png ![([^]]*)](([^\\/]+.png)) Replace constructs like Correct Answer: B. extend is used to create a new column or modify an existing column. With extend is used to create a new column or modify an existing column. Search: - **Correct Answer:** [A-Z]. (.+) Replace: $1 Quick datasets: df = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/titanic.csv\") display(df) Intelisence not working in VS","title":"Visual Studio Code Tricks"},{"location":"Misc/4_VSTrics/#intelisence-not-working-in-vs","text":"","title":"Intelisence not working in VS"},{"location":"Misc/5_WhichDatastsToUse/","text":"Overview If you're new to data modeling and want to learn about how to create fact and dimension tables, it's best to start with some sample databases that are designed for practice. Here are some good options: 1. AdventureWorks (Microsoft SQL Server Sample Database) What It Is : AdventureWorks is a sample database provided by Microsoft. It contains data for a fictional company that sells bicycles and related products. Where to Get It : Download AdventureWorks 2. Northwind Traders (Microsoft Access Database) What It Is : Northwind is another sample database that simulates a trading company. It includes data about customers, products, orders, and suppliers. Where to Get It : Download Northwind Database 3. TPC-H Benchmark Dataset What It Is : TPC-H is a dataset used for testing database performance. It contains data for a wholesale supplier, including orders, customers, parts, and suppliers. Where to Get It : TPC-H Benchmark Dataset 4. IMDB Movie Dataset What It Is : The IMDb dataset has information about movies, actors, directors, and genres. Where to Get It : IMDb Datasets 5. Retail Transaction Data (from Kaggle) What It Is : There are several retail transaction datasets on Kaggle, like the \"Online Retail Dataset\" or \"Instacart Market Basket Analysis.\" Where to Get It : UCI link . Kaggle Link Instacart Market Basket Analysis 6. Airbnb Listings Dataset What It Is : This dataset contains details about Airbnb properties, like prices, locations, host details, and reviews. Where to Get It : Airbnb Listings Dataset 7. Chinook Database What It Is : Chinook is a sample database that simulates a digital media store. It includes data about artists, albums, media tracks, customers, and sales. Where to Get It : Download Chinook Database Summary Start with AdventureWorks or Northwind if you want something easy to get started with. These databases are ready to use and perfect for beginners. If you\u2019re looking for more real-world examples, Kaggle\u2019s retail or Airbnb datasets are also good options.","title":"Free Datasets for Data Practice"},{"location":"Misc/5_WhichDatastsToUse/#overview","text":"If you're new to data modeling and want to learn about how to create fact and dimension tables, it's best to start with some sample databases that are designed for practice. Here are some good options:","title":"Overview"},{"location":"Misc/5_WhichDatastsToUse/#1-adventureworks-microsoft-sql-server-sample-database","text":"What It Is : AdventureWorks is a sample database provided by Microsoft. It contains data for a fictional company that sells bicycles and related products. Where to Get It : Download AdventureWorks","title":"1. AdventureWorks (Microsoft SQL Server Sample Database)"},{"location":"Misc/5_WhichDatastsToUse/#2-northwind-traders-microsoft-access-database","text":"What It Is : Northwind is another sample database that simulates a trading company. It includes data about customers, products, orders, and suppliers. Where to Get It : Download Northwind Database","title":"2. Northwind Traders (Microsoft Access Database)"},{"location":"Misc/5_WhichDatastsToUse/#3-tpc-h-benchmark-dataset","text":"What It Is : TPC-H is a dataset used for testing database performance. It contains data for a wholesale supplier, including orders, customers, parts, and suppliers. Where to Get It : TPC-H Benchmark Dataset","title":"3. TPC-H Benchmark Dataset"},{"location":"Misc/5_WhichDatastsToUse/#4-imdb-movie-dataset","text":"What It Is : The IMDb dataset has information about movies, actors, directors, and genres. Where to Get It : IMDb Datasets","title":"4. IMDB Movie Dataset"},{"location":"Misc/5_WhichDatastsToUse/#5-retail-transaction-data-from-kaggle","text":"What It Is : There are several retail transaction datasets on Kaggle, like the \"Online Retail Dataset\" or \"Instacart Market Basket Analysis.\" Where to Get It : UCI link . Kaggle Link Instacart Market Basket Analysis","title":"5. Retail Transaction Data (from Kaggle)"},{"location":"Misc/5_WhichDatastsToUse/#6-airbnb-listings-dataset","text":"What It Is : This dataset contains details about Airbnb properties, like prices, locations, host details, and reviews. Where to Get It : Airbnb Listings Dataset","title":"6. Airbnb Listings Dataset"},{"location":"Misc/5_WhichDatastsToUse/#7-chinook-database","text":"What It Is : Chinook is a sample database that simulates a digital media store. It includes data about artists, albums, media tracks, customers, and sales. Where to Get It : Download Chinook Database","title":"7. Chinook Database"},{"location":"Misc/5_WhichDatastsToUse/#summary","text":"Start with AdventureWorks or Northwind if you want something easy to get started with. These databases are ready to use and perfect for beginners. If you\u2019re looking for more real-world examples, Kaggle\u2019s retail or Airbnb datasets are also good options.","title":"Summary"},{"location":"Misc/6_RunningAppsInBg/","text":"How to Run a Service in the Background on Linux and Check if It\u2019s Running In this article, I will show you how to run a service in the background. I've used Apache Airflow as an example, but you can apply these methods to other services as well. Running a Service in the Background Using nohup (Linux/MacOS): The nohup command allows you to run the Airflow scheduler in the background and ensures it continues running even after you close the terminal. bash nohup airflow scheduler > scheduler.log 2>&1 & Here, nohup keeps the process running, > scheduler.log 2>&1 saves the output to a log file, and & runs it in the background. Using screen (Linux/MacOS): screen is a terminal multiplexer that lets you start a terminal session that you can detach from and reattach to later. bash screen -S airflow-scheduler airflow scheduler After starting the scheduler, detach the screen session by pressing Ctrl + A , then D . To reattach to the session, use: bash screen -r airflow-scheduler Using tmux (Linux/MacOS): tmux is similar to screen , providing another way to manage terminal sessions that you can detach and reattach to. bash tmux new -s airflow-scheduler airflow scheduler Detach from the session with Ctrl + B , then D . Reattach with: bash tmux attach -t airflow-scheduler Using systemd (Linux): To manage the Airflow scheduler as a service, you can create a systemd service file. This method is more permanent and reliable. Create a file at /etc/systemd/system/airflow-scheduler.service with the following content: ```ini [Unit] Description=Airflow Scheduler After=network.target [Service] ExecStart=/usr/local/bin/airflow scheduler Restart=always User=airflow Group=airflow StandardOutput=syslog StandardError=syslog SyslogIdentifier=airflow-scheduler [Install] WantedBy=multi-user.target ``` Then enable and start the service: bash sudo systemctl enable airflow-scheduler sudo systemctl start airflow-scheduler Using & (Linux/MacOS): For a quick and simple way to run the Airflow scheduler in the background, just append & to the command: bash airflow scheduler & Using Docker (If Airflow is running inside a container): If you're using Docker, you can run the Airflow scheduler in detached mode using the following command: bash docker run -d --name airflow-scheduler apache/airflow:latest scheduler This command will start the scheduler in the background inside the Docker container. Checking if a Service is Running Checking the Airflow Scheduler: To see if the Airflow scheduler is running, use the ps command: bash ps aux | grep 'airflow scheduler' This command will list the running processes, including the Airflow scheduler if it's active. Checking Apache Web Server: For Apache, which is a widely used web server, you can check its status with: bash sudo systemctl status apache2 If Apache is running, you'll see \"active (running)\" in the status output. Conclusion So, the popular commands to run services in the background in linux like os are: nohup , screen , tmux and systemctl","title":"Running Service in Background"},{"location":"Misc/6_RunningAppsInBg/#how-to-run-a-service-in-the-background-on-linux-and-check-if-its-running","text":"In this article, I will show you how to run a service in the background. I've used Apache Airflow as an example, but you can apply these methods to other services as well.","title":"How to Run a Service in the Background on Linux and Check if It\u2019s Running"},{"location":"Misc/6_RunningAppsInBg/#running-a-service-in-the-background","text":"Using nohup (Linux/MacOS): The nohup command allows you to run the Airflow scheduler in the background and ensures it continues running even after you close the terminal. bash nohup airflow scheduler > scheduler.log 2>&1 & Here, nohup keeps the process running, > scheduler.log 2>&1 saves the output to a log file, and & runs it in the background. Using screen (Linux/MacOS): screen is a terminal multiplexer that lets you start a terminal session that you can detach from and reattach to later. bash screen -S airflow-scheduler airflow scheduler After starting the scheduler, detach the screen session by pressing Ctrl + A , then D . To reattach to the session, use: bash screen -r airflow-scheduler Using tmux (Linux/MacOS): tmux is similar to screen , providing another way to manage terminal sessions that you can detach and reattach to. bash tmux new -s airflow-scheduler airflow scheduler Detach from the session with Ctrl + B , then D . Reattach with: bash tmux attach -t airflow-scheduler Using systemd (Linux): To manage the Airflow scheduler as a service, you can create a systemd service file. This method is more permanent and reliable. Create a file at /etc/systemd/system/airflow-scheduler.service with the following content: ```ini [Unit] Description=Airflow Scheduler After=network.target [Service] ExecStart=/usr/local/bin/airflow scheduler Restart=always User=airflow Group=airflow StandardOutput=syslog StandardError=syslog SyslogIdentifier=airflow-scheduler [Install] WantedBy=multi-user.target ``` Then enable and start the service: bash sudo systemctl enable airflow-scheduler sudo systemctl start airflow-scheduler Using & (Linux/MacOS): For a quick and simple way to run the Airflow scheduler in the background, just append & to the command: bash airflow scheduler & Using Docker (If Airflow is running inside a container): If you're using Docker, you can run the Airflow scheduler in detached mode using the following command: bash docker run -d --name airflow-scheduler apache/airflow:latest scheduler This command will start the scheduler in the background inside the Docker container.","title":"Running a Service in the Background"},{"location":"Misc/6_RunningAppsInBg/#checking-if-a-service-is-running","text":"Checking the Airflow Scheduler: To see if the Airflow scheduler is running, use the ps command: bash ps aux | grep 'airflow scheduler' This command will list the running processes, including the Airflow scheduler if it's active. Checking Apache Web Server: For Apache, which is a widely used web server, you can check its status with: bash sudo systemctl status apache2 If Apache is running, you'll see \"active (running)\" in the status output.","title":"Checking if a Service is Running"},{"location":"Misc/6_RunningAppsInBg/#conclusion","text":"So, the popular commands to run services in the background in linux like os are: nohup , screen , tmux and systemctl","title":"Conclusion"},{"location":"Misc/7_Azure_Budget/","text":"How to set a Azure Budget","title":"How to set a Azure Budget"},{"location":"Misc/7_Azure_Budget/#how-to-set-a-azure-budget","text":"","title":"How to set a Azure Budget"},{"location":"Misc/9_WhyUbuntuIsGood/","text":"Why Ubuntu is a Valuable OS for Microsoft Ecosystem Ubuntu is an important operating system for Microsoft ecosystem, particularly Azure. Here\u2019s why: Cloud Popularity : Ubuntu is the most widely used Linux distribution in cloud environments, including Azure. Many Azure Virtual Machines (VMs), containers, and Databricks clusters operate on Ubuntu, making it essential to know for cloud-related tasks. Seamless Azure Integration : Ubuntu is fully supported by Azure services. Tools like Azure CLI and DevOps agents are optimized for Ubuntu, allowing for smooth integration and efficient management. Databricks Support : Azure Databricks relies on Ubuntu for its virtual machines, including both worker and driver nodes. Knowing Ubuntu helps you manage and optimize these clusters effectively. Command-Line Skills : Azure frequently uses command-line interfaces. Ubuntu\u2019s native bash shell is ideal for scripting and managing Azure resources, boosting your productivity and effectiveness. Open-Source Benefits : Ubuntu is free and open-source, which means you can learn and experiment without worrying about licensing costs. This aligns well with Azure\u2019s support for open-source technologies. Practical Usage : Many enterprises deploy Ubuntu on Azure for their production environments. Learning Ubuntu gives you practical skills that are directly applicable in the workplace.","title":"Why Ubuntu is good to learn"},{"location":"Misc/9_WhyUbuntuIsGood/#why-ubuntu-is-a-valuable-os-for-microsoft-ecosystem","text":"Ubuntu is an important operating system for Microsoft ecosystem, particularly Azure. Here\u2019s why: Cloud Popularity : Ubuntu is the most widely used Linux distribution in cloud environments, including Azure. Many Azure Virtual Machines (VMs), containers, and Databricks clusters operate on Ubuntu, making it essential to know for cloud-related tasks. Seamless Azure Integration : Ubuntu is fully supported by Azure services. Tools like Azure CLI and DevOps agents are optimized for Ubuntu, allowing for smooth integration and efficient management. Databricks Support : Azure Databricks relies on Ubuntu for its virtual machines, including both worker and driver nodes. Knowing Ubuntu helps you manage and optimize these clusters effectively. Command-Line Skills : Azure frequently uses command-line interfaces. Ubuntu\u2019s native bash shell is ideal for scripting and managing Azure resources, boosting your productivity and effectiveness. Open-Source Benefits : Ubuntu is free and open-source, which means you can learn and experiment without worrying about licensing costs. This aligns well with Azure\u2019s support for open-source technologies. Practical Usage : Many enterprises deploy Ubuntu on Azure for their production environments. Learning Ubuntu gives you practical skills that are directly applicable in the workplace.","title":"Why Ubuntu is a Valuable OS for Microsoft Ecosystem"},{"location":"Misc/markdown_pdf_export_html/","text":"","title":"Markdown pdf export html"},{"location":"Misc/MarkdownColor.md/Color/","text":"Color Palettes I found this from the internet. This could be very useful for technical writers who want a handy color rereence guid. 1. Burnt Sienna Orange + Bedazzled Blue Color Palette Hex Codes: #3d5a80 , #98c1d9 , #e0fbfc , #ee6c4d , #293241 2. Imperial Red + Space Cadet Blue Color Palette Hex Codes: #2b2d42 , #8d99ae , #edf2f4 , #ef233c , #d90429 3. Orange + Honey Yellow + Prussian Blue Color Palette Hex Codes: #8ecae6 , #219ebc , #023047 , #ffb703 , #fb8500 4. Candy Pink + Rose Desert + Y in Mn Blue Color Palette Hex Codes: #355070 , #6d597a , #b56576 , #e56b6f , #eaac8b 5. Paradise Pink + Caribbean Green + NCS Blue Color Palette Hex Codes: #ef476f , #ffd166 , #06d6a0 , #118ab2 , #073b4c 6. Lemon Meringue + Prussian Blue Color Palette Hex Codes: #003049 , #d62828 , #f77f00 , #fcbf49 , #eae2b7 7. Orange Web + Oxford Blue Color Palette Hex Codes: #000000 , #14213d , #fca311 , #e5e5e5 , #ffffff 8. Carolina Blue + CG Blue Color Palette Hex Codes: #ffffff , #00171f , #003459 , #007ea7 , #00a8e8 9. Cyber Yellow + Royal Dark Blue Color Palette Hex Codes: #00296b , #003f88 , #00509d , #fdc500 , #ffd500 10. Shades of Blue Color Palette Hex Codes: #03045e , #0077b6 , #00b4d8 , #90e0ef , #caf0f8 11. Midnight Eagle Green + Metallic Seaweed Blue Color Palette Hex Codes: #177e89 , #084c61 , #db3a34 , #ffc857 , #323031 12. Sage + Ming + Indigo Dye Blue Color Palette Hex Codes: #033f63 , #28666e , #7c9885 , #b5b682 , #fedc97 13. Almond + Purple Navy + Oxford Blue Color Palette Hex Codes: #f1dac4 , #a69cac , #474973 , #161b33 , #0d0c1d 14. Ruby + Bright Yellow Crayola + Sky Blue Crayola Color Palette Hex Codes: #d81159 , #8f2d56 , #218380 , #fbb13c , #73d2de 15. Atomic Tangerine + Pacific Blue + Yale Blue Color Palette Hex Codes: #f79256 , #fbd1a2 , #7dcfb6 , #00b2ca , #1d4e89 16. Burnt Sienna + Cadet Blue + Columbia Blue Color Palette Hex Codes: #dd6e42 , #e8dab2 , #4f6d7a , #c0d6df , #eaeaea 17. Jet + Ming + Indigo Dye Blue Color Palette Hex Codes: #353535 , #3c6e71 , #ffffff , #d9d9d9 , #284b63 18. Sunglow + Sizzling Red + Crayola Blue Color Palette Hex Codes: #ff595e , #ffca3a , #8ac926 , #1982c4 , #6a4c93 19. Light Salmon + French Pink + Baby Blue Color Palette Hex Codes: #70d6ff , #ff70a6 , #ff9770 , #ffd670 , #e9ff70 20. Black Coffee + Duke Blue + True Blue Color Palette Hex Codes: #3c3744 , #090c9b , #3066be , #b4c5e4 , #fbfff1","title":"Color Reference"},{"location":"Misc/MarkdownColor.md/Color/#color-palettes","text":"I found this from the internet. This could be very useful for technical writers who want a handy color rereence guid.","title":"Color Palettes"},{"location":"Misc/MarkdownColor.md/Color/#1-burnt-sienna-orange-bedazzled-blue-color-palette","text":"Hex Codes: #3d5a80 , #98c1d9 , #e0fbfc , #ee6c4d , #293241","title":"1. Burnt Sienna Orange + Bedazzled Blue Color Palette"},{"location":"Misc/MarkdownColor.md/Color/#2-imperial-red-space-cadet-blue-color-palette","text":"Hex Codes: #2b2d42 , #8d99ae , #edf2f4 , #ef233c , #d90429","title":"2. Imperial Red + Space Cadet Blue Color Palette"},{"location":"Misc/MarkdownColor.md/Color/#3-orange-honey-yellow-prussian-blue-color-palette","text":"Hex Codes: #8ecae6 , #219ebc , #023047 , #ffb703 , #fb8500","title":"3. Orange + Honey Yellow + Prussian Blue Color Palette"},{"location":"Misc/MarkdownColor.md/Color/#4-candy-pink-rose-desert-y-in-mn-blue-color-palette","text":"Hex Codes: #355070 , #6d597a , #b56576 , #e56b6f , #eaac8b","title":"4. Candy Pink + Rose Desert + Y in Mn Blue Color Palette"},{"location":"Misc/MarkdownColor.md/Color/#5-paradise-pink-caribbean-green-ncs-blue-color-palette","text":"Hex Codes: #ef476f , #ffd166 , #06d6a0 , #118ab2 , #073b4c","title":"5. Paradise Pink + Caribbean Green + NCS Blue Color Palette"},{"location":"Misc/MarkdownColor.md/Color/#6-lemon-meringue-prussian-blue-color-palette","text":"Hex Codes: #003049 , #d62828 , #f77f00 , #fcbf49 , #eae2b7","title":"6. Lemon Meringue + Prussian Blue Color Palette"},{"location":"Misc/MarkdownColor.md/Color/#7-orange-web-oxford-blue-color-palette","text":"Hex Codes: #000000 , #14213d , #fca311 , #e5e5e5 , #ffffff","title":"7. Orange Web + Oxford Blue Color Palette"},{"location":"Misc/MarkdownColor.md/Color/#8-carolina-blue-cg-blue-color-palette","text":"Hex Codes: #ffffff , #00171f , #003459 , #007ea7 , #00a8e8","title":"8. Carolina Blue + CG Blue Color Palette"},{"location":"Misc/MarkdownColor.md/Color/#9-cyber-yellow-royal-dark-blue-color-palette","text":"Hex Codes: #00296b , #003f88 , #00509d , #fdc500 , #ffd500","title":"9. Cyber Yellow + Royal Dark Blue Color Palette"},{"location":"Misc/MarkdownColor.md/Color/#10-shades-of-blue-color-palette","text":"Hex Codes: #03045e , #0077b6 , #00b4d8 , #90e0ef , #caf0f8","title":"10. Shades of Blue Color Palette"},{"location":"Misc/MarkdownColor.md/Color/#11-midnight-eagle-green-metallic-seaweed-blue-color-palette","text":"Hex Codes: #177e89 , #084c61 , #db3a34 , #ffc857 , #323031","title":"11. Midnight Eagle Green + Metallic Seaweed Blue Color Palette"},{"location":"Misc/MarkdownColor.md/Color/#12-sage-ming-indigo-dye-blue-color-palette","text":"Hex Codes: #033f63 , #28666e , #7c9885 , #b5b682 , #fedc97","title":"12. Sage + Ming + Indigo Dye Blue Color Palette"},{"location":"Misc/MarkdownColor.md/Color/#13-almond-purple-navy-oxford-blue-color-palette","text":"Hex Codes: #f1dac4 , #a69cac , #474973 , #161b33 , #0d0c1d","title":"13. Almond + Purple Navy + Oxford Blue Color Palette"},{"location":"Misc/MarkdownColor.md/Color/#14-ruby-bright-yellow-crayola-sky-blue-crayola-color-palette","text":"Hex Codes: #d81159 , #8f2d56 , #218380 , #fbb13c , #73d2de","title":"14. Ruby + Bright Yellow Crayola + Sky Blue Crayola Color Palette"},{"location":"Misc/MarkdownColor.md/Color/#15-atomic-tangerine-pacific-blue-yale-blue-color-palette","text":"Hex Codes: #f79256 , #fbd1a2 , #7dcfb6 , #00b2ca , #1d4e89","title":"15. Atomic Tangerine + Pacific Blue + Yale Blue Color Palette"},{"location":"Misc/MarkdownColor.md/Color/#16-burnt-sienna-cadet-blue-columbia-blue-color-palette","text":"Hex Codes: #dd6e42 , #e8dab2 , #4f6d7a , #c0d6df , #eaeaea","title":"16. Burnt Sienna + Cadet Blue + Columbia Blue Color Palette"},{"location":"Misc/MarkdownColor.md/Color/#17-jet-ming-indigo-dye-blue-color-palette","text":"Hex Codes: #353535 , #3c6e71 , #ffffff , #d9d9d9 , #284b63","title":"17. Jet + Ming + Indigo Dye Blue Color Palette"},{"location":"Misc/MarkdownColor.md/Color/#18-sunglow-sizzling-red-crayola-blue-color-palette","text":"Hex Codes: #ff595e , #ffca3a , #8ac926 , #1982c4 , #6a4c93","title":"18. Sunglow + Sizzling Red + Crayola Blue Color Palette"},{"location":"Misc/MarkdownColor.md/Color/#19-light-salmon-french-pink-baby-blue-color-palette","text":"Hex Codes: #70d6ff , #ff70a6 , #ff9770 , #ffd670 , #e9ff70","title":"19. Light Salmon + French Pink + Baby Blue Color Palette"},{"location":"Misc/MarkdownColor.md/Color/#20-black-coffee-duke-blue-true-blue-color-palette","text":"Hex Codes: #3c3744 , #090c9b , #3066be , #b4c5e4 , #fbfff1","title":"20. Black Coffee + Duke Blue + True Blue Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/","text":"Color Palettes 1. Burnt Sienna Orange + Bedazzled Blue Color Palette Hex Codes: #3d5a80 , #98c1d9 , #e0fbfc , #ee6c4d , #293241 2. Imperial Red + Space Cadet Blue Color Palette Hex Codes: #2b2d42 , #8d99ae , #edf2f4 , #ef233c , #d90429 3. Orange + Honey Yellow + Prussian Blue Color Palette Hex Codes: #8ecae6 , #219ebc , #023047 , #ffb703 , #fb8500 4. Candy Pink + Rose Desert + Y in Mn Blue Color Palette Hex Codes: #355070 , #6d597a , #b56576 , #e56b6f , #eaac8b 5. Paradise Pink + Caribbean Green + NCS Blue Color Palette Hex Codes: #ef476f , #ffd166 , #06d6a0 , #118ab2 , #073b4c 6. Lemon Meringue + Prussian Blue Color Palette Hex Codes: #003049 , #d62828 , #f77f00 , #fcbf49 , #eae2b7 7. Orange Web + Oxford Blue Color Palette Hex Codes: #000000 , #14213d , #fca311 , #e5e5e5 , #ffffff 8. Carolina Blue + CG Blue Color Palette Hex Codes: #ffffff , #00171f , #003459 , #007ea7 , #00a8e8 9. Cyber Yellow + Royal Dark Blue Color Palette Hex Codes: #00296b , #003f88 , #00509d , #fdc500 , #ffd500 10. Shades of Blue Color Palette Hex Codes: #03045e , #0077b6 , #00b4d8 , #90e0ef , #caf0f8 11. Midnight Eagle Green + Metallic Seaweed Blue Color Palette Hex Codes: #177e89 , #084c61 , #db3a34 , #ffc857 , #323031 12. Sage + Ming + Indigo Dye Blue Color Palette Hex Codes: #033f63 , #28666e , #7c9885 , #b5b682 , #fedc97 13. Almond + Purple Navy + Oxford Blue Color Palette Hex Codes: #f1dac4 , #a69cac , #474973 , #161b33 , #0d0c1d 14. Ruby + Bright Yellow Crayola + Sky Blue Crayola Color Palette Hex Codes: #d81159 , #8f2d56 , #218380 , #fbb13c , #73d2de 15. Atomic Tangerine + Pacific Blue + Yale Blue Color Palette Hex Codes: #f79256 , #fbd1a2 , #7dcfb6 , #00b2ca , #1d4e89 16. Burnt Sienna + Cadet Blue + Columbia Blue Color Palette Hex Codes: #dd6e42 , #e8dab2 , #4f6d7a , #c0d6df , #eaeaea 17. Jet + Ming + Indigo Dye Blue Color Palette Hex Codes: #353535 , #3c6e71 , #ffffff , #d9d9d9 , #284b63 18. Sunglow + Sizzling Red + Crayola Blue Color Palette Hex Codes: #ff595e , #ffca3a , #8ac926 , #1982c4 , #6a4c93 19. Light Salmon + French Pink + Baby Blue Color Palette Hex Codes: #70d6ff , #ff70a6 , #ff9770 , #ffd670 , #e9ff70 20. Black Coffee + Duke Blue + True Blue Color Palette Hex Codes: #3c3744 , #090c9b , #3066be , #b4c5e4 , #fbfff1","title":"Color Palettes"},{"location":"Misc/MarkdownColor.md/images/Color/#color-palettes","text":"","title":"Color Palettes"},{"location":"Misc/MarkdownColor.md/images/Color/#1-burnt-sienna-orange-bedazzled-blue-color-palette","text":"Hex Codes: #3d5a80 , #98c1d9 , #e0fbfc , #ee6c4d , #293241","title":"1. Burnt Sienna Orange + Bedazzled Blue Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/#2-imperial-red-space-cadet-blue-color-palette","text":"Hex Codes: #2b2d42 , #8d99ae , #edf2f4 , #ef233c , #d90429","title":"2. Imperial Red + Space Cadet Blue Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/#3-orange-honey-yellow-prussian-blue-color-palette","text":"Hex Codes: #8ecae6 , #219ebc , #023047 , #ffb703 , #fb8500","title":"3. Orange + Honey Yellow + Prussian Blue Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/#4-candy-pink-rose-desert-y-in-mn-blue-color-palette","text":"Hex Codes: #355070 , #6d597a , #b56576 , #e56b6f , #eaac8b","title":"4. Candy Pink + Rose Desert + Y in Mn Blue Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/#5-paradise-pink-caribbean-green-ncs-blue-color-palette","text":"Hex Codes: #ef476f , #ffd166 , #06d6a0 , #118ab2 , #073b4c","title":"5. Paradise Pink + Caribbean Green + NCS Blue Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/#6-lemon-meringue-prussian-blue-color-palette","text":"Hex Codes: #003049 , #d62828 , #f77f00 , #fcbf49 , #eae2b7","title":"6. Lemon Meringue + Prussian Blue Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/#7-orange-web-oxford-blue-color-palette","text":"Hex Codes: #000000 , #14213d , #fca311 , #e5e5e5 , #ffffff","title":"7. Orange Web + Oxford Blue Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/#8-carolina-blue-cg-blue-color-palette","text":"Hex Codes: #ffffff , #00171f , #003459 , #007ea7 , #00a8e8","title":"8. Carolina Blue + CG Blue Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/#9-cyber-yellow-royal-dark-blue-color-palette","text":"Hex Codes: #00296b , #003f88 , #00509d , #fdc500 , #ffd500","title":"9. Cyber Yellow + Royal Dark Blue Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/#10-shades-of-blue-color-palette","text":"Hex Codes: #03045e , #0077b6 , #00b4d8 , #90e0ef , #caf0f8","title":"10. Shades of Blue Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/#11-midnight-eagle-green-metallic-seaweed-blue-color-palette","text":"Hex Codes: #177e89 , #084c61 , #db3a34 , #ffc857 , #323031","title":"11. Midnight Eagle Green + Metallic Seaweed Blue Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/#12-sage-ming-indigo-dye-blue-color-palette","text":"Hex Codes: #033f63 , #28666e , #7c9885 , #b5b682 , #fedc97","title":"12. Sage + Ming + Indigo Dye Blue Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/#13-almond-purple-navy-oxford-blue-color-palette","text":"Hex Codes: #f1dac4 , #a69cac , #474973 , #161b33 , #0d0c1d","title":"13. Almond + Purple Navy + Oxford Blue Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/#14-ruby-bright-yellow-crayola-sky-blue-crayola-color-palette","text":"Hex Codes: #d81159 , #8f2d56 , #218380 , #fbb13c , #73d2de","title":"14. Ruby + Bright Yellow Crayola + Sky Blue Crayola Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/#15-atomic-tangerine-pacific-blue-yale-blue-color-palette","text":"Hex Codes: #f79256 , #fbd1a2 , #7dcfb6 , #00b2ca , #1d4e89","title":"15. Atomic Tangerine + Pacific Blue + Yale Blue Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/#16-burnt-sienna-cadet-blue-columbia-blue-color-palette","text":"Hex Codes: #dd6e42 , #e8dab2 , #4f6d7a , #c0d6df , #eaeaea","title":"16. Burnt Sienna + Cadet Blue + Columbia Blue Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/#17-jet-ming-indigo-dye-blue-color-palette","text":"Hex Codes: #353535 , #3c6e71 , #ffffff , #d9d9d9 , #284b63","title":"17. Jet + Ming + Indigo Dye Blue Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/#18-sunglow-sizzling-red-crayola-blue-color-palette","text":"Hex Codes: #ff595e , #ffca3a , #8ac926 , #1982c4 , #6a4c93","title":"18. Sunglow + Sizzling Red + Crayola Blue Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/#19-light-salmon-french-pink-baby-blue-color-palette","text":"Hex Codes: #70d6ff , #ff70a6 , #ff9770 , #ffd670 , #e9ff70","title":"19. Light Salmon + French Pink + Baby Blue Color Palette"},{"location":"Misc/MarkdownColor.md/images/Color/#20-black-coffee-duke-blue-true-blue-color-palette","text":"Hex Codes: #3c3744 , #090c9b , #3066be , #b4c5e4 , #fbfff1","title":"20. Black Coffee + Duke Blue + True Blue Color Palette"},{"location":"MongoDB/HowMongoDBStoresFiles/","text":"Overview How MongoDB would store your normal SQL Data(simple text)? Storing unstructured data like files in MongoDB How GridFS Splits Files Storing a sample file Resume.pdf Let's See This in Practice Let's recap the splitting process Now, let's see the actual details Detailed Explanation Summary Overview MongoDB is a document database. Document database? You mean, it's good for storing PDFs/Word files? No . The reason it is called a Document database is not because it stores PDFs/Word docs well(it does). But, because whatever data it stores\u2014whether it's files or SQL table data\u2014 it stores as JSON files . What? Yes, even for storing something simple like Name and Roll Number, it will be saved like this: {Name: \"Donald\", Roll Number: 100} . And for storing a PDF, the 0s and 1s (binary data of the PDF) are stored as entries in JSON files. So, whatever data you store, it\u2019s stored in a JSON document. Hence, it's called a document database. Let's see below how the data, structured(sql tables) and unstructured(pdf documents) are stored in MongoDB. How MongoDB would store your normal SQL Data(simple text)? Suppose, we have a simple MSSQL table but, we want to store the data now in MongoDB. How would MongoDB store it. When you save this data in MongoDB, each row becomes a JSON file. These documents are stored inside a collection (like a table) called countries inside a database called Earth . Now, let's do some practical to see it in reality. Follow the steps below. To run MongoDB commands, you can install MongoDB Shell from here . Open CMD and key in mongosh . It will log in and start a test database: Create the Earth Database. Just enter Earth and it will create the database. No need to create it separately. Now, insert the Data into the countries Collection: This will create the collection. No need to create it beforehand. ```javascript db.countries.insertMany([ { \"Country\": \"USA\", \"Population\": 300, \"Language\": \"English\" }, { \"Country\": \"Cuba\", \"Population\": 100, \"Language\": \"Spanish\" } ]); ``` <img src=\"images/custom-image-2024-07-20-02-22-46.png\" alt=\"Description of the image\" style=\"max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\"> You can see the data in MongoDB Compass To make sure the data is inserted properly. you can query the collection using this command: db.countries.find().pretty(); . This command will display the documents stored in the countries collection in a readable format. Storing unstructured data like files in MongoDB Let's now see how MongoDB stores a PDF file. Say we have a 1 MB PDF file named Resume.pdf . How would MongoDB store it? MongoDB uses a specification called GridFS to store large files. GridFS divides the file into smaller chunks and stores each chunk as a separate document. How GridFS Splits Files When you save a file in MongoDB using GridFS, the file is split into smaller pieces (around 255 KB). For a 1 MB PDF file, it will be split into 4 smaller files. Each file-piece is stored in the fs.chunks collection as a JSON file. Yes, your Adobe PDF's binary data is split into pieces, and the 01$#$#$ is put inside the JSON file. There will be one metadata file for the entire PDF stored as a JSON file in the fs.files collection. JSON, JSON everywhere. Storing a sample file Resume.pdf When you save the Resume.pdf file in MongoDB, it is broken into chunks and stored across two collections: fs.files and fs.chunks . File Metadata - fs.files : Each file has a metadata document in the fs.files collection. json { \"_id\": ObjectId(\"...\"), \"filename\": \"Resume.pdf\", \"length\": 1048576, // Size in bytes (1 MB) \"chunkSize\": 261120, // Default chunk size (255 KB) \"uploadDate\": ISODate(\"2024-07-20T12:34:56Z\"), \"md5\": \"...\" } File Chunks - fs.chunks : The file data is stored in chunks in the fs.chunks collection. Each chunk is linked to the file via the files_id . json { \"_id\": ObjectId(\"...\"), \"files_id\": ObjectId(\"...\"), // Reference to fs.files document \"n\": 0, // Chunk number \"data\": <binary data $###$WhenYouOpenAdobePDFinNotepad01010101> } { \"_id\": ObjectId(\"...\"), \"files_id\": ObjectId(\"...\"), // Reference to fs.files document \"n\": 1, // Chunk number \"data\": <binary data $###$WhenYouOpenAdobePDFinNotepad01010101> } // More chunks follow... Let's See This in Practice To run MongoDB commands, you can install MongoDB Shell from here . Now, let's try to create a Python code to upload a sample file into MongoDB. Install Required Packages : Make sure you have pymongo installed. You can install it using pip: bash pip install pymongo gridfs Python Code to Store Resume.pdf : ```python from pymongo import MongoClient import gridfs Connect to MongoDB client = MongoClient('mongodb://localhost:27017') db = client['Earth'] Create a GridFS bucket fs = gridfs.GridFS(db) Open the PDF file and store it in MongoDB with open('Resume.pdf', 'rb') as f: file_id = fs.put(f, filename='Resume.pdf') print(f'File stored with id: {file_id}') ``` Verify the File Storage : You can verify that the file has been stored correctly by querying the fs.files collection: ```python # Verify file storage stored_file = db.fs.files.find_one({'filename': 'Resume.pdf'}) print(stored_file) ``` Let's recap the splitting process When you upload Resume.pdf using GridFS: - The file is split into pieces. The default chunk size is 255 KB. - For a 1 MB file, it would be split into around 4 pieces (1 MB / 255 KB \u2248 4). - Each piece is stored as a separate JSON file in the fs.chunks collection. Yes, the Adobe PDF's binary data is stuffed inside the JSON file. - There will be one JSON file in the fs.files collection to store metadata about the real file, including its length, chunk size, and upload date. - Each chunk document in fs.chunks contains a reference to the file's metadata document in fs.files . Now, let's see the actual details In actual fact, each row in a SQL table when saved in MongoDB becomes a document. Documents are not exactly .JSON files. Rather, they are stored in BSON (Binary JSON) format within MongoDB, which is an internal storage format optimized for performance and space efficiency. Detailed Explanation SQL to MongoDB : When migrating data from a SQL table to MongoDB, each row from the SQL table becomes a document in a MongoDB collection. Documents : These documents are similar to JSON objects but are stored internally as BSON. BSON allows MongoDB to efficiently store and retrieve data. Storage in MongoDB : MongoDB does not store each document as a separate file on the filesystem. Instead, all documents within a collection are stored together in large database files managed by MongoDB\u2019s storage engine. These database files typically reside in the dbpath directory of your MongoDB installation and have extensions like .wt for WiredTiger. Accessing Data : When you query MongoDB, it retrieves the BSON documents from these large files and converts them to JSON-like structures for ease of use in your application. Summary For conceptual understanding, you can think of each document as a JSON object. However, in reality, these documents are stored in a more efficient binary format (BSON) within MongoDB's internal database files.","title":"How MongoDB Stores Data"},{"location":"MongoDB/HowMongoDBStoresFiles/#overview","text":"MongoDB is a document database. Document database? You mean, it's good for storing PDFs/Word files? No . The reason it is called a Document database is not because it stores PDFs/Word docs well(it does). But, because whatever data it stores\u2014whether it's files or SQL table data\u2014 it stores as JSON files . What? Yes, even for storing something simple like Name and Roll Number, it will be saved like this: {Name: \"Donald\", Roll Number: 100} . And for storing a PDF, the 0s and 1s (binary data of the PDF) are stored as entries in JSON files. So, whatever data you store, it\u2019s stored in a JSON document. Hence, it's called a document database. Let's see below how the data, structured(sql tables) and unstructured(pdf documents) are stored in MongoDB.","title":"Overview"},{"location":"MongoDB/HowMongoDBStoresFiles/#how-mongodb-would-store-your-normal-sql-datasimple-text","text":"Suppose, we have a simple MSSQL table but, we want to store the data now in MongoDB. How would MongoDB store it. When you save this data in MongoDB, each row becomes a JSON file. These documents are stored inside a collection (like a table) called countries inside a database called Earth . Now, let's do some practical to see it in reality. Follow the steps below. To run MongoDB commands, you can install MongoDB Shell from here . Open CMD and key in mongosh . It will log in and start a test database: Create the Earth Database. Just enter Earth and it will create the database. No need to create it separately. Now, insert the Data into the countries Collection: This will create the collection. No need to create it beforehand. ```javascript db.countries.insertMany([ { \"Country\": \"USA\", \"Population\": 300, \"Language\": \"English\" }, { \"Country\": \"Cuba\", \"Population\": 100, \"Language\": \"Spanish\" } ]); ``` <img src=\"images/custom-image-2024-07-20-02-22-46.png\" alt=\"Description of the image\" style=\"max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\"> You can see the data in MongoDB Compass To make sure the data is inserted properly. you can query the collection using this command: db.countries.find().pretty(); . This command will display the documents stored in the countries collection in a readable format.","title":"How MongoDB would store your normal SQL Data(simple text)?"},{"location":"MongoDB/HowMongoDBStoresFiles/#storing-unstructured-data-like-files-in-mongodb","text":"Let's now see how MongoDB stores a PDF file. Say we have a 1 MB PDF file named Resume.pdf . How would MongoDB store it? MongoDB uses a specification called GridFS to store large files. GridFS divides the file into smaller chunks and stores each chunk as a separate document.","title":"Storing unstructured data like files in MongoDB"},{"location":"MongoDB/HowMongoDBStoresFiles/#how-gridfs-splits-files","text":"When you save a file in MongoDB using GridFS, the file is split into smaller pieces (around 255 KB). For a 1 MB PDF file, it will be split into 4 smaller files. Each file-piece is stored in the fs.chunks collection as a JSON file. Yes, your Adobe PDF's binary data is split into pieces, and the 01$#$#$ is put inside the JSON file. There will be one metadata file for the entire PDF stored as a JSON file in the fs.files collection. JSON, JSON everywhere.","title":"How GridFS Splits Files"},{"location":"MongoDB/HowMongoDBStoresFiles/#storing-a-sample-file-resumepdf","text":"When you save the Resume.pdf file in MongoDB, it is broken into chunks and stored across two collections: fs.files and fs.chunks . File Metadata - fs.files : Each file has a metadata document in the fs.files collection. json { \"_id\": ObjectId(\"...\"), \"filename\": \"Resume.pdf\", \"length\": 1048576, // Size in bytes (1 MB) \"chunkSize\": 261120, // Default chunk size (255 KB) \"uploadDate\": ISODate(\"2024-07-20T12:34:56Z\"), \"md5\": \"...\" } File Chunks - fs.chunks : The file data is stored in chunks in the fs.chunks collection. Each chunk is linked to the file via the files_id . json { \"_id\": ObjectId(\"...\"), \"files_id\": ObjectId(\"...\"), // Reference to fs.files document \"n\": 0, // Chunk number \"data\": <binary data $###$WhenYouOpenAdobePDFinNotepad01010101> } { \"_id\": ObjectId(\"...\"), \"files_id\": ObjectId(\"...\"), // Reference to fs.files document \"n\": 1, // Chunk number \"data\": <binary data $###$WhenYouOpenAdobePDFinNotepad01010101> } // More chunks follow...","title":"Storing a sample file Resume.pdf"},{"location":"MongoDB/HowMongoDBStoresFiles/#lets-see-this-in-practice","text":"To run MongoDB commands, you can install MongoDB Shell from here . Now, let's try to create a Python code to upload a sample file into MongoDB. Install Required Packages : Make sure you have pymongo installed. You can install it using pip: bash pip install pymongo gridfs Python Code to Store Resume.pdf : ```python from pymongo import MongoClient import gridfs","title":"Let's See This in Practice"},{"location":"MongoDB/HowMongoDBStoresFiles/#connect-to-mongodb","text":"client = MongoClient('mongodb://localhost:27017') db = client['Earth']","title":"Connect to MongoDB"},{"location":"MongoDB/HowMongoDBStoresFiles/#create-a-gridfs-bucket","text":"fs = gridfs.GridFS(db)","title":"Create a GridFS bucket"},{"location":"MongoDB/HowMongoDBStoresFiles/#open-the-pdf-file-and-store-it-in-mongodb","text":"with open('Resume.pdf', 'rb') as f: file_id = fs.put(f, filename='Resume.pdf') print(f'File stored with id: {file_id}') ``` Verify the File Storage : You can verify that the file has been stored correctly by querying the fs.files collection: ```python # Verify file storage stored_file = db.fs.files.find_one({'filename': 'Resume.pdf'}) print(stored_file) ```","title":"Open the PDF file and store it in MongoDB"},{"location":"MongoDB/HowMongoDBStoresFiles/#lets-recap-the-splitting-process","text":"When you upload Resume.pdf using GridFS: - The file is split into pieces. The default chunk size is 255 KB. - For a 1 MB file, it would be split into around 4 pieces (1 MB / 255 KB \u2248 4). - Each piece is stored as a separate JSON file in the fs.chunks collection. Yes, the Adobe PDF's binary data is stuffed inside the JSON file. - There will be one JSON file in the fs.files collection to store metadata about the real file, including its length, chunk size, and upload date. - Each chunk document in fs.chunks contains a reference to the file's metadata document in fs.files .","title":"Let's recap the splitting process"},{"location":"MongoDB/HowMongoDBStoresFiles/#now-lets-see-the-actual-details","text":"In actual fact, each row in a SQL table when saved in MongoDB becomes a document. Documents are not exactly .JSON files. Rather, they are stored in BSON (Binary JSON) format within MongoDB, which is an internal storage format optimized for performance and space efficiency.","title":"Now, let's see the actual details"},{"location":"MongoDB/HowMongoDBStoresFiles/#detailed-explanation","text":"SQL to MongoDB : When migrating data from a SQL table to MongoDB, each row from the SQL table becomes a document in a MongoDB collection. Documents : These documents are similar to JSON objects but are stored internally as BSON. BSON allows MongoDB to efficiently store and retrieve data. Storage in MongoDB : MongoDB does not store each document as a separate file on the filesystem. Instead, all documents within a collection are stored together in large database files managed by MongoDB\u2019s storage engine. These database files typically reside in the dbpath directory of your MongoDB installation and have extensions like .wt for WiredTiger. Accessing Data : When you query MongoDB, it retrieves the BSON documents from these large files and converts them to JSON-like structures for ease of use in your application.","title":"Detailed Explanation"},{"location":"MongoDB/HowMongoDBStoresFiles/#summary","text":"For conceptual understanding, you can think of each document as a JSON object. However, in reality, these documents are stored in a more efficient binary format (BSON) within MongoDB's internal database files.","title":"Summary"},{"location":"MongoDB/MongbDB_Vs_Atlas_VsCosmosDB/","text":"MongoDB was released in 2009. It's a NoSQL document database and stores data in JSON files. Actualy in Binary Json Files. MongoDB Vs MongoDB Atlas","title":"MongoDB Vs CosmosDB"},{"location":"MongoDB/MongoDBCommands/","text":"MongoDB Command Cheatsheet Using the MongoDB Shell Connecting to MongoDB Description Command Connect to a MongoDB instance mongo Connect to a specific database mongo <database_name> Connect to a remote MongoDB server mongo <hostname>:<port>/<database_name> Connect via mongosh mongosh Connect to specific host and port via mongosh mongosh --host <host> --port <port> --authenticationDatabase admin -u <user> -p <pwd> Connect via mongosh with connection string mongosh \"mongodb://<user>:<password>@192.168.1.1:27017\" Connect to MongoDB Atlas via mongosh mongosh \"mongodb+srv://cluster-name.abcde.mongodb.net/<dbname>\" --apiVersion 1 --username <username> Database Operations Description Command Show all databases show dbs Use a specific database use <database_name> Get the current database db Drop the current database db.dropDatabase() Collection Operations Description Command Show all collections in the current database show collections Create a collection db.createCollection(\"<collection_name>\") Drop a collection db.<collection_name>.drop() CRUD Operations Create Description Command Insert a document db.coll.insertOne({name: \"Max\"}) Insert multiple documents (ordered) db.coll.insertMany([{name: \"Max\"}, {name: \"Alex\"}]) Insert multiple documents (unordered) db.coll.insertMany([{name: \"Max\"}, {name: \"Alex\"}], {ordered: false}) Insert document with current date db.coll.insertOne({date: ISODate()}) Insert document with write concern db.coll.insertOne({name: \"Max\"}, {\"writeConcern\": {\"w\": \"majority\", \"wtimeout\": 5000}}) Read Description Command Find one document db.coll.findOne() Find all documents db.coll.find() Find all documents (pretty print) db.coll.find().pretty() Find documents with a query db.coll.find({name: \"Max\", age: 32}) Find documents with date db.coll.find({date: ISODate(\"2020-09-25T13:57:17.180Z\")}) Explain query execution stats db.coll.find({name: \"Max\", age: 32}).explain(\"executionStats\") Get distinct values of a field db.coll.distinct(\"name\") Count documents with a query db.coll.countDocuments({age: 32}) Get estimated document count db.coll.estimatedDocumentCount() Update Description Command Update a document (set fields) db.coll.updateOne({\"_id\": 1}, {$set: {\"year\": 2016, name: \"Max\"}}) Update a document (unset fields) db.coll.updateOne({\"_id\": 1}, {$unset: {\"year\": 1}}) Rename a field db.coll.updateOne({\"_id\": 1}, {$rename: {\"year\": \"date\"}}) Increment a field db.coll.updateOne({\"_id\": 1}, {$inc: {\"year\": 5}}) Multiply a field db.coll.updateOne({\"_id\": 1}, {$mul: {price: NumberDecimal(\"1.25\"), qty: 2}}) Set minimum value of a field db.coll.updateOne({\"_id\": 1}, {$min: {\"imdb\": 5}}) Set maximum value of a field db.coll.updateOne({\"_id\": 1}, {$max: {\"imdb\": 8}}) Set current date db.coll.updateOne({\"_id\": 1}, {$currentDate: {\"lastModified\": true}}) Set current date as timestamp db.coll.updateOne({\"_id\": 1}, {$currentDate: {\"lastModified\": {$type: \"timestamp\"}}}) Array updates (push) db.coll.updateOne({\"_id\": 1}, {$push: {\"array\": 1}}) Array updates (pull) db.coll.updateOne({\"_id\": 1}, {$pull: {\"array\": 1}}) Array updates (addToSet) db.coll.updateOne({\"_id\": 1}, {$addToSet: {\"array\": 2}}) Array updates (pop last element) db.coll.updateOne({\"_id\": 1}, {$pop: {\"array\": 1}}) Array updates (pop first element) db.coll.updateOne({\"_id\": 1}, {$pop: {\"array\": -1}}) Array updates (pullAll) db.coll.updateOne({\"_id\": 1}, {$pullAll: {\"array\": [3, 4, 5]}}) Array updates (push with each) db.coll.updateOne({\"_id\": 1}, {$push: {\"scores\": {$each: [90, 92]}}}) Array updates (push with sort) db.coll.updateOne({\"_id\": 2}, {$push: {\"scores\": {$each: [40, 60], $sort: 1}}}) Update array element db.coll.updateOne({\"_id\": 1, \"grades\": 80}, {$set: {\"grades.$\": 82}}) Update all array elements db.coll.updateMany({}, {$inc: {\"grades.$[]\": 10}}) Update array elements with filter db.coll.updateMany({}, {$set: {\"grades.$[element]\": 100}}, {multi: true, arrayFilters: [{\"element\": {$gte: 100}}]}) Find and update db.coll.findOneAndUpdate({\"name\": \"Max\"}, {$inc: {\"points\": 5}}, {returnNewDocument: true}) Upsert (update or insert) db.coll.updateOne({\"_id\": 1}, {$set: {item: \"apple\"}, $setOnInsert: {defaultQty: 100}}, {upsert: true}) Replace a document db.coll.replaceOne({\"name\": \"Max\"}, {\"firstname\": \"Maxime\", \"surname\": \"Beugnet\"}) Update with write concern db.coll.updateMany({}, {$set: {\"x\": 1}}, {\"writeConcern\": {\"w\": \"majority\", \"wtimeout\": 5000}}) Delete Description Command Delete one document db.coll.deleteOne({name: \"Max\"}) Delete multiple documents db.coll.deleteMany({name: \"Max\"}, {\"writeConcern\": {\"w\": \"majority\", \"wtimeout\": 5000}}) Delete all documents db.coll.deleteMany({}) Find and delete db.coll.findOneAndDelete({\"name\": \"Max\"}) Indexing Description Command Create an index on a field db.<collection_name>.createIndex({<field>: 1}) Create a unique index db.<collection_name>.createIndex({<field>: 1}, {unique: true}) List all indexes on a collection db.<collection_name>.getIndexes() Drop an index db.<collection_name>.dropIndex(\"<index_name>\") Hide an index db.coll.hideIndex(\"name_1\") Unhide an index db.coll.unhideIndex(\"name_1\") Aggregation Description Command Aggregation framework db.<collection_name>.aggregate([ { $match: { <field>: <value> } }, { $group: { _id: \"$<group_field>\", total: { $sum: \"$<sum_field>\" } } }, { $sort: {total: -1} } ]) Backup and Restore Description Command Backup a database mongodump --db <database_name> --out <backup_directory> Restore a database mongorestore --db <database_name> <backup_directory>/<database_name> User Management Description Command Create a new user db.createUser({ user: \"<username>\", pwd: \"<password>\", roles: [ { role: \"<role>\", db: \"<database>\" } ] }) Show users show users Drop a user db.dropUser(\"<username>\") Server Administration Description Command Server status db.serverStatus() Database statistics db.stats() Collection statistics db.<collection_name>.stats() Current operations db.currentOp() Kill an operation db.killOp(<operation_id>) Lock the database db.fsyncLock() Unlock the database db.fsyncUnlock() Get collection names db.getCollectionNames() Get collection info db.getCollectionInfos() Print collection stats db.printCollectionStats() Replication info db.getReplicationInfo() Print replication info db.printReplicationInfo() Server info db.hello() Host info db.hostInfo() Shutdown server db.shutdownServer() Profiling status db.getProfilingStatus() Set profiling level db.setProfilingLevel(1, 200) Enable free monitoring db.enableFreeMonitoring() Disable free monitoring db.disableFreeMonitoring() Get free monitoring status db.getFreeMonitoringStatus() Handy Commands Description Command Use admin database use admin Create root user db.createUser({\"user\": \"root\", \"pwd\": passwordPrompt(), \"roles\": [\"root\"]}) Drop root user db.dropUser(\"root\") Authenticate user db.auth( \"user\", passwordPrompt() ) Switch to test database use test Get sibling database db.getSiblingDB(\"dbname\") Get current operations db.currentOp() Kill operation db.killOp(123) Get collection stats db.printCollectionStats() Get server status db.serverStatus() Create a view db.createView(\"viewName\", \"sourceColl\", [{$project:{department: 1}}]) Change Streams Description Command Watch for changes watchCursor = db.coll.watch([ { $match : {\"operationType\" : \"insert\" } } ]) Iterate change stream while (!watchCursor.isExhausted()){ if (watchCursor.hasNext()){ print(tojson(watchCursor.next())); } } Replica Set Description Command Replica set status rs.status() Initiate replica set rs.initiate({\"_id\": \"RS1\", members: [ { _id: 0, host: \"mongodb1.net:27017\" }, { _id: 1, host: \"mongodb2.net:27017\" }, { _id: 2, host: \"mongodb3.net:27017\" }]}) Add a member rs.add(\"mongodb4.net:27017\") Add an arbiter rs.addArb(\"mongodb5.net:27017\") Remove a member rs.remove(\"mongodb1.net:27017\") Get replica set config rs.conf() Replica set hello rs.hello() Print replication info rs.printReplicationInfo() Print secondary replication info rs.printSecondaryReplicationInfo() Reconfigure replica set rs.reconfig(config) Set read preference db.getMongo().setReadPref('secondaryPreferred') Step down primary rs.stepDown(20, 5) Sharded Cluster Description Command Print sharding status db.printShardingStatus() Sharding status sh.status() Add a shard sh.addShard(\"rs1/mongodb1.example.net:27017\") Shard a collection sh.shardCollection(\"mydb.coll\", {zipcode: 1}) Move a chunk sh.moveChunk(\"mydb.coll\", { zipcode: \"53187\" }, \"shard0019\") Split a chunk at a key sh.splitAt(\"mydb.coll\", {x: 70}) Split a chunk by find query sh.splitFind(\"mydb.coll\", {x: 70}) Start balancer sh.startBalancer() Stop balancer sh.stopBalancer() Disable balancing sh.disableBalancing(\"mydb.coll\") Enable balancing sh.enableBalancing(\"mydb.coll\") Get balancer state sh.getBalancerState() Set balancer state sh.setBalancerState(true/false) Is balancer running sh.isBalancerRunning() Start auto merger sh.startAutoMerger() Stop auto merger sh.stopAutoMerger() Enable auto merger sh.enableAutoMerger() Disable auto merger sh.disableAutoMerger() Update zone key range sh.updateZoneKeyRange(\"mydb.coll\", {state: \"NY\", zip: MinKey }, { state: \"NY\", zip: MaxKey }, \"NY\") Remove range from zone sh.removeRangeFromZone(\"mydb.coll\", {state: \"NY\", zip: MinKey }, { state: \"NY\", zip: MaxKey }) Add shard to zone sh.addShardToZone(\"shard0000\", \"NYC\") Remove shard from zone sh.removeShardFromZone(\"shard0000\", \"NYC\")","title":"MongoDB Commands"},{"location":"MongoDB/MongoDBCommands/#mongodb-command-cheatsheet","text":"","title":"MongoDB Command Cheatsheet"},{"location":"MongoDB/MongoDBCommands/#using-the-mongodb-shell","text":"","title":"Using the MongoDB Shell"},{"location":"MongoDB/MongoDBCommands/#connecting-to-mongodb","text":"Description Command Connect to a MongoDB instance mongo Connect to a specific database mongo <database_name> Connect to a remote MongoDB server mongo <hostname>:<port>/<database_name> Connect via mongosh mongosh Connect to specific host and port via mongosh mongosh --host <host> --port <port> --authenticationDatabase admin -u <user> -p <pwd> Connect via mongosh with connection string mongosh \"mongodb://<user>:<password>@192.168.1.1:27017\" Connect to MongoDB Atlas via mongosh mongosh \"mongodb+srv://cluster-name.abcde.mongodb.net/<dbname>\" --apiVersion 1 --username <username>","title":"Connecting to MongoDB"},{"location":"MongoDB/MongoDBCommands/#database-operations","text":"Description Command Show all databases show dbs Use a specific database use <database_name> Get the current database db Drop the current database db.dropDatabase()","title":"Database Operations"},{"location":"MongoDB/MongoDBCommands/#collection-operations","text":"Description Command Show all collections in the current database show collections Create a collection db.createCollection(\"<collection_name>\") Drop a collection db.<collection_name>.drop()","title":"Collection Operations"},{"location":"MongoDB/MongoDBCommands/#crud-operations","text":"","title":"CRUD Operations"},{"location":"MongoDB/MongoDBCommands/#create","text":"Description Command Insert a document db.coll.insertOne({name: \"Max\"}) Insert multiple documents (ordered) db.coll.insertMany([{name: \"Max\"}, {name: \"Alex\"}]) Insert multiple documents (unordered) db.coll.insertMany([{name: \"Max\"}, {name: \"Alex\"}], {ordered: false}) Insert document with current date db.coll.insertOne({date: ISODate()}) Insert document with write concern db.coll.insertOne({name: \"Max\"}, {\"writeConcern\": {\"w\": \"majority\", \"wtimeout\": 5000}})","title":"Create"},{"location":"MongoDB/MongoDBCommands/#read","text":"Description Command Find one document db.coll.findOne() Find all documents db.coll.find() Find all documents (pretty print) db.coll.find().pretty() Find documents with a query db.coll.find({name: \"Max\", age: 32}) Find documents with date db.coll.find({date: ISODate(\"2020-09-25T13:57:17.180Z\")}) Explain query execution stats db.coll.find({name: \"Max\", age: 32}).explain(\"executionStats\") Get distinct values of a field db.coll.distinct(\"name\") Count documents with a query db.coll.countDocuments({age: 32}) Get estimated document count db.coll.estimatedDocumentCount()","title":"Read"},{"location":"MongoDB/MongoDBCommands/#update","text":"Description Command Update a document (set fields) db.coll.updateOne({\"_id\": 1}, {$set: {\"year\": 2016, name: \"Max\"}}) Update a document (unset fields) db.coll.updateOne({\"_id\": 1}, {$unset: {\"year\": 1}}) Rename a field db.coll.updateOne({\"_id\": 1}, {$rename: {\"year\": \"date\"}}) Increment a field db.coll.updateOne({\"_id\": 1}, {$inc: {\"year\": 5}}) Multiply a field db.coll.updateOne({\"_id\": 1}, {$mul: {price: NumberDecimal(\"1.25\"), qty: 2}}) Set minimum value of a field db.coll.updateOne({\"_id\": 1}, {$min: {\"imdb\": 5}}) Set maximum value of a field db.coll.updateOne({\"_id\": 1}, {$max: {\"imdb\": 8}}) Set current date db.coll.updateOne({\"_id\": 1}, {$currentDate: {\"lastModified\": true}}) Set current date as timestamp db.coll.updateOne({\"_id\": 1}, {$currentDate: {\"lastModified\": {$type: \"timestamp\"}}}) Array updates (push) db.coll.updateOne({\"_id\": 1}, {$push: {\"array\": 1}}) Array updates (pull) db.coll.updateOne({\"_id\": 1}, {$pull: {\"array\": 1}}) Array updates (addToSet) db.coll.updateOne({\"_id\": 1}, {$addToSet: {\"array\": 2}}) Array updates (pop last element) db.coll.updateOne({\"_id\": 1}, {$pop: {\"array\": 1}}) Array updates (pop first element) db.coll.updateOne({\"_id\": 1}, {$pop: {\"array\": -1}}) Array updates (pullAll) db.coll.updateOne({\"_id\": 1}, {$pullAll: {\"array\": [3, 4, 5]}}) Array updates (push with each) db.coll.updateOne({\"_id\": 1}, {$push: {\"scores\": {$each: [90, 92]}}}) Array updates (push with sort) db.coll.updateOne({\"_id\": 2}, {$push: {\"scores\": {$each: [40, 60], $sort: 1}}}) Update array element db.coll.updateOne({\"_id\": 1, \"grades\": 80}, {$set: {\"grades.$\": 82}}) Update all array elements db.coll.updateMany({}, {$inc: {\"grades.$[]\": 10}}) Update array elements with filter db.coll.updateMany({}, {$set: {\"grades.$[element]\": 100}}, {multi: true, arrayFilters: [{\"element\": {$gte: 100}}]}) Find and update db.coll.findOneAndUpdate({\"name\": \"Max\"}, {$inc: {\"points\": 5}}, {returnNewDocument: true}) Upsert (update or insert) db.coll.updateOne({\"_id\": 1}, {$set: {item: \"apple\"}, $setOnInsert: {defaultQty: 100}}, {upsert: true}) Replace a document db.coll.replaceOne({\"name\": \"Max\"}, {\"firstname\": \"Maxime\", \"surname\": \"Beugnet\"}) Update with write concern db.coll.updateMany({}, {$set: {\"x\": 1}}, {\"writeConcern\": {\"w\": \"majority\", \"wtimeout\": 5000}})","title":"Update"},{"location":"MongoDB/MongoDBCommands/#delete","text":"Description Command Delete one document db.coll.deleteOne({name: \"Max\"}) Delete multiple documents db.coll.deleteMany({name: \"Max\"}, {\"writeConcern\": {\"w\": \"majority\", \"wtimeout\": 5000}}) Delete all documents db.coll.deleteMany({}) Find and delete db.coll.findOneAndDelete({\"name\": \"Max\"})","title":"Delete"},{"location":"MongoDB/MongoDBCommands/#indexing","text":"Description Command Create an index on a field db.<collection_name>.createIndex({<field>: 1}) Create a unique index db.<collection_name>.createIndex({<field>: 1}, {unique: true}) List all indexes on a collection db.<collection_name>.getIndexes() Drop an index db.<collection_name>.dropIndex(\"<index_name>\") Hide an index db.coll.hideIndex(\"name_1\") Unhide an index db.coll.unhideIndex(\"name_1\")","title":"Indexing"},{"location":"MongoDB/MongoDBCommands/#aggregation","text":"Description Command Aggregation framework db.<collection_name>.aggregate([ { $match: { <field>: <value> } }, { $group: { _id: \"$<group_field>\", total: { $sum: \"$<sum_field>\" } } }, { $sort: {total: -1} } ])","title":"Aggregation"},{"location":"MongoDB/MongoDBCommands/#backup-and-restore","text":"Description Command Backup a database mongodump --db <database_name> --out <backup_directory> Restore a database mongorestore --db <database_name> <backup_directory>/<database_name>","title":"Backup and Restore"},{"location":"MongoDB/MongoDBCommands/#user-management","text":"Description Command Create a new user db.createUser({ user: \"<username>\", pwd: \"<password>\", roles: [ { role: \"<role>\", db: \"<database>\" } ] }) Show users show users Drop a user db.dropUser(\"<username>\")","title":"User Management"},{"location":"MongoDB/MongoDBCommands/#server-administration","text":"Description Command Server status db.serverStatus() Database statistics db.stats() Collection statistics db.<collection_name>.stats() Current operations db.currentOp() Kill an operation db.killOp(<operation_id>) Lock the database db.fsyncLock() Unlock the database db.fsyncUnlock() Get collection names db.getCollectionNames() Get collection info db.getCollectionInfos() Print collection stats db.printCollectionStats() Replication info db.getReplicationInfo() Print replication info db.printReplicationInfo() Server info db.hello() Host info db.hostInfo() Shutdown server db.shutdownServer() Profiling status db.getProfilingStatus() Set profiling level db.setProfilingLevel(1, 200) Enable free monitoring db.enableFreeMonitoring() Disable free monitoring db.disableFreeMonitoring() Get free monitoring status db.getFreeMonitoringStatus()","title":"Server Administration"},{"location":"MongoDB/MongoDBCommands/#handy-commands","text":"Description Command Use admin database use admin Create root user db.createUser({\"user\": \"root\", \"pwd\": passwordPrompt(), \"roles\": [\"root\"]}) Drop root user db.dropUser(\"root\") Authenticate user db.auth( \"user\", passwordPrompt() ) Switch to test database use test Get sibling database db.getSiblingDB(\"dbname\") Get current operations db.currentOp() Kill operation db.killOp(123) Get collection stats db.printCollectionStats() Get server status db.serverStatus() Create a view db.createView(\"viewName\", \"sourceColl\", [{$project:{department: 1}}])","title":"Handy Commands"},{"location":"MongoDB/MongoDBCommands/#change-streams","text":"Description Command Watch for changes watchCursor = db.coll.watch([ { $match : {\"operationType\" : \"insert\" } } ]) Iterate change stream while (!watchCursor.isExhausted()){ if (watchCursor.hasNext()){ print(tojson(watchCursor.next())); } }","title":"Change Streams"},{"location":"MongoDB/MongoDBCommands/#replica-set","text":"Description Command Replica set status rs.status() Initiate replica set rs.initiate({\"_id\": \"RS1\", members: [ { _id: 0, host: \"mongodb1.net:27017\" }, { _id: 1, host: \"mongodb2.net:27017\" }, { _id: 2, host: \"mongodb3.net:27017\" }]}) Add a member rs.add(\"mongodb4.net:27017\") Add an arbiter rs.addArb(\"mongodb5.net:27017\") Remove a member rs.remove(\"mongodb1.net:27017\") Get replica set config rs.conf() Replica set hello rs.hello() Print replication info rs.printReplicationInfo() Print secondary replication info rs.printSecondaryReplicationInfo() Reconfigure replica set rs.reconfig(config) Set read preference db.getMongo().setReadPref('secondaryPreferred') Step down primary rs.stepDown(20, 5)","title":"Replica Set"},{"location":"MongoDB/MongoDBCommands/#sharded-cluster","text":"Description Command Print sharding status db.printShardingStatus() Sharding status sh.status() Add a shard sh.addShard(\"rs1/mongodb1.example.net:27017\") Shard a collection sh.shardCollection(\"mydb.coll\", {zipcode: 1}) Move a chunk sh.moveChunk(\"mydb.coll\", { zipcode: \"53187\" }, \"shard0019\") Split a chunk at a key sh.splitAt(\"mydb.coll\", {x: 70}) Split a chunk by find query sh.splitFind(\"mydb.coll\", {x: 70}) Start balancer sh.startBalancer() Stop balancer sh.stopBalancer() Disable balancing sh.disableBalancing(\"mydb.coll\") Enable balancing sh.enableBalancing(\"mydb.coll\") Get balancer state sh.getBalancerState() Set balancer state sh.setBalancerState(true/false) Is balancer running sh.isBalancerRunning() Start auto merger sh.startAutoMerger() Stop auto merger sh.stopAutoMerger() Enable auto merger sh.enableAutoMerger() Disable auto merger sh.disableAutoMerger() Update zone key range sh.updateZoneKeyRange(\"mydb.coll\", {state: \"NY\", zip: MinKey }, { state: \"NY\", zip: MaxKey }, \"NY\") Remove range from zone sh.removeRangeFromZone(\"mydb.coll\", {state: \"NY\", zip: MinKey }, { state: \"NY\", zip: MaxKey }) Add shard to zone sh.addShardToZone(\"shard0000\", \"NYC\") Remove shard from zone sh.removeShardFromZone(\"shard0000\", \"NYC\")","title":"Sharded Cluster"},{"location":"PowerPlatform/CalculationGroups/","text":"","title":"CalculationGroups"},{"location":"PowerPlatform/CustomConnectors/","text":"Background Steps to creating Custom Connectors for Power Platform Example Action Using the Custom Connector in Power Apps Example Usage Background Power Platform is great in automation, but its true potential unlocks when you can connect it to non-standard data sources. This is where custom connectors come in, acting as bridges between Power Platform and external APIs. In this guide I will show you how to create custom connectors: Steps to creating Custom Connectors for Power Platform Building the Connector in Power Automate: Launch Power Automate: Sign in and navigate to the \"Solutions\" section. Create a new solution or open an existing one. Add a Custom Connector: Within the solution, expand the \"New\" menu and select \"Automation\" > \"Custom connector.\" Define Connection Details: Choose \"Create from blank\" and provide a descriptive name for your connector. Import API Definition (Optional): If the API offers an OpenAPI (Swagger) definition file (.json or .yaml), you can import it to automatically populate actions and data models. Otherwise, you'll need to define them manually. Define Actions: Here's where the magic happens! Create actions representing the functionalities you want to expose in Power Apps. Each action should have a clear name, a concise description, and well-defined parameters (inputs) and outputs. Parameters: Specify the data types (text, number, etc.) required for each action. Outputs: Define the structure of the data returned by the action (often matching the API's response format). Example Action Let's say you're building a connector for a weather API. You might create an action named \"GetWeather\" that accepts a \"city\" parameter (text) and returns an output containing \"temperature,\" \"humidity,\" and \"weather description\" (all text). Configure Authentication: Specify the authentication method your chosen API requires. This could involve API keys, OAuth, or other methods. Test and Validate: The \"Test\" tab allows you to send test requests with sample data to ensure your connector functions as expected. Refine your definitions until the tests pass successfully. Save and Publish: Once everything is working smoothly, save your connector and then publish it to make it available in your environment. Using the Custom Connector in Power Apps Create a Power App: Build a new Power App or open an existing one. Connect to the Custom Connector: Navigate to the \"Data\" pane and select \"Add data.\" Choose the \"Connectors\" tab and locate your custom connector by name. Click on it to establish a connection. Utilize the Connector Actions: Within your Power App formulas and expressions, you can now leverage the actions you defined in your custom connector. Example Usage In your weather app, you could use the \"GetWeather\" action within a formula to display the current temperature and weather description for a user-entered city.","title":"Building Custom Connectors"},{"location":"PowerPlatform/CustomConnectors/#background","text":"Power Platform is great in automation, but its true potential unlocks when you can connect it to non-standard data sources. This is where custom connectors come in, acting as bridges between Power Platform and external APIs. In this guide I will show you how to create custom connectors:","title":"Background"},{"location":"PowerPlatform/CustomConnectors/#steps-to-creating-custom-connectors-for-power-platform","text":"Building the Connector in Power Automate: Launch Power Automate: Sign in and navigate to the \"Solutions\" section. Create a new solution or open an existing one. Add a Custom Connector: Within the solution, expand the \"New\" menu and select \"Automation\" > \"Custom connector.\" Define Connection Details: Choose \"Create from blank\" and provide a descriptive name for your connector. Import API Definition (Optional): If the API offers an OpenAPI (Swagger) definition file (.json or .yaml), you can import it to automatically populate actions and data models. Otherwise, you'll need to define them manually. Define Actions: Here's where the magic happens! Create actions representing the functionalities you want to expose in Power Apps. Each action should have a clear name, a concise description, and well-defined parameters (inputs) and outputs. Parameters: Specify the data types (text, number, etc.) required for each action. Outputs: Define the structure of the data returned by the action (often matching the API's response format).","title":"Steps to creating Custom Connectors for Power Platform"},{"location":"PowerPlatform/CustomConnectors/#example-action","text":"Let's say you're building a connector for a weather API. You might create an action named \"GetWeather\" that accepts a \"city\" parameter (text) and returns an output containing \"temperature,\" \"humidity,\" and \"weather description\" (all text). Configure Authentication: Specify the authentication method your chosen API requires. This could involve API keys, OAuth, or other methods. Test and Validate: The \"Test\" tab allows you to send test requests with sample data to ensure your connector functions as expected. Refine your definitions until the tests pass successfully. Save and Publish: Once everything is working smoothly, save your connector and then publish it to make it available in your environment.","title":"Example Action"},{"location":"PowerPlatform/CustomConnectors/#using-the-custom-connector-in-power-apps","text":"Create a Power App: Build a new Power App or open an existing one. Connect to the Custom Connector: Navigate to the \"Data\" pane and select \"Add data.\" Choose the \"Connectors\" tab and locate your custom connector by name. Click on it to establish a connection. Utilize the Connector Actions: Within your Power App formulas and expressions, you can now leverage the actions you defined in your custom connector.","title":"Using the Custom Connector in Power Apps"},{"location":"PowerPlatform/CustomConnectors/#example-usage","text":"In your weather app, you could use the \"GetWeather\" action within a formula to display the current temperature and weather description for a user-entered city.","title":"Example Usage"},{"location":"PowerPlatform/ECMCaptureFlow/","text":"Overview Traditional method of document capture Alternative method - using Power Platform Let's get started Our Workflow How to Create the Workflow Conclusion Further Reading Overview Image capture and extraction workflows are heavily used in the Banking and Insurance sectors. Almost all institutions use products like OpenText inteligent capture, Kofax Capture, Datacap etc to Capture documents from multiple channels and store them in backend ECM systems like Opentext content server, SharePoint server etc. Traditional method of document capture In the banking and insurance sectors, the usual way to handle document capture is shown in the diagram below: Alternative method - using Power Platform I'll demonstrate how you can use Power Platform to achieve some of these functions easily. I'll guide you through a simplified solution to get started with this approach. Let's get started Our Workflow This guide outlines how to capture and extract data from invoices and store them in SharePoint using a simple workflow. Invoices are placed in a dedicated folder on OneDrive and processed automatically when a new file is detected by the Power Automate OneDrive trigger. How to Create the Workflow Follow these steps to set up the automated workflow: In Power Automate, click + Create > Automated cloud flow . Provide a flow name, enter When a file is created in the search box, and then select When a file is created (OneDrive for Business) . Click on Next Step , then + New step . To specify the folder to monitor, click on the folder icon under Parameters and select the OneDrive folder that will contain the invoices. Click + Add an action , search for Extract information from invoice , and select AI Builder > Extract information from invoices . Configure the Extract information from invoices step by clicking on it and then selecting the Parameters tab. For Invoice File , click the lightning bolt icon , then select File content . Add another action by selecting SharePoint > Create file . Configure the Create file action with the following details: Site Address : Enter the SharePoint site URL where the document library is located. Folder Path : Choose the document library. File Name : Select When a file is created > File name . File Content : Choose When a file is created > File content . Add an action for SharePoint > Update file properties . Configure it by providing: The site address hosting the document library. The document library from the dropdown menu. Id : Select Create file > body > ItemId . Map the properties of the document with values extracted from the invoice in the previous steps. To test the flow, click Test , opt for a manual test, and place a file in the monitored OneDrive folder. After a short period, the flow will trigger, and the SharePoint library will be updated with the document and all extracted metadata. Below is how the file will appear in the document library, showcasing the extracted metadata: Conclusion Choosing between AI-based methods and traditional document capture software like Kofax Capture or Captiva Capture involves a careful evaluation of various factors: Licensing Costs : Traditional software often has licensing fees based on usage volume. For instance, a Kofax Capture license for processing 100,000 pages annually may cost approximately $16,526.00 Kofax Capture License Cost . Functionality and Modules : OpenText Intelligent Capture (formerly Captiva) includes a suite of modules designed for different aspects of document handling, such as Standard Import, Standard Export, and Completion (for manual verification), as well as eInput (for browser-based capture). These features may integrate seamlessly into the workflow of financial or insurance environments, offering out-of-the-box solutions. Integration and Customization : Microsoft Power Automate allows for the creation of customized document capture workflows, which can be advantageous for those already utilizing the Microsoft 365 platform. It offers a way to avoid substantial licensing fees while still achieving a high level of functionality. Scan-to-network-folder : A very typical document capture is to scan to a network folder and use Captiv standard import to crate batches. I have seen that this workflow can be easily created using Power Autoamte. All you have to do is configure your scanner to scan to the oneDrive folder. Long story short: The decision hinges on your company's specific needs, existing infrastructure, and budget. For future reference, should this subject arise, consider conducting a POC to show the power of Power Automate to your management team and let them take a call! Further Reading Use the invoice processing prebuilt model in Power Automate .","title":"Power Automate Or Kofax/Captiva?"},{"location":"PowerPlatform/ECMCaptureFlow/#overview","text":"Image capture and extraction workflows are heavily used in the Banking and Insurance sectors. Almost all institutions use products like OpenText inteligent capture, Kofax Capture, Datacap etc to Capture documents from multiple channels and store them in backend ECM systems like Opentext content server, SharePoint server etc.","title":"Overview"},{"location":"PowerPlatform/ECMCaptureFlow/#traditional-method-of-document-capture","text":"In the banking and insurance sectors, the usual way to handle document capture is shown in the diagram below:","title":"Traditional method of document capture"},{"location":"PowerPlatform/ECMCaptureFlow/#alternative-method-using-power-platform","text":"I'll demonstrate how you can use Power Platform to achieve some of these functions easily. I'll guide you through a simplified solution to get started with this approach.","title":"Alternative method - using Power Platform"},{"location":"PowerPlatform/ECMCaptureFlow/#lets-get-started","text":"","title":"Let's get started"},{"location":"PowerPlatform/ECMCaptureFlow/#our-workflow","text":"This guide outlines how to capture and extract data from invoices and store them in SharePoint using a simple workflow. Invoices are placed in a dedicated folder on OneDrive and processed automatically when a new file is detected by the Power Automate OneDrive trigger.","title":"Our Workflow"},{"location":"PowerPlatform/ECMCaptureFlow/#how-to-create-the-workflow","text":"Follow these steps to set up the automated workflow: In Power Automate, click + Create > Automated cloud flow . Provide a flow name, enter When a file is created in the search box, and then select When a file is created (OneDrive for Business) . Click on Next Step , then + New step . To specify the folder to monitor, click on the folder icon under Parameters and select the OneDrive folder that will contain the invoices. Click + Add an action , search for Extract information from invoice , and select AI Builder > Extract information from invoices . Configure the Extract information from invoices step by clicking on it and then selecting the Parameters tab. For Invoice File , click the lightning bolt icon , then select File content . Add another action by selecting SharePoint > Create file . Configure the Create file action with the following details: Site Address : Enter the SharePoint site URL where the document library is located. Folder Path : Choose the document library. File Name : Select When a file is created > File name . File Content : Choose When a file is created > File content . Add an action for SharePoint > Update file properties . Configure it by providing: The site address hosting the document library. The document library from the dropdown menu. Id : Select Create file > body > ItemId . Map the properties of the document with values extracted from the invoice in the previous steps. To test the flow, click Test , opt for a manual test, and place a file in the monitored OneDrive folder. After a short period, the flow will trigger, and the SharePoint library will be updated with the document and all extracted metadata. Below is how the file will appear in the document library, showcasing the extracted metadata:","title":"How to Create the Workflow"},{"location":"PowerPlatform/ECMCaptureFlow/#conclusion","text":"Choosing between AI-based methods and traditional document capture software like Kofax Capture or Captiva Capture involves a careful evaluation of various factors: Licensing Costs : Traditional software often has licensing fees based on usage volume. For instance, a Kofax Capture license for processing 100,000 pages annually may cost approximately $16,526.00 Kofax Capture License Cost . Functionality and Modules : OpenText Intelligent Capture (formerly Captiva) includes a suite of modules designed for different aspects of document handling, such as Standard Import, Standard Export, and Completion (for manual verification), as well as eInput (for browser-based capture). These features may integrate seamlessly into the workflow of financial or insurance environments, offering out-of-the-box solutions. Integration and Customization : Microsoft Power Automate allows for the creation of customized document capture workflows, which can be advantageous for those already utilizing the Microsoft 365 platform. It offers a way to avoid substantial licensing fees while still achieving a high level of functionality. Scan-to-network-folder : A very typical document capture is to scan to a network folder and use Captiv standard import to crate batches. I have seen that this workflow can be easily created using Power Autoamte. All you have to do is configure your scanner to scan to the oneDrive folder. Long story short: The decision hinges on your company's specific needs, existing infrastructure, and budget. For future reference, should this subject arise, consider conducting a POC to show the power of Power Automate to your management team and let them take a call!","title":"Conclusion"},{"location":"PowerPlatform/ECMCaptureFlow/#further-reading","text":"Use the invoice processing prebuilt model in Power Automate .","title":"Further Reading"},{"location":"PowerPlatform/EnableMicrosoftSyntex/","text":"Overview Steps to activate Microsoft Syntex Overview Microsoft Syntex uses content and AI to enable end-to-end intelligent document processing solutions. With Microsoft Syntex you can: - Quickly process and extract information from common business documents like contracts, invoices, and receipts. - Identify field values in structured documents (forms, invoices) and extract information from unstructured documents (letters, contracts). Image Tagging, Taxonomy Tagging, and Translation are also available. Steps to activate Microsoft Syntex Access the Microsoft 365 Admin Center : Log in to your Microsoft 365 admin center with an account that has Global admin or SharePoint admin permissions. Navigate to Files and Content Settings : In the admin center, go to Setup . Under the Files and content section, select Use content AI with Microsoft Syntex . Manage Microsoft Syntex : On the Use content AI with Microsoft Syntex page, click Manage Microsoft Syntex . Choose the Service to Set Up : Select the specific Microsoft Syntex service that you want to set up. You can choose from various services like prebuilt document processing, structured and freeform document processing, unstructured document processing, content assembly, image tagging, taxonomy tagging, translation, Syntex eSignature, optical character recognition, Microsoft 365 Archive (Preview), and Microsoft 365 Backup (Preview). Configure Options : Customize the options based on your requirements. Click Save to apply the settings. Here are direct links to setup instructions for each service: - Set up prebuilt document processing - Set up structured and freeform document processing - Set up unstructured document processing - Set up content assembly - Set up image tagging - Set up taxonomy tagging - Set up document translation - Set up Syntex eSignature - Set up optical character recognition - Set up Microsoft 365 Archive (Preview) - Set up Microsoft 365 Backup (Preview)","title":"Enable Microsoft Syntex on your Office 365 tenant"},{"location":"PowerPlatform/EnableMicrosoftSyntex/#overview","text":"Microsoft Syntex uses content and AI to enable end-to-end intelligent document processing solutions. With Microsoft Syntex you can: - Quickly process and extract information from common business documents like contracts, invoices, and receipts. - Identify field values in structured documents (forms, invoices) and extract information from unstructured documents (letters, contracts). Image Tagging, Taxonomy Tagging, and Translation are also available.","title":"Overview"},{"location":"PowerPlatform/EnableMicrosoftSyntex/#steps-to-activate-microsoft-syntex","text":"Access the Microsoft 365 Admin Center : Log in to your Microsoft 365 admin center with an account that has Global admin or SharePoint admin permissions. Navigate to Files and Content Settings : In the admin center, go to Setup . Under the Files and content section, select Use content AI with Microsoft Syntex . Manage Microsoft Syntex : On the Use content AI with Microsoft Syntex page, click Manage Microsoft Syntex . Choose the Service to Set Up : Select the specific Microsoft Syntex service that you want to set up. You can choose from various services like prebuilt document processing, structured and freeform document processing, unstructured document processing, content assembly, image tagging, taxonomy tagging, translation, Syntex eSignature, optical character recognition, Microsoft 365 Archive (Preview), and Microsoft 365 Backup (Preview). Configure Options : Customize the options based on your requirements. Click Save to apply the settings. Here are direct links to setup instructions for each service: - Set up prebuilt document processing - Set up structured and freeform document processing - Set up unstructured document processing - Set up content assembly - Set up image tagging - Set up taxonomy tagging - Set up document translation - Set up Syntex eSignature - Set up optical character recognition - Set up Microsoft 365 Archive (Preview) - Set up Microsoft 365 Backup (Preview)","title":"Steps to activate Microsoft Syntex"},{"location":"PowerPlatform/EnableSyntexOnYourDocumentLibrary/","text":"Create a model on a local SharePoint site with Microsoft Syntex Create a model on a local site Create a model on a local SharePoint site with Microsoft Syntex Create a model on a local site From a SharePoint document library, select the files you want to analyze, and then select Classify and extract . The first time you use this feature, you're activating Syntex on your site. You'll see the following message. Note You must have the Manage Web Site permission to perform administration tasks and manage content for the site. This would be a site owner. Once the feature is activated, anyone with the Manage Lists permission will be able to create and manage models. Select Activate to continue. You'll see the following message. Select Create a model . On the Create a model panel, type the name of the model, add a description, and then select Create . Proceed to train your custom model or to configure your trained model using the files that you selected. When done, the Add to library panel opens. On the Add to library panel, you'll see the name of your SharePoint site and the document library that the model will be applied to. If you want to apply the model to a different library, select Back to libraries , and choose the library you want to use. Then select Add . On the model home page, in the Where the model is applied on this site section, you can see the libraries that have the model applied. To apply the model to other libraries on the site, select Apply model .","title":"Create a model on SharePoint - Syntex"},{"location":"PowerPlatform/EnableSyntexOnYourDocumentLibrary/#create-a-model-on-a-local-sharepoint-site-with-microsoft-syntex","text":"","title":"Create a model on a local SharePoint site with Microsoft Syntex"},{"location":"PowerPlatform/EnableSyntexOnYourDocumentLibrary/#create-a-model-on-a-local-site","text":"From a SharePoint document library, select the files you want to analyze, and then select Classify and extract . The first time you use this feature, you're activating Syntex on your site. You'll see the following message. Note You must have the Manage Web Site permission to perform administration tasks and manage content for the site. This would be a site owner. Once the feature is activated, anyone with the Manage Lists permission will be able to create and manage models. Select Activate to continue. You'll see the following message. Select Create a model . On the Create a model panel, type the name of the model, add a description, and then select Create . Proceed to train your custom model or to configure your trained model using the files that you selected. When done, the Add to library panel opens. On the Add to library panel, you'll see the name of your SharePoint site and the document library that the model will be applied to. If you want to apply the model to a different library, select Back to libraries , and choose the library you want to use. Then select Add . On the model home page, in the Where the model is applied on this site section, you can see the libraries that have the model applied. To apply the model to other libraries on the site, select Apply model .","title":"Create a model on a local site"},{"location":"PowerPlatform/GoogleProviderPowerPages/","text":"Overview Steps to follow Set up Google in Power Pages Register an App in Google Add the API Set up your consent screen Enter your top-level domain Add credentials Enter site settings in Power Pages Appendix Overview Power Apps portals & Dynamics 365 portals are now called Power Pages. Power Pages websites can be open-to-all or you can have register/sign-in using Google, LinkedIn, Twitter, and Facebook. Here, I will show you how to use Google sign-in on your Power Pages websites. The protocol behind this is OAuth2.0. Using this Google allow users to access your site using their google accounts without exposing their userid/passwords. Steps to follow Set up Google in Power Pages In your Power Pages site, select Set up > Identity providers . Google > More Commands ( \u2026 ) > Configure . Select Next . Under Reply URL , select Copy . Select Open Google . Register an App in Google Add the API Open the Google Developers Console . And create an API project. In the left side panel, select APIs & Services . Select + Enable APIs and Services . Search for and enable Google People API . Set up your consent screen In the left side panel, select Credentials > Configure consent screen . Select the External user type and click Create . Enter the name of the application and select your organization's user support email address. Upload a logo image file if necessary. Enter the URLs of your site's home page, privacy policy, and terms of service, if applicable. Enter an email address where Google can send you developer notifications. Enter your top-level domain Under Authorized domains , select + Add Domain . Enter your site's top-level domain; for example, powerappsportals.com . Select Save and Continue . Add credentials In the left side panel, select Credentials . Select Create credentials > OAuth client ID . Select Web application as the application type. Enter any name e.g. Web sign-in . This is internal. Not shown. Under Authorized JavaScript origins , select + Add URI . Enter your site's URL; for example, https://mySite.powerappsportals.com . Under Authorized redirect URIs , select + Add URI . Enter your site's URL followed by /signin ; for example, https://mySite.powerappsportals.com/signin . Select Create . In the OAuth client created window, select the copy icons to copy the Client ID and Client secret . Click CLOSE . Enter site settings in Power Pages Return to the Power Pages Configure identity provider page you left earlier. Under Configure site settings , paste the following values: Client ID\u200b : Paste the Client ID you copied . Client secret : Paste the Client secret you copied. Click Continue then Close Appendix How to open Power Page Design Studio Go to Power Pages(https://make.powerpages.microsoft.com/) Select the Microsoft Dataverse environment Locate your site in the Active sites list.","title":"Google Authentication"},{"location":"PowerPlatform/GoogleProviderPowerPages/#overview","text":"Power Apps portals & Dynamics 365 portals are now called Power Pages. Power Pages websites can be open-to-all or you can have register/sign-in using Google, LinkedIn, Twitter, and Facebook. Here, I will show you how to use Google sign-in on your Power Pages websites. The protocol behind this is OAuth2.0. Using this Google allow users to access your site using their google accounts without exposing their userid/passwords.","title":"Overview"},{"location":"PowerPlatform/GoogleProviderPowerPages/#steps-to-follow","text":"","title":"Steps to follow"},{"location":"PowerPlatform/GoogleProviderPowerPages/#set-up-google-in-power-pages","text":"In your Power Pages site, select Set up > Identity providers . Google > More Commands ( \u2026 ) > Configure . Select Next . Under Reply URL , select Copy . Select Open Google .","title":"Set up Google in Power Pages"},{"location":"PowerPlatform/GoogleProviderPowerPages/#register-an-app-in-google","text":"","title":"Register an App in Google"},{"location":"PowerPlatform/GoogleProviderPowerPages/#add-the-api","text":"Open the Google Developers Console . And create an API project. In the left side panel, select APIs & Services . Select + Enable APIs and Services . Search for and enable Google People API .","title":"Add the API"},{"location":"PowerPlatform/GoogleProviderPowerPages/#set-up-your-consent-screen","text":"In the left side panel, select Credentials > Configure consent screen . Select the External user type and click Create . Enter the name of the application and select your organization's user support email address. Upload a logo image file if necessary. Enter the URLs of your site's home page, privacy policy, and terms of service, if applicable. Enter an email address where Google can send you developer notifications.","title":"Set up your consent screen"},{"location":"PowerPlatform/GoogleProviderPowerPages/#enter-your-top-level-domain","text":"Under Authorized domains , select + Add Domain . Enter your site's top-level domain; for example, powerappsportals.com . Select Save and Continue .","title":"Enter your top-level domain"},{"location":"PowerPlatform/GoogleProviderPowerPages/#add-credentials","text":"In the left side panel, select Credentials . Select Create credentials > OAuth client ID . Select Web application as the application type. Enter any name e.g. Web sign-in . This is internal. Not shown. Under Authorized JavaScript origins , select + Add URI . Enter your site's URL; for example, https://mySite.powerappsportals.com . Under Authorized redirect URIs , select + Add URI . Enter your site's URL followed by /signin ; for example, https://mySite.powerappsportals.com/signin . Select Create . In the OAuth client created window, select the copy icons to copy the Client ID and Client secret . Click CLOSE .","title":"Add credentials"},{"location":"PowerPlatform/GoogleProviderPowerPages/#enter-site-settings-in-power-pages","text":"Return to the Power Pages Configure identity provider page you left earlier. Under Configure site settings , paste the following values: Client ID\u200b : Paste the Client ID you copied . Client secret : Paste the Client secret you copied. Click Continue then Close","title":"Enter site settings in Power Pages"},{"location":"PowerPlatform/GoogleProviderPowerPages/#appendix","text":"How to open Power Page Design Studio Go to Power Pages(https://make.powerpages.microsoft.com/) Select the Microsoft Dataverse environment Locate your site in the Active sites list.","title":"Appendix"},{"location":"PowerPlatform/HealthClinicDataverseSecurity/","text":"Health Clinic Management System Background A health clinic employs Microsoft's Power Platform, built on Microsoft Dataverse, to manage its patient records, appointments, and internal communications efficiently and securely. The system includes: Patient Records Management (Model-driven App) Appointment Scheduling (Canvas App) Internal Communications (Microsoft Teams integration with automated workflows) Security Requirements and Implementation 1. Licensing - All staff members are provided with Power Apps licenses, determining their base level of access to the Power Platform ecosystem. 2. Environment and Business Unit Setup - The clinic establishes a single Dataverse environment as the primary container for its applications and data. - Within this environment, departments such as Pediatrics and Orthopedics are set up as distinct Business Units , reflecting the clinic's hierarchical and matrix organizational structure. 3. Hierarchical and Matrix Data Access Structure - Senior medical staff are granted broad access across business units due to their role, allowing them to oversee patient care across departments. - Specialized medical staff , such as those with cross-departmental expertise, are given matrix access to multiple business units, enabling them to access patient records in both Pediatrics and Orthopedics. 4. Security Roles, Teams, and Ownership - Custom security roles are created and assigned to staff based on their department and job function. These roles define access at the business unit level, table/record level, and even field level for sensitive information. - Teams are formed to group staff by their function and department. These teams facilitate efficient permission management and reflect the clinic's organizational structure within Dataverse. - Medical Staff Teams : Include doctors and nurses, with access permissions to patient records based on their specific roles and the hierarchical structure. - Receptionist Team : Has access to the Appointment Scheduling app but limited access to patient records, ensuring they can manage appointments without viewing sensitive health information. - Group Teams (Microsoft 365 groups) are used for internal communications, integrating seamlessly with Microsoft Teams for efficient collaboration without compromising patient data security. 5. Table/Record Ownership and Access Controls - Patient records are owned by the clinic but are accessible only to authorized medical personnel based on their security roles and team memberships. - Appointment data is managed by receptionists, who are the primary owners of this data, ensuring that only they can make changes to the schedule. Advanced Security Measures Row-Level Security is implemented to ensure that only medical staff involved in a patient's care can access their records. Field-Level Security restricts access to highly sensitive patient information, such as medical history, to authorized senior medical staff only. Custom connectors to external services (e.g., SMS notifications for appointments) are secured and managed based on the credentials of the service, ensuring data is shared securely and in compliance with health data protection regulations. Conclusion Hope this gave you good overview of how Dataverse security concepts are implemented.","title":"HealthClinicDataverseSecurity"},{"location":"PowerPlatform/HealthClinicDataverseSecurity/#health-clinic-management-system","text":"","title":"Health Clinic Management System"},{"location":"PowerPlatform/HealthClinicDataverseSecurity/#background","text":"A health clinic employs Microsoft's Power Platform, built on Microsoft Dataverse, to manage its patient records, appointments, and internal communications efficiently and securely. The system includes: Patient Records Management (Model-driven App) Appointment Scheduling (Canvas App) Internal Communications (Microsoft Teams integration with automated workflows)","title":"Background"},{"location":"PowerPlatform/HealthClinicDataverseSecurity/#security-requirements-and-implementation","text":"1. Licensing - All staff members are provided with Power Apps licenses, determining their base level of access to the Power Platform ecosystem. 2. Environment and Business Unit Setup - The clinic establishes a single Dataverse environment as the primary container for its applications and data. - Within this environment, departments such as Pediatrics and Orthopedics are set up as distinct Business Units , reflecting the clinic's hierarchical and matrix organizational structure. 3. Hierarchical and Matrix Data Access Structure - Senior medical staff are granted broad access across business units due to their role, allowing them to oversee patient care across departments. - Specialized medical staff , such as those with cross-departmental expertise, are given matrix access to multiple business units, enabling them to access patient records in both Pediatrics and Orthopedics. 4. Security Roles, Teams, and Ownership - Custom security roles are created and assigned to staff based on their department and job function. These roles define access at the business unit level, table/record level, and even field level for sensitive information. - Teams are formed to group staff by their function and department. These teams facilitate efficient permission management and reflect the clinic's organizational structure within Dataverse. - Medical Staff Teams : Include doctors and nurses, with access permissions to patient records based on their specific roles and the hierarchical structure. - Receptionist Team : Has access to the Appointment Scheduling app but limited access to patient records, ensuring they can manage appointments without viewing sensitive health information. - Group Teams (Microsoft 365 groups) are used for internal communications, integrating seamlessly with Microsoft Teams for efficient collaboration without compromising patient data security. 5. Table/Record Ownership and Access Controls - Patient records are owned by the clinic but are accessible only to authorized medical personnel based on their security roles and team memberships. - Appointment data is managed by receptionists, who are the primary owners of this data, ensuring that only they can make changes to the schedule.","title":"Security Requirements and Implementation"},{"location":"PowerPlatform/HealthClinicDataverseSecurity/#advanced-security-measures","text":"Row-Level Security is implemented to ensure that only medical staff involved in a patient's care can access their records. Field-Level Security restricts access to highly sensitive patient information, such as medical history, to authorized senior medical staff only. Custom connectors to external services (e.g., SMS notifications for appointments) are secured and managed based on the credentials of the service, ensuring data is shared securely and in compliance with health data protection regulations.","title":"Advanced Security Measures"},{"location":"PowerPlatform/HealthClinicDataverseSecurity/#conclusion","text":"Hope this gave you good overview of how Dataverse security concepts are implemented.","title":"Conclusion"},{"location":"PowerPlatform/HelloDataverse/","text":"What is Dataverse? What are Dataverse verions? Dataverse Storage - behind the scenes Dataverse vs. MSSQL Tables: A Quick Comparison Long story short How to choose? What is Dataverse? Dataverse is Power Platform's Database - for simplicty. What are Dataverse verions? Microsoft Dataverse: Premium edition. Requires PowerApps subscription. Dataverse for Teams: Free edition. Dataverse Storage - behind the scenes Dataverse vs. MSSQL Tables: A Quick Comparison Hello readers! Hope you had a wonderful day. Today, let's explore one of the important aspects of Power Platform: Dataverse. How does it differ from other options like MSSQL? What's its importance? Let's find out! Long story short Dataverse and MSSQL, while both store data in tables within the Microsoft ecosystem, there are many differences between them. Here is a quick breakdown to help you understand the key differences: Feature Dataverse MSSQL Schema Flexible Fixed Data Types Various ( complex included) Similar ( granular control ) Relationships Built-in Foreign Keys Data Manipulation User-friendly interface ( low-code ) T-SQL (code), Programming languages Security RBAC ( roles ), Auditing Manual setup, Auditing (configuration needed) Table Types Standard, Virtual, Custom, Elastic ( limited ) Base, Temporary, Views, Table-Valued Functions Views Limited ( virtual tables ) Traditional views ( complex ) Business Logic (Table Level) Power Automate workflows Stored Procedures, Triggers Business Logic (Column Level) Validation Rules, Workflows Constraints, Triggers Ideal for User-friendly app development, Built-in security, Power Platform Integration Complex data management, Granular control, Complex data models Table Types: Also, here are some differences between the types of tables allowed in both. Feature Dataverse MSSQL Standard Tables \u2713 \u2713 Virtual Tables (simplified data views) \u2713 Views (complex data views) Custom Tables (more control) \u2713 Elastic Tables (large datasets) \u2713 (limited) How to choose? For Power Platform users, Dataverse stands out as the go-to choice. User-friendly interface and low-code approach make it ideal for simple data models. Built-in security and lots of OOTB tables for various applications will give you a head start. These pre-built tables can be used immediately and further customized for specific needs. Alternatively, choose MSSQL if you have a complex data model, want to build things from scratch and want to store a huge volume of data.","title":"Hello Dataverse"},{"location":"PowerPlatform/HelloDataverse/#what-is-dataverse","text":"Dataverse is Power Platform's Database - for simplicty.","title":"What is Dataverse?"},{"location":"PowerPlatform/HelloDataverse/#what-are-dataverse-verions","text":"Microsoft Dataverse: Premium edition. Requires PowerApps subscription. Dataverse for Teams: Free edition.","title":"What are Dataverse verions?"},{"location":"PowerPlatform/HelloDataverse/#dataverse-storage-behind-the-scenes","text":"","title":"Dataverse Storage - behind the scenes"},{"location":"PowerPlatform/HelloDataverse/#dataverse-vs-mssql-tables-a-quick-comparison","text":"Hello readers! Hope you had a wonderful day. Today, let's explore one of the important aspects of Power Platform: Dataverse. How does it differ from other options like MSSQL? What's its importance? Let's find out!","title":"Dataverse vs. MSSQL Tables: A Quick Comparison"},{"location":"PowerPlatform/HelloDataverse/#long-story-short","text":"Dataverse and MSSQL, while both store data in tables within the Microsoft ecosystem, there are many differences between them. Here is a quick breakdown to help you understand the key differences: Feature Dataverse MSSQL Schema Flexible Fixed Data Types Various ( complex included) Similar ( granular control ) Relationships Built-in Foreign Keys Data Manipulation User-friendly interface ( low-code ) T-SQL (code), Programming languages Security RBAC ( roles ), Auditing Manual setup, Auditing (configuration needed) Table Types Standard, Virtual, Custom, Elastic ( limited ) Base, Temporary, Views, Table-Valued Functions Views Limited ( virtual tables ) Traditional views ( complex ) Business Logic (Table Level) Power Automate workflows Stored Procedures, Triggers Business Logic (Column Level) Validation Rules, Workflows Constraints, Triggers Ideal for User-friendly app development, Built-in security, Power Platform Integration Complex data management, Granular control, Complex data models Table Types: Also, here are some differences between the types of tables allowed in both. Feature Dataverse MSSQL Standard Tables \u2713 \u2713 Virtual Tables (simplified data views) \u2713 Views (complex data views) Custom Tables (more control) \u2713 Elastic Tables (large datasets) \u2713 (limited)","title":"Long story short"},{"location":"PowerPlatform/HelloDataverse/#how-to-choose","text":"For Power Platform users, Dataverse stands out as the go-to choice. User-friendly interface and low-code approach make it ideal for simple data models. Built-in security and lots of OOTB tables for various applications will give you a head start. These pre-built tables can be used immediately and further customized for specific needs. Alternatively, choose MSSQL if you have a complex data model, want to build things from scratch and want to store a huge volume of data.","title":"How to choose?"},{"location":"PowerPlatform/HelloDynamics365/","text":"List of Dynamics 365 products Here\u2019s a table showcasing various products within the Dynamics 365 ecosystem. Keep in mind that new products may be added or existing ones may change names over time, so this table might not always reflect the latest updates: Product Description Link Dynamics 365 Sales Focuses on sales processes, lead management, and pipeline tracking. Enhance revenue generation. Sales Dynamics 365 Marketing Personalize customer journeys, create targeted campaigns, and analyze marketing performance. Marketing Dynamics 365 Finance Manage financial operations, track expenses, and optimize financial processes. Finance Dynamics 365 Supply Chain Mgmt Improve supply chain visibility, efficiency, and collaboration across the supply network. Supply Chain Dynamics 365 Field Service Streamline field service operations, manage work orders, and empower field technicians. Field Service Dynamics 365 Commerce Manage e-commerce, point-of-sale, and store operations in the retail industry. Commerce Dynamics 365 Human Resources Focuses on workforce management, employee engagement, and talent acquisition. Human Resources Dynamics 365 Business Central Integrated solution for small and medium-sized businesses, covering finance, sales, service, and ops. Business Central Dynamics 365 Customer Insights Unify customer data, gain actionable insights, and understand customer behavior and preferences. Customer Insights Dynamics 365 Project Operations Optimize project management, resource allocation, and project financials for project-based organizations. Project Operations Dynamics 365 Customer Service Enhance agent productivity with a browser-like, tabbed experience. Agents can work on multiple cases and conversations simultaneously. It uses AI tools like Smart Assist to identify similar cases and relevant articles, streamlining customer interactions. Customer Service","title":"Dynamics 365 Ecosystem"},{"location":"PowerPlatform/HelloDynamics365/#list-of-dynamics-365-products","text":"Here\u2019s a table showcasing various products within the Dynamics 365 ecosystem. Keep in mind that new products may be added or existing ones may change names over time, so this table might not always reflect the latest updates: Product Description Link Dynamics 365 Sales Focuses on sales processes, lead management, and pipeline tracking. Enhance revenue generation. Sales Dynamics 365 Marketing Personalize customer journeys, create targeted campaigns, and analyze marketing performance. Marketing Dynamics 365 Finance Manage financial operations, track expenses, and optimize financial processes. Finance Dynamics 365 Supply Chain Mgmt Improve supply chain visibility, efficiency, and collaboration across the supply network. Supply Chain Dynamics 365 Field Service Streamline field service operations, manage work orders, and empower field technicians. Field Service Dynamics 365 Commerce Manage e-commerce, point-of-sale, and store operations in the retail industry. Commerce Dynamics 365 Human Resources Focuses on workforce management, employee engagement, and talent acquisition. Human Resources Dynamics 365 Business Central Integrated solution for small and medium-sized businesses, covering finance, sales, service, and ops. Business Central Dynamics 365 Customer Insights Unify customer data, gain actionable insights, and understand customer behavior and preferences. Customer Insights Dynamics 365 Project Operations Optimize project management, resource allocation, and project financials for project-based organizations. Project Operations Dynamics 365 Customer Service Enhance agent productivity with a browser-like, tabbed experience. Agents can work on multiple cases and conversations simultaneously. It uses AI tools like Smart Assist to identify similar cases and relevant articles, streamlining customer interactions. Customer Service","title":"List of Dynamics 365 products"},{"location":"PowerPlatform/HelloPowerPlatform/","text":"Microsoft Power Platform Key Products Power Apps - For web and mobile apps. PowerApps is a low-code/no-code platform to create apps quickly. Its data can live in SharePoint, Dynamics, M365, or anywhere. Its language is PowerFX (like Excel formulas). Power apps are canvas apps (any data) and model-driven apps (Dataverse only). Power Automate - Workflows to automate work tasks. Power BI - Data dashboards and reports. Power Pages - For websites. Supporting Tools Copilot Studio [Power Virtual Agents] - Tool to automate the automation. Connectors - Connect to Dropbox, Twitter, etc. Approx 900. AI Builder - Add AI functionality to Power Automate and Power Apps. Dataverse - Backend data for Power Platform. Power FX - Programming language for Power Platform. A typical power platform project Receive invoice emails from vendors (Office 365 Outlook) -> Store attachments in SharePoint (Microsoft SharePoint) -> Send for approval in Teams (Microsoft Teams) -> Enter approved invoices in ERP (Oracle) -> Send confirmation email to vendors (Office 365 Outlook) Dataverse Dataverse ~ Power Platform DB. Tables: Standard (OOTB), managed (part of a solution, read-only), custom. 4TB storage limit. You can apply business logic to tables! E.g., If country is US, then postal code mandatory. Copilot Copilot was earlier called Power Virtual Agents. This is like GPT-4/Bing. Here, you just say in plain English what you want. CoPilot will create the power automate/app for you! Power FX This is the language of Power Platform. It's like Excel formulas. It's used in PowerApps, Dataverse, and Copilot Studio. Power Platform and Microsoft Teams All Power Platform components can be used from within MS Teams. Power Platform & Dynamics 365 Power Apps: All Dynamics 365 customer engagement apps are model-driven apps (built in PowerApps). The data for such apps is in Dataverse. Power BI: Can create reports from Dynamics 365. Power Automate: Business process flows are created using Power Automate. Copilot Studio: Call/SMS/Facebook Msgs -> Dynamics 365 Customer Service App -> Copilot -> Live agent. Power Pages: Self-support websites. Power Platform and Azure Power Platform and Azure offer countless ways to create end-to-end solutions. For instance, an airline project where: Azure API Management hosts a custom API for airline system communication. A coordinator handles notifications, assigns flights to Teams channels, and sends them to Power Apps. Azure Functions process Graph API calls from a storage queue, sending notifications to Teams and streaming events to Azure Event Hubs. Azure Bot Service powers a custom bot messaging service for flight updates in Teams. Azure Data Lake stores event data from Event Hubs for long-term retention and analytics with Power BI. Dataverse OOTB features Here is a picture showing OOTB features of Dataverse. Dataflows ETL tool for Power Platform Ecosystem. Uses very little code. Uses Power Query (like Excel). No infra required, fully on cloud. No separate license, use Power BI / Power Apps license. Common Data Model Common Data Model is a ready-made collection of tables. For healthcare, you have a patient table and an admission table. With this, you don't have to design tables and choose columns. Microsoft has partnered with industries like healthcare, automobile, and banking to create CDM. Long story short: Use Common Data Model. Save yourself from complex data modeling tasks. Data Sources | Triggers & Actions | Connector Types Data Sources: - Tabular: SharePoint, SQL Server, Dataverse - Function Based: 365 Users, Azure Blobs Connectors: - Simply put, they link data sources to the Power Platform. - Types: standard, premium, custom Triggers and Actions: - Triggers: Start the flow in Power Automate, like a spark! - Actions: Perform tasks or operations. Dataverse Table Creation AKA Dataverse Model \"Does 'Build a Dataverse model' sound intimidating? Don't worry, it's simply about creating tables in the Dataverse database.\" Dataverse vs. MSSQL Feature Dataverse MSSQL Focus Business Applications Data Management Schema Flexible Fixed Data Types Limited Complete Relationships Built-in Foreign Keys Data Manipulation User-friendly interface, low-code T-SQL, Programming Languages Security Role-Based Access Control Manual Setup Table Types Standard, Custom, Virtual, Elastic Base, Temporary, Views, Table-Valued Functions Views Limited (Virtual Tables) Traditional Views Business Logic (Table Level) Power Automate Workflows Stored Procedures, Triggers Business Logic (Column Level) Validation Rules, Workflows Constraints, Triggers","title":"PowerPlatform Ecosystem"},{"location":"PowerPlatform/HelloPowerPlatform/#microsoft-power-platform","text":"","title":"Microsoft Power Platform"},{"location":"PowerPlatform/HelloPowerPlatform/#key-products","text":"Power Apps - For web and mobile apps. PowerApps is a low-code/no-code platform to create apps quickly. Its data can live in SharePoint, Dynamics, M365, or anywhere. Its language is PowerFX (like Excel formulas). Power apps are canvas apps (any data) and model-driven apps (Dataverse only). Power Automate - Workflows to automate work tasks. Power BI - Data dashboards and reports. Power Pages - For websites.","title":"Key Products"},{"location":"PowerPlatform/HelloPowerPlatform/#supporting-tools","text":"Copilot Studio [Power Virtual Agents] - Tool to automate the automation. Connectors - Connect to Dropbox, Twitter, etc. Approx 900. AI Builder - Add AI functionality to Power Automate and Power Apps. Dataverse - Backend data for Power Platform. Power FX - Programming language for Power Platform.","title":"Supporting Tools"},{"location":"PowerPlatform/HelloPowerPlatform/#a-typical-power-platform-project","text":"Receive invoice emails from vendors (Office 365 Outlook) -> Store attachments in SharePoint (Microsoft SharePoint) -> Send for approval in Teams (Microsoft Teams) -> Enter approved invoices in ERP (Oracle) -> Send confirmation email to vendors (Office 365 Outlook)","title":"A typical power platform project"},{"location":"PowerPlatform/HelloPowerPlatform/#dataverse","text":"Dataverse ~ Power Platform DB. Tables: Standard (OOTB), managed (part of a solution, read-only), custom. 4TB storage limit. You can apply business logic to tables! E.g., If country is US, then postal code mandatory.","title":"Dataverse"},{"location":"PowerPlatform/HelloPowerPlatform/#copilot","text":"Copilot was earlier called Power Virtual Agents. This is like GPT-4/Bing. Here, you just say in plain English what you want. CoPilot will create the power automate/app for you!","title":"Copilot"},{"location":"PowerPlatform/HelloPowerPlatform/#power-fx","text":"This is the language of Power Platform. It's like Excel formulas. It's used in PowerApps, Dataverse, and Copilot Studio.","title":"Power FX"},{"location":"PowerPlatform/HelloPowerPlatform/#power-platform-and-microsoft-teams","text":"All Power Platform components can be used from within MS Teams.","title":"Power Platform and Microsoft Teams"},{"location":"PowerPlatform/HelloPowerPlatform/#power-platform-dynamics-365","text":"Power Apps: All Dynamics 365 customer engagement apps are model-driven apps (built in PowerApps). The data for such apps is in Dataverse. Power BI: Can create reports from Dynamics 365. Power Automate: Business process flows are created using Power Automate. Copilot Studio: Call/SMS/Facebook Msgs -> Dynamics 365 Customer Service App -> Copilot -> Live agent. Power Pages: Self-support websites.","title":"Power Platform &amp; Dynamics 365"},{"location":"PowerPlatform/HelloPowerPlatform/#power-platform-and-azure","text":"Power Platform and Azure offer countless ways to create end-to-end solutions. For instance, an airline project where: Azure API Management hosts a custom API for airline system communication. A coordinator handles notifications, assigns flights to Teams channels, and sends them to Power Apps. Azure Functions process Graph API calls from a storage queue, sending notifications to Teams and streaming events to Azure Event Hubs. Azure Bot Service powers a custom bot messaging service for flight updates in Teams. Azure Data Lake stores event data from Event Hubs for long-term retention and analytics with Power BI.","title":"Power Platform and Azure"},{"location":"PowerPlatform/HelloPowerPlatform/#dataverse-ootb-features","text":"Here is a picture showing OOTB features of Dataverse.","title":"Dataverse OOTB features"},{"location":"PowerPlatform/HelloPowerPlatform/#dataflows","text":"ETL tool for Power Platform Ecosystem. Uses very little code. Uses Power Query (like Excel). No infra required, fully on cloud. No separate license, use Power BI / Power Apps license.","title":"Dataflows"},{"location":"PowerPlatform/HelloPowerPlatform/#common-data-model","text":"Common Data Model is a ready-made collection of tables. For healthcare, you have a patient table and an admission table. With this, you don't have to design tables and choose columns. Microsoft has partnered with industries like healthcare, automobile, and banking to create CDM. Long story short: Use Common Data Model. Save yourself from complex data modeling tasks.","title":"Common Data Model"},{"location":"PowerPlatform/HelloPowerPlatform/#data-sources-triggers-actions-connector-types","text":"Data Sources: - Tabular: SharePoint, SQL Server, Dataverse - Function Based: 365 Users, Azure Blobs Connectors: - Simply put, they link data sources to the Power Platform. - Types: standard, premium, custom Triggers and Actions: - Triggers: Start the flow in Power Automate, like a spark! - Actions: Perform tasks or operations.","title":"Data Sources | Triggers &amp; Actions | Connector Types"},{"location":"PowerPlatform/HelloPowerPlatform/#dataverse-table-creation-aka-dataverse-model","text":"\"Does 'Build a Dataverse model' sound intimidating? Don't worry, it's simply about creating tables in the Dataverse database.\"","title":"Dataverse Table Creation AKA Dataverse Model"},{"location":"PowerPlatform/HelloPowerPlatform/#dataverse-vs-mssql","text":"Feature Dataverse MSSQL Focus Business Applications Data Management Schema Flexible Fixed Data Types Limited Complete Relationships Built-in Foreign Keys Data Manipulation User-friendly interface, low-code T-SQL, Programming Languages Security Role-Based Access Control Manual Setup Table Types Standard, Custom, Virtual, Elastic Base, Temporary, Views, Table-Valued Functions Views Limited (Virtual Tables) Traditional Views Business Logic (Table Level) Power Automate Workflows Stored Procedures, Triggers Business Logic (Column Level) Validation Rules, Workflows Constraints, Triggers","title":"Dataverse vs. MSSQL"},{"location":"PowerPlatform/ModelDrivenApps/","text":"What are model-driven apps? What are model-driven apps? In canvas apps you have complete control over everything. In model-driven apps the layout done already on the components you choose. Canvas apps are built screen-by-screen. Model-driven apps are created with few simple steps. Remember: Canvas apps can have many data sources. Model-driven apps must be on dataverse tables. Model-driven apps start with a page: Different types of pages are Dataverse table, Dashboard, URL, Web resource, and Custom Remember: Two pages for each Dataverse table","title":"Model-Driven Apps"},{"location":"PowerPlatform/ModelDrivenApps/#what-are-model-driven-apps","text":"In canvas apps you have complete control over everything. In model-driven apps the layout done already on the components you choose. Canvas apps are built screen-by-screen. Model-driven apps are created with few simple steps. Remember: Canvas apps can have many data sources. Model-driven apps must be on dataverse tables. Model-driven apps start with a page: Different types of pages are Dataverse table, Dashboard, URL, Web resource, and Custom Remember: Two pages for each Dataverse table","title":"What are model-driven apps?"},{"location":"PowerPlatform/OnPremiseGateway/","text":"On-Premise to Cloud ETL Using DataFlows & On-Premise Gateway Background Case Study: We have a large number of xml files in a local folder. We want to move it to Dataverse. In this article I will show you how you can do it using on-premise gateway and Dataflow in Power platform. This ETL is particularly useful in bank settings where xml files from many sources like right-fax servers are recieved and they have to be moved to backend database.Even though its not a full-solution as rightfax servers also send documents along with control files. But, the document part can be handled seprately handled. What is an On-Premise Gateway? An on-premise gateway is software installed on a local system that enables access to local files and databases from Power BI, Power Apps, Power Automate, Azure Analysis Services, and Azure Logic Apps. There are two modes available: Personal, which allows use only with Power BI, and Standard, which supports all mentioned applications. Installation requirements include Windows 10 (64-bit) or Windows Server 2019 and .NET Framework 4.8. Note that these requirements may change in the future. Installing the Gateway Download the standard gateway. In the gateway installer, maintain the default installation path, accept the terms of use, and then select \"Install.\" Enter the email address associated with your Office 365 organizational account and select \"Sign in.\" Choose \"Register a new gateway on this computer\" and click \"Next.\" Enter a unique name for the gateway and a recovery key. This key is crucial for recovering or relocating your gateway in the future. Click \"Configure.\" Review the information in the final window. Since the same account is used for Power BI, Power Apps, and Power Automate, the gateway will be accessible for all three services. Select \"Close.\" Use Gateway - Import files from local folder Open PowerApps -> Dataflows -> New Dataflow -> Start from Blank -> Provide a name -> Click \"Create.\" On the \"Get Data\" page, click Folder . Provide credentials and choose the source folder on the Connect to Data Source page. Ensure the account has read/write privileges by checking the folder's properties under the security tab. If all settings are correct, files will be loaded and displayed on the preview page. Click Combine or Transform Data . In this example, I used Combine . One common issue during XML conversion is that DataFlow assigns its own locale, which is often overlooked during testing but causes the flow to fail during actual execution. To resolve this, go to \"Options,\" select \"Regional Settings,\" choose the appropriate locale (e.g., English (United States)), and click OK Then click \"Next.\" This will open the mapping page where you can load data into an existing table or create a new table with automatic mapping. Once you are finished, click \"Next.\" Now, decide how to run the flow. You can run it ad hoc manually or at a predetermined interval. Once selected, click \"Publish\" to publish the flow. Once published successfully, you can view your dataflow in the \"My DataFlow\" tab. Since the XML data was exported into Dataverse, you can view the table and its data from the \"Tables\" page. Voila! Your data is now in Dataverse and can be accessed using a wide range of M365 apps. Using the Gateway in Microsoft Fabric You can reuse the gateway! For example, if you want to collaborate: Troubleshooting If the connection fails, ensure that your user has the appropriate permissions for the folder:","title":"Onpremise Gateway"},{"location":"PowerPlatform/OnPremiseGateway/#on-premise-to-cloud-etl-using-dataflows-on-premise-gateway","text":"","title":"On-Premise to Cloud ETL Using DataFlows &amp; On-Premise Gateway"},{"location":"PowerPlatform/OnPremiseGateway/#background","text":"Case Study: We have a large number of xml files in a local folder. We want to move it to Dataverse. In this article I will show you how you can do it using on-premise gateway and Dataflow in Power platform. This ETL is particularly useful in bank settings where xml files from many sources like right-fax servers are recieved and they have to be moved to backend database.Even though its not a full-solution as rightfax servers also send documents along with control files. But, the document part can be handled seprately handled.","title":"Background"},{"location":"PowerPlatform/OnPremiseGateway/#what-is-an-on-premise-gateway","text":"An on-premise gateway is software installed on a local system that enables access to local files and databases from Power BI, Power Apps, Power Automate, Azure Analysis Services, and Azure Logic Apps. There are two modes available: Personal, which allows use only with Power BI, and Standard, which supports all mentioned applications. Installation requirements include Windows 10 (64-bit) or Windows Server 2019 and .NET Framework 4.8. Note that these requirements may change in the future.","title":"What is an On-Premise Gateway?"},{"location":"PowerPlatform/OnPremiseGateway/#installing-the-gateway","text":"Download the standard gateway. In the gateway installer, maintain the default installation path, accept the terms of use, and then select \"Install.\" Enter the email address associated with your Office 365 organizational account and select \"Sign in.\" Choose \"Register a new gateway on this computer\" and click \"Next.\" Enter a unique name for the gateway and a recovery key. This key is crucial for recovering or relocating your gateway in the future. Click \"Configure.\" Review the information in the final window. Since the same account is used for Power BI, Power Apps, and Power Automate, the gateway will be accessible for all three services. Select \"Close.\"","title":"Installing the Gateway"},{"location":"PowerPlatform/OnPremiseGateway/#use-gateway-import-files-from-local-folder","text":"Open PowerApps -> Dataflows -> New Dataflow -> Start from Blank -> Provide a name -> Click \"Create.\" On the \"Get Data\" page, click Folder . Provide credentials and choose the source folder on the Connect to Data Source page. Ensure the account has read/write privileges by checking the folder's properties under the security tab. If all settings are correct, files will be loaded and displayed on the preview page. Click Combine or Transform Data . In this example, I used Combine . One common issue during XML conversion is that DataFlow assigns its own locale, which is often overlooked during testing but causes the flow to fail during actual execution. To resolve this, go to \"Options,\" select \"Regional Settings,\" choose the appropriate locale (e.g., English (United States)), and click OK Then click \"Next.\" This will open the mapping page where you can load data into an existing table or create a new table with automatic mapping. Once you are finished, click \"Next.\" Now, decide how to run the flow. You can run it ad hoc manually or at a predetermined interval. Once selected, click \"Publish\" to publish the flow. Once published successfully, you can view your dataflow in the \"My DataFlow\" tab. Since the XML data was exported into Dataverse, you can view the table and its data from the \"Tables\" page. Voila! Your data is now in Dataverse and can be accessed using a wide range of M365 apps.","title":"Use Gateway - Import files from local folder"},{"location":"PowerPlatform/OnPremiseGateway/#using-the-gateway-in-microsoft-fabric","text":"You can reuse the gateway! For example, if you want to collaborate:","title":"Using the Gateway in Microsoft Fabric"},{"location":"PowerPlatform/OnPremiseGateway/#troubleshooting","text":"If the connection fails, ensure that your user has the appropriate permissions for the folder:","title":"Troubleshooting"},{"location":"PowerPlatform/PowerAutomateIsWorkflowTeams/","text":"Workflow = Power Automate(Teams) = make.powerautomate.com Key Features and Benefits Getting Started Conclusion Workflow = Power Automate(Teams) = make.powerautomate.com The Workflow app within Microsoft Teams seamlessly integrates Power Automate directly into your Teams environment. It is the same old Power Automate app for Teams, but renamed as Workflow . Key Features and Benefits Previously known as \"Power Automate,\" the app was rebranded as \"Workflow\" around November 2023. The Workflow app allows you to create Power Automate workflows from scratch or utilize pre-built templates. Access these templates directly within Teams, making it convenient to get started with automation. All workflows created within the Workflow app are powered by Power Automate. The app is completely identical to make.powerautomate.com . Getting Started Install the Workflow App : Head to the Teams store and search for \"Workflow.\" Install the app to get started. Once installed, you'll find it in your Teams app bar. Create Your First Workflow : Click on the Workflow app icon in Teams. Choose to create a new workflow from scratch or explore the available templates. Customize your workflow by adding actions, conditions, and triggers. Conclusion Workflow = Power Automate(Teams) = make.powerautomate.com","title":"Workflow is Power Automate"},{"location":"PowerPlatform/PowerAutomateIsWorkflowTeams/#workflow-power-automateteams-makepowerautomatecom","text":"The Workflow app within Microsoft Teams seamlessly integrates Power Automate directly into your Teams environment. It is the same old Power Automate app for Teams, but renamed as Workflow .","title":"Workflow = Power Automate(Teams) = make.powerautomate.com"},{"location":"PowerPlatform/PowerAutomateIsWorkflowTeams/#key-features-and-benefits","text":"Previously known as \"Power Automate,\" the app was rebranded as \"Workflow\" around November 2023. The Workflow app allows you to create Power Automate workflows from scratch or utilize pre-built templates. Access these templates directly within Teams, making it convenient to get started with automation. All workflows created within the Workflow app are powered by Power Automate. The app is completely identical to make.powerautomate.com .","title":"Key Features and Benefits"},{"location":"PowerPlatform/PowerAutomateIsWorkflowTeams/#getting-started","text":"Install the Workflow App : Head to the Teams store and search for \"Workflow.\" Install the app to get started. Once installed, you'll find it in your Teams app bar. Create Your First Workflow : Click on the Workflow app icon in Teams. Choose to create a new workflow from scratch or explore the available templates. Customize your workflow by adding actions, conditions, and triggers.","title":"Getting Started"},{"location":"PowerPlatform/PowerAutomateIsWorkflowTeams/#conclusion","text":"Workflow = Power Automate(Teams) = make.powerautomate.com","title":"Conclusion"},{"location":"PowerPlatform/PowerPlatformAdminCentral/","text":"TBD Power Platform Admin Central(PPAC) - Managing Users, Roles and Teams. PPAC provides a centralized admin experience to manage users/security roles and Teams. Background In this article I will show you what are application users. Whey they are used. How we can create them. I will use a simple case-study so that you can related to it easily. Application users - case study Say you have a power automate workflow(leave application) which adds new records in dataverse when a form is submitted by users. Now we have so many employees, we can't give all their ids access to the backend database. And its not feasible as well. They just want to submit the form. What is the soluton: Create an application user for the leave application workflow. Give the application user read/write access to the tables. Add an Application user Application users system accounts for background tasks. You shouldn't update them or change their security roles. They don't consume any license. The application users are given access to environment's data on behalf of the user who's using the application. Go to PPAC https://admin.powerplatform.microsoft.com/ -> Environments -> Select the environment Click Settings Click Users + permissions -> Application users . Click + New app user . Certainly! Let's delve into managing application users in the Power Platform admin center . This process is crucial for ensuring smooth access and security within your environment. Below, I'll outline the steps involved: Manage Application Users in the Power Platform Admin Center 1. View Application Users in an Environment Sign in to the Power Platform admin center as a System Administrator . Select Environments , and then choose an environment from the list. Navigate to Settings and select Users + permissions . Click on Application users to view and manage application users. 2. Create an Application User You can create an unlicensed application user in your environment. This user will have access to your environment's data on behalf of the application. In an environment, you can only have one application user for each Microsoft Entra\u2013registered application . Here's how: Sign in to the Power Platform admin center as a System Administrator . Select Environments , choose an environment, and go to Settings . Click on Users + permissions , then select Application users . Click + New app user to open the Create a new app user page. Choose the registered Microsoft Entra application created for the user. Assign a business unit and select security roles for the new application user. Save your changes and create the user. 3. View or Edit Application User Details Sign in to the Power Platform admin center as a System Administrator . Select Environments , choose an environment, and go to Settings . Click on Users + permissions , then select Application users . You can view or edit the details of an application user here. Remember that deleting an application user is currently not supported. For more detailed information, you can refer to the official Microsoft Learn article .","title":"PowerPlatformAdminCentral"},{"location":"PowerPlatform/PowerPlatformAdminCentral/#background","text":"In this article I will show you what are application users. Whey they are used. How we can create them. I will use a simple case-study so that you can related to it easily.","title":"Background"},{"location":"PowerPlatform/PowerPlatformAdminCentral/#application-users-case-study","text":"Say you have a power automate workflow(leave application) which adds new records in dataverse when a form is submitted by users. Now we have so many employees, we can't give all their ids access to the backend database. And its not feasible as well. They just want to submit the form. What is the soluton: Create an application user for the leave application workflow. Give the application user read/write access to the tables.","title":"Application users - case study"},{"location":"PowerPlatform/PowerPlatformAdminCentral/#add-an-application-user","text":"Application users system accounts for background tasks. You shouldn't update them or change their security roles. They don't consume any license. The application users are given access to environment's data on behalf of the user who's using the application. Go to PPAC https://admin.powerplatform.microsoft.com/ -> Environments -> Select the environment Click Settings Click Users + permissions -> Application users . Click + New app user . Certainly! Let's delve into managing application users in the Power Platform admin center . This process is crucial for ensuring smooth access and security within your environment. Below, I'll outline the steps involved:","title":"Add an Application user"},{"location":"PowerPlatform/PowerPlatformAdminCentral/#manage-application-users-in-the-power-platform-admin-center","text":"","title":"Manage Application Users in the Power Platform Admin Center"},{"location":"PowerPlatform/PowerPlatformAdminCentral/#1-view-application-users-in-an-environment","text":"Sign in to the Power Platform admin center as a System Administrator . Select Environments , and then choose an environment from the list. Navigate to Settings and select Users + permissions . Click on Application users to view and manage application users.","title":"1. View Application Users in an Environment"},{"location":"PowerPlatform/PowerPlatformAdminCentral/#2-create-an-application-user","text":"You can create an unlicensed application user in your environment. This user will have access to your environment's data on behalf of the application. In an environment, you can only have one application user for each Microsoft Entra\u2013registered application . Here's how: Sign in to the Power Platform admin center as a System Administrator . Select Environments , choose an environment, and go to Settings . Click on Users + permissions , then select Application users . Click + New app user to open the Create a new app user page. Choose the registered Microsoft Entra application created for the user. Assign a business unit and select security roles for the new application user. Save your changes and create the user.","title":"2. Create an Application User"},{"location":"PowerPlatform/PowerPlatformAdminCentral/#3-view-or-edit-application-user-details","text":"Sign in to the Power Platform admin center as a System Administrator . Select Environments , choose an environment, and go to Settings . Click on Users + permissions , then select Application users . You can view or edit the details of an application user here. Remember that deleting an application user is currently not supported. For more detailed information, you can refer to the official Microsoft Learn article .","title":"3. View or Edit Application User Details"},{"location":"PowerPlatform/PowerPlatformQ%26A/","text":"Note: Highlight the answers to reveal them. 1. What are the benefits of using Common Data Model? - It eliminates the need for data modeling. - It enables interoperability between different systems and applications. - It provides a standard user interface for data management. Answer: Common Data Model enables interoperability between different systems and applications. 2. Someone adds an item in SharePoint, which prompts a flow to run in Power Automate. What type of operation was used to start your flow? - Trigger. - Action. - Function-based. Answer: A trigger is an operation that tells a workflow to begin or prompts some type of action. 3. A client likes the idea of implementing a Microsoft Power Platform solution but is concerned about the ability to interact with a custom API. How should you respond? - Microsoft Power Platform has over 900 connectors to use in these situations. - Microsoft Power Platform offers the ability to create custom connectors, which allow you to connect to Power Apps and Power Automate. - Microsoft Power Platform uses connectors that hold a series of functions available for developers. Answer: You can build out a custom connector to bridge your app or workflow to the API. 4. Someone asks you to describe a connector. How would you respond? - Connectors hold a series of functions available for developers. - Connectors connect your data source to your app, workflow, or dashboard. - Connectors are a cloud-based service that makes it practical and simple for line-of-business users to build workflows that automate time-consuming business tasks and processes across applications and services. Answer: Connectors allow functions and information to pass from your data source to your app or workflow. 4. In Microsoft Dataverse, where is your data being stored? Tables Forms SharePoint Access Answer: In Dataverse, data is stored in tables. 5. What is the benefit of building a model-driven app? Logic-focused design, no code required, and different UIs across multiple devices Complex responsiveness with different UIs across multiple devices Apps can be distributed by using complex coding and different UIs across mobile devices Component-focused design, no code required, complex responsiveness with similar UI across multiple devices, and apps can be distributed as solutions Answer: The benefits of building a model-driven app are component-focused design, no code required, complex responsiveness with similar UI across multiple devices, and apps can be distributed as solutions. 6. What are the three important areas to focus on when creating model-driven apps? Licenses, business data, and composing the app App design pattern, business data, and defining business process Modeling business data, defining business process, and composing the app Model licensing design, defining business process, and composing the app Answer: When creating a model-driven app, it's important to focus on modeling business data to see where data is read from and saved, the business process including who the users are, and composing the app to make it easily accessible.","title":"Pl-900-Q&A"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch/","text":"Integrate AI Search and Azure AI Document Intelligence Overview In this module, you'll learn how to create a AI Search custom skill that calls a model in Azure AI Document Intelligence to improve indexing documents. Learning objectives Describe how a custom skill can enrich content passed through an Azure AI Search pipeline. Build a custom skill that calls an Azure AI Document Intelligence solution to index data from forms. Case Study You have thousands of marriage certificates stored in a data lake and wish to search them using key-value pairs (e.g., \"Girlfriend_Name = 'Rita'\"). You are evaluating two options: Solution 1 - ADLS Files -> Azure Cognitive Search You implement Azure Cognitive Search, which will automatically process the files by opening them, performing OCR, and indexing the extracted data. It offers many built-in features, including sentiment analysis. However, the cornerstone of effective search capabilities is the quality of the OCR output. Since the OCR provided by Azure Cognitive Search is basic, any search functionality relying on this OCR output may yield suboptimal results. Solution 2 - ADLS Files -> Azure AI Document Intelligence -> Azure Cognitive Search By creating a custom extraction model tailored for your marriage certificates, Azure AI Document Intelligence can provide highly accurate OCR results. After processing the certificates with this custom model, you then index the extracted data using Azure Cognitive Search. This approach enables robust search capabilities that significantly surpass the effectiveness of using Azure Cognitive Search's basic OCR alone. How to implement Solution 2 To integrate Azure AI Document Intelligence into the AI Search indexing process: Use AAIDI document model(prebuilt/custom) to extract data from the marriage certificates. Write a web service that can integrate custom skill with Azure AI Document Intelligence resource. In this module, you'll use an Azure Function to host this service. Add a custom web API skill, with the correct configuration to the AI Search skillset. This skill should be configured to send requests to the web service. What is a skill in Azure Cognitive Search Skills are like identifying the language of documents, recognizing logos, or detecting names of places. Skills can pre-built or custom-built. During the indexing process, these skills are applied one after another, to make the ocred data more searchable. They add more Indexing data to make Azure cognitive search better. Types of custom skill Azure Machine Learning (AML) Custom Skills You can use a model from Azure Machine Learning to create a custom skill. Custom Web API Skills These are simply custom skills provided as web api. The connect to services like Azure Document Intellingece etc. Build an Azure AI Document Intelligence custom skill Appendix Stages of Indexing process There are five stages to the indexing process: Document Cracking. In document cracking, the indexer opens the content files and extracts their content. Field Mappings. Fields such as titles, names, dates, and more are extracted from the content. You can use field mappings to control how they're stored in the index. Skillset Execution. In the optional skillset execution stage, custom AI processing is done on the content to enrich the final index. Output field mappings. If you're using a custom skillset, its output is mapped to index fields in this stage. Push to index. The results of the indexing process are stored in the index in Azure AI Search.","title":"Integrate AI Search and Azure AI Document Intelligence"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch/#integrate-ai-search-and-azure-ai-document-intelligence","text":"","title":"Integrate AI Search and Azure AI Document Intelligence"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch/#overview","text":"In this module, you'll learn how to create a AI Search custom skill that calls a model in Azure AI Document Intelligence to improve indexing documents. Learning objectives Describe how a custom skill can enrich content passed through an Azure AI Search pipeline. Build a custom skill that calls an Azure AI Document Intelligence solution to index data from forms.","title":"Overview"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch/#case-study","text":"You have thousands of marriage certificates stored in a data lake and wish to search them using key-value pairs (e.g., \"Girlfriend_Name = 'Rita'\"). You are evaluating two options:","title":"Case Study"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch/#solution-1-adls-files-azure-cognitive-search","text":"You implement Azure Cognitive Search, which will automatically process the files by opening them, performing OCR, and indexing the extracted data. It offers many built-in features, including sentiment analysis. However, the cornerstone of effective search capabilities is the quality of the OCR output. Since the OCR provided by Azure Cognitive Search is basic, any search functionality relying on this OCR output may yield suboptimal results.","title":"Solution 1 - ADLS Files -&gt; Azure Cognitive Search"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch/#solution-2-adls-files-azure-ai-document-intelligence-azure-cognitive-search","text":"By creating a custom extraction model tailored for your marriage certificates, Azure AI Document Intelligence can provide highly accurate OCR results. After processing the certificates with this custom model, you then index the extracted data using Azure Cognitive Search. This approach enables robust search capabilities that significantly surpass the effectiveness of using Azure Cognitive Search's basic OCR alone.","title":"Solution 2 - ADLS Files -&gt; Azure AI Document Intelligence -&gt; Azure Cognitive Search"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch/#how-to-implement-solution-2","text":"To integrate Azure AI Document Intelligence into the AI Search indexing process: Use AAIDI document model(prebuilt/custom) to extract data from the marriage certificates. Write a web service that can integrate custom skill with Azure AI Document Intelligence resource. In this module, you'll use an Azure Function to host this service. Add a custom web API skill, with the correct configuration to the AI Search skillset. This skill should be configured to send requests to the web service.","title":"How to implement Solution 2"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch/#what-is-a-skill-in-azure-cognitive-search","text":"Skills are like identifying the language of documents, recognizing logos, or detecting names of places. Skills can pre-built or custom-built. During the indexing process, these skills are applied one after another, to make the ocred data more searchable. They add more Indexing data to make Azure cognitive search better.","title":"What is a skill in Azure Cognitive Search"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch/#types-of-custom-skill","text":"","title":"Types of custom skill"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch/#azure-machine-learning-aml-custom-skills","text":"You can use a model from Azure Machine Learning to create a custom skill.","title":"Azure Machine Learning (AML) Custom Skills"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch/#custom-web-api-skills","text":"These are simply custom skills provided as web api. The connect to services like Azure Document Intellingece etc.","title":"Custom Web API Skills"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch/#build-an-azure-ai-document-intelligence-custom-skill","text":"","title":"Build an Azure AI Document Intelligence custom skill"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch/#appendix","text":"","title":"Appendix"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch/#stages-of-indexing-process","text":"There are five stages to the indexing process: Document Cracking. In document cracking, the indexer opens the content files and extracts their content. Field Mappings. Fields such as titles, names, dates, and more are extracted from the content. You can use field mappings to control how they're stored in the index. Skillset Execution. In the optional skillset execution stage, custom AI processing is done on the content to enrich the final index. Output field mappings. If you're using a custom skillset, its output is mapped to index fields in this stage. Push to index. The results of the indexing process are stored in the index in Azure AI Search.","title":"Stages of Indexing process"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_Q%26A/","text":"You have a composed model that consists of three custom models. You're writing code that sends forms to the composed model and you need to check which of the custom models was used to analyze each form. Which property should you use from the returned JSON? modelId. status. docType. Ans: docType. Explanation: The docType property includes the model ID of the custom model that was used to analyze the document. You're trying to create a composed model but you're receiving an error. Which of the following should you check? That the custom models were trained with labels. That the custom models all have the same model ID. That the custom models all have the same list of fields. Ans: That the custom models were trained with labels. Explanation: Only custom models that have been trained with labeled example forms can be added to a composed model You have a large set of documents with varying structures that contain customer name and address information. You want to extract entities for each customer. Which prebuilt model should you use? Read model. General document model. ID document model. Answer: General document model. Explanation: The general document model is the only one that supports entity extraction. You are using the prebuilt layout model to analyze a document with many checkboxes. You want to find out whether each box is checked or empty. What object should you use in the returned JSON code? Selection marks. Bounding boxes. Confidence indicators. Answer: Selection marks. Explanation: Selection marks record checkboxes and radio buttons and include whether they're selected or not. You submit a Word document to the Azure AI Document Intelligence general document model for analysis but you receive an error. The file is A4 size, contains 1 MB of data, and is not password-protected. How should you resolve the error? Change from the free tier to the standard tier. Submit the document one page at a time. Convert the document to PDF format. Answer: Convert the document to PDF format. Explanation: Word documents are not supported by Azure AI Document Intelligence but PDF documents are supported. Azure AI Document Intelligence is designed to analyze scanned and photographed paper documents , not documents that are already in a digital format so you should consider using another technology to extract the data in Word documents. A person plans to use an Azure Document Intelligence prebuilt invoice model. To extract document data using the model, what are two calls they need to make to the API? Train Model and Get Model Labels Analyze Invoice and Get Analyze Invoice Result Create Azure Document Intelligence and Get Analyze Invoice Result Answer: Analyze Invoice and Get Analyze Invoice Result Explanation: The Analyze Invoice function starts the form analysis and returns a result ID, which they can pass in a subsequent call to the Get Analyze Invoice Result function to retrieve the results. A person needs to build an application that submits expense claims and extracts the merchant, date, and total from scanned receipts. What's the best way to do this? Use the Read API of the Computer Vision service. Use Azure Document Intelligence's Layout service Use Azure Document Intelligence's prebuilt receipts model Answer: Use Azure Document Intelligence's prebuilt receipts model Explanation: Use the Azure Document Intelligence's prebuilt receipts model. It can intelligently extract the required fields even if the scanned receipts have different names in them. A person is building a custom model with Azure Document Intelligence services. What is required to train a model? Along with the form to analyze, JSON files need to be provided. Training must be done through language specific SDKs. Nothing else is required. Answer: Along with the form to analyze, JSON files need to be provided. Explanation: The labels needed in training are referenced in the ocr.json files, labels.json files, and single fields.json file.","title":"AAIDI Q&A"},{"location":"PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence/","text":"Azure AI Document Intelligence What is Azure AI Document Intelligence? Azure AI Document Intelligence (Form Recognizer) is a machine-learning AI service that extracts text from documents(JPG, PNG, BMP, PDF, or TIFF). It offers REST APIs client library SDKs(Python, C#, Java, and JavaScript) for programmers. Azure Document Intelligence uses Optical Character Recognition (OCR) capabilities and deep learning models to extract text, key-value pairs, selection marks, and tables from documents. The service offers two types of models - Prebuilt models are ready-to-use models for common documents like receipts and credit cards. Custom models are used when a document has unusual and unique format. The service is accessible via SDKs in Python, C#, Java, and JavaScript. Models in Azure AI Document Intelligence Azure AI Document Intelligence (formerly Form Recognizer) offers various models for using this service in your application. Here are the models: Prebuilt Models : These are pre-trained, ready-to-use models which you can use rightaway without any customization: General-Document-Read : Extracts text and key-value pairs from documents. General-Document-Layout : Analyzes the layout and structure of documents. Contract : Extracts details from contracts. Health Insurance Card : Extracts information from health insurance cards. ID Document : Extracts data from identification documents. Invoice : Extracts relevant details from invoices. Receipt : Extracts transaction information from receipts. US Tax Forms (e.g., 1040, 1098, W2) : Extracts data from various US tax forms. Marriage Certificate : Extracts details from marriage certificates. Credit Card : Extracts information from credit cards. Business Card (deprecated) : Previously used for business card extraction. Custom Models : When you have forms with unusual or unique formats, you can create and train your own custom models in Azure AI Document Intelligence. A custom model can provide field extraction for the data that is unique to your form and generate data targeted to your unique business application. Custom Classification Model : Allows you to create custom classifiers. Neural Model : Use custom neural models for inconsistent, semi-structured or unstructured forms. Template Model : Use custom template models when your forms have a consistent visual template. The formatting and layout should be consistent across all completed examples of the form. Composed Model : You can build hundreds of custom models within one Azure AI Document Intelligence resource and combine them into a composed model using Azure AI Document Intelligence Studio's GUI or the StartCreateComposedModelAsync() method in code. Add-On Capabilities : These enhance existing models: Font Property Extraction : Extracts font-related information. Azure AI Document Intelligence Vs Azure AI Vision Azure AI Document Intelligence is based on Azure AI Vision services. For basic text extraction from images, Azure AI Vision OCR is appropriate. For more detailed document analysis, such as identifying key/value pairs, tables, and contextual information, Azure AI Document Intelligence is more suitable. Tools for AAIDI Azure AI Document Intelligence tools offer Document Intelligence Studio(https://formrecognizer.appliedai.azure.com/) or (https://documentintelligence.ai.azure.com/studio) to use it without much coding. Here, you can find a range off pre-built models and option to create custom models. To embed AADI in your applications, coding is required. For instance, users can scan receipts through a mobile app, which uses this service to extract detailed information for integration into a CRM database. AADI provides APIs for each pre-built model. You can use C#/.NET, Java, Python, and JavaScript to access them. Also, you can access the service via its RESTful web service for other languages. Azure Document Intelligence Studio Azure Document Intelligence services are accessible through both SDKs and APIs, but if you prefer a more visual approach, you can use the Azure Document Intelligence Studio. This web application lets you manage your projects more interactively. In the Studio, you can set up various types of projects like document analysis models, which include capabilities for extracting text, tables, and structure from documents and images. You can also work with prebuilt models for specific document types or even train your own custom models to fit your specific needs. It's a flexible tool that caters to a variety of document processing requirements. Use prebuilt document model Create an Azure AI Document Intelligence resource Before you can call the Azure AI Document Intelligence service, you must create a resource to host that service in Azure: In a browser tab, open the Azure portal at https://portal.azure.com, signing in with the Microsoft account associated with your Azure subscription. On the Azure portal home page, navigate to the top search box and type Document Intelligence and then press Enter. On the Document Intelligence page, select Create. On the Create Document Intelligence page, use the following to configure your resource: Subscription: Your Azure subscription. Resource group: Select or create a resource group with a unique name such as DocIntelligenceResources. Region: select a region near you. Name: Enter a globally unique name. Pricing tier: select Free F0 (if you don't have a Free tier available, select Standard S0). Then select Review + create, and Create. Wait while Azure creates the Azure AI Document Intelligence resource. When the deployment is complete, select Go to resource. Keep this page open for the rest of this exercise. Use the Read model Let's start by using the Azure AI Document Intelligence Studio and the Read model to analyze a document with multiple languages. You'll connect Azure AI Document Intelligence Studio to the resource you just created to perform the analysis: Open a new browser tab and go to the Azure AI Document Intelligence Studio at https://documentintelligence.ai.azure.com/studio. Under Document Analysis, select the Read tile. If you are asked to sign into your account, use your Azure credentials. If you are asked which Azure AI Document Intelligence resource to use, select the subscription and resource name you used when you created the Azure AI Document Intelligence resource. In the list of documents on the left, select read-german.png. When the analysis is complete, the text extracted from the image is shown on the right in the Content tab. Review this text and compare it to the text in the original image for accuracy. Select the Result tab. This tab displays the extracted JSON code. Scroll to the bottom of the JSON code in the Result tab. Notice that the read model has detected the language of each span. Most spans are in German (language code de) but the last span is in English (language code en). Use composed model Once you've created a set of custom models, you must assemble them into a composed model. You can do this in a Graphical User Interface (GUI) by using Azure AI Document Intelligence Studio, or by using the StartCreateComposedModelAsync() method in custom code. In the results from the composed model, you can determine which custom model has been used for the analysis by checking the docType field. The maximum number of custom models that can be added to a single composed model is 100. File type requirement Note: AAIDI can't extract data from word documents or excel. It only supports, photographed documents like JPG, PNG, BMP, TIFF and PDF. This is also true for traditional document capture tools like OpenText Intelligent Capture(Captiva InputAccel) whre you can use word document but an intermediate image conversion module converts it to image beore ocring. Here are the technical reuiqrment for files: Format must be JPG, PNG, BMP, PDF (text or scanned), or TIFF. The file size must be less than 500 MB for paid (S0) tier and 4 MB for free (F0) tier. Image dimensions must be between 50 x 50 pixels and 10000 x 10000 pixels. The total size of the training data set must be 500 pages or less.","title":"Azure AI Document Intelligence"},{"location":"PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence/#azure-ai-document-intelligence","text":"","title":"Azure AI Document Intelligence"},{"location":"PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence/#what-is-azure-ai-document-intelligence","text":"Azure AI Document Intelligence (Form Recognizer) is a machine-learning AI service that extracts text from documents(JPG, PNG, BMP, PDF, or TIFF). It offers REST APIs client library SDKs(Python, C#, Java, and JavaScript) for programmers. Azure Document Intelligence uses Optical Character Recognition (OCR) capabilities and deep learning models to extract text, key-value pairs, selection marks, and tables from documents. The service offers two types of models - Prebuilt models are ready-to-use models for common documents like receipts and credit cards. Custom models are used when a document has unusual and unique format. The service is accessible via SDKs in Python, C#, Java, and JavaScript.","title":"What is Azure AI Document Intelligence?"},{"location":"PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence/#models-in-azure-ai-document-intelligence","text":"Azure AI Document Intelligence (formerly Form Recognizer) offers various models for using this service in your application. Here are the models: Prebuilt Models : These are pre-trained, ready-to-use models which you can use rightaway without any customization: General-Document-Read : Extracts text and key-value pairs from documents. General-Document-Layout : Analyzes the layout and structure of documents. Contract : Extracts details from contracts. Health Insurance Card : Extracts information from health insurance cards. ID Document : Extracts data from identification documents. Invoice : Extracts relevant details from invoices. Receipt : Extracts transaction information from receipts. US Tax Forms (e.g., 1040, 1098, W2) : Extracts data from various US tax forms. Marriage Certificate : Extracts details from marriage certificates. Credit Card : Extracts information from credit cards. Business Card (deprecated) : Previously used for business card extraction. Custom Models : When you have forms with unusual or unique formats, you can create and train your own custom models in Azure AI Document Intelligence. A custom model can provide field extraction for the data that is unique to your form and generate data targeted to your unique business application. Custom Classification Model : Allows you to create custom classifiers. Neural Model : Use custom neural models for inconsistent, semi-structured or unstructured forms. Template Model : Use custom template models when your forms have a consistent visual template. The formatting and layout should be consistent across all completed examples of the form. Composed Model : You can build hundreds of custom models within one Azure AI Document Intelligence resource and combine them into a composed model using Azure AI Document Intelligence Studio's GUI or the StartCreateComposedModelAsync() method in code. Add-On Capabilities : These enhance existing models: Font Property Extraction : Extracts font-related information.","title":"Models in Azure AI Document Intelligence"},{"location":"PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence/#azure-ai-document-intelligence-vs-azure-ai-vision","text":"Azure AI Document Intelligence is based on Azure AI Vision services. For basic text extraction from images, Azure AI Vision OCR is appropriate. For more detailed document analysis, such as identifying key/value pairs, tables, and contextual information, Azure AI Document Intelligence is more suitable.","title":"Azure AI Document Intelligence Vs Azure AI Vision"},{"location":"PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence/#tools-for-aaidi","text":"Azure AI Document Intelligence tools offer Document Intelligence Studio(https://formrecognizer.appliedai.azure.com/) or (https://documentintelligence.ai.azure.com/studio) to use it without much coding. Here, you can find a range off pre-built models and option to create custom models. To embed AADI in your applications, coding is required. For instance, users can scan receipts through a mobile app, which uses this service to extract detailed information for integration into a CRM database. AADI provides APIs for each pre-built model. You can use C#/.NET, Java, Python, and JavaScript to access them. Also, you can access the service via its RESTful web service for other languages.","title":"Tools for AAIDI"},{"location":"PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence/#azure-document-intelligence-studio","text":"Azure Document Intelligence services are accessible through both SDKs and APIs, but if you prefer a more visual approach, you can use the Azure Document Intelligence Studio. This web application lets you manage your projects more interactively. In the Studio, you can set up various types of projects like document analysis models, which include capabilities for extracting text, tables, and structure from documents and images. You can also work with prebuilt models for specific document types or even train your own custom models to fit your specific needs. It's a flexible tool that caters to a variety of document processing requirements.","title":"Azure Document Intelligence Studio"},{"location":"PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence/#use-prebuilt-document-model","text":"Create an Azure AI Document Intelligence resource Before you can call the Azure AI Document Intelligence service, you must create a resource to host that service in Azure: In a browser tab, open the Azure portal at https://portal.azure.com, signing in with the Microsoft account associated with your Azure subscription. On the Azure portal home page, navigate to the top search box and type Document Intelligence and then press Enter. On the Document Intelligence page, select Create. On the Create Document Intelligence page, use the following to configure your resource: Subscription: Your Azure subscription. Resource group: Select or create a resource group with a unique name such as DocIntelligenceResources. Region: select a region near you. Name: Enter a globally unique name. Pricing tier: select Free F0 (if you don't have a Free tier available, select Standard S0). Then select Review + create, and Create. Wait while Azure creates the Azure AI Document Intelligence resource. When the deployment is complete, select Go to resource. Keep this page open for the rest of this exercise. Use the Read model Let's start by using the Azure AI Document Intelligence Studio and the Read model to analyze a document with multiple languages. You'll connect Azure AI Document Intelligence Studio to the resource you just created to perform the analysis: Open a new browser tab and go to the Azure AI Document Intelligence Studio at https://documentintelligence.ai.azure.com/studio. Under Document Analysis, select the Read tile. If you are asked to sign into your account, use your Azure credentials. If you are asked which Azure AI Document Intelligence resource to use, select the subscription and resource name you used when you created the Azure AI Document Intelligence resource. In the list of documents on the left, select read-german.png. When the analysis is complete, the text extracted from the image is shown on the right in the Content tab. Review this text and compare it to the text in the original image for accuracy. Select the Result tab. This tab displays the extracted JSON code. Scroll to the bottom of the JSON code in the Result tab. Notice that the read model has detected the language of each span. Most spans are in German (language code de) but the last span is in English (language code en).","title":"Use prebuilt document model"},{"location":"PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence/#use-composed-model","text":"Once you've created a set of custom models, you must assemble them into a composed model. You can do this in a Graphical User Interface (GUI) by using Azure AI Document Intelligence Studio, or by using the StartCreateComposedModelAsync() method in custom code. In the results from the composed model, you can determine which custom model has been used for the analysis by checking the docType field. The maximum number of custom models that can be added to a single composed model is 100.","title":"Use composed model"},{"location":"PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence/#file-type-requirement","text":"Note: AAIDI can't extract data from word documents or excel. It only supports, photographed documents like JPG, PNG, BMP, TIFF and PDF. This is also true for traditional document capture tools like OpenText Intelligent Capture(Captiva InputAccel) whre you can use word document but an intermediate image conversion module converts it to image beore ocring. Here are the technical reuiqrment for files: Format must be JPG, PNG, BMP, PDF (text or scanned), or TIFF. The file size must be less than 500 MB for paid (S0) tier and 4 MB for free (F0) tier. Image dimensions must be between 50 x 50 pixels and 10000 x 10000 pixels. The total size of the training data set must be 500 pages or less.","title":"File type requirement"},{"location":"PowerPlatform/DocumentIntelligence/DocumentAutomation/","text":"Background How its done traditionally Enter Power Platform's AI Builder Introduing Document automation base kit Conclusion Background This article may be beneficial for ECM and Data Capture specialists exploring alternative solutions offered by Power Platform to traditional Document Capture tools such as Captiva InputAccel (OpenText Intelligent Capture), Kofax Capture, and IBM Datacap. How its done traditionally In most major banks, insurance companies, and pharmaceutical organizations, document capture tools like OpenText Intelligent Capture or Kofax are utilized to capture documents from scanners, emails, and other sources. These tools extract metadata and then forward both the documents and metadata to backend ECM systems such as OpenText Documentum, IBM FileNet, or SharePoint. Multi-channel capture using traditional products A key feature of these Document Capture products is their capability to automatically classify and extract information, allowing users to validate this information. Captiva Document Processing Modules OpenText facilitates this process through modules like ScanPlus for capture, Classification for automatic classification, Extraction for metadata extraction, and IndexPlus/Completion for manual validation. Over the past two decades, EMC Captiva, Kofax, and now OpenText Intelligent Capture have dominated the market. These products have been widely adopted across large banks, insurance corporations, and the pharmaceutical sector. Captiva Capture provides a centralized development tool called Captiva Designer for creating, configuring, deploying, and testing the capture system end-to-end.. Enter Power Platform's AI Builder In recent years, Microsoft has made notable strides in the Document Capture market. I've demonstrated how straightforward it is to develop a workflow using Power Automate and AI Builder that extracts data from fixed-template documents like passports and driving licenses. AI Builder also provides ready-to-use models for processing standard documents such as invoices and receipts, which can be utilized as they are or customized with ease. For instance, if you need to extract information from Singapore NRIC or Indian PAN Cards, you only need about 5 sample images in PNG or JPEG format. Simply create the fields and outline the areas you want AI Builder to recognize and extract. It takes less than 30 minutes to create a custom model if your documents adhere to a very standard fixed template. While this capability is powerful, it does not include an operator-based validation feature, which means the extracted data might not always be 100% accurate. However, you can deisgn a solution for that too. You can create an end-to-end solution that combines a Power Automate workflow for data extraction using AI Builder, Power Apps for manual validation, and Dataverse or SharePoint to manage storage and indexing queues. Yet, building such a solution from scratch can be complex. Introduing Document automation base kit Microsoft now offers a Document Automation Base \u2014a ready-to-use, complete solution that includes AI Builder, Power Automate, and Power Apps for document capture, extraction, and validation. Document automation base solution has these components: Email Importer Flow: Documents received by email are imported for processing. If you receive documents differently, you can adjust this to collect documents from places like SharePoint or cloud storage. Document Processor Flow: This step uses AI to extract data from the documents. It triggers when a document reaches the 'Extracting data' state in the queue. The data extracted by the AI is then stored in Dataverse, a storage system. Validator Flow: This flow adds business rules to the process. It starts after the data has been extracted and stored. If the data meets certain criteria (like confidence levels or specific data thresholds), it might be approved automatically; otherwise, it goes for manual review. By default, this step does not have preset rules and sends all documents for manual review. Automation Application: This is where manual review and approval of documents take place. Users can see all documents in the pipeline, open them to view the data, make necessary edits, and approve them. This application also allows the process owner to configure the AI model and track documents through various states from receiving and extracting to validating and final approval. Conclusion The key question is whether the Power Platform's AI-Builder and related solutions can effectively handle real-world document processing in sectors like banking. From my observations, the Power Platform may not be suitable for large-scale scanning where industrial scanners are linked to desktops and data validation is performed by dedicated operators, typical of setups outsourced to third-party operators like Iron Mountain. Power Automate lacks support for scanner-connected setups and may struggle with high-volume scanning due to its web-based nature. However, it could work for processes involving already digitized documents needing OCR processing and ECM integration. While solutions like OpenText Intelligent Capture\u2019s web-based Captiva Capture Web Client might be partly replaceable by Power Automate, the durability of AI Builder's OCR engines compared to established ones like Nuance OCR from Captiva is less certain. Below, you'll find a table comparing traditional scanning products with the Power Platform's solutions. Feature / Module Microsoft Power Platform Document Automation Base Kit OpenText Intelligent Capture Document Import Email import, SharePoint and cloud storage integration. Modules like ScanPlus and Standard Import handle various sources including emails, MFP, Fax, browser-connected scanners etc.. AI-Powered Data Extraction AI Builder for configurable data extraction. Uses modules like NuanceOCR, Classification, Recognition and IndexPlus for OCR, classification, and data extraction from various document types. OCR Engine Built-in OCR capabilities within AI Builder. NuanceOCR for high-accuracy OCR, capable of handling both machine-printed and handwritten texts. Data Storage and Management Uses Dataverse for data management and storage. Focuses on processing and exporting data; does not typically store documents long-term. Process Customization Customizable through Power Apps for various workflows. Extensive customization options with C#/VB.NET scripting, advanced recognition settings, and use of RESTful services. Validation and Review Validator flow for manual and automated data validation. Modules like IndexPlus, Completion for manual review and validation, with robust tools for automated data validation. Business Rule Application Validator flow for applying business rules. Advanced scripting capabilities for applying complex business logic through modules like IndexPlus. Integration with Other Systems Limitless integration capability with M365 and Azure ecosystem Ready-made modules like Documentum Export, SharePoint Export, XML Export, SAP Export, makes it very easy to integrate with ECM, ERP, BPM systems. User Interface and Experience Power Apps-based centralized application. Mostly windows desktop applications also browser based application like Captiva Web Client. Specific Modules Import, AI Extraction, Validation, and Review modules. Comprehensive suite including ScanPlus, IndexPlus, Completion, Web Services Input/Output, and more. Scalability and Distribution Auto-scalling. ScaleServer for load distribution and clustering for HA and DR.","title":"Document automation base kit"},{"location":"PowerPlatform/DocumentIntelligence/DocumentAutomation/#background","text":"This article may be beneficial for ECM and Data Capture specialists exploring alternative solutions offered by Power Platform to traditional Document Capture tools such as Captiva InputAccel (OpenText Intelligent Capture), Kofax Capture, and IBM Datacap.","title":"Background"},{"location":"PowerPlatform/DocumentIntelligence/DocumentAutomation/#how-its-done-traditionally","text":"In most major banks, insurance companies, and pharmaceutical organizations, document capture tools like OpenText Intelligent Capture or Kofax are utilized to capture documents from scanners, emails, and other sources. These tools extract metadata and then forward both the documents and metadata to backend ECM systems such as OpenText Documentum, IBM FileNet, or SharePoint. Multi-channel capture using traditional products A key feature of these Document Capture products is their capability to automatically classify and extract information, allowing users to validate this information. Captiva Document Processing Modules OpenText facilitates this process through modules like ScanPlus for capture, Classification for automatic classification, Extraction for metadata extraction, and IndexPlus/Completion for manual validation. Over the past two decades, EMC Captiva, Kofax, and now OpenText Intelligent Capture have dominated the market. These products have been widely adopted across large banks, insurance corporations, and the pharmaceutical sector. Captiva Capture provides a centralized development tool called Captiva Designer for creating, configuring, deploying, and testing the capture system end-to-end..","title":"How its done traditionally"},{"location":"PowerPlatform/DocumentIntelligence/DocumentAutomation/#enter-power-platforms-ai-builder","text":"In recent years, Microsoft has made notable strides in the Document Capture market. I've demonstrated how straightforward it is to develop a workflow using Power Automate and AI Builder that extracts data from fixed-template documents like passports and driving licenses. AI Builder also provides ready-to-use models for processing standard documents such as invoices and receipts, which can be utilized as they are or customized with ease. For instance, if you need to extract information from Singapore NRIC or Indian PAN Cards, you only need about 5 sample images in PNG or JPEG format. Simply create the fields and outline the areas you want AI Builder to recognize and extract. It takes less than 30 minutes to create a custom model if your documents adhere to a very standard fixed template. While this capability is powerful, it does not include an operator-based validation feature, which means the extracted data might not always be 100% accurate. However, you can deisgn a solution for that too. You can create an end-to-end solution that combines a Power Automate workflow for data extraction using AI Builder, Power Apps for manual validation, and Dataverse or SharePoint to manage storage and indexing queues. Yet, building such a solution from scratch can be complex.","title":"Enter Power Platform's AI Builder"},{"location":"PowerPlatform/DocumentIntelligence/DocumentAutomation/#introduing-document-automation-base-kit","text":"Microsoft now offers a Document Automation Base \u2014a ready-to-use, complete solution that includes AI Builder, Power Automate, and Power Apps for document capture, extraction, and validation. Document automation base solution has these components: Email Importer Flow: Documents received by email are imported for processing. If you receive documents differently, you can adjust this to collect documents from places like SharePoint or cloud storage. Document Processor Flow: This step uses AI to extract data from the documents. It triggers when a document reaches the 'Extracting data' state in the queue. The data extracted by the AI is then stored in Dataverse, a storage system. Validator Flow: This flow adds business rules to the process. It starts after the data has been extracted and stored. If the data meets certain criteria (like confidence levels or specific data thresholds), it might be approved automatically; otherwise, it goes for manual review. By default, this step does not have preset rules and sends all documents for manual review. Automation Application: This is where manual review and approval of documents take place. Users can see all documents in the pipeline, open them to view the data, make necessary edits, and approve them. This application also allows the process owner to configure the AI model and track documents through various states from receiving and extracting to validating and final approval.","title":"Introduing Document automation base kit"},{"location":"PowerPlatform/DocumentIntelligence/DocumentAutomation/#conclusion","text":"The key question is whether the Power Platform's AI-Builder and related solutions can effectively handle real-world document processing in sectors like banking. From my observations, the Power Platform may not be suitable for large-scale scanning where industrial scanners are linked to desktops and data validation is performed by dedicated operators, typical of setups outsourced to third-party operators like Iron Mountain. Power Automate lacks support for scanner-connected setups and may struggle with high-volume scanning due to its web-based nature. However, it could work for processes involving already digitized documents needing OCR processing and ECM integration. While solutions like OpenText Intelligent Capture\u2019s web-based Captiva Capture Web Client might be partly replaceable by Power Automate, the durability of AI Builder's OCR engines compared to established ones like Nuance OCR from Captiva is less certain. Below, you'll find a table comparing traditional scanning products with the Power Platform's solutions. Feature / Module Microsoft Power Platform Document Automation Base Kit OpenText Intelligent Capture Document Import Email import, SharePoint and cloud storage integration. Modules like ScanPlus and Standard Import handle various sources including emails, MFP, Fax, browser-connected scanners etc.. AI-Powered Data Extraction AI Builder for configurable data extraction. Uses modules like NuanceOCR, Classification, Recognition and IndexPlus for OCR, classification, and data extraction from various document types. OCR Engine Built-in OCR capabilities within AI Builder. NuanceOCR for high-accuracy OCR, capable of handling both machine-printed and handwritten texts. Data Storage and Management Uses Dataverse for data management and storage. Focuses on processing and exporting data; does not typically store documents long-term. Process Customization Customizable through Power Apps for various workflows. Extensive customization options with C#/VB.NET scripting, advanced recognition settings, and use of RESTful services. Validation and Review Validator flow for manual and automated data validation. Modules like IndexPlus, Completion for manual review and validation, with robust tools for automated data validation. Business Rule Application Validator flow for applying business rules. Advanced scripting capabilities for applying complex business logic through modules like IndexPlus. Integration with Other Systems Limitless integration capability with M365 and Azure ecosystem Ready-made modules like Documentum Export, SharePoint Export, XML Export, SAP Export, makes it very easy to integrate with ECM, ERP, BPM systems. User Interface and Experience Power Apps-based centralized application. Mostly windows desktop applications also browser based application like Captiva Web Client. Specific Modules Import, AI Extraction, Validation, and Review modules. Comprehensive suite including ScanPlus, IndexPlus, Completion, Web Services Input/Output, and more. Scalability and Distribution Auto-scalling. ScaleServer for load distribution and clustering for HA and DR.","title":"Conclusion"},{"location":"Python/1.0_Sets/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Python Sets for busy people Set items don't have a serial number (index) No Index. No Order. You can't refer to an item like mySet[2] No duplicate items No Lists/Dictionaries Can add new items: mySet.add(4) : Adds a single element to the set. mySet.update([4, 5]) : Adds multiple elements to the set. Can remove items: mySet.remove(element) : Removes a specific element. Raises a KeyError if not found. mySet.discard(element) : Removes a specific element. No issues if not found. mySet.pop() : Removes and returns a random element. KeyError if set empty. mySet.clear() : Removes all elements. Slate clean. Check if an element exists: element in mySet : True if item present, else False . Get the number of elements: len(mySet) : Total no of items in set. Copy a set: mySet.copy() : Creates a shallow copy of the set. Union of sets: mySet.union(other_set) or mySet | other_set : Combines all elements from both sets, without duplicates. Intersection of sets: mySet.intersection(other_set) or mySet & other_set : Returns elements common to both sets. Difference of sets: mySet.difference(other_set) or mySet - other_set : Returns elements in the first set but not in the second. Symmetric difference of sets: mySet.symmetric_difference(other_set) or mySet ^ other_set : Returns elements in either set, but not in both. Note: Changeable items (also called mutable) are items that can be modified after they are created. For example: Lists : You can add, remove, or change elements. Dictionaries : You can add, remove, or change key-value pairs. These items cannot be added to a set because sets need items that do not change. Unchangeable items (also called immutable) are items that cannot be modified after they are created. For example: Numbers : Once created, their values cannot be changed. Strings : Any modification creates a new string. Tuples : Their elements cannot be changed once created. These items can be added to a set because their values stay the same. Python Sets A Python set is like your travel kit. Collection of unique items. There can be different items. But, they should be unique. Set items don't have serial numbers (Index). Without a serial number, you can't do something like mySet[2]=\"Guava\" . All items in a set must be different. Otherwise, how would you tell them apart? If your set has two apples, which one is which? But, you can remove items from a set. You can take out an apple and add a guava. Don't think about removing an apple and adding another apple. Sets can't contain a list or a dictionary . Period. They can contain tuples , but these tuples can't have lists or dictionaries inside them. (It won't cause an error, but it can make the code unstable.) Python Sets Properties So, here are some properties of Python Sets: Items have No Index: Python stores Set items but does not keep track of their order. This means there is no first item, second item, etc. For example, if you input `apple`, `orange`, `banana`, you might get `banana`, `apple`, `orange` as the output. ```python mySet = {1, 2, 3} print(mySet) # Output could be {1, 2, 3} or {3, 1, 2} or any permutation mySet[0] # THIS IS AN ERROR. No one is sitting at 0. There is no order, no index. ``` No Duplicates: Since items in a set do not have serial numbers, duplicates are not allowed. If you try to add two apples, how would you distinguish between them? Therefore, when you add duplicates to a set, Python automatically removes the duplicates. mySet = {1, 2, 2, 3} print(mySet) # Output: {1, 2, 3} No In-Place Replace. Add/remove instead. You can add/remove items, but can't change an item's value directly. Can't in-place replace items. First, remove the old one and add the new one. ```python mySet = {1, 2, 3} mySet.remove(2) # OK mySet.add(4) # OK mySet[0] = 5 # ERROR ``` No Lists/Dictionaries, Tuples Are OK. Sets use hashing, so you can't store lists or dictionaries in them. However, you can store tuples. Just make sure these tuples don't contain lists or dictionaries inside them. ```python # Valid elements mySet = {1, \"hello\", (1, 2)} # TUPLES OK # Invalid elements mySet = {[1, 2], {\"key\": \"value\"}} # ERROR, NO LISTS, NO DICTS ``` When to use sets Sets for Python are very useful when you need keep unique items and do quick membership checks. Here are some scenarios where sets are frequently used: Removing Duplicates Use Case : When you need to ensure that a collection of elements contains no duplicates. Example : Removing duplicates from a list. python items = [1, 2, 2, 3, 4, 4, 5] unique_items = list(set(items)) # [1, 2, 3, 4, 5] Membership Testing Use Case : When you need to check if an element exists in a collection. Sets provide average O(1) time complexity for membership tests. Example : Checking if an item exists in a collection. python allowed_items = {\"apple\", \"banana\", \"cherry\"} if \"banana\" in allowed_items: print(\"Banana is allowed\") Set Operations Use Case : When you need to perform operations like union, intersection, difference, and symmetric difference between collections. Example : Finding common elements between two sets. python set1 = {1, 2, 3} set2 = {3, 4, 5} common_items = set1 & set2 # {3} Data Validation Use Case : When validating data to ensure uniqueness, such as checking for duplicate entries in a dataset. Example : Validating unique user IDs. python user_ids = [101, 102, 103, 101] unique_user_ids = set(user_ids) if len(user_ids) != len(unique_user_ids): print(\"Duplicate user IDs found\") Tracking Unique Elements Use Case : When you need to keep track of unique items encountered during processing. Example : Tracking unique words in a text. python text = \"hello world hello\" words = text.split() unique_words = set(words) # {\"hello\", \"world\"} Efficient Data Lookups Use Case : When you need a data structure that allows for fast lookups, insertions, and deletions. Example : Keeping track of visited URLs in a web crawler. python visited_urls = set() visited_urls.add(\"https://example.com\") if \"https://example.com\" in visited_urls: print(\"URL already visited\") Test your knowledge Highlight the answer section to reveal! Question - set.update() What will be the output of the following statement? thisset = {\"apple\", \"banana\", \"cherry\", False, True, 0} print(thisset) Answer: {'apple', 'banana', 'cherry', False, True} Question - set.add() What will be the output of the following statement? thisset = {\"apple\", \"banana\", \"cherry\"} thisset.add(\"apple\") print(thisset) Answer: {'apple', 'banana', 'cherry'} Question - set.discard() What will be the output of the following statement? thisset = {1, 2, 3, 4, 5} thisset.discard(6) print(thisset) Answer: {1, 2, 3, 4, 5} Question - set.remove() What will be the output of the following statement? thisset = {1, 2, 3, 4, 5} thisset.remove(6) print(thisset) Answer: Raises a KeyError Question - set.update() What will be the output of the following statement? thisset = {\"apple\", \"banana\", \"cherry\"} thisset.update([\"orange\", \"mango\"]) print(thisset) Answer: {'apple', 'banana', 'cherry', 'orange', 'mango'} Question - set.copy() What will be the output of the following statement? thisset = {\"apple\", \"banana\", \"cherry\"} newset = thisset.copy() thisset.add(\"orange\") print(newset) Answer: {'apple', 'banana', 'cherry'} Question - set membership What will be the output of the following statement? thisset = {1, 2, 3, 4, 5} result = 3 in thisset print(result) Answer: True Question - set intersection What will be the output of the following statement? thisset1 = {1, 2, 3} thisset2 = {3, 4, 5} result = thisset1 & thisset2 print(result) Answer: {3} Question - set union What will be the output of the following statement? thisset1 = {1, 2, 3} thisset2 = {3, 4, 5} result = thisset1 | thisset2 print(result) Answer: {1, 2, 3, 4, 5} Question - set difference What will be the output of the following statement? thisset1 = {1, 2, 3} thisset2 = {3, 4, 5} result = thisset1 - thisset2 print(result) Answer: {1, 2} Set Operations and Properties Operation Syntax Description & Example Union x1.union(x2) x1 &#124; x2 Combines all elements from both sets, without duplicates. x1 = {1, 2, 3} x2 = {3, 4, 5} x1.union(x2) Output: {1, 2, 3, 4, 5} Intersection x1.intersection(x2) x1 & x2 Returns elements common to both sets. x1 = {1, 2, 3} x2 = {3, 4, 5} x1 & x2 Output: {3} Difference x1.difference(x2) x1 - x2 Returns elements in the first set but not in the second. x1 = {1, 2, 3} x2 = {3, 4, 5} x1 - x2 Output: {1, 2} Symmetric Difference x1.symmetric_difference(x2) x1 ^ x2 Elements in either set, but not both. x1 = {1, 2, 3} x2 = {3, 4, 5} x1 ^ x2 Output: {1, 2, 4, 5} Subset x1.issubset(x2) x1 <= x2 Checks if all elements of one set are in another. x1 = {1, 2} x2 = {1, 2, 3} x1 <= x2 Output: True Superset x1.issuperset(x2) x1 >= x2 Checks if one set contains all elements of another. x1 = {1, 2, 3} x2 = {1, 2} x1 >= x2 Output: True Disjoint x1.isdisjoint(x2) Checks if two sets have no elements in common. x1 = {1, 2, 3} x2 = {4, 5, 6} x1.isdisjoint(x2) Output: True Add Element x1.add(element) Adds a single element to the set. x1 = {1, 2, 3} x1.add(4) Output: {1, 2, 3, 4} Remove Element x1.remove(element) Removes a specific element from the set. x1 = {1, 2, 3} x1.remove(2) Output: {1, 3} Discard Element x1.discard(element) Removes a specific element if it is present. x1 = {1, 2, 3} x1.discard(2) Output: {1, 3} Clear Set x1.clear() Removes all elements from the set. x1 = {1, 2, 3} x1.clear() Output: set() Copy Set x1.copy() Creates a shallow copy of the set. x1 = {1, 2, 3} x2 = x1.copy() Output: x2 = {1, 2, 3} Update Set x1.update(x2) Adds elements from another set. x1 = {1, 2} x2 = {3, 4} x1.update(x2) Output: {1, 2, 3, 4} Intersection Update x1.intersection_update(x2) Updates the set, keeping only elements found in it and another set. x1 = {1, 2, 3} x2 = {2, 3, 4} x1.intersection_update(x2) Output: {2, 3} Difference Update x1.difference_update(x2) Updates the set, removing elements found in another set. x1 = {1, 2, 3} x2 = {2, 3, 4} x1.difference_update(x2) Output: {1} Symmetric Difference Update x1.symmetric_difference_update(x2) Updates the set, keeping only elements found in either set, but not both. x1 = {1, 2, 3} x2 = {2, 3, 4} x1.symmetric_difference_update(x2) Output: {1, 4}","title":"Sets"},{"location":"Python/1.0_Sets/#python-sets-for-busy-people","text":"Set items don't have a serial number (index) No Index. No Order. You can't refer to an item like mySet[2] No duplicate items No Lists/Dictionaries Can add new items: mySet.add(4) : Adds a single element to the set. mySet.update([4, 5]) : Adds multiple elements to the set. Can remove items: mySet.remove(element) : Removes a specific element. Raises a KeyError if not found. mySet.discard(element) : Removes a specific element. No issues if not found. mySet.pop() : Removes and returns a random element. KeyError if set empty. mySet.clear() : Removes all elements. Slate clean. Check if an element exists: element in mySet : True if item present, else False . Get the number of elements: len(mySet) : Total no of items in set. Copy a set: mySet.copy() : Creates a shallow copy of the set. Union of sets: mySet.union(other_set) or mySet | other_set : Combines all elements from both sets, without duplicates. Intersection of sets: mySet.intersection(other_set) or mySet & other_set : Returns elements common to both sets. Difference of sets: mySet.difference(other_set) or mySet - other_set : Returns elements in the first set but not in the second. Symmetric difference of sets: mySet.symmetric_difference(other_set) or mySet ^ other_set : Returns elements in either set, but not in both.","title":"Python Sets for busy people"},{"location":"Python/1.0_Sets/#python-sets","text":"A Python set is like your travel kit. Collection of unique items. There can be different items. But, they should be unique. Set items don't have serial numbers (Index). Without a serial number, you can't do something like mySet[2]=\"Guava\" . All items in a set must be different. Otherwise, how would you tell them apart? If your set has two apples, which one is which? But, you can remove items from a set. You can take out an apple and add a guava. Don't think about removing an apple and adding another apple. Sets can't contain a list or a dictionary . Period. They can contain tuples , but these tuples can't have lists or dictionaries inside them. (It won't cause an error, but it can make the code unstable.)","title":"Python Sets"},{"location":"Python/1.0_Sets/#python-sets-properties","text":"So, here are some properties of Python Sets:","title":"Python Sets Properties"},{"location":"Python/1.0_Sets/#items-have-no-index","text":"Python stores Set items but does not keep track of their order. This means there is no first item, second item, etc. For example, if you input `apple`, `orange`, `banana`, you might get `banana`, `apple`, `orange` as the output. ```python mySet = {1, 2, 3} print(mySet) # Output could be {1, 2, 3} or {3, 1, 2} or any permutation mySet[0] # THIS IS AN ERROR. No one is sitting at 0. There is no order, no index. ```","title":"Items have No Index:"},{"location":"Python/1.0_Sets/#no-duplicates","text":"Since items in a set do not have serial numbers, duplicates are not allowed. If you try to add two apples, how would you distinguish between them? Therefore, when you add duplicates to a set, Python automatically removes the duplicates. mySet = {1, 2, 2, 3} print(mySet) # Output: {1, 2, 3}","title":"No Duplicates:"},{"location":"Python/1.0_Sets/#no-in-place-replace-addremove-instead","text":"You can add/remove items, but can't change an item's value directly. Can't in-place replace items. First, remove the old one and add the new one. ```python mySet = {1, 2, 3} mySet.remove(2) # OK mySet.add(4) # OK mySet[0] = 5 # ERROR ```","title":"No In-Place Replace. Add/remove instead."},{"location":"Python/1.0_Sets/#no-listsdictionaries-tuples-are-ok","text":"Sets use hashing, so you can't store lists or dictionaries in them. However, you can store tuples. Just make sure these tuples don't contain lists or dictionaries inside them. ```python # Valid elements mySet = {1, \"hello\", (1, 2)} # TUPLES OK # Invalid elements mySet = {[1, 2], {\"key\": \"value\"}} # ERROR, NO LISTS, NO DICTS ```","title":"No Lists/Dictionaries, Tuples Are OK."},{"location":"Python/1.0_Sets/#when-to-use-sets","text":"Sets for Python are very useful when you need keep unique items and do quick membership checks. Here are some scenarios where sets are frequently used:","title":"When to use sets"},{"location":"Python/1.0_Sets/#removing-duplicates","text":"Use Case : When you need to ensure that a collection of elements contains no duplicates. Example : Removing duplicates from a list. python items = [1, 2, 2, 3, 4, 4, 5] unique_items = list(set(items)) # [1, 2, 3, 4, 5]","title":"Removing Duplicates"},{"location":"Python/1.0_Sets/#membership-testing","text":"Use Case : When you need to check if an element exists in a collection. Sets provide average O(1) time complexity for membership tests. Example : Checking if an item exists in a collection. python allowed_items = {\"apple\", \"banana\", \"cherry\"} if \"banana\" in allowed_items: print(\"Banana is allowed\")","title":"Membership Testing"},{"location":"Python/1.0_Sets/#set-operations","text":"Use Case : When you need to perform operations like union, intersection, difference, and symmetric difference between collections. Example : Finding common elements between two sets. python set1 = {1, 2, 3} set2 = {3, 4, 5} common_items = set1 & set2 # {3}","title":"Set Operations"},{"location":"Python/1.0_Sets/#data-validation","text":"Use Case : When validating data to ensure uniqueness, such as checking for duplicate entries in a dataset. Example : Validating unique user IDs. python user_ids = [101, 102, 103, 101] unique_user_ids = set(user_ids) if len(user_ids) != len(unique_user_ids): print(\"Duplicate user IDs found\")","title":"Data Validation"},{"location":"Python/1.0_Sets/#tracking-unique-elements","text":"Use Case : When you need to keep track of unique items encountered during processing. Example : Tracking unique words in a text. python text = \"hello world hello\" words = text.split() unique_words = set(words) # {\"hello\", \"world\"}","title":"Tracking Unique Elements"},{"location":"Python/1.0_Sets/#efficient-data-lookups","text":"Use Case : When you need a data structure that allows for fast lookups, insertions, and deletions. Example : Keeping track of visited URLs in a web crawler. python visited_urls = set() visited_urls.add(\"https://example.com\") if \"https://example.com\" in visited_urls: print(\"URL already visited\")","title":"Efficient Data Lookups"},{"location":"Python/1.0_Sets/#test-your-knowledge","text":"Highlight the answer section to reveal!","title":"Test your knowledge"},{"location":"Python/1.0_Sets/#question-setupdate","text":"What will be the output of the following statement? thisset = {\"apple\", \"banana\", \"cherry\", False, True, 0} print(thisset) Answer: {'apple', 'banana', 'cherry', False, True}","title":"Question - set.update()"},{"location":"Python/1.0_Sets/#question-setadd","text":"What will be the output of the following statement? thisset = {\"apple\", \"banana\", \"cherry\"} thisset.add(\"apple\") print(thisset) Answer: {'apple', 'banana', 'cherry'}","title":"Question - set.add()"},{"location":"Python/1.0_Sets/#question-setdiscard","text":"What will be the output of the following statement? thisset = {1, 2, 3, 4, 5} thisset.discard(6) print(thisset) Answer: {1, 2, 3, 4, 5}","title":"Question - set.discard()"},{"location":"Python/1.0_Sets/#question-setremove","text":"What will be the output of the following statement? thisset = {1, 2, 3, 4, 5} thisset.remove(6) print(thisset) Answer: Raises a KeyError","title":"Question - set.remove()"},{"location":"Python/1.0_Sets/#question-setupdate_1","text":"What will be the output of the following statement? thisset = {\"apple\", \"banana\", \"cherry\"} thisset.update([\"orange\", \"mango\"]) print(thisset) Answer: {'apple', 'banana', 'cherry', 'orange', 'mango'}","title":"Question - set.update()"},{"location":"Python/1.0_Sets/#question-setcopy","text":"What will be the output of the following statement? thisset = {\"apple\", \"banana\", \"cherry\"} newset = thisset.copy() thisset.add(\"orange\") print(newset) Answer: {'apple', 'banana', 'cherry'}","title":"Question - set.copy()"},{"location":"Python/1.0_Sets/#question-set-membership","text":"What will be the output of the following statement? thisset = {1, 2, 3, 4, 5} result = 3 in thisset print(result) Answer: True","title":"Question - set membership"},{"location":"Python/1.0_Sets/#question-set-intersection","text":"What will be the output of the following statement? thisset1 = {1, 2, 3} thisset2 = {3, 4, 5} result = thisset1 & thisset2 print(result) Answer: {3}","title":"Question - set intersection"},{"location":"Python/1.0_Sets/#question-set-union","text":"What will be the output of the following statement? thisset1 = {1, 2, 3} thisset2 = {3, 4, 5} result = thisset1 | thisset2 print(result) Answer: {1, 2, 3, 4, 5}","title":"Question - set union"},{"location":"Python/1.0_Sets/#question-set-difference","text":"What will be the output of the following statement? thisset1 = {1, 2, 3} thisset2 = {3, 4, 5} result = thisset1 - thisset2 print(result) Answer: {1, 2}","title":"Question - set difference"},{"location":"Python/1.0_Sets/#set-operations-and-properties","text":"Operation Syntax Description & Example Union x1.union(x2) x1 &#124; x2 Combines all elements from both sets, without duplicates. x1 = {1, 2, 3} x2 = {3, 4, 5} x1.union(x2) Output: {1, 2, 3, 4, 5} Intersection x1.intersection(x2) x1 & x2 Returns elements common to both sets. x1 = {1, 2, 3} x2 = {3, 4, 5} x1 & x2 Output: {3} Difference x1.difference(x2) x1 - x2 Returns elements in the first set but not in the second. x1 = {1, 2, 3} x2 = {3, 4, 5} x1 - x2 Output: {1, 2} Symmetric Difference x1.symmetric_difference(x2) x1 ^ x2 Elements in either set, but not both. x1 = {1, 2, 3} x2 = {3, 4, 5} x1 ^ x2 Output: {1, 2, 4, 5} Subset x1.issubset(x2) x1 <= x2 Checks if all elements of one set are in another. x1 = {1, 2} x2 = {1, 2, 3} x1 <= x2 Output: True Superset x1.issuperset(x2) x1 >= x2 Checks if one set contains all elements of another. x1 = {1, 2, 3} x2 = {1, 2} x1 >= x2 Output: True Disjoint x1.isdisjoint(x2) Checks if two sets have no elements in common. x1 = {1, 2, 3} x2 = {4, 5, 6} x1.isdisjoint(x2) Output: True Add Element x1.add(element) Adds a single element to the set. x1 = {1, 2, 3} x1.add(4) Output: {1, 2, 3, 4} Remove Element x1.remove(element) Removes a specific element from the set. x1 = {1, 2, 3} x1.remove(2) Output: {1, 3} Discard Element x1.discard(element) Removes a specific element if it is present. x1 = {1, 2, 3} x1.discard(2) Output: {1, 3} Clear Set x1.clear() Removes all elements from the set. x1 = {1, 2, 3} x1.clear() Output: set() Copy Set x1.copy() Creates a shallow copy of the set. x1 = {1, 2, 3} x2 = x1.copy() Output: x2 = {1, 2, 3} Update Set x1.update(x2) Adds elements from another set. x1 = {1, 2} x2 = {3, 4} x1.update(x2) Output: {1, 2, 3, 4} Intersection Update x1.intersection_update(x2) Updates the set, keeping only elements found in it and another set. x1 = {1, 2, 3} x2 = {2, 3, 4} x1.intersection_update(x2) Output: {2, 3} Difference Update x1.difference_update(x2) Updates the set, removing elements found in another set. x1 = {1, 2, 3} x2 = {2, 3, 4} x1.difference_update(x2) Output: {1} Symmetric Difference Update x1.symmetric_difference_update(x2) Updates the set, keeping only elements found in either set, but not both. x1 = {1, 2, 3} x2 = {2, 3, 4} x1.symmetric_difference_update(x2) Output: {1, 4}","title":"Set Operations and Properties"},{"location":"Python/1.1.0_assert_methods/","text":"Understanding Assert Methods in Python Assert methods are used in Python to check if a certain condition is true. They help in verifying that your code is working as expected. If the condition is not met, the program stops and shows an error message. Key Points Basic Assertion : You use the assert keyword followed by a condition. If the condition is True , the program continues. If the condition is False , the program raises an AssertionError . Example : python x = 5 assert x == 5 # This will not raise an error because the condition is True Assertion with Error Message : You can add a message to the assert statement. This message will be shown if the assertion fails. Example : python x = 3 assert x == 5, \"x should be 5\" # This will raise an AssertionError with the message Common Use Cases : Testing : Assertions are often used in testing to check if functions return the expected results. Debugging : They help in finding bugs by ensuring conditions are met at different stages of the code. Using Assertions in Functions : You can use assert to check input values or results within functions. Example : python def divide(a, b): assert b != 0, \"Division by zero is not allowed\" return a / b Here, if b is zero, the assertion will fail, and an error message will be displayed. Takeaways Assert Methods are a simple way to verify that your code behaves as expected. Use Assertions to catch errors early and ensure your program runs correctly. Keep Messages Clear to make debugging easier when assertions fail. In summary, assert methods are useful for checking conditions in your code and help in debugging and testing by providing clear error messages when things go wrong.","title":"assert methods"},{"location":"Python/1.1.0_assert_methods/#understanding-assert-methods-in-python","text":"Assert methods are used in Python to check if a certain condition is true. They help in verifying that your code is working as expected. If the condition is not met, the program stops and shows an error message.","title":"Understanding Assert Methods in Python"},{"location":"Python/1.1.0_assert_methods/#key-points","text":"Basic Assertion : You use the assert keyword followed by a condition. If the condition is True , the program continues. If the condition is False , the program raises an AssertionError . Example : python x = 5 assert x == 5 # This will not raise an error because the condition is True Assertion with Error Message : You can add a message to the assert statement. This message will be shown if the assertion fails. Example : python x = 3 assert x == 5, \"x should be 5\" # This will raise an AssertionError with the message Common Use Cases : Testing : Assertions are often used in testing to check if functions return the expected results. Debugging : They help in finding bugs by ensuring conditions are met at different stages of the code. Using Assertions in Functions : You can use assert to check input values or results within functions. Example : python def divide(a, b): assert b != 0, \"Division by zero is not allowed\" return a / b Here, if b is zero, the assertion will fail, and an error message will be displayed.","title":"Key Points"},{"location":"Python/1.1.0_assert_methods/#takeaways","text":"Assert Methods are a simple way to verify that your code behaves as expected. Use Assertions to catch errors early and ensure your program runs correctly. Keep Messages Clear to make debugging easier when assertions fail. In summary, assert methods are useful for checking conditions in your code and help in debugging and testing by providing clear error messages when things go wrong.","title":"Takeaways"},{"location":"Python/1.1.1_decorators/","text":"Understanding Decorators in Python In Python, decorators are a handy feature that lets you modify/enhance functions/methods without changing their actual code. Decorators are likes wrappers that can change how a function behaves. How Decorators Work Basic Concept : A decorator is a function that takes another function and extends its behavior without modifying it. It\u2019s like adding extra toppings to a basic dish to make it more interesting. Using a Decorator : You apply a decorator using the @ symbol followed by the decorator's name right above the function you want to modify. For example: python @my_decorator def my_function(): print(\"Hello!\") Here, @my_decorator is applied to my_function . Creating a Decorator : A decorator itself is just a function that returns another function. Here\u2019s a simple example: ```python def my_decorator(func): def wrapper(): print(\"Something is happening before the function.\") func() print(\"Something is happening after the function.\") return wrapper @my_decorator def say_hello(): print(\"Hello!\") say_hello() `` When say_hello()` is called, you\u2019ll see extra messages before and after the \"Hello!\" message. Why Use Decorators? : They help in adding common functionality (like logging, timing, etc.) to multiple functions easily, keeping your code clean and organized. Key Points Flexibility : Decorators let you add functionality to functions or methods without changing their core logic. Reusability : Once created, a decorator can be used on multiple functions, making your code DRY (Don't Repeat Yourself). Built-in Decorators : Python also provides some built-in decorators like @staticmethod , @classmethod , and @property . What is unittest ? unittest is a built-in Python module that helps you test your code. Think of it like a way to check if your code is doing what it's supposed to. Here\u2019s a simple breakdown: Why Use unittest ? Imagine you\u2019re building a machine that makes coffee. You'd want to test if it makes coffee properly, right? unittest does something similar for your code. It helps you check if your functions and classes are working as expected. How Does It Work? Write Test Cases : You write test cases that check if your code is correct. For example, if you have a function that adds two numbers, you\u2019d write a test to check if it actually adds them correctly. Run Tests : You run these test cases to see if your code passes all the checks. If everything works fine, you\u2019re good to go. If not, you\u2019ll see where things went wrong and can fix them. Basic Example Here's how you might use unittest : import unittest # Function to test def add(a, b): return a + b # Test case class TestAddFunction(unittest.TestCase): def test_add_positive(self): self.assertEqual(add(2, 3), 5) # Test if 2 + 3 equals 5 def test_add_negative(self): self.assertEqual(add(-1, -1), -2) # Test if -1 + -1 equals -2 # Run the tests if __name__ == \"__main__\": unittest.main() In this example, unittest helps us test if our add function works with positive and negative numbers. Why It Matters Using unittest helps you catch bugs early, so you don't find out about them later when it's harder to fix. It\u2019s like checking your coffee machine before your guests arrive to make sure it works! So, unittest is a handy tool to make sure your code does what it\u2019s supposed to do, making your coding life easier and your code more reliable.","title":"decorators"},{"location":"Python/1.1.1_decorators/#understanding-decorators-in-python","text":"In Python, decorators are a handy feature that lets you modify/enhance functions/methods without changing their actual code. Decorators are likes wrappers that can change how a function behaves.","title":"Understanding Decorators in Python"},{"location":"Python/1.1.1_decorators/#how-decorators-work","text":"Basic Concept : A decorator is a function that takes another function and extends its behavior without modifying it. It\u2019s like adding extra toppings to a basic dish to make it more interesting. Using a Decorator : You apply a decorator using the @ symbol followed by the decorator's name right above the function you want to modify. For example: python @my_decorator def my_function(): print(\"Hello!\") Here, @my_decorator is applied to my_function . Creating a Decorator : A decorator itself is just a function that returns another function. Here\u2019s a simple example: ```python def my_decorator(func): def wrapper(): print(\"Something is happening before the function.\") func() print(\"Something is happening after the function.\") return wrapper @my_decorator def say_hello(): print(\"Hello!\") say_hello() `` When say_hello()` is called, you\u2019ll see extra messages before and after the \"Hello!\" message. Why Use Decorators? : They help in adding common functionality (like logging, timing, etc.) to multiple functions easily, keeping your code clean and organized.","title":"How Decorators Work"},{"location":"Python/1.1.1_decorators/#key-points","text":"Flexibility : Decorators let you add functionality to functions or methods without changing their core logic. Reusability : Once created, a decorator can be used on multiple functions, making your code DRY (Don't Repeat Yourself). Built-in Decorators : Python also provides some built-in decorators like @staticmethod , @classmethod , and @property . What is unittest ? unittest is a built-in Python module that helps you test your code. Think of it like a way to check if your code is doing what it's supposed to. Here\u2019s a simple breakdown:","title":"Key Points"},{"location":"Python/1.1.1_decorators/#why-use-unittest","text":"Imagine you\u2019re building a machine that makes coffee. You'd want to test if it makes coffee properly, right? unittest does something similar for your code. It helps you check if your functions and classes are working as expected.","title":"Why Use unittest?"},{"location":"Python/1.1.1_decorators/#how-does-it-work","text":"Write Test Cases : You write test cases that check if your code is correct. For example, if you have a function that adds two numbers, you\u2019d write a test to check if it actually adds them correctly. Run Tests : You run these test cases to see if your code passes all the checks. If everything works fine, you\u2019re good to go. If not, you\u2019ll see where things went wrong and can fix them.","title":"How Does It Work?"},{"location":"Python/1.1.1_decorators/#basic-example","text":"Here's how you might use unittest : import unittest # Function to test def add(a, b): return a + b # Test case class TestAddFunction(unittest.TestCase): def test_add_positive(self): self.assertEqual(add(2, 3), 5) # Test if 2 + 3 equals 5 def test_add_negative(self): self.assertEqual(add(-1, -1), -2) # Test if -1 + -1 equals -2 # Run the tests if __name__ == \"__main__\": unittest.main() In this example, unittest helps us test if our add function works with positive and negative numbers.","title":"Basic Example"},{"location":"Python/1.1.1_decorators/#why-it-matters","text":"Using unittest helps you catch bugs early, so you don't find out about them later when it's harder to fix. It\u2019s like checking your coffee machine before your guests arrive to make sure it works! So, unittest is a handy tool to make sure your code does what it\u2019s supposed to do, making your coding life easier and your code more reliable.","title":"Why It Matters"},{"location":"Python/1.1.2_argv/","text":"argv argv stands for \"argument vector.\" It\u2019s a feature in Python that allows you to pass command-line arguments to your script when you run it. This is useful when you want your script to handle different inputs without changing the code itself. How It Works When you run a Python script from the command line, you can provide extra information after the script name. For example: python myscript.py arg1 arg2 arg3 In this case, arg1 , arg2 , and arg3 are command-line arguments. argv helps you capture and use these arguments in your script. Using argv in a Script Here\u2019s a simple example: import sys # Print all arguments print(\"All arguments:\", sys.argv) # Access individual arguments if len(sys.argv) > 1: print(\"First argument:\", sys.argv[1]) Explanation sys.argv is a list that contains all command-line arguments passed to the script. sys.argv[0] is always the script name ( myscript.py in this case). sys.argv[1] , sys.argv[2] , etc., are the actual arguments you pass to the script. Example If you run: python myscript.py hello world The output will be: All arguments: ['myscript.py', 'hello', 'world'] First argument: hello Why It\u2019s Useful argv is handy for creating scripts that need to handle different inputs or configurations without hardcoding values. For example, a script that processes files can use argv to specify which file to process, making the script flexible and reusable.","title":"argv"},{"location":"Python/1.1.2_argv/#argv","text":"argv stands for \"argument vector.\" It\u2019s a feature in Python that allows you to pass command-line arguments to your script when you run it. This is useful when you want your script to handle different inputs without changing the code itself.","title":"argv"},{"location":"Python/1.1.2_argv/#how-it-works","text":"When you run a Python script from the command line, you can provide extra information after the script name. For example: python myscript.py arg1 arg2 arg3 In this case, arg1 , arg2 , and arg3 are command-line arguments. argv helps you capture and use these arguments in your script.","title":"How It Works"},{"location":"Python/1.1.2_argv/#using-argv-in-a-script","text":"Here\u2019s a simple example: import sys # Print all arguments print(\"All arguments:\", sys.argv) # Access individual arguments if len(sys.argv) > 1: print(\"First argument:\", sys.argv[1])","title":"Using argv in a Script"},{"location":"Python/1.1.2_argv/#explanation","text":"sys.argv is a list that contains all command-line arguments passed to the script. sys.argv[0] is always the script name ( myscript.py in this case). sys.argv[1] , sys.argv[2] , etc., are the actual arguments you pass to the script.","title":"Explanation"},{"location":"Python/1.1.2_argv/#example","text":"If you run: python myscript.py hello world The output will be: All arguments: ['myscript.py', 'hello', 'world'] First argument: hello","title":"Example"},{"location":"Python/1.1.2_argv/#why-its-useful","text":"argv is handy for creating scripts that need to handle different inputs or configurations without hardcoding values. For example, a script that processes files can use argv to specify which file to process, making the script flexible and reusable.","title":"Why It\u2019s Useful"},{"location":"Python/1.1.3_Diff_And_Patch/","text":"Understanding diff and patch When working with code, especially with a team, it's important to handle changes efficiently. Two handy tools for this are diff and patch . diff The diff command helps you compare two files line by line and shows you the differences. This makes it easy to see what changes have been made. Example: Suppose you have two versions of a Python file, file1.py and file2.py . You want to check what changes were made from file1.py to file2.py . diff file1.py file2.py The output might look like this: 3c3 < print(\"Hello, World!\") --- > print(\"Hello, Universe!\") This output means that line 3 in file1.py , which prints \"Hello, World!\", has been changed to print \"Hello, Universe!\" in file2.py . patch The patch command lets you apply changes to a file based on a diff file. This is useful for sharing updates or fixes without needing to send the entire file. Example: You have a diff file, changes.diff , which contains the differences between two versions of a file. You want to apply these changes to file1.py . patch file1.py < changes.diff This command updates file1.py with the changes specified in changes.diff .","title":"diffAndpatch"},{"location":"Python/1.1.3_Diff_And_Patch/#understanding-diff-and-patch","text":"When working with code, especially with a team, it's important to handle changes efficiently. Two handy tools for this are diff and patch .","title":"Understanding diff and patch"},{"location":"Python/1.1.3_Diff_And_Patch/#diff","text":"The diff command helps you compare two files line by line and shows you the differences. This makes it easy to see what changes have been made. Example: Suppose you have two versions of a Python file, file1.py and file2.py . You want to check what changes were made from file1.py to file2.py . diff file1.py file2.py The output might look like this: 3c3 < print(\"Hello, World!\") --- > print(\"Hello, Universe!\") This output means that line 3 in file1.py , which prints \"Hello, World!\", has been changed to print \"Hello, Universe!\" in file2.py .","title":"diff"},{"location":"Python/1.1.3_Diff_And_Patch/#patch","text":"The patch command lets you apply changes to a file based on a diff file. This is useful for sharing updates or fixes without needing to send the entire file. Example: You have a diff file, changes.diff , which contains the differences between two versions of a file. You want to apply these changes to file1.py . patch file1.py < changes.diff This command updates file1.py with the changes specified in changes.diff .","title":"patch"},{"location":"Python/1.1.3_error_handling/","text":"Understanding try and except in Python In Python, the try and except statements are used to handle errors and exceptions gracefully. This is very useful when you want your program to keep running even if something goes wrong. How try and except Work try Block : The code that might cause an error is put inside the try block. Python tries to execute this code. except Block : If an error occurs in the try block, the execution jumps to the except block. You can specify the type of error you want to handle. Example: Counting Character Frequency in a File Let\u2019s look at an example that reads a file does something with it. We\u2019ll use try and except to handle any errors that might occur when opening the file. def character_frequency(filename): \"\"\"Counts the frequency of each character in the given file.\"\"\" # First, try to open the file try: f = open(filename) except OSError: return None # Return None if the file cannot be opened # Now, process the file Raising Errors in Python In Python, you can raise (or trigger) errors intentionally using the raise statement. This is useful when you want to raise errors if certain conditions arise. This approach can help in debugging. Why Raise Errors? Input Validation : Ensure that the input to a function is valid. Preventing Illegal Operations : Stop the program from performing operations that aren't allowed. Custom Error Messages : Provide clear messages to help understand what went wrong. How to Raise Errors You can raise built-in exceptions or define your own custom exceptions. Here\u2019s how: Raising Built-in Exceptions Python has several built-in exceptions like ValueError , TypeError , IndexError , etc. You can raise these using the raise statement. Example: Raising a ValueError def divide(a, b): if b == 0: raise ValueError(\"The denominator cannot be zero.\") return a / b try: result = divide(10, 0) except ValueError as e: print(e) In this example: - The function divide raises a ValueError if the denominator is zero. - The try block catches the error and prints the error message. Custom Exceptions You can define your own exceptions by creating a class that inherits from the Exception class. Example: Custom Exception class NegativeNumberError(Exception): pass def check_positive(number): if number < 0: raise NegativeNumberError(\"Negative numbers are not allowed.\") return number try: print(check_positive(-5)) except NegativeNumberError as e: print(e) In this example: - NegativeNumberError is a custom exception. - The function check_positive raises NegativeNumberError if the input is negative. - The try block catches the error and prints the message. Common Use Cases Validating Function Arguments ```python def sqrt(x): if x < 0: raise ValueError(\"Cannot compute the square root of a negative number.\") return x ** 0.5 try: print(sqrt(-9)) except ValueError as e: print(e) ``` Handling Invalid States ```python def withdraw(amount, balance): if amount > balance: raise RuntimeError(\"Insufficient funds.\") return balance - amount try: print(withdraw(100, 50)) except RuntimeError as e: print(e) ``` Summary Raising Errors : Use the raise statement to trigger exceptions. Built-in Exceptions : Raise common errors like ValueError and TypeError . Custom Exceptions : Create custom error types for specific situations. assert statements","title":"Error Handling"},{"location":"Python/1.1.3_error_handling/#understanding-try-and-except-in-python","text":"In Python, the try and except statements are used to handle errors and exceptions gracefully. This is very useful when you want your program to keep running even if something goes wrong.","title":"Understanding try and except in Python"},{"location":"Python/1.1.3_error_handling/#how-try-and-except-work","text":"try Block : The code that might cause an error is put inside the try block. Python tries to execute this code. except Block : If an error occurs in the try block, the execution jumps to the except block. You can specify the type of error you want to handle.","title":"How try and except Work"},{"location":"Python/1.1.3_error_handling/#example-counting-character-frequency-in-a-file","text":"Let\u2019s look at an example that reads a file does something with it. We\u2019ll use try and except to handle any errors that might occur when opening the file. def character_frequency(filename): \"\"\"Counts the frequency of each character in the given file.\"\"\" # First, try to open the file try: f = open(filename) except OSError: return None # Return None if the file cannot be opened # Now, process the file","title":"Example: Counting Character Frequency in a File"},{"location":"Python/1.1.3_error_handling/#raising-errors-in-python","text":"In Python, you can raise (or trigger) errors intentionally using the raise statement. This is useful when you want to raise errors if certain conditions arise. This approach can help in debugging.","title":"Raising Errors in Python"},{"location":"Python/1.1.3_error_handling/#why-raise-errors","text":"Input Validation : Ensure that the input to a function is valid. Preventing Illegal Operations : Stop the program from performing operations that aren't allowed. Custom Error Messages : Provide clear messages to help understand what went wrong.","title":"Why Raise Errors?"},{"location":"Python/1.1.3_error_handling/#how-to-raise-errors","text":"You can raise built-in exceptions or define your own custom exceptions. Here\u2019s how:","title":"How to Raise Errors"},{"location":"Python/1.1.3_error_handling/#raising-built-in-exceptions","text":"Python has several built-in exceptions like ValueError , TypeError , IndexError , etc. You can raise these using the raise statement. Example: Raising a ValueError def divide(a, b): if b == 0: raise ValueError(\"The denominator cannot be zero.\") return a / b try: result = divide(10, 0) except ValueError as e: print(e) In this example: - The function divide raises a ValueError if the denominator is zero. - The try block catches the error and prints the error message.","title":"Raising Built-in Exceptions"},{"location":"Python/1.1.3_error_handling/#custom-exceptions","text":"You can define your own exceptions by creating a class that inherits from the Exception class. Example: Custom Exception class NegativeNumberError(Exception): pass def check_positive(number): if number < 0: raise NegativeNumberError(\"Negative numbers are not allowed.\") return number try: print(check_positive(-5)) except NegativeNumberError as e: print(e) In this example: - NegativeNumberError is a custom exception. - The function check_positive raises NegativeNumberError if the input is negative. - The try block catches the error and prints the message.","title":"Custom Exceptions"},{"location":"Python/1.1.3_error_handling/#common-use-cases","text":"Validating Function Arguments ```python def sqrt(x): if x < 0: raise ValueError(\"Cannot compute the square root of a negative number.\") return x ** 0.5 try: print(sqrt(-9)) except ValueError as e: print(e) ``` Handling Invalid States ```python def withdraw(amount, balance): if amount > balance: raise RuntimeError(\"Insufficient funds.\") return balance - amount try: print(withdraw(100, 50)) except RuntimeError as e: print(e) ```","title":"Common Use Cases"},{"location":"Python/1.1.3_error_handling/#summary","text":"Raising Errors : Use the raise statement to trigger exceptions. Built-in Exceptions : Raise common errors like ValueError and TypeError . Custom Exceptions : Create custom error types for specific situations. assert statements","title":"Summary"},{"location":"Python/1.1.4_pdb/","text":"Using pdb for Debugging Debugging is an important part of writing code, and Python has a built-in tool called pdb to help with this. What is pdb ? pdb is Python's interactive debugger. It lets you pause your code, inspect variables, and step through the code line by line to find out what's going wrong. Setting Breakpoints Breakpoints allow you to pause your code at specific points to see what's happening at that moment. Example: In your Python script, you can set a breakpoint like this: import pdb; pdb.set_trace() def add(a, b): return a + b result = add(2, 3) print(result) When you run this script, it will pause before returning the result, allowing you to inspect variables and step through the code. Stepping Through Code pdb lets you execute your code line by line to see what happens at each step. Example: Using the same script, once pdb pauses execution, you can step through the code. (pdb) n # 'n' is for next line (pdb) p a # 'p' is for print 2 (pdb) p b 3 (pdb) c # 'c' is for continue Inspecting and Modifying Variables You can check and change the values of variables at any point during execution. Example: While debugging, you might find the value of a is not as expected. You can modify it. (pdb) p a 2 (pdb) a = 5 (pdb) p a 5 Evaluating Expressions pdb lets you run Python expressions interactively, so you can test code snippets in real-time. Example: (pdb) p a + b 8 Why Use pdb ? Using pdb gives you a deeper understanding of your code's execution flow compared to just using print statements. Instead of scattering print statements throughout your code, you can use pdb to explore and diagnose issues interactively. This makes debugging more efficient and helps you find the root cause of problems more effectively.","title":"pdb"},{"location":"Python/1.1.4_pdb/#using-pdb-for-debugging","text":"Debugging is an important part of writing code, and Python has a built-in tool called pdb to help with this.","title":"Using pdb for Debugging"},{"location":"Python/1.1.4_pdb/#what-is-pdb","text":"pdb is Python's interactive debugger. It lets you pause your code, inspect variables, and step through the code line by line to find out what's going wrong.","title":"What is pdb?"},{"location":"Python/1.1.4_pdb/#setting-breakpoints","text":"Breakpoints allow you to pause your code at specific points to see what's happening at that moment. Example: In your Python script, you can set a breakpoint like this: import pdb; pdb.set_trace() def add(a, b): return a + b result = add(2, 3) print(result) When you run this script, it will pause before returning the result, allowing you to inspect variables and step through the code.","title":"Setting Breakpoints"},{"location":"Python/1.1.4_pdb/#stepping-through-code","text":"pdb lets you execute your code line by line to see what happens at each step. Example: Using the same script, once pdb pauses execution, you can step through the code. (pdb) n # 'n' is for next line (pdb) p a # 'p' is for print 2 (pdb) p b 3 (pdb) c # 'c' is for continue","title":"Stepping Through Code"},{"location":"Python/1.1.4_pdb/#inspecting-and-modifying-variables","text":"You can check and change the values of variables at any point during execution. Example: While debugging, you might find the value of a is not as expected. You can modify it. (pdb) p a 2 (pdb) a = 5 (pdb) p a 5","title":"Inspecting and Modifying Variables"},{"location":"Python/1.1.4_pdb/#evaluating-expressions","text":"pdb lets you run Python expressions interactively, so you can test code snippets in real-time. Example: (pdb) p a + b 8","title":"Evaluating Expressions"},{"location":"Python/1.1.4_pdb/#why-use-pdb","text":"Using pdb gives you a deeper understanding of your code's execution flow compared to just using print statements. Instead of scattering print statements throughout your code, you can use pdb to explore and diagnose issues interactively. This makes debugging more efficient and helps you find the root cause of problems more effectively.","title":"Why Use pdb?"},{"location":"Python/1.1.5_pyformat/","text":"# Using `%` and `.format()` in Python ## Introduction In Python, formatting strings is a common task. Two popular methods to do this are using the `%` operator and the `.format()` method. Let's understand how to use them with simple examples. ## The `%` Operator The `%` operator is a traditional way to format strings. It places values in a string using placeholders. ### Example: name = \"Alice\" age = 30 formatted_string = \"My name is %s and I am %d years old.\" % (name, age) print(formatted_string) Output: My name is Alice and I am 30 years old. Here, `%s` is a placeholder for a string, and `%d` is a placeholder for an integer. ### More Placeholders: - `%s` - String - `%d` - Integer - `%f` - Floating-point number ## The `.format()` Method The `.format()` method is more flexible and was introduced in Python 3. It uses curly braces `{}` as placeholders in a string. ### Example: name = \"Bob\" age = 25 formatted_string = \"My name is {} and I am {} years old.\".format(name, age) print(formatted_string) Output: My name is Bob and I am 25 years old. Here, `{}` are placeholders that get replaced by values passed to the `format()` method. ### Using Index Numbers: You can specify the order of values using index numbers inside `{}`. #### Example: formatted_string = \"My name is {0} and I am {1} years old. {0} loves coding.\".format(name, age) print(formatted_string) Output: My name is Bob and I am 25 years old. Bob loves coding. ### Using Keywords: You can also use keywords to make the code more readable. #### Example: formatted_string = \"My name is {name} and I am {age} years old.\".format(name=\"Charlie\", age=22) print(formatted_string) Output: My name is Charlie and I am 22 years old. ## Conclusion Both `%` and `.format()` are useful for formatting strings in Python. The `%` operator is simple and good for basic formatting. The `.format()` method is more powerful and flexible for more complex string formatting needs. Happy coding!","title":"format method"},{"location":"Python/1.10_Func_Modl_Summary/","text":"Functions, Methods, Modules, and Libraries in Python Here, I have consolidated all the points related to Functions, Methods, Modules and Library in Python. Hope you will find it useful. Type Description Examples Functions Standalone blocks of code inside a def . math.sqrt() , os.path.exists() , json.loads() Methods Functions within a class, always linked to objects. list.append() , str.upper() , dict.items() Modules A Python file with a .py extension containing functions, classes, and other code. import math from ikea import add Libraries A collection of modules. Libraries contain similar modules for similar tasks. import pandas as pd Importing Modules and Differences Syntax Description Example Difference import module Imports the full module. Use moduleName.functionName to access functions. import math print(math.sqrt(16)) Keeps the namespace clean, avoids naming conflicts. from module import function Imports specific functions directly, no need for the module name prefix. from ikea import add print(add(1, 2)) Convenient for using specific functions frequently. from module import * Imports all functions, classes, and variables from the module directly into the current namespace. from math import * print(sqrt(16)) Can cause naming conflicts, harder to track where functions/classes come from. Generally not recommended for larger codebases. Installing Libraries Command Description Example pip install libraryname Installs a library from the command prompt or terminal. pip install pandas os.system('pip install libraryname') Installs a library from within your Python code using the os module to run shell commands. import os os.system('pip install pandas') Managing Libraries When Sharing Python Code Step Description Example pip freeze > requirements.txt Create a requirements.txt file that lists all the libraries your script needs. pip freeze > requirements.txt pip install -r requirements.txt Install dependencies on another system. pip install -r requirements.txt Example Workflow Step Description Code Create the Python script Write your Python script and save it to a file. echo \"import pandas as pd ... \" > data_analysis.py Install pandas if not installed Ensure that the pandas library is installed. pip install pandas Generate requirements.txt Create a requirements.txt file listing all required libraries. pip freeze > requirements.txt Clone project or copy files Copy your project files to the target system. scp user@development_system:/path/to/project/* /path/to/local/directory/ Create virtual environment (Optional) Create a virtual environment for your project. python -m venv myenv source myenv/bin/activate Install required libraries Install the required libraries on the target system using requirements.txt . pip install -r requirements.txt Run the script Execute your Python script on the target system. python data_analysis.py","title":"LFM Summary"},{"location":"Python/1.10_Func_Modl_Summary/#functions-methods-modules-and-libraries-in-python","text":"Here, I have consolidated all the points related to Functions, Methods, Modules and Library in Python. Hope you will find it useful. Type Description Examples Functions Standalone blocks of code inside a def . math.sqrt() , os.path.exists() , json.loads() Methods Functions within a class, always linked to objects. list.append() , str.upper() , dict.items() Modules A Python file with a .py extension containing functions, classes, and other code. import math from ikea import add Libraries A collection of modules. Libraries contain similar modules for similar tasks. import pandas as pd","title":"Functions, Methods, Modules, and Libraries in Python"},{"location":"Python/1.10_Func_Modl_Summary/#importing-modules-and-differences","text":"Syntax Description Example Difference import module Imports the full module. Use moduleName.functionName to access functions. import math print(math.sqrt(16)) Keeps the namespace clean, avoids naming conflicts. from module import function Imports specific functions directly, no need for the module name prefix. from ikea import add print(add(1, 2)) Convenient for using specific functions frequently. from module import * Imports all functions, classes, and variables from the module directly into the current namespace. from math import * print(sqrt(16)) Can cause naming conflicts, harder to track where functions/classes come from. Generally not recommended for larger codebases.","title":"Importing Modules and Differences"},{"location":"Python/1.10_Func_Modl_Summary/#installing-libraries","text":"Command Description Example pip install libraryname Installs a library from the command prompt or terminal. pip install pandas os.system('pip install libraryname') Installs a library from within your Python code using the os module to run shell commands. import os os.system('pip install pandas')","title":"Installing Libraries"},{"location":"Python/1.10_Func_Modl_Summary/#managing-libraries-when-sharing-python-code","text":"Step Description Example pip freeze > requirements.txt Create a requirements.txt file that lists all the libraries your script needs. pip freeze > requirements.txt pip install -r requirements.txt Install dependencies on another system. pip install -r requirements.txt","title":"Managing Libraries When Sharing Python Code"},{"location":"Python/1.10_Func_Modl_Summary/#example-workflow","text":"Step Description Code Create the Python script Write your Python script and save it to a file. echo \"import pandas as pd ... \" > data_analysis.py Install pandas if not installed Ensure that the pandas library is installed. pip install pandas Generate requirements.txt Create a requirements.txt file listing all required libraries. pip freeze > requirements.txt Clone project or copy files Copy your project files to the target system. scp user@development_system:/path/to/project/* /path/to/local/directory/ Create virtual environment (Optional) Create a virtual environment for your project. python -m venv myenv source myenv/bin/activate Install required libraries Install the required libraries on the target system using requirements.txt . pip install -r requirements.txt Run the script Execute your Python script on the target system. python data_analysis.py","title":"Example Workflow"},{"location":"Python/1.11_ifelifelse/","text":"Table of contents {: .text-delta } 1. TOC {:toc} if: elif: else: in Python if : Checks a condition. If true, runs the code block. elif : Checks another condition if the previous ones were false. Runs the code block if true. else : Runs the code block if all previous conditions were false. Some ifelifelse examples #demo.py def main(): print(find_email(sys.argv)) # If demo.py file is run directly (not imported as a module), then run the main() function. if __name__ == \"__main__\": main() if user_input.lower() == \"yes\": print(\"blabla\") stock = \"APPL\" NASDAQ = [\"FB\", \"APPL\", \"NVDIA\"] if stock in NASDAQ: print(\"..\") Code Examples Code Examples user_input = \"Yes\" if user_input.lower() == \"yes\" : print ( \"Yes\" ) else : print ( \"No\" ) fruit = \"apple\" fruits = [ \"apple\" , \"banana\" , \"cherry\" ] if fruit in fruits: print (f \"{fruit} is in the list\" ) else : print (f \"{fruit} is not in the list\" ) user_input = \"Yes\" if user_input.lower() == \"yes\" : print ( \"User said yes\" ) else : print ( \"User did not say yes\" ) fruit = \"apple\" fruits = [ \"apple\" , \"banana\" , \"cherry\" ] if fruit in fruits: print (f \"{fruit} is in the list\" ) else : print (f \"{fruit} is not in the list\" ) num = 10 if num % 2 == 0 : print ( \"Even\" ) else : print ( \"Odd\" ) value = \"100\" if isinstance(value, str ): print ( \"Value is a string\" ) else : print ( \"Value is not a string\" ) n = None if n is None : print ( \"n is None\" ) else : print ( \"n is not None\" ) permissions = [ 'read' , 'write' ] if 'admin' in permissions: print ( \"Has admin access\" ) elif 'write' in permissions: print ( \"Has write access\" ) else : print ( \"Has read-only access\" ) config = { \"debug\" : True } if config.get( \"debug\" ): print ( \"Debugging mode is on\" ) else : print ( \"Debugging mode is off\" ) color = \"red\" if color == \"blue\" : print ( \"Color is blue\" ) elif color == \"red\" : print ( \"Color is red\" ) else : print ( \"Color is neither blue nor red\" ) x = 3 y = \"3\" if x == int (y): print ( \"Equal values\" ) else : print ( \"Different values\" ) temperature = 35 if temperature > 30 : print ( \"It's hot\" ) elif 20 < = temperature < = 30 : print ( \"It's warm\" ) else : print ( \"It's cold\" )","title":"ifelifelse"},{"location":"Python/1.11_ifelifelse/#if-elif-else-in-python","text":"if : Checks a condition. If true, runs the code block. elif : Checks another condition if the previous ones were false. Runs the code block if true. else : Runs the code block if all previous conditions were false.","title":"if: elif: else: in Python"},{"location":"Python/1.11_ifelifelse/#some-ifelifelse-examples","text":"#demo.py def main(): print(find_email(sys.argv)) # If demo.py file is run directly (not imported as a module), then run the main() function. if __name__ == \"__main__\": main() if user_input.lower() == \"yes\": print(\"blabla\") stock = \"APPL\" NASDAQ = [\"FB\", \"APPL\", \"NVDIA\"] if stock in NASDAQ: print(\"..\") Code Examples Code Examples user_input = \"Yes\" if user_input.lower() == \"yes\" : print ( \"Yes\" ) else : print ( \"No\" ) fruit = \"apple\" fruits = [ \"apple\" , \"banana\" , \"cherry\" ] if fruit in fruits: print (f \"{fruit} is in the list\" ) else : print (f \"{fruit} is not in the list\" ) user_input = \"Yes\" if user_input.lower() == \"yes\" : print ( \"User said yes\" ) else : print ( \"User did not say yes\" ) fruit = \"apple\" fruits = [ \"apple\" , \"banana\" , \"cherry\" ] if fruit in fruits: print (f \"{fruit} is in the list\" ) else : print (f \"{fruit} is not in the list\" ) num = 10 if num % 2 == 0 : print ( \"Even\" ) else : print ( \"Odd\" ) value = \"100\" if isinstance(value, str ): print ( \"Value is a string\" ) else : print ( \"Value is not a string\" ) n = None if n is None : print ( \"n is None\" ) else : print ( \"n is not None\" ) permissions = [ 'read' , 'write' ] if 'admin' in permissions: print ( \"Has admin access\" ) elif 'write' in permissions: print ( \"Has write access\" ) else : print ( \"Has read-only access\" ) config = { \"debug\" : True } if config.get( \"debug\" ): print ( \"Debugging mode is on\" ) else : print ( \"Debugging mode is off\" ) color = \"red\" if color == \"blue\" : print ( \"Color is blue\" ) elif color == \"red\" : print ( \"Color is red\" ) else : print ( \"Color is neither blue nor red\" ) x = 3 y = \"3\" if x == int (y): print ( \"Equal values\" ) else : print ( \"Different values\" ) temperature = 35 if temperature > 30 : print ( \"It's hot\" ) elif 20 < = temperature < = 30 : print ( \"It's warm\" ) else : print ( \"It's cold\" )","title":"Some ifelifelse examples"},{"location":"Python/1.12_Operators/","text":"(1 == \"1\") is False but (1 < \"1\") is ERROR Why? When Python uses the == operator, it first checks if the two items are of the same type. If the types are different, Python will immediately return False without thinking twice. Hence, you get False and not an error. But when Python uses relational operators like ( < , > , <= , >= ), it wants the items to be of the same type. Why? Because you can't compare oranges with apples! Relational comparisons need the items to be of the same type to decide the order. Summary: Equality ( == ) : Checks if the two values are the same type. Returns `False` if the types are different, without further comparison. print(1 == \"1\") # This will return False Relational Operators ( < , > , etc.) : Require the values to be of the same or comparable types to evaluate their order. Raise a `TypeError` if the types are not comparable. print(1 < \"1\") # This will raise a TypeError Use and not &","title":"operators"},{"location":"Python/1.12_Operators/#1-1-is-false-but-1-1-is-error-why","text":"When Python uses the == operator, it first checks if the two items are of the same type. If the types are different, Python will immediately return False without thinking twice. Hence, you get False and not an error. But when Python uses relational operators like ( < , > , <= , >= ), it wants the items to be of the same type. Why? Because you can't compare oranges with apples! Relational comparisons need the items to be of the same type to decide the order.","title":"(1 == \"1\") is False but (1 &lt; \"1\") is ERROR Why?"},{"location":"Python/1.12_Operators/#use-and-not","text":"","title":"Use and not &amp;"},{"location":"Python/1.13_For_Loops/","text":"Python for loop Here us the syntax for item in sequence: Break, continue, and pass are control flow statements in Python that are used to change the behavior of loops (and in the case of pass, to do nothing). Here's a short explanation of each item: Break break : Terminates the loop completely and transfers control to the first statement after the loop. python for number in range(10): if number == 5: break print(number) This loop will print numbers 0 to 4 and then stop. Continue continue : Skips the rest of the code inside the current loop iteration and moves to the next iteration of the loop. python for number in range(10): if number % 2 == 0: continue print(number) This loop will print all odd numbers between 0 and 9. Pass pass : Does nothing and is used as a placeholder in loops, function definitions, or conditionals where syntactically some code is required but no action is needed. python for number in range(10): if number < 5: pass # Placeholder for future code else: print(number) This loop will print numbers 5 to 9, doing nothing for numbers less than 5. Else else (used with loops) : Executes when the loop completes normally (i.e., not terminated by a break statement). python for number in range(5): print(number) else: print(\"Loop completed without break\") This will print numbers 0 to 4 and then \"Loop completed without break\". Continue vs Break vs Pass continue : Skips to the next iteration of the loop. break : Exits the loop immediately. pass : Does nothing, acts as a placeholder. For loops Examples Code Examples Code Examples # Print \"Access Denied\" 5 times for _ in range(5): print(\"Access Denied\") # Using list comprehension for conditional operations numbers = [1, 2, 3, 4, 5, 6] even_numbers = [num for num in numbers if num % 2 == 0] print(even_numbers) # Output: [2, 4, 6] # Using enumerate to get index and value fruits = [\"apple\", \"banana\", \"cherry\"] for index, fruit in enumerate(fruits): print(f\"{index}: {fruit}\") # Using zip to iterate over two lists names = [\"Alice\", \"Bob\", \"Charlie\"] scores = [85, 90, 95] for name, score in zip(names, scores): print(f\"{name} scored {score}\") # Using a dictionary in a for loop user_info = {\"name\": \"Alice\", \"age\": 25, \"city\": \"New York\"} for key, value in user_info.items(): print(f\"{key}: {value}\") # Nested loops to print a multiplication table for i in range(1, 6): for j in range(1, 6): print(f\"{i} x {j} = {i * j}\") print() # Using the range function with a step for i in range(0, 20, 5): print(i) # Using a set in a for loop unique_numbers = {1, 2, 3, 4, 5} for num in unique_numbers: print(num) # Using a for loop with else numbers = [1, 2, 3, 4, 5] for number in numbers: if number == 3: print(\"Found 3!\") break else: print(\"3 not found\") # Using a generator expression in a for loop squares = (x * x for x in range(10)) for square in squares: print(square)","title":"for loops"},{"location":"Python/1.13_For_Loops/#python-for-loop","text":"Here us the syntax for item in sequence: Break, continue, and pass are control flow statements in Python that are used to change the behavior of loops (and in the case of pass, to do nothing). Here's a short explanation of each item:","title":"Python for loop"},{"location":"Python/1.13_For_Loops/#break","text":"break : Terminates the loop completely and transfers control to the first statement after the loop. python for number in range(10): if number == 5: break print(number) This loop will print numbers 0 to 4 and then stop.","title":"Break"},{"location":"Python/1.13_For_Loops/#continue","text":"continue : Skips the rest of the code inside the current loop iteration and moves to the next iteration of the loop. python for number in range(10): if number % 2 == 0: continue print(number) This loop will print all odd numbers between 0 and 9.","title":"Continue"},{"location":"Python/1.13_For_Loops/#pass","text":"pass : Does nothing and is used as a placeholder in loops, function definitions, or conditionals where syntactically some code is required but no action is needed. python for number in range(10): if number < 5: pass # Placeholder for future code else: print(number) This loop will print numbers 5 to 9, doing nothing for numbers less than 5.","title":"Pass"},{"location":"Python/1.13_For_Loops/#else","text":"else (used with loops) : Executes when the loop completes normally (i.e., not terminated by a break statement). python for number in range(5): print(number) else: print(\"Loop completed without break\") This will print numbers 0 to 4 and then \"Loop completed without break\".","title":"Else"},{"location":"Python/1.13_For_Loops/#continue-vs-break-vs-pass","text":"continue : Skips to the next iteration of the loop. break : Exits the loop immediately. pass : Does nothing, acts as a placeholder.","title":"Continue vs Break vs Pass"},{"location":"Python/1.13_For_Loops/#for-loops-examples","text":"Code Examples Code Examples # Print \"Access Denied\" 5 times for _ in range(5): print(\"Access Denied\") # Using list comprehension for conditional operations numbers = [1, 2, 3, 4, 5, 6] even_numbers = [num for num in numbers if num % 2 == 0] print(even_numbers) # Output: [2, 4, 6] # Using enumerate to get index and value fruits = [\"apple\", \"banana\", \"cherry\"] for index, fruit in enumerate(fruits): print(f\"{index}: {fruit}\") # Using zip to iterate over two lists names = [\"Alice\", \"Bob\", \"Charlie\"] scores = [85, 90, 95] for name, score in zip(names, scores): print(f\"{name} scored {score}\") # Using a dictionary in a for loop user_info = {\"name\": \"Alice\", \"age\": 25, \"city\": \"New York\"} for key, value in user_info.items(): print(f\"{key}: {value}\") # Nested loops to print a multiplication table for i in range(1, 6): for j in range(1, 6): print(f\"{i} x {j} = {i * j}\") print() # Using the range function with a step for i in range(0, 20, 5): print(i) # Using a set in a for loop unique_numbers = {1, 2, 3, 4, 5} for num in unique_numbers: print(num) # Using a for loop with else numbers = [1, 2, 3, 4, 5] for number in numbers: if number == 3: print(\"Found 3!\") break else: print(\"3 not found\") # Using a generator expression in a for loop squares = (x * x for x in range(10)) for square in squares: print(square)","title":"For loops Examples"},{"location":"Python/1.14_enumerate/","text":"Python enumerate() function Enumerate is a buil-in funciton in Python. Using this you can loop over a list (or any iterable) and at the same time you will have a counter telling you which round you are in. So you will get outputs like 0a, 1b, 2c.. So, using enumerate, you get an index(trip counter) and item(item) in the list. for index, item in enumerate(sequence):","title":"1.14 enumerate"},{"location":"Python/1.14_enumerate/#python-enumerate-function","text":"Enumerate is a buil-in funciton in Python. Using this you can loop over a list (or any iterable) and at the same time you will have a counter telling you which round you are in. So you will get outputs like 0a, 1b, 2c.. So, using enumerate, you get an index(trip counter) and item(item) in the list. for index, item in enumerate(sequence):","title":"Python enumerate() function"},{"location":"Python/1.15_range_function/","text":"Python range() function The range() function gives you a bunch of numbers . An ordered bunch of numbers . Simple series like 1, 2, 3, 4 . # A typical use of range. This gives you 0, 1, 2 range(3) Now, the output type of the range() function is a range object. Don't get confused, the output of range() is always a bunch of numbers. The range() function and for loops are like best buddies in Python. One creates a sequence of numbers, and the other loops through it. range() syntax range() examples print(*(i for i in range(3))) 0 1 2 range(3) -> 0, 1, 2 range(10)[-1] 9 print(*(fruits[i] for i in range(len(fruits)))) apple banana cherry date * unpacks the items print(*(i for i in range(10, 0, -2))) 10 8 6 4 2 Backwards using minus number list ( range ( 5 )) [0,1,2,3,4] print(*(fruits[i] for i in range(len(fruits))))","title":"range function"},{"location":"Python/1.15_range_function/#python-range-function","text":"The range() function gives you a bunch of numbers . An ordered bunch of numbers . Simple series like 1, 2, 3, 4 . # A typical use of range. This gives you 0, 1, 2 range(3) Now, the output type of the range() function is a range object. Don't get confused, the output of range() is always a bunch of numbers. The range() function and for loops are like best buddies in Python. One creates a sequence of numbers, and the other loops through it.","title":"Python range() function"},{"location":"Python/1.15_range_function/#range-syntax","text":"","title":"range() syntax"},{"location":"Python/1.15_range_function/#range-examples","text":"print(*(i for i in range(3))) 0 1 2 range(3) -> 0, 1, 2 range(10)[-1] 9 print(*(fruits[i] for i in range(len(fruits)))) apple banana cherry date * unpacks the items print(*(i for i in range(10, 0, -2))) 10 8 6 4 2 Backwards using minus number list ( range ( 5 )) [0,1,2,3,4] print(*(fruits[i] for i in range(len(fruits))))","title":"range() examples"},{"location":"Python/1.16_built_in_functions/","text":"Table of contents {: .text-delta } 1. TOC {:toc} os Library in Python Note : Import os at the beginning of your script: import os Function Example Get Current Working Directory cwd = os.getcwd() print(f\"Current Working Directory: {cwd}\") Change Directory os.chdir(\"/path/to/directory\") print(f\"Changed Directory: {os.getcwd()}\") List Directory Contents contents = os.listdir(\".\") print(f\"Directory Contents: {contents}\") Make New Directory os.makedirs(\"new_dir/sub_dir\", exist_ok=True) print(\"Directories created\") Remove Directory os.rmdir(\"new_dir/sub_dir\") print(\"Sub-directory removed\") Remove File os.remove(\"example.txt\") print(\"File removed\") Rename File or Directory os.rename(\"old_name.txt\", \"new_name.txt\") print(\"File renamed\") Get File Size size = os.path.getsize(\"example.txt\") print(f\"File size: {size} bytes\") Check if Path Exists exists = os.path.exists(\"example.txt\") print(f\"Path exists: {exists}\") Check if Path is File is_file = os.path.isfile(\"example.txt\") print(f\"Is a file: {is_file}\") Check if Path is Directory is_dir = os.path.isdir(\"example\") print(f\"Is a directory: {is_dir}\") Join Paths path = os.path.join(\"folder\", \"subfolder\", \"file.txt\") print(f\"Joined path: {path}\") Get Absolute Path absolute_path = os.path.abspath(\"example.txt\") print(f\"Absolute path: {absolute_path}\") Get Directory Name dir_name = os.path.dirname(\"/path/to/example.txt\") print(f\"Directory name: {dir_name}\") Get Base Name base_name = os.path.basename(\"/path/to/example.txt\") print(f\"Base name: {base_name}\") Split Path head, tail = os.path.split(\"/path/to/example.txt\") print(f\"Head: {head}, Tail: {tail}\") Split File Extension root, ext = os.path.splitext(\"example.txt\") print(f\"Root: {root}, Extension: {ext}\") Execute a Shell Command os.system(\"echo Hello World\") Get Environment Variables path = os.getenv(\"PATH\") print(f\"PATH: {path}\") Set Environment Variables os.environ[\"MY_VAR\"] = \"my_value\" print(f\"MY_VAR: {os.getenv('MY_VAR')}\") Walk Through Directory Tree for dirpath, dirnames, filenames in os.walk(\".\"): print(f\"Found directory: {dirpath}\") for file_name in filenames: print(f\"\\t{file_name}\") File Permissions os.chmod(\"example.txt\", 0o644) print(\"Permissions changed\") File Ownership os.chown(\"example.txt\", uid, gid) print(\"Ownership changed\") Create a Symbolic Link os.symlink(\"source.txt\", \"link.txt\") print(\"Symbolic link created\") Create a Hard Link os.link(\"source.txt\", \"hard_link.txt\") print(\"Hard link created\") Get File Statistics stats = os.stat(\"example.txt\") print(f\"File statistics: {stats}\") os.path in Python os.path is a part of Python's standard library that helps you work with file paths easily. os.path functions work on all operating systems like Windows, macOS, and Linux. You don\u2019t have to worry about different path styles on different systems. Here is an example which will include all main functianlities of os.path. import os # Example file path example_path = \"example/directory/file.txt\" # Get the directory name directory = os.path.dirname(example_path) print(f\"Directory name: {directory}\") # Output: example/directory # Get the base name file_name = os.path.basename(example_path) print(f\"File name: {file_name}\") # Output: file.txt # Join paths joined_path = os.path.join(\"example\", \"directory\", \"newfile.txt\") print(f\"Joined path: {joined_path}\") # Output: example/directory/newfile.txt # Check if path exists path_exists = os.path.exists(example_path) print(f\"Path exists: {path_exists}\") # Check if it is a file is_file = os.path.isfile(example_path) print(f\"Is a file: {is_file}\") # Check if it is a directory is_directory = os.path.isdir(directory) print(f\"Is a directory: {is_directory}\") # Get the absolute path absolute_path = os.path.abspath(example_path) print(f\"Absolute path: {absolute_path}\") # Split the path into directory and file name split_path = os.path.split(example_path) print(f\"Split path: {split_path}\") # Output: ('example/directory', 'file.txt') # Get the file extension file_extension = os.path.splitext(example_path)[1] print(f\"File extension: {file_extension}\") # Output: .txt # Example relative path relative_path = os.path.relpath(example_path, start=\"example\") print(f\"Relative path: {relative_path}\") # Output: directory/file.txt # Check the size of the file (only if it exists) if os.path.exists(example_path): file_size = os.path.getsize(example_path) print(f\"File size: {file_size} bytes\") # Check the last modification time (only if it exists) if os.path.exists(example_path): modification_time = os.path.getmtime(example_path) print(f\"Last modification time: {modification_time}\") # Demonstrate creating a new directory new_directory_path = \"example/new_directory\" if not os.path.exists(new_directory_path): os.makedirs(new_directory_path) print(f\"Created new directory: {new_directory_path}\") else: print(f\"Directory already exists: {new_directory_path}\") This program shows the following os.path functions: - os.path.dirname() : Get the directory name from a path. - os.path.basename() : Get the file name from a path. - os.path.join() : Join multiple path components. - os.path.exists() : Check if a path exists. - os.path.isfile() : Check if a path is a file. - os.path.isdir() : Check if a path is a directory. - os.path.abspath() : Get the absolute path. - os.path.split() : Split a path into directory and file name. - os.path.splitext() : Get the file extension. - os.path.relpath() : Get the relative path. - os.path.getsize() : Get the size of a file. - os.path.getmtime() : Get the last modification time of a file. - os.makedirs() : Create a new directory. Here is an example showing how os.path can be used so that the code handles both Windows and MAC folder systems without problems: import os # Example file path components folder = \"example\" subfolder = \"directory\" filename = \"file.txt\" # Join the components into a full path full_path = os.path.join(folder, subfolder, filename) print(f\"Full path: {full_path}\") # Check if the path exists path_exists = os.path.exists(full_path) print(f\"Path exists: {path_exists}\") # Get the absolute path absolute_path = os.path.abspath(full_path) print(f\"Absolute path: {absolute_path}\") # Example output for different operating systems if os.name == 'nt': # Windows print(f\"Windows style path: {full_path}\") else: # Mac/Linux print(f\"Unix style path: {full_path}\") By using os.path , you can make sure the code works in both windows and Linux/Mac and handles (backslash \\ for Windows and forward slash / for Mac/Linux). Python built-in functions These are built-in functions that are available to Python interpreter always. You don't have to pip install anything for this. I compiled the list from the following link Function Description & Syntax Usage Example abs(x) : Returns the absolute value of a number. abs(-5) returns 5 aiter(iterable) : Returns an asynchronous iterator for an asynchronous iterable. async for item in aiter(async_iterable): pass all(iterable) : Returns True if all elements of an iterable are true. all([True, True, False]) returns False anext(async_iterator) : Retrieves the next item from an asynchronous iterator. await anext(async_iterator) any(iterable) : Returns True if any element of an iterable is true. any([False, True, False]) returns True ascii(object) : Returns a string representation of an object with non-ASCII characters escaped. ascii('\u00fc') returns '\\\\xfc' bin(x) : Converts an integer to a binary string. bin(10) returns '0b1010' bool(x) : Converts a value to a boolean, returning either True or False. bool(1) returns True breakpoint() : Drops you into the debugger at the call site. breakpoint() bytearray([source[, encoding[, errors]]]) : Returns a new array of bytes. bytearray(b'hello') returns bytearray(b'hello') bytes([source[, encoding[, errors]]]) : Returns a new bytes object. bytes('hello', 'utf-8') returns b'hello' callable(object) : Returns True if the object appears callable. callable(len) returns True chr(i) : Returns the character that represents the specified unicode. chr(97) returns 'a' classmethod(function) : Converts a method into a class method. class C: @classmethod def f(cls): pass compile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1) : Compiles source into a code object that can be executed by exec() or eval(). compile('print(1)', '', 'exec') complex([real[, imag]]) : Creates a complex number. complex(1, 2) returns (1+2j) delattr(object, name) : Deletes the named attribute from an object. delattr(obj, 'attr') dict(**kwargs) : Creates a new dictionary. dict(a=1, b=2) returns {'a': 1, 'b': 2} dir([object]) : Returns a list of the attributes and methods of any object. dir() divmod(a, b) : Returns a tuple containing the quotient and remainder when dividing two numbers. divmod(10, 3) returns (3, 1) enumerate(iterable, start=0) : Returns an enumerate object. enumerate(['a', 'b'], 1) returns [(1, 'a'), (2, 'b')] eval(expression, globals=None, locals=None) : Evaluates the specified expression. eval('1 + 2') returns 3 exec(object[, globals[, locals]]) : Executes the specified code. exec('print(\"Hello, World!\")') filter(function, iterable) : Constructs an iterator from elements of iterable for which function returns true. filter(lambda x: x > 0, [1, -2, 3, 0]) float([x]) : Converts a number or string to a floating point number. float('3.14') returns 3.14 format(value[, format_spec]) : Formats a value using a format specification. format(255, '02x') returns 'ff' frozenset([iterable]) : Returns a new frozenset object, optionally with elements taken from iterable. frozenset([1, 2, 3]) returns frozenset({1, 2, 3}) getattr(object, name[, default]) : Returns the value of the named attribute of an object. getattr(obj, 'attr', None) globals() : Returns a dictionary representing the current global symbol table. globals() hasattr(object, name) : Returns True if the object has the named attribute. hasattr(obj, 'attr') hash(object) : Returns the hash value of the object. hash('hello') help([object]) : Invokes the built-in help system. help(print) hex(x) : Converts an integer to a hexadecimal string. hex(255) returns '0xff' id(object) : Returns the identity of an object. id(3) input([prompt]) : Reads a line from input. input('Enter your name: ') int([x[, base]]) : Converts a number or string to an integer. int('10') returns 10 isinstance(object, classinfo) : Returns True if the object is an instance of the class or of a subclass thereof. isinstance(3, int) issubclass(class, classinfo) : Returns True if the class is a subclass of classinfo. issubclass(bool, int) iter(object[, sentinel]) : Returns an iterator object. iter([1, 2, 3]) len(s) : Returns the length of an object. len('hello') returns 5 list([iterable]) : Creates a new list. list('hello') returns ['h', 'e', 'l', 'l', 'o'] locals() : Updates and returns a dictionary representing the current local symbol table. locals() map(function, iterable, ...) : Applies function to every item of iterable and returns an iterator. map(str.upper, ['a', 'b', 'c']) max(iterable, *[, key, default]) : Returns the largest item in an iterable or the largest of two or more arguments. max([1, 2, 3]) memoryview(obj) : Returns a memory view object. memoryview(b'abc') min(iterable, *[, key, default]) : Returns the smallest item in an iterable or the smallest of two or more arguments. min([1, 2, 3]) next(iterator[, default]) : Retrieves the next item from the iterator. next(iter([1, 2, 3])) object() : Returns a new featureless object. object() oct(x) : Converts an integer to an octal string. oct(8) returns '0o10' open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None) : Opens a file and returns a corresponding file object. open('file.txt', 'r') ord(c) : Returns the Unicode code point for a single character. ord('a') returns 97 pow(x, y[, z]) : Returns x to the power of y; if z is present, returns x to the power of y, modulo z. pow(2, 3) returns 8 print(*objects, sep=' ', end='\\n', file=sys.stdout, flush=False) : Prints the objects to the text stream file, separated by sep and followed by end. print('hello') property(fget=None, fset=None, fdel=None, doc=None) : Returns a property attribute. class C: @property def x(self): return self._x range(stop) : Returns an immutable sequence of numbers from 0 to stop repr(object) : Returns a string containing a printable representation of an object. reversed(seq) : Returns a reversed iterator. list(reversed([1, 2, 3])) returns [3, 2, 1] round(number[, ndigits]) : Rounds a number to a given precision in decimal digits. round(3.14159, 2) returns 3.14 set([iterable]) : Returns a new set object, optionally with elements taken from iterable. set([1, 2, 2, 3]) returns {1, 2, 3} setattr(object, name, value) : Sets the value of the named attribute of an object. setattr(obj, 'attr', 10) slice(stop) : Returns a slice object representing the set of indices specified by range(start, stop, step). slice(1, 5, 2) returns slice(1, 5, 2) sorted(iterable, *, key=None, reverse=False) : Returns a new sorted list from the items in iterable. sorted([3, 1, 2]) returns [1, 2, 3] staticmethod(function) : Converts a method into a static method. class C: @staticmethod def f(): pass str(object='') : Returns a string version of object. str(123) returns '123' sum(iterable, /, start=0) : Sums the items of an iterable from left to right and returns the total. sum([1, 2, 3]) returns 6 super([type[, object-or-type]]) : Returns a proxy object that delegates method calls to a parent or sibling class of type. super().method() tuple([iterable]) : Returns a tuple object, optionally with elements taken from iterable. tuple([1, 2, 3]) returns (1, 2, 3) type(object) : Returns the type of an object. type(123) returns <class 'int'> vars([object]) : Returns the __dict__ attribute for a module, class, instance, or any other object with a __dict__ attribute. vars() zip(*iterables) : Returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. list(zip([1, 2, 3], ['a', 'b', 'c'])) returns [(1, 'a'), (2, 'b'), (3, 'c')] __import__(name, globals=None, locals=None, fromlist=(), level=0) : Invoked by the import statement. __import__('math')","title":"1.16 built in functions"},{"location":"Python/1.16_built_in_functions/#os-library-in-python","text":"Note : Import os at the beginning of your script: import os Function Example Get Current Working Directory cwd = os.getcwd() print(f\"Current Working Directory: {cwd}\") Change Directory os.chdir(\"/path/to/directory\") print(f\"Changed Directory: {os.getcwd()}\") List Directory Contents contents = os.listdir(\".\") print(f\"Directory Contents: {contents}\") Make New Directory os.makedirs(\"new_dir/sub_dir\", exist_ok=True) print(\"Directories created\") Remove Directory os.rmdir(\"new_dir/sub_dir\") print(\"Sub-directory removed\") Remove File os.remove(\"example.txt\") print(\"File removed\") Rename File or Directory os.rename(\"old_name.txt\", \"new_name.txt\") print(\"File renamed\") Get File Size size = os.path.getsize(\"example.txt\") print(f\"File size: {size} bytes\") Check if Path Exists exists = os.path.exists(\"example.txt\") print(f\"Path exists: {exists}\") Check if Path is File is_file = os.path.isfile(\"example.txt\") print(f\"Is a file: {is_file}\") Check if Path is Directory is_dir = os.path.isdir(\"example\") print(f\"Is a directory: {is_dir}\") Join Paths path = os.path.join(\"folder\", \"subfolder\", \"file.txt\") print(f\"Joined path: {path}\") Get Absolute Path absolute_path = os.path.abspath(\"example.txt\") print(f\"Absolute path: {absolute_path}\") Get Directory Name dir_name = os.path.dirname(\"/path/to/example.txt\") print(f\"Directory name: {dir_name}\") Get Base Name base_name = os.path.basename(\"/path/to/example.txt\") print(f\"Base name: {base_name}\") Split Path head, tail = os.path.split(\"/path/to/example.txt\") print(f\"Head: {head}, Tail: {tail}\") Split File Extension root, ext = os.path.splitext(\"example.txt\") print(f\"Root: {root}, Extension: {ext}\") Execute a Shell Command os.system(\"echo Hello World\") Get Environment Variables path = os.getenv(\"PATH\") print(f\"PATH: {path}\") Set Environment Variables os.environ[\"MY_VAR\"] = \"my_value\" print(f\"MY_VAR: {os.getenv('MY_VAR')}\") Walk Through Directory Tree for dirpath, dirnames, filenames in os.walk(\".\"): print(f\"Found directory: {dirpath}\") for file_name in filenames: print(f\"\\t{file_name}\") File Permissions os.chmod(\"example.txt\", 0o644) print(\"Permissions changed\") File Ownership os.chown(\"example.txt\", uid, gid) print(\"Ownership changed\") Create a Symbolic Link os.symlink(\"source.txt\", \"link.txt\") print(\"Symbolic link created\") Create a Hard Link os.link(\"source.txt\", \"hard_link.txt\") print(\"Hard link created\") Get File Statistics stats = os.stat(\"example.txt\") print(f\"File statistics: {stats}\")","title":"os Library in Python"},{"location":"Python/1.16_built_in_functions/#ospath-in-python","text":"os.path is a part of Python's standard library that helps you work with file paths easily. os.path functions work on all operating systems like Windows, macOS, and Linux. You don\u2019t have to worry about different path styles on different systems. Here is an example which will include all main functianlities of os.path. import os # Example file path example_path = \"example/directory/file.txt\" # Get the directory name directory = os.path.dirname(example_path) print(f\"Directory name: {directory}\") # Output: example/directory # Get the base name file_name = os.path.basename(example_path) print(f\"File name: {file_name}\") # Output: file.txt # Join paths joined_path = os.path.join(\"example\", \"directory\", \"newfile.txt\") print(f\"Joined path: {joined_path}\") # Output: example/directory/newfile.txt # Check if path exists path_exists = os.path.exists(example_path) print(f\"Path exists: {path_exists}\") # Check if it is a file is_file = os.path.isfile(example_path) print(f\"Is a file: {is_file}\") # Check if it is a directory is_directory = os.path.isdir(directory) print(f\"Is a directory: {is_directory}\") # Get the absolute path absolute_path = os.path.abspath(example_path) print(f\"Absolute path: {absolute_path}\") # Split the path into directory and file name split_path = os.path.split(example_path) print(f\"Split path: {split_path}\") # Output: ('example/directory', 'file.txt') # Get the file extension file_extension = os.path.splitext(example_path)[1] print(f\"File extension: {file_extension}\") # Output: .txt # Example relative path relative_path = os.path.relpath(example_path, start=\"example\") print(f\"Relative path: {relative_path}\") # Output: directory/file.txt # Check the size of the file (only if it exists) if os.path.exists(example_path): file_size = os.path.getsize(example_path) print(f\"File size: {file_size} bytes\") # Check the last modification time (only if it exists) if os.path.exists(example_path): modification_time = os.path.getmtime(example_path) print(f\"Last modification time: {modification_time}\") # Demonstrate creating a new directory new_directory_path = \"example/new_directory\" if not os.path.exists(new_directory_path): os.makedirs(new_directory_path) print(f\"Created new directory: {new_directory_path}\") else: print(f\"Directory already exists: {new_directory_path}\") This program shows the following os.path functions: - os.path.dirname() : Get the directory name from a path. - os.path.basename() : Get the file name from a path. - os.path.join() : Join multiple path components. - os.path.exists() : Check if a path exists. - os.path.isfile() : Check if a path is a file. - os.path.isdir() : Check if a path is a directory. - os.path.abspath() : Get the absolute path. - os.path.split() : Split a path into directory and file name. - os.path.splitext() : Get the file extension. - os.path.relpath() : Get the relative path. - os.path.getsize() : Get the size of a file. - os.path.getmtime() : Get the last modification time of a file. - os.makedirs() : Create a new directory. Here is an example showing how os.path can be used so that the code handles both Windows and MAC folder systems without problems: import os # Example file path components folder = \"example\" subfolder = \"directory\" filename = \"file.txt\" # Join the components into a full path full_path = os.path.join(folder, subfolder, filename) print(f\"Full path: {full_path}\") # Check if the path exists path_exists = os.path.exists(full_path) print(f\"Path exists: {path_exists}\") # Get the absolute path absolute_path = os.path.abspath(full_path) print(f\"Absolute path: {absolute_path}\") # Example output for different operating systems if os.name == 'nt': # Windows print(f\"Windows style path: {full_path}\") else: # Mac/Linux print(f\"Unix style path: {full_path}\") By using os.path , you can make sure the code works in both windows and Linux/Mac and handles (backslash \\ for Windows and forward slash / for Mac/Linux).","title":"os.path in Python"},{"location":"Python/1.16_built_in_functions/#python-built-in-functions","text":"These are built-in functions that are available to Python interpreter always. You don't have to pip install anything for this. I compiled the list from the following link Function Description & Syntax Usage Example abs(x) : Returns the absolute value of a number. abs(-5) returns 5 aiter(iterable) : Returns an asynchronous iterator for an asynchronous iterable. async for item in aiter(async_iterable): pass all(iterable) : Returns True if all elements of an iterable are true. all([True, True, False]) returns False anext(async_iterator) : Retrieves the next item from an asynchronous iterator. await anext(async_iterator) any(iterable) : Returns True if any element of an iterable is true. any([False, True, False]) returns True ascii(object) : Returns a string representation of an object with non-ASCII characters escaped. ascii('\u00fc') returns '\\\\xfc' bin(x) : Converts an integer to a binary string. bin(10) returns '0b1010' bool(x) : Converts a value to a boolean, returning either True or False. bool(1) returns True breakpoint() : Drops you into the debugger at the call site. breakpoint() bytearray([source[, encoding[, errors]]]) : Returns a new array of bytes. bytearray(b'hello') returns bytearray(b'hello') bytes([source[, encoding[, errors]]]) : Returns a new bytes object. bytes('hello', 'utf-8') returns b'hello' callable(object) : Returns True if the object appears callable. callable(len) returns True chr(i) : Returns the character that represents the specified unicode. chr(97) returns 'a' classmethod(function) : Converts a method into a class method. class C: @classmethod def f(cls): pass compile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1) : Compiles source into a code object that can be executed by exec() or eval(). compile('print(1)', '', 'exec') complex([real[, imag]]) : Creates a complex number. complex(1, 2) returns (1+2j) delattr(object, name) : Deletes the named attribute from an object. delattr(obj, 'attr') dict(**kwargs) : Creates a new dictionary. dict(a=1, b=2) returns {'a': 1, 'b': 2} dir([object]) : Returns a list of the attributes and methods of any object. dir() divmod(a, b) : Returns a tuple containing the quotient and remainder when dividing two numbers. divmod(10, 3) returns (3, 1) enumerate(iterable, start=0) : Returns an enumerate object. enumerate(['a', 'b'], 1) returns [(1, 'a'), (2, 'b')] eval(expression, globals=None, locals=None) : Evaluates the specified expression. eval('1 + 2') returns 3 exec(object[, globals[, locals]]) : Executes the specified code. exec('print(\"Hello, World!\")') filter(function, iterable) : Constructs an iterator from elements of iterable for which function returns true. filter(lambda x: x > 0, [1, -2, 3, 0]) float([x]) : Converts a number or string to a floating point number. float('3.14') returns 3.14 format(value[, format_spec]) : Formats a value using a format specification. format(255, '02x') returns 'ff' frozenset([iterable]) : Returns a new frozenset object, optionally with elements taken from iterable. frozenset([1, 2, 3]) returns frozenset({1, 2, 3}) getattr(object, name[, default]) : Returns the value of the named attribute of an object. getattr(obj, 'attr', None) globals() : Returns a dictionary representing the current global symbol table. globals() hasattr(object, name) : Returns True if the object has the named attribute. hasattr(obj, 'attr') hash(object) : Returns the hash value of the object. hash('hello') help([object]) : Invokes the built-in help system. help(print) hex(x) : Converts an integer to a hexadecimal string. hex(255) returns '0xff' id(object) : Returns the identity of an object. id(3) input([prompt]) : Reads a line from input. input('Enter your name: ') int([x[, base]]) : Converts a number or string to an integer. int('10') returns 10 isinstance(object, classinfo) : Returns True if the object is an instance of the class or of a subclass thereof. isinstance(3, int) issubclass(class, classinfo) : Returns True if the class is a subclass of classinfo. issubclass(bool, int) iter(object[, sentinel]) : Returns an iterator object. iter([1, 2, 3]) len(s) : Returns the length of an object. len('hello') returns 5 list([iterable]) : Creates a new list. list('hello') returns ['h', 'e', 'l', 'l', 'o'] locals() : Updates and returns a dictionary representing the current local symbol table. locals() map(function, iterable, ...) : Applies function to every item of iterable and returns an iterator. map(str.upper, ['a', 'b', 'c']) max(iterable, *[, key, default]) : Returns the largest item in an iterable or the largest of two or more arguments. max([1, 2, 3]) memoryview(obj) : Returns a memory view object. memoryview(b'abc') min(iterable, *[, key, default]) : Returns the smallest item in an iterable or the smallest of two or more arguments. min([1, 2, 3]) next(iterator[, default]) : Retrieves the next item from the iterator. next(iter([1, 2, 3])) object() : Returns a new featureless object. object() oct(x) : Converts an integer to an octal string. oct(8) returns '0o10' open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None) : Opens a file and returns a corresponding file object. open('file.txt', 'r') ord(c) : Returns the Unicode code point for a single character. ord('a') returns 97 pow(x, y[, z]) : Returns x to the power of y; if z is present, returns x to the power of y, modulo z. pow(2, 3) returns 8 print(*objects, sep=' ', end='\\n', file=sys.stdout, flush=False) : Prints the objects to the text stream file, separated by sep and followed by end. print('hello') property(fget=None, fset=None, fdel=None, doc=None) : Returns a property attribute. class C: @property def x(self): return self._x range(stop) : Returns an immutable sequence of numbers from 0 to stop repr(object) : Returns a string containing a printable representation of an object. reversed(seq) : Returns a reversed iterator. list(reversed([1, 2, 3])) returns [3, 2, 1] round(number[, ndigits]) : Rounds a number to a given precision in decimal digits. round(3.14159, 2) returns 3.14 set([iterable]) : Returns a new set object, optionally with elements taken from iterable. set([1, 2, 2, 3]) returns {1, 2, 3} setattr(object, name, value) : Sets the value of the named attribute of an object. setattr(obj, 'attr', 10) slice(stop) : Returns a slice object representing the set of indices specified by range(start, stop, step). slice(1, 5, 2) returns slice(1, 5, 2) sorted(iterable, *, key=None, reverse=False) : Returns a new sorted list from the items in iterable. sorted([3, 1, 2]) returns [1, 2, 3] staticmethod(function) : Converts a method into a static method. class C: @staticmethod def f(): pass str(object='') : Returns a string version of object. str(123) returns '123' sum(iterable, /, start=0) : Sums the items of an iterable from left to right and returns the total. sum([1, 2, 3]) returns 6 super([type[, object-or-type]]) : Returns a proxy object that delegates method calls to a parent or sibling class of type. super().method() tuple([iterable]) : Returns a tuple object, optionally with elements taken from iterable. tuple([1, 2, 3]) returns (1, 2, 3) type(object) : Returns the type of an object. type(123) returns <class 'int'> vars([object]) : Returns the __dict__ attribute for a module, class, instance, or any other object with a __dict__ attribute. vars() zip(*iterables) : Returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. list(zip([1, 2, 3], ['a', 'b', 'c'])) returns [(1, 'a'), (2, 'b'), (3, 'c')] __import__(name, globals=None, locals=None, fromlist=(), level=0) : Invoked by the import statement. __import__('math')","title":"Python built-in functions"},{"location":"Python/1.17_withStatement/","text":"The with Statement in Python In Python, the with statement is very useful when you need to clean up after the code finishes. Cleanup includes things like closing connections and freeing memory. It also makes the code very clean and readable. In C#, we have the using function, which is similar to with . Let's find out more details about the with function in this article. What is the with Statement? The with statement ensures that resources are properly acquired and released, handling setup and cleanup tasks automatically. It guarantees that resources are released when the block of code is done, even if exceptions occur. Basic Syntax Here's the basic syntax of the with statement: Why Use with ? Automatic Resource Management: The with statement ensures that resources are released as soon as the block of code is executed, even if errors occur. Cleaner Code: Reduces the need for explicit resource management (like closing files), leading to cleaner and more readable code. Examples Let's look at some practical and funny examples to see how the with statement works. Working with Files Opening and reading a file: with open('lovers.txt', 'r') as file: content = file.read() print(\"Reading love notes...\") print(content) Writing to a file: new_lovers = [\"Romeo\", \"Juliet\", \"Casablanca\"] with open('lovers.txt', 'a') as file: for lover in new_lovers: file.write(lover + \" \u2764\ufe0f\\n\") print(f\"{lover} added to the list of lovers!\") Using Locks for Thread Safety When working with threads, you might need to use locks to prevent race conditions. The with statement makes this simple: import threading lock = threading.Lock() with lock: print(\"Securing the love triangle...\") # Critical section of code print(\"Thread-safe love triangle management\") Handling Database Connections Managing database connections can be complex, but the with statement can simplify it: Custom Context Managers You can create your own context managers using classes and the __enter__ and __exit__ methods: class LoveContext: def __enter__(self): print(\"Entering the love zone \u2764\ufe0f\") return self def __exit__(self, exc_type, exc_value, traceback): print(\"Exiting the love zone \ud83d\udc94\") with LoveContext(): print(\"Inside the love zone\") This will output: Entering the love zone \u2764\ufe0f Inside the love zone Exiting the love zone \ud83d\udc94 Conclusion The usinng function in C# is same as the with function in Python. This is an example of using using function using (StreamReader reader = new StreamReader(\"example.txt\")) { string content = reader.ReadToEnd(); Console.WriteLine(content); }","title":"with statement"},{"location":"Python/1.17_withStatement/#the-with-statement-in-python","text":"In Python, the with statement is very useful when you need to clean up after the code finishes. Cleanup includes things like closing connections and freeing memory. It also makes the code very clean and readable. In C#, we have the using function, which is similar to with . Let's find out more details about the with function in this article.","title":"The with Statement in Python"},{"location":"Python/1.17_withStatement/#what-is-the-with-statement","text":"The with statement ensures that resources are properly acquired and released, handling setup and cleanup tasks automatically. It guarantees that resources are released when the block of code is done, even if exceptions occur.","title":"What is the with Statement?"},{"location":"Python/1.17_withStatement/#basic-syntax","text":"Here's the basic syntax of the with statement:","title":"Basic Syntax"},{"location":"Python/1.17_withStatement/#why-use-with","text":"Automatic Resource Management: The with statement ensures that resources are released as soon as the block of code is executed, even if errors occur. Cleaner Code: Reduces the need for explicit resource management (like closing files), leading to cleaner and more readable code.","title":"Why Use with?"},{"location":"Python/1.17_withStatement/#examples","text":"Let's look at some practical and funny examples to see how the with statement works. Working with Files Opening and reading a file: with open('lovers.txt', 'r') as file: content = file.read() print(\"Reading love notes...\") print(content) Writing to a file: new_lovers = [\"Romeo\", \"Juliet\", \"Casablanca\"] with open('lovers.txt', 'a') as file: for lover in new_lovers: file.write(lover + \" \u2764\ufe0f\\n\") print(f\"{lover} added to the list of lovers!\") Using Locks for Thread Safety When working with threads, you might need to use locks to prevent race conditions. The with statement makes this simple: import threading lock = threading.Lock() with lock: print(\"Securing the love triangle...\") # Critical section of code print(\"Thread-safe love triangle management\") Handling Database Connections Managing database connections can be complex, but the with statement can simplify it: Custom Context Managers You can create your own context managers using classes and the __enter__ and __exit__ methods: class LoveContext: def __enter__(self): print(\"Entering the love zone \u2764\ufe0f\") return self def __exit__(self, exc_type, exc_value, traceback): print(\"Exiting the love zone \ud83d\udc94\") with LoveContext(): print(\"Inside the love zone\") This will output: Entering the love zone \u2764\ufe0f Inside the love zone Exiting the love zone \ud83d\udc94","title":"Examples"},{"location":"Python/1.17_withStatement/#conclusion","text":"The usinng function in C# is same as the with function in Python. This is an example of using using function using (StreamReader reader = new StreamReader(\"example.txt\")) { string content = reader.ReadToEnd(); Console.WriteLine(content); }","title":"Conclusion"},{"location":"Python/1.18_unittest_pytest/","text":"Introduction to pytest pytest is a popular tool for testing Python code. It's known for being easy to use and having many useful features. Key Features of pytest Simple Syntax : You write tests with plain Python functions starting with test_ . Use standard assert statements for checks. This makes it easy to start testing. Rich Assertions : pytest provides clear failure reports. If a test fails, it shows what went wrong with details on variables and expressions. Fixtures : Fixtures help set up and clean up before and after tests. They let you reuse code for preparing test data or resources. Parametrization : You can run the same test with different inputs using pytest . This saves you from writing multiple similar tests. Plugins and Extensions : There are many plugins to add extra features like running tests in parallel, checking code coverage, and more. Test Discovery : pytest finds and runs tests automatically by looking for files and functions that follow certain naming rules. These features make pytest a powerful and flexible tool for testing Python code. Writing Tests with pytest To get started with pytest , install it using pip: pip install pytest Create a test file (e.g., test_sample.py ) and write a simple test function: def test_addition(): assert 1 + 1 == 2 Run the test using the pytest command: pytest pytest will automatically find and run the test, providing a summary of the results. Advanced Features Fixtures : Define a fixture using the @pytest.fixture decorator to provide setup code for your tests: ```python import pytest @pytest.fixture def sample_data(): return [1, 2, 3] def test_sum(sample_data): assert sum(sample_data) == 6 ``` Parametrization : Use the @pytest.mark.parametrize decorator to run tests with multiple sets of parameters: ```python import pytest @pytest.mark.parametrize(\"a, b, expected\", [ (1, 1, 2), (2, 3, 5), (5, 5, 10) ]) def test_add(a, b, expected): assert a + b == expected ``` Unit test example rearrange.py #!/usr/bin/env python3 import re def rearrange_name(full_name): match = re.search(r\"^([\\w .]*), ([\\w .]*)$\", full_name) return \"{} {}\".format(match[2], match[1]) rearrange_test.py #!/usr/bin/env python3 import unittest from rearrange import rearrange_name class TestRearrange(unittest.TestCase): def test_rearrange(self): input_name = \"Lovelace, Ada\" expected_output = \"Ada Lovelace\" self.assertEqual(rearrange_name(input_name), expected_output) # Run the tests unittest.main() Running the Test chmod +x rearrange_test.py ./rearrange_test.py Unittest vs. Pytest Unittest is built into Python and uses an object-oriented approach with special assert methods. Pytest needs to be imported and uses a functional approach with simple assert statements. Unittest automatically detects tests but needs to be run from the command line. Pytest runs tests automatically with the test_ prefix. Both can work together; you can use pytest with unittest code. Choose based on your preference: unittest for built-in simplicity or pytest for easier, more readable tests. Conclusion pytest is a handy tool for testing Python code. It\u2019s easy to use, has many features, and is well-supported by the community. Ideal for both small and big projects, it helps you write reliable tests quickly. Some important concepts regarding testing Black-box test : This is a type of testing where the tester doesn\u2019t look at the internal code of the software. They just check if the software works as expected based on inputs and outputs. Integration test : This test checks if different parts of the software work well together. It ensures that the combined parts function correctly with each other and with external systems. Isolation : In testing, isolation means ensuring that a test only checks one specific part of the software. Any success or failure should be due to the part being tested, not because of something else. Regression test : This is a test done after fixing a bug to make sure that the bug doesn\u2019t come back. It checks if the new changes have affected the existing functionality. Load test : This type of testing checks how well the software performs under heavy use. It ensures that the software can handle a large number of users or high data loads without crashing. Smoke test : This is a basic test to check if the main functions of the software are working. It\u2019s like a quick check to see if the software is stable enough for more detailed testing. White-box test : This is a type of testing where the tester looks at the internal code and structure of the software. They check if the internal workings are correct and efficient. Test-driven development : This is a software development process where tests are written before the actual code. Developers first write a test for a small part of the functionality, then write the code to pass that test, and finally, improve the code while keeping the test passing.","title":"pytest"},{"location":"Python/1.18_unittest_pytest/#introduction-to-pytest","text":"pytest is a popular tool for testing Python code. It's known for being easy to use and having many useful features.","title":"Introduction to pytest"},{"location":"Python/1.18_unittest_pytest/#key-features-of-pytest","text":"Simple Syntax : You write tests with plain Python functions starting with test_ . Use standard assert statements for checks. This makes it easy to start testing. Rich Assertions : pytest provides clear failure reports. If a test fails, it shows what went wrong with details on variables and expressions. Fixtures : Fixtures help set up and clean up before and after tests. They let you reuse code for preparing test data or resources. Parametrization : You can run the same test with different inputs using pytest . This saves you from writing multiple similar tests. Plugins and Extensions : There are many plugins to add extra features like running tests in parallel, checking code coverage, and more. Test Discovery : pytest finds and runs tests automatically by looking for files and functions that follow certain naming rules. These features make pytest a powerful and flexible tool for testing Python code.","title":"Key Features of pytest"},{"location":"Python/1.18_unittest_pytest/#writing-tests-with-pytest","text":"To get started with pytest , install it using pip: pip install pytest Create a test file (e.g., test_sample.py ) and write a simple test function: def test_addition(): assert 1 + 1 == 2 Run the test using the pytest command: pytest pytest will automatically find and run the test, providing a summary of the results.","title":"Writing Tests with pytest"},{"location":"Python/1.18_unittest_pytest/#advanced-features","text":"Fixtures : Define a fixture using the @pytest.fixture decorator to provide setup code for your tests: ```python import pytest @pytest.fixture def sample_data(): return [1, 2, 3] def test_sum(sample_data): assert sum(sample_data) == 6 ``` Parametrization : Use the @pytest.mark.parametrize decorator to run tests with multiple sets of parameters: ```python import pytest @pytest.mark.parametrize(\"a, b, expected\", [ (1, 1, 2), (2, 3, 5), (5, 5, 10) ]) def test_add(a, b, expected): assert a + b == expected ```","title":"Advanced Features"},{"location":"Python/1.18_unittest_pytest/#unit-test-example","text":"","title":"Unit test example"},{"location":"Python/1.18_unittest_pytest/#rearrangepy","text":"#!/usr/bin/env python3 import re def rearrange_name(full_name): match = re.search(r\"^([\\w .]*), ([\\w .]*)$\", full_name) return \"{} {}\".format(match[2], match[1])","title":"rearrange.py"},{"location":"Python/1.18_unittest_pytest/#rearrange_testpy","text":"#!/usr/bin/env python3 import unittest from rearrange import rearrange_name class TestRearrange(unittest.TestCase): def test_rearrange(self): input_name = \"Lovelace, Ada\" expected_output = \"Ada Lovelace\" self.assertEqual(rearrange_name(input_name), expected_output) # Run the tests unittest.main()","title":"rearrange_test.py"},{"location":"Python/1.18_unittest_pytest/#running-the-test","text":"chmod +x rearrange_test.py ./rearrange_test.py","title":"Running the Test"},{"location":"Python/1.18_unittest_pytest/#unittest-vs-pytest","text":"Unittest is built into Python and uses an object-oriented approach with special assert methods. Pytest needs to be imported and uses a functional approach with simple assert statements. Unittest automatically detects tests but needs to be run from the command line. Pytest runs tests automatically with the test_ prefix. Both can work together; you can use pytest with unittest code. Choose based on your preference: unittest for built-in simplicity or pytest for easier, more readable tests.","title":"Unittest vs. Pytest"},{"location":"Python/1.18_unittest_pytest/#conclusion","text":"pytest is a handy tool for testing Python code. It\u2019s easy to use, has many features, and is well-supported by the community. Ideal for both small and big projects, it helps you write reliable tests quickly.","title":"Conclusion"},{"location":"Python/1.18_unittest_pytest/#some-important-concepts-regarding-testing","text":"Black-box test : This is a type of testing where the tester doesn\u2019t look at the internal code of the software. They just check if the software works as expected based on inputs and outputs. Integration test : This test checks if different parts of the software work well together. It ensures that the combined parts function correctly with each other and with external systems. Isolation : In testing, isolation means ensuring that a test only checks one specific part of the software. Any success or failure should be due to the part being tested, not because of something else. Regression test : This is a test done after fixing a bug to make sure that the bug doesn\u2019t come back. It checks if the new changes have affected the existing functionality. Load test : This type of testing checks how well the software performs under heavy use. It ensures that the software can handle a large number of users or high data loads without crashing. Smoke test : This is a basic test to check if the main functions of the software are working. It\u2019s like a quick check to see if the software is stable enough for more detailed testing. White-box test : This is a type of testing where the tester looks at the internal code and structure of the software. They check if the internal workings are correct and efficient. Test-driven development : This is a software development process where tests are written before the actual code. Developers first write a test for a small part of the functionality, then write the code to pass that test, and finally, improve the code while keeping the test passing.","title":"Some important concepts regarding testing"},{"location":"Python/1.19_if_name_main.md/","text":"Understanding if __name__ == '__main__': In Python, if __name__ == '__main__': is used to determine whether a script is run directly or imported as a module. This helps manage code execution depending on the context. How It Works Direct Execution : When you run a script directly (e.g., python my_script.py ), __name__ is set to '__main__' . The code inside the if __name__ == '__main__': block will run. Imported as a Module : When you import the script into another script (e.g., import my_script ), __name__ is set to the module's name (e.g., 'my_script' ). The code inside the if __name__ == '__main__': block will not execute. Examples Basic Example ```python # script1.py def say_hello(): print(\"Hello!\") if name == ' main ': say_hello() ``` Running python script1.py will output \"Hello!\" because say_hello() is called directly. Testing Functions ```python # script2.py def add(a, b): return a + b if name == ' main ': result = add(2, 3) print(f\"Result: {result}\") ``` Running python script2.py will print \"Result: 5\". When imported, add() can be used without printing the result. Module Import Example ```python # script3.py def greet(name): print(f\"Hi, {name}!\") if name == ' main ': greet(\"Alice\") ``` ```python # script4.py import script3 script3.greet(\"Bob\") ``` Running python script4.py will print \"Hi, Bob!\" but not \"Hi, Alice!\" since greet(\"Alice\") in script3 only runs when script3.py is executed directly. Using Command-Line Arguments ```python # script5.py import sys def main(): if len(sys.argv) > 1: print(f\"Arguments: {sys.argv[1:]}\") else: print(\"No arguments provided\") if name == ' main ': main() ``` Running python script5.py arg1 arg2 will print \"Arguments: ['arg1', 'arg2']\". When imported, main() won't run automatically. Takeaways Direct Execution : Code in if __name__ == '__main__': runs only when the script is executed directly. Module Import : Code in this block does not run when the script is imported as a module. Use Cases : Ideal for testing code, running scripts, or executing code that should not run on import. Using if __name__ == '__main__': helps keep code modular and avoids unintended executions when scripts are imported as modules.","title":"if__name__main"},{"location":"Python/1.19_if_name_main.md/#understanding-if-__name__-__main__","text":"In Python, if __name__ == '__main__': is used to determine whether a script is run directly or imported as a module. This helps manage code execution depending on the context.","title":"Understanding if __name__ == '__main__':"},{"location":"Python/1.19_if_name_main.md/#how-it-works","text":"Direct Execution : When you run a script directly (e.g., python my_script.py ), __name__ is set to '__main__' . The code inside the if __name__ == '__main__': block will run. Imported as a Module : When you import the script into another script (e.g., import my_script ), __name__ is set to the module's name (e.g., 'my_script' ). The code inside the if __name__ == '__main__': block will not execute.","title":"How It Works"},{"location":"Python/1.19_if_name_main.md/#examples","text":"Basic Example ```python # script1.py def say_hello(): print(\"Hello!\") if name == ' main ': say_hello() ``` Running python script1.py will output \"Hello!\" because say_hello() is called directly. Testing Functions ```python # script2.py def add(a, b): return a + b if name == ' main ': result = add(2, 3) print(f\"Result: {result}\") ``` Running python script2.py will print \"Result: 5\". When imported, add() can be used without printing the result. Module Import Example ```python # script3.py def greet(name): print(f\"Hi, {name}!\") if name == ' main ': greet(\"Alice\") ``` ```python # script4.py import script3 script3.greet(\"Bob\") ``` Running python script4.py will print \"Hi, Bob!\" but not \"Hi, Alice!\" since greet(\"Alice\") in script3 only runs when script3.py is executed directly. Using Command-Line Arguments ```python # script5.py import sys def main(): if len(sys.argv) > 1: print(f\"Arguments: {sys.argv[1:]}\") else: print(\"No arguments provided\") if name == ' main ': main() ``` Running python script5.py arg1 arg2 will print \"Arguments: ['arg1', 'arg2']\". When imported, main() won't run automatically.","title":"Examples"},{"location":"Python/1.19_if_name_main.md/#takeaways","text":"Direct Execution : Code in if __name__ == '__main__': runs only when the script is executed directly. Module Import : Code in this block does not run when the script is imported as a module. Use Cases : Ideal for testing code, running scripts, or executing code that should not run on import. Using if __name__ == '__main__': helps keep code modular and avoids unintended executions when scripts are imported as modules.","title":"Takeaways"},{"location":"Python/1.1_Tuples/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Python (Tuples) Tuples are collections of items where items have serial numbers, but they can't be changed - No add/remove/reorder. They can have duplicates. Tuples are like a necklace. Once you create them you can't change them. You can't add/remove items. You can't reorder items. Technically they are ordered collections of immutable objects. Phew! Python has 4 built-in data types - LSTD - List, Set, Tuple, Dictionary. Tuples and lists both store items, but there are key differences. **Tuples** can't be changed once created; you can't add, remove, or modify items. **Lists** can be changed; you can add, remove, or modify items. Both can have duplicate items. Tuples vs. Lists Tuples and lists both store items, but there are key differences. **Tuples** can't be changed once created; you can't add, remove, or modify items. **Lists** can be changed; you can add, remove, or modify items. Both can have duplicate items. # Tuples can have duplicates my_tuple = (1, 2, 2, 3, 4, 4) print(my_tuple) # Output: (1, 2, 2, 3, 4, 4) # Lists can also have duplicates my_list = [1, 2, 2, 3, 4, 4] print(my_list) # Output: [1, 2, 2, 3, 4, 4] Tuples use `()`, and lists use `[]`. Use tuples for fixed data and lists for data that changes. Features of Tuples Ordered and Indexed Tuple items have serial numbers(Index). And they stay the same. The first item stays first, the second stays second, and so on. I.e., Tuples are 'ordered' my_tuple = (1, 2, 3) print(my_tuple[0]) # Output: 1 print(my_tuple[1]) # Output: 2 Immutable(Rigid) Once created, you can't change the items in a tuple. This means you can't add, remove, or replace items in a tuple. This makes tuples very reliable for storing data that shouldn't change. my_tuple = (1, 2, 3) # Trying to change a value will cause an error my_tuple[0] = 10 # Error: TypeError: 'tuple' object does not support item assignment # Trying to add an item will cause an error my_tuple.append(4) # Error: AttributeError: 'tuple' object has no attribute 'append' # Trying to remove an item will cause an error my_tuple.remove(2) # Error: AttributeError: 'tuple' object has no attribute 'remove' Hetro..Hetrogenxblabla - urrgh! All datatypes welcome! Tuples can hold items of different types, including other tuples. mixed_tuple = (1, \"hello\", 3.14, (2, 4)) print(mixed_tuple) # Output: (1, 'hello', 3.14, (2, 4)) Tuples take less memory than others Creating Tuples There are a few ways to create tuples in Python: Using Parentheses () # Creating an empty tuple empty_tuple = () # Creating a tuple with items my_tuple = (1, 2, 3) Without Parentheses You can also create a tuple by simply separating items with commas. my_tuple = 1, 2, 3 Using the tuple() Function You can convert other data types (like lists) to tuples using the tuple() function. my_list = [1, 2, 3] my_tuple = tuple(my_list) Common Errors Trying to Modify a Tuple Since tuples are immutable, trying to change an item will result in an error. my_tuple = (1, 2, 3) my_tuple[0] = 10 # Error: TypeError: 'tuple' object does not support item assignment Forgetting Comma in Single-Item Tuples A tuple with one item needs a comma, otherwise, Python will not recognize it as a tuple. single_item_tuple = (1,) print(type(single_item_tuple)) # Output: <class 'tuple'> not_a_tuple = (1) print(type(not_a_tuple)) # Output: <class 'int'> Examples of Using Tuples Storing Coordinates Tuples are great for storing fixed sets of data like coordinates. coordinates = (10.0, 20.0) print(coordinates) # Output: (10.0, 20.0) Returning Multiple Values from Functions Functions can return multiple values using tuples. def get_name_and_age(): name = \"Alice\" age = 30 return name, age name, age = get_name_and_age() print(name) # Output: Alice print(age) # Output: 30 Using Tuples as Dictionary Keys Because tuples are immutable, they can be used as keys in dictionaries. location_data = {} location = (10.0, 20.0) location_data[location] = \"Location 1\" print(location_data) # Output: {(10.0, 20.0): 'Location 1'} Did you know this? \"Hello, %s! You're %s years old.\" % (\"Kim John\", 150) Summary Tuples : Like lists but can't modify. Create : Using () , commas, or tuple() . Features : Ordered, immutable, and can hold different data types. Use Cases : Fixed collections, multiple return values, dictionary keys. Tuples are useful to put constant items in a group. They are easy to understand and make code easy to read. Test your knowledge Highlight the answer section to reveal! Question - Tuple Indexing What will be the output of the following statement? my_tuple = (1, 2, 3, 4, 5) print(my_tuple[-1]) Answer: 5 Question - Tuple Containing a List What will happen if you try to include a list inside a tuple? my_tuple = (1, [2, 3], 4) my_tuple[1][0] = 5 print(my_tuple) Answer: (1, [5, 3], 4) Question - Tuple Concatenation What will be the output of the following statement? tuple1 = (1, 2) tuple2 = (3, 4) print(tuple1 + tuple2) Answer: (1, 2, 3, 4) Question - Immutable Tuples What will happen if you try to change an element of a tuple? my_tuple = (1, 2, 3) my_tuple[0] = 4 Answer: TypeError: 'tuple' object does not support item assignment Question - Single-Item Tuple How do you create a tuple with a single item? single_item_tuple = (1) print(type(single_item_tuple)) Answer: <class 'int'> Question - Tuple Methods Which method would you use to find the index of a value in a tuple? my_tuple = (10, 20, 30, 40) print(my_tuple.index(30)) Answer: 2 Question - Tuple Length What will be the output of the following statement? my_tuple = (1, 2, 3, 4, 5) print(len(my_tuple)) Answer: 5 Question - Nested Tuples What will be the output of the following statement? nested_tuple = (1, (2, 3), 4) print(nested_tuple[1]) Answer: (2, 3) Question - Tuple Slicing What will be the output of the following statement? my_tuple = (1, 2, 3, 4, 5) print(my_tuple[1:3]) Answer: (2, 3) Question - Tuple Containment What will be the output of the following statement? my_tuple = (1, 2, 3, 4, 5) print(3 in my_tuple) Answer: True Question - Tuple Unpacking What will be the output of the following statement? my_tuple = (1, 2, 3) a, b, c = my_tuple print(a, b, c) Answer: 1 2 3 Question - Tuple Multiplication What will be the output of the following statement? my_tuple = (1, 2, 3) print(my_tuple * 2) Answer: (1, 2, 3, 1, 2, 3) Question - Tuple Conversion What will be the output of the following statement? my_list = [1, 2, 3] my_tuple = tuple(my_list) print(my_tuple) Answer: (1, 2, 3) Question - Tuple with Mixed Types What will be the output of the following statement? my_tuple = (1, \"hello\", 3.14) print(my_tuple[1]) Answer: hello Question - Tuple Indexing What will be the output of the following statement? my_tuple = (1, 2, 3, 4, 5) print(my_tuple[-1]) Answer: 5 Question - Tuple Containing a List What will happen if you try to include a list inside a tuple? my_tuple = (1, [2, 3], 4) my_tuple[1][0] = 5 print(my_tuple) Answer: (1, [5, 3], 4) Question - Tuple Concatenation What will be the output of the following statement? tuple1 = (1, 2) tuple2 = (3, 4) print(tuple1 + tuple2) Answer: (1, 2, 3, 4) Question - Immutable Tuples What will happen if you try to change an element of a tuple? my_tuple = (1, 2, 3) my_tuple[0] = 4 Answer: TypeError: 'tuple' object does not support item assignment Question - Single-Item Tuple How do you create a tuple with a single item? single_item_tuple = (1) print(type(single_item_tuple)) Answer: <class 'int'> Question - Tuple Methods Which method would you use to find the index of a value in a tuple? my_tuple = (10, 20, 30, 40) print(my_tuple.index(30)) Answer: 2 Question - Tuple Length What will be the output of the following statement? my_tuple = (1, 2, 3, 4, 5) print(len(my_tuple)) Answer: 5 Question - Nested Tuples What will be the output of the following statement? nested_tuple = (1, (2, 3), 4) print(nested_tuple[1]) Answer: (2, 3) Question - Tuple Slicing What will be the output of the following statement? my_tuple = (1, 2, 3, 4, 5) print(my_tuple[1:3]) Answer: (2, 3) Question - Tuple Containment What will be the output of the following statement? my_tuple = (1, 2, 3, 4, 5) print(3 in my_tuple) Answer: True Question - Tuple Unpacking What will be the output of the following statement? my_tuple = (1, 2, 3) a, b, c = my_tuple print(a, b, c) Answer: 1 2 3 Question - Tuple Multiplication What will be the output of the following statement? my_tuple = (1, 2, 3) print(my_tuple * 2) Answer: (1, 2, 3, 1, 2, 3) Question - Tuple Conversion What will be the output of the following statement? my_list = [1, 2, 3] my_tuple = tuple(my_list) print(my_tuple) Answer: (1, 2, 3) Question - Tuple with Mixed Types What will be the output of the following statement? my_tuple = (1, \"hello\", 3.14) print(my_tuple[1]) Answer: hello Question - Tuple Assignment What will happen when you try to reassign a tuple variable? my_tuple = (1, 2, 3) my_tuple = (4, 5, 6) print(my_tuple) Answer: (4, 5, 6) Question - Tuple Immutability with List What will be the output of the following statement? my_tuple = (1, [2, 3], 4) my_tuple[1].append(5) print(my_tuple) Answer: (1, [2, 3, 5], 4) Examples def biography_list(people): # Iterate over each \"person\" in the given \"people\" list of tuples. for item in people: # This is the highlight of the code name, age, profession = item # The tuple gets assigned into 3 variables # Format the required sentence and place the 3 variables # in the correct placeholders using the .format() method. print(\"{} is {} years old and works as {}\".format(name, age, profession)) # Call to the function: biography_list([(\"Dilwali\", 30, \"a Dulhania\"), (\"Raj\", 35, \"nam to suna hi hoga\"), (\"Maria\", 25, \"oh maria\")]) # Output: # Ira is 30 years old and works as a Chef # Raj is 35 years old and works as a Lawyer # Maria is 25 years old and works as an Engineer","title":"Tuples"},{"location":"Python/1.1_Tuples/#python-tuples","text":"Tuples are collections of items where items have serial numbers, but they can't be changed - No add/remove/reorder. They can have duplicates. Tuples are like a necklace. Once you create them you can't change them. You can't add/remove items. You can't reorder items. Technically they are ordered collections of immutable objects. Phew! Python has 4 built-in data types - LSTD - List, Set, Tuple, Dictionary. Tuples and lists both store items, but there are key differences. **Tuples** can't be changed once created; you can't add, remove, or modify items. **Lists** can be changed; you can add, remove, or modify items. Both can have duplicate items.","title":"Python (Tuples)"},{"location":"Python/1.1_Tuples/#tuples-vs-lists","text":"Tuples and lists both store items, but there are key differences. **Tuples** can't be changed once created; you can't add, remove, or modify items. **Lists** can be changed; you can add, remove, or modify items. Both can have duplicate items. # Tuples can have duplicates my_tuple = (1, 2, 2, 3, 4, 4) print(my_tuple) # Output: (1, 2, 2, 3, 4, 4) # Lists can also have duplicates my_list = [1, 2, 2, 3, 4, 4] print(my_list) # Output: [1, 2, 2, 3, 4, 4] Tuples use `()`, and lists use `[]`. Use tuples for fixed data and lists for data that changes.","title":"Tuples vs. Lists"},{"location":"Python/1.1_Tuples/#features-of-tuples","text":"","title":"Features of Tuples"},{"location":"Python/1.1_Tuples/#ordered-and-indexed","text":"Tuple items have serial numbers(Index). And they stay the same. The first item stays first, the second stays second, and so on. I.e., Tuples are 'ordered' my_tuple = (1, 2, 3) print(my_tuple[0]) # Output: 1 print(my_tuple[1]) # Output: 2","title":"Ordered and Indexed"},{"location":"Python/1.1_Tuples/#immutablerigid","text":"Once created, you can't change the items in a tuple. This means you can't add, remove, or replace items in a tuple. This makes tuples very reliable for storing data that shouldn't change. my_tuple = (1, 2, 3) # Trying to change a value will cause an error my_tuple[0] = 10 # Error: TypeError: 'tuple' object does not support item assignment # Trying to add an item will cause an error my_tuple.append(4) # Error: AttributeError: 'tuple' object has no attribute 'append' # Trying to remove an item will cause an error my_tuple.remove(2) # Error: AttributeError: 'tuple' object has no attribute 'remove'","title":"Immutable(Rigid)"},{"location":"Python/1.1_Tuples/#hetrohetrogenxblabla-urrgh-all-datatypes-welcome","text":"Tuples can hold items of different types, including other tuples. mixed_tuple = (1, \"hello\", 3.14, (2, 4)) print(mixed_tuple) # Output: (1, 'hello', 3.14, (2, 4)) Tuples take less memory than others","title":"Hetro..Hetrogenxblabla - urrgh! All datatypes welcome!"},{"location":"Python/1.1_Tuples/#creating-tuples","text":"There are a few ways to create tuples in Python:","title":"Creating Tuples"},{"location":"Python/1.1_Tuples/#using-parentheses","text":"# Creating an empty tuple empty_tuple = () # Creating a tuple with items my_tuple = (1, 2, 3)","title":"Using Parentheses ()"},{"location":"Python/1.1_Tuples/#without-parentheses","text":"You can also create a tuple by simply separating items with commas. my_tuple = 1, 2, 3","title":"Without Parentheses"},{"location":"Python/1.1_Tuples/#using-the-tuple-function","text":"You can convert other data types (like lists) to tuples using the tuple() function. my_list = [1, 2, 3] my_tuple = tuple(my_list)","title":"Using the tuple() Function"},{"location":"Python/1.1_Tuples/#common-errors","text":"","title":"Common Errors"},{"location":"Python/1.1_Tuples/#trying-to-modify-a-tuple","text":"Since tuples are immutable, trying to change an item will result in an error. my_tuple = (1, 2, 3) my_tuple[0] = 10 # Error: TypeError: 'tuple' object does not support item assignment","title":"Trying to Modify a Tuple"},{"location":"Python/1.1_Tuples/#forgetting-comma-in-single-item-tuples","text":"A tuple with one item needs a comma, otherwise, Python will not recognize it as a tuple. single_item_tuple = (1,) print(type(single_item_tuple)) # Output: <class 'tuple'> not_a_tuple = (1) print(type(not_a_tuple)) # Output: <class 'int'>","title":"Forgetting Comma in Single-Item Tuples"},{"location":"Python/1.1_Tuples/#examples-of-using-tuples","text":"","title":"Examples of Using Tuples"},{"location":"Python/1.1_Tuples/#storing-coordinates","text":"Tuples are great for storing fixed sets of data like coordinates. coordinates = (10.0, 20.0) print(coordinates) # Output: (10.0, 20.0)","title":"Storing Coordinates"},{"location":"Python/1.1_Tuples/#returning-multiple-values-from-functions","text":"Functions can return multiple values using tuples. def get_name_and_age(): name = \"Alice\" age = 30 return name, age name, age = get_name_and_age() print(name) # Output: Alice print(age) # Output: 30","title":"Returning Multiple Values from Functions"},{"location":"Python/1.1_Tuples/#using-tuples-as-dictionary-keys","text":"Because tuples are immutable, they can be used as keys in dictionaries. location_data = {} location = (10.0, 20.0) location_data[location] = \"Location 1\" print(location_data) # Output: {(10.0, 20.0): 'Location 1'} Did you know this? \"Hello, %s! You're %s years old.\" % (\"Kim John\", 150)","title":"Using Tuples as Dictionary Keys"},{"location":"Python/1.1_Tuples/#summary","text":"Tuples : Like lists but can't modify. Create : Using () , commas, or tuple() . Features : Ordered, immutable, and can hold different data types. Use Cases : Fixed collections, multiple return values, dictionary keys. Tuples are useful to put constant items in a group. They are easy to understand and make code easy to read.","title":"Summary"},{"location":"Python/1.1_Tuples/#test-your-knowledge","text":"Highlight the answer section to reveal!","title":"Test your knowledge"},{"location":"Python/1.1_Tuples/#question-tuple-indexing","text":"What will be the output of the following statement? my_tuple = (1, 2, 3, 4, 5) print(my_tuple[-1]) Answer: 5","title":"Question - Tuple Indexing"},{"location":"Python/1.1_Tuples/#question-tuple-containing-a-list","text":"What will happen if you try to include a list inside a tuple? my_tuple = (1, [2, 3], 4) my_tuple[1][0] = 5 print(my_tuple) Answer: (1, [5, 3], 4)","title":"Question - Tuple Containing a List"},{"location":"Python/1.1_Tuples/#question-tuple-concatenation","text":"What will be the output of the following statement? tuple1 = (1, 2) tuple2 = (3, 4) print(tuple1 + tuple2) Answer: (1, 2, 3, 4)","title":"Question - Tuple Concatenation"},{"location":"Python/1.1_Tuples/#question-immutable-tuples","text":"What will happen if you try to change an element of a tuple? my_tuple = (1, 2, 3) my_tuple[0] = 4 Answer: TypeError: 'tuple' object does not support item assignment","title":"Question - Immutable Tuples"},{"location":"Python/1.1_Tuples/#question-single-item-tuple","text":"How do you create a tuple with a single item? single_item_tuple = (1) print(type(single_item_tuple)) Answer: <class 'int'>","title":"Question - Single-Item Tuple"},{"location":"Python/1.1_Tuples/#question-tuple-methods","text":"Which method would you use to find the index of a value in a tuple? my_tuple = (10, 20, 30, 40) print(my_tuple.index(30)) Answer: 2","title":"Question - Tuple Methods"},{"location":"Python/1.1_Tuples/#question-tuple-length","text":"What will be the output of the following statement? my_tuple = (1, 2, 3, 4, 5) print(len(my_tuple)) Answer: 5","title":"Question - Tuple Length"},{"location":"Python/1.1_Tuples/#question-nested-tuples","text":"What will be the output of the following statement? nested_tuple = (1, (2, 3), 4) print(nested_tuple[1]) Answer: (2, 3)","title":"Question - Nested Tuples"},{"location":"Python/1.1_Tuples/#question-tuple-slicing","text":"What will be the output of the following statement? my_tuple = (1, 2, 3, 4, 5) print(my_tuple[1:3]) Answer: (2, 3)","title":"Question - Tuple Slicing"},{"location":"Python/1.1_Tuples/#question-tuple-containment","text":"What will be the output of the following statement? my_tuple = (1, 2, 3, 4, 5) print(3 in my_tuple) Answer: True","title":"Question - Tuple Containment"},{"location":"Python/1.1_Tuples/#question-tuple-unpacking","text":"What will be the output of the following statement? my_tuple = (1, 2, 3) a, b, c = my_tuple print(a, b, c) Answer: 1 2 3","title":"Question - Tuple Unpacking"},{"location":"Python/1.1_Tuples/#question-tuple-multiplication","text":"What will be the output of the following statement? my_tuple = (1, 2, 3) print(my_tuple * 2) Answer: (1, 2, 3, 1, 2, 3)","title":"Question - Tuple Multiplication"},{"location":"Python/1.1_Tuples/#question-tuple-conversion","text":"What will be the output of the following statement? my_list = [1, 2, 3] my_tuple = tuple(my_list) print(my_tuple) Answer: (1, 2, 3)","title":"Question - Tuple Conversion"},{"location":"Python/1.1_Tuples/#question-tuple-with-mixed-types","text":"What will be the output of the following statement? my_tuple = (1, \"hello\", 3.14) print(my_tuple[1]) Answer: hello","title":"Question - Tuple with Mixed Types"},{"location":"Python/1.1_Tuples/#question-tuple-indexing_1","text":"What will be the output of the following statement? my_tuple = (1, 2, 3, 4, 5) print(my_tuple[-1]) Answer: 5","title":"Question - Tuple Indexing"},{"location":"Python/1.1_Tuples/#question-tuple-containing-a-list_1","text":"What will happen if you try to include a list inside a tuple? my_tuple = (1, [2, 3], 4) my_tuple[1][0] = 5 print(my_tuple) Answer: (1, [5, 3], 4)","title":"Question - Tuple Containing a List"},{"location":"Python/1.1_Tuples/#question-tuple-concatenation_1","text":"What will be the output of the following statement? tuple1 = (1, 2) tuple2 = (3, 4) print(tuple1 + tuple2) Answer: (1, 2, 3, 4)","title":"Question - Tuple Concatenation"},{"location":"Python/1.1_Tuples/#question-immutable-tuples_1","text":"What will happen if you try to change an element of a tuple? my_tuple = (1, 2, 3) my_tuple[0] = 4 Answer: TypeError: 'tuple' object does not support item assignment","title":"Question - Immutable Tuples"},{"location":"Python/1.1_Tuples/#question-single-item-tuple_1","text":"How do you create a tuple with a single item? single_item_tuple = (1) print(type(single_item_tuple)) Answer: <class 'int'>","title":"Question - Single-Item Tuple"},{"location":"Python/1.1_Tuples/#question-tuple-methods_1","text":"Which method would you use to find the index of a value in a tuple? my_tuple = (10, 20, 30, 40) print(my_tuple.index(30)) Answer: 2","title":"Question - Tuple Methods"},{"location":"Python/1.1_Tuples/#question-tuple-length_1","text":"What will be the output of the following statement? my_tuple = (1, 2, 3, 4, 5) print(len(my_tuple)) Answer: 5","title":"Question - Tuple Length"},{"location":"Python/1.1_Tuples/#question-nested-tuples_1","text":"What will be the output of the following statement? nested_tuple = (1, (2, 3), 4) print(nested_tuple[1]) Answer: (2, 3)","title":"Question - Nested Tuples"},{"location":"Python/1.1_Tuples/#question-tuple-slicing_1","text":"What will be the output of the following statement? my_tuple = (1, 2, 3, 4, 5) print(my_tuple[1:3]) Answer: (2, 3)","title":"Question - Tuple Slicing"},{"location":"Python/1.1_Tuples/#question-tuple-containment_1","text":"What will be the output of the following statement? my_tuple = (1, 2, 3, 4, 5) print(3 in my_tuple) Answer: True","title":"Question - Tuple Containment"},{"location":"Python/1.1_Tuples/#question-tuple-unpacking_1","text":"What will be the output of the following statement? my_tuple = (1, 2, 3) a, b, c = my_tuple print(a, b, c) Answer: 1 2 3","title":"Question - Tuple Unpacking"},{"location":"Python/1.1_Tuples/#question-tuple-multiplication_1","text":"What will be the output of the following statement? my_tuple = (1, 2, 3) print(my_tuple * 2) Answer: (1, 2, 3, 1, 2, 3)","title":"Question - Tuple Multiplication"},{"location":"Python/1.1_Tuples/#question-tuple-conversion_1","text":"What will be the output of the following statement? my_list = [1, 2, 3] my_tuple = tuple(my_list) print(my_tuple) Answer: (1, 2, 3)","title":"Question - Tuple Conversion"},{"location":"Python/1.1_Tuples/#question-tuple-with-mixed-types_1","text":"What will be the output of the following statement? my_tuple = (1, \"hello\", 3.14) print(my_tuple[1]) Answer: hello","title":"Question - Tuple with Mixed Types"},{"location":"Python/1.1_Tuples/#question-tuple-assignment","text":"What will happen when you try to reassign a tuple variable? my_tuple = (1, 2, 3) my_tuple = (4, 5, 6) print(my_tuple) Answer: (4, 5, 6)","title":"Question - Tuple Assignment"},{"location":"Python/1.1_Tuples/#question-tuple-immutability-with-list","text":"What will be the output of the following statement? my_tuple = (1, [2, 3], 4) my_tuple[1].append(5) print(my_tuple) Answer: (1, [2, 3, 5], 4)","title":"Question - Tuple Immutability with List"},{"location":"Python/1.1_Tuples/#examples","text":"def biography_list(people): # Iterate over each \"person\" in the given \"people\" list of tuples. for item in people: # This is the highlight of the code name, age, profession = item # The tuple gets assigned into 3 variables # Format the required sentence and place the 3 variables # in the correct placeholders using the .format() method. print(\"{} is {} years old and works as {}\".format(name, age, profession)) # Call to the function: biography_list([(\"Dilwali\", 30, \"a Dulhania\"), (\"Raj\", 35, \"nam to suna hi hoga\"), (\"Maria\", 25, \"oh maria\")]) # Output: # Ira is 30 years old and works as a Chef # Raj is 35 years old and works as a Lawyer # Maria is 25 years old and works as an Engineer","title":"Examples"},{"location":"Python/1.2_Tuples_Advanced/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Advanced Tuples In this article, we\u2019ll explore some advanced features and uses of tuples in Python. Using Parentheses with Tuples Interpolating Values in a String When using the % operator to insert values into a string, you need to wrap the values in a tuple using parentheses. # Correct way print(\"Hi, %s! You are %s years old.\" % (\"Ravi\", 30)) # Output: 'Hi, Ravi! You are 30 years old.' # Incorrect way print(\"Hi, %s! You are %s years old.\" % \"Ravi\", 30) # Output: TypeError: not enough arguments for format string In the first example, the values are wrapped in a tuple, so it works. The second example raises an error because the values are not in a tuple. Creating Single-Item Tuples To make a tuple with only one item, you need to include a comma after the item. one_word = \"Pranam\", print(one_word) # Output: ('Pranam',) one_number = (88,) print(one_number) # Output: (88,) The comma is necessary to make it a tuple and not just a regular string or number. Using the tuple() Constructor You can use the tuple() function to make tuples from a list, set, dictionary, or string. If you call tuple() without any arguments, it creates an empty tuple. Examples: print(tuple([\"Asharam Bapu\", 28, 5.9, \"India\"])) # Output: ('Asharam Bapu', 28, 5.9, 'India') print(tuple(\"Developer\")) # Output: ('D', 'e', 'v', 'e', 'l', 'o', 'p', 'e', 'r') print(tuple({ \"make\": \"Honda\", \"model\": \"Civic\", \"year\": 2021, }.values())) # Output: ('Honda', 'Civic', 2021) print(tuple()) # Output: () Accessing Items in a Tuple: Indexing You can get items from a tuple using their index numbers. Indexes start from 0. Example: person = (\"Sita\", 22, 5.4, \"Nepal\") print(person[0]) # Output: 'Sita' print(person[1]) # Output: 22 print(person[3]) # Output: 'Nepal' You can also use negative indexes to get items from the end. Example: print(person[-1]) # Output: 'Nepal' print(person[-2]) # Output: 5.4 Retrieving Multiple Items From a Tuple: Slicing Slicing allows you to get parts of a tuple. Example: days = (\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\") print(days[:3]) # Output: ('Sunday', 'Monday', 'Tuesday') print(days[3:]) # Output: ('Wednesday', 'Thursday', 'Friday', 'Saturday') Exploring Tuple Immutability Tuples are immutable, which means you can\u2019t change, add, or remove items after creating them. Example: person = (\"Sita\", 22, 5.4, \"Nepal\") # Trying to change a value will cause an error person[3] = \"India\" # Output: TypeError: 'tuple' object does not support item assignment # Trying to delete an item will cause an error del person[2] # Output: TypeError: 'tuple' object doesn't support item deletion Packing and Unpacking Tuples Example: # Packing a tuple coordinates = (19.0760, 72.8777) # Unpacking a tuple lat, lon = coordinates print(lat) # Output: 19.0760 print(lon) # Output: 72.8777 Returning Tuples From Functions Functions can return multiple values as tuples. Example: def min_max(values): if not values: raise ValueError(\"input list must not be empty\") return min(values), max(values) result = min_max([7, 2, 8, 4, 5]) print(result) # Output: (2, 8) print(type(result)) # Output: <class 'tuple'> Concatenating and Repeating Tuples Concatenating Tuples name = (\"Rohit\",) surname = (\"Sharma\",) full_name = name + surname print(full_name) # Output: ('Rohit', 'Sharma') Repeating Tuples numbers = (1, 2, 3) print(numbers * 2) # Output: (1, 2, 3, 1, 2, 3) Reversing and Sorting Tuples Reversing a Tuple days = (\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\") reversed_days = days[::-1] print(reversed_days) # Output: ('Saturday', 'Friday', 'Thursday', 'Wednesday', 'Tuesday', 'Monday', 'Sunday') Sorting a Tuple ages = (45, 23, 67, 12, 34) print(sorted(ages)) # Output: [12, 23, 34, 45, 67] Traversing Tuples in Python Using a for Loop monthly_sales = ( (\"January\", 12000), (\"February\", 14000), (\"March\", 13000), (\"April\", 15000), ) total_sales = 0 for month, sales in monthly_sales: total_sales += sales print(total_sales) # Output: 54000 Using a List Comprehension numbers = (\"10\", \"20\", \"30\") print(tuple(int(num) for num in numbers)) # Output: (10, 20, 30) Other Features of Tuples .count() and .index() Methods fruits = (\"mango\", \"banana\", \"apple\", \"mango\", \"mango\", \"kiwi\", \"banana\") print(fruits.count(\"mango\")) # Output: 3 print(fruits.index(\"kiwi\")) # Output: 5 Membership Tests languages = (\"Hindi\", \"English\", \"Tamil\", \"Telugu\") print(\"Tamil\" in languages) # Output: True print(\"Bengali\" not in languages) # Output: True Getting the Length of a Tuple details = (\"Rajesh\", \"Teacher\", \"Delhi\", 45) print(len(details)) # Output: 4 Comparing Tuples print((3, 4) == (3, 4)) # Output: True print((10, 15, 20) < (20, 15, 10)) # Output: True print((7, 8, 9) <= (7, 8, 9)) # Output: True Common Traps When creating a one-item tuple, don't forget the trailing comma. items = (99,) print(type(items)) # Output: <class 'tuple'> Tuples containing mutable objects, like lists, can't be used as dictionary keys. # This will raise an error cities_info = { (\"Mumbai\", [18.975, 72.8258]): \"Financial Capital\", } # Output: TypeError: unhashable type: 'list'","title":"Advanced Tuples"},{"location":"Python/1.2_Tuples_Advanced/#advanced-tuples","text":"In this article, we\u2019ll explore some advanced features and uses of tuples in Python.","title":"Advanced Tuples"},{"location":"Python/1.2_Tuples_Advanced/#using-parentheses-with-tuples","text":"","title":"Using Parentheses with Tuples"},{"location":"Python/1.2_Tuples_Advanced/#interpolating-values-in-a-string","text":"When using the % operator to insert values into a string, you need to wrap the values in a tuple using parentheses. # Correct way print(\"Hi, %s! You are %s years old.\" % (\"Ravi\", 30)) # Output: 'Hi, Ravi! You are 30 years old.' # Incorrect way print(\"Hi, %s! You are %s years old.\" % \"Ravi\", 30) # Output: TypeError: not enough arguments for format string In the first example, the values are wrapped in a tuple, so it works. The second example raises an error because the values are not in a tuple.","title":"Interpolating Values in a String"},{"location":"Python/1.2_Tuples_Advanced/#creating-single-item-tuples","text":"To make a tuple with only one item, you need to include a comma after the item. one_word = \"Pranam\", print(one_word) # Output: ('Pranam',) one_number = (88,) print(one_number) # Output: (88,) The comma is necessary to make it a tuple and not just a regular string or number.","title":"Creating Single-Item Tuples"},{"location":"Python/1.2_Tuples_Advanced/#using-the-tuple-constructor","text":"You can use the tuple() function to make tuples from a list, set, dictionary, or string. If you call tuple() without any arguments, it creates an empty tuple.","title":"Using the tuple() Constructor"},{"location":"Python/1.2_Tuples_Advanced/#examples","text":"print(tuple([\"Asharam Bapu\", 28, 5.9, \"India\"])) # Output: ('Asharam Bapu', 28, 5.9, 'India') print(tuple(\"Developer\")) # Output: ('D', 'e', 'v', 'e', 'l', 'o', 'p', 'e', 'r') print(tuple({ \"make\": \"Honda\", \"model\": \"Civic\", \"year\": 2021, }.values())) # Output: ('Honda', 'Civic', 2021) print(tuple()) # Output: ()","title":"Examples:"},{"location":"Python/1.2_Tuples_Advanced/#accessing-items-in-a-tuple-indexing","text":"You can get items from a tuple using their index numbers. Indexes start from 0.","title":"Accessing Items in a Tuple: Indexing"},{"location":"Python/1.2_Tuples_Advanced/#example","text":"person = (\"Sita\", 22, 5.4, \"Nepal\") print(person[0]) # Output: 'Sita' print(person[1]) # Output: 22 print(person[3]) # Output: 'Nepal' You can also use negative indexes to get items from the end.","title":"Example:"},{"location":"Python/1.2_Tuples_Advanced/#example_1","text":"print(person[-1]) # Output: 'Nepal' print(person[-2]) # Output: 5.4","title":"Example:"},{"location":"Python/1.2_Tuples_Advanced/#retrieving-multiple-items-from-a-tuple-slicing","text":"Slicing allows you to get parts of a tuple.","title":"Retrieving Multiple Items From a Tuple: Slicing"},{"location":"Python/1.2_Tuples_Advanced/#example_2","text":"days = (\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\") print(days[:3]) # Output: ('Sunday', 'Monday', 'Tuesday') print(days[3:]) # Output: ('Wednesday', 'Thursday', 'Friday', 'Saturday')","title":"Example:"},{"location":"Python/1.2_Tuples_Advanced/#exploring-tuple-immutability","text":"Tuples are immutable, which means you can\u2019t change, add, or remove items after creating them.","title":"Exploring Tuple Immutability"},{"location":"Python/1.2_Tuples_Advanced/#example_3","text":"person = (\"Sita\", 22, 5.4, \"Nepal\") # Trying to change a value will cause an error person[3] = \"India\" # Output: TypeError: 'tuple' object does not support item assignment # Trying to delete an item will cause an error del person[2] # Output: TypeError: 'tuple' object doesn't support item deletion","title":"Example:"},{"location":"Python/1.2_Tuples_Advanced/#packing-and-unpacking-tuples","text":"","title":"Packing and Unpacking Tuples"},{"location":"Python/1.2_Tuples_Advanced/#example_4","text":"# Packing a tuple coordinates = (19.0760, 72.8777) # Unpacking a tuple lat, lon = coordinates print(lat) # Output: 19.0760 print(lon) # Output: 72.8777","title":"Example:"},{"location":"Python/1.2_Tuples_Advanced/#returning-tuples-from-functions","text":"Functions can return multiple values as tuples.","title":"Returning Tuples From Functions"},{"location":"Python/1.2_Tuples_Advanced/#example_5","text":"def min_max(values): if not values: raise ValueError(\"input list must not be empty\") return min(values), max(values) result = min_max([7, 2, 8, 4, 5]) print(result) # Output: (2, 8) print(type(result)) # Output: <class 'tuple'>","title":"Example:"},{"location":"Python/1.2_Tuples_Advanced/#concatenating-and-repeating-tuples","text":"","title":"Concatenating and Repeating Tuples"},{"location":"Python/1.2_Tuples_Advanced/#concatenating-tuples","text":"name = (\"Rohit\",) surname = (\"Sharma\",) full_name = name + surname print(full_name) # Output: ('Rohit', 'Sharma')","title":"Concatenating Tuples"},{"location":"Python/1.2_Tuples_Advanced/#repeating-tuples","text":"numbers = (1, 2, 3) print(numbers * 2) # Output: (1, 2, 3, 1, 2, 3)","title":"Repeating Tuples"},{"location":"Python/1.2_Tuples_Advanced/#reversing-and-sorting-tuples","text":"","title":"Reversing and Sorting Tuples"},{"location":"Python/1.2_Tuples_Advanced/#reversing-a-tuple","text":"days = (\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\") reversed_days = days[::-1] print(reversed_days) # Output: ('Saturday', 'Friday', 'Thursday', 'Wednesday', 'Tuesday', 'Monday', 'Sunday')","title":"Reversing a Tuple"},{"location":"Python/1.2_Tuples_Advanced/#sorting-a-tuple","text":"ages = (45, 23, 67, 12, 34) print(sorted(ages)) # Output: [12, 23, 34, 45, 67]","title":"Sorting a Tuple"},{"location":"Python/1.2_Tuples_Advanced/#traversing-tuples-in-python","text":"","title":"Traversing Tuples in Python"},{"location":"Python/1.2_Tuples_Advanced/#using-a-for-loop","text":"monthly_sales = ( (\"January\", 12000), (\"February\", 14000), (\"March\", 13000), (\"April\", 15000), ) total_sales = 0 for month, sales in monthly_sales: total_sales += sales print(total_sales) # Output: 54000","title":"Using a for Loop"},{"location":"Python/1.2_Tuples_Advanced/#using-a-list-comprehension","text":"numbers = (\"10\", \"20\", \"30\") print(tuple(int(num) for num in numbers)) # Output: (10, 20, 30)","title":"Using a List Comprehension"},{"location":"Python/1.2_Tuples_Advanced/#other-features-of-tuples","text":"","title":"Other Features of Tuples"},{"location":"Python/1.2_Tuples_Advanced/#count-and-index-methods","text":"fruits = (\"mango\", \"banana\", \"apple\", \"mango\", \"mango\", \"kiwi\", \"banana\") print(fruits.count(\"mango\")) # Output: 3 print(fruits.index(\"kiwi\")) # Output: 5","title":".count() and .index() Methods"},{"location":"Python/1.2_Tuples_Advanced/#membership-tests","text":"languages = (\"Hindi\", \"English\", \"Tamil\", \"Telugu\") print(\"Tamil\" in languages) # Output: True print(\"Bengali\" not in languages) # Output: True","title":"Membership Tests"},{"location":"Python/1.2_Tuples_Advanced/#getting-the-length-of-a-tuple","text":"details = (\"Rajesh\", \"Teacher\", \"Delhi\", 45) print(len(details)) # Output: 4","title":"Getting the Length of a Tuple"},{"location":"Python/1.2_Tuples_Advanced/#comparing-tuples","text":"print((3, 4) == (3, 4)) # Output: True print((10, 15, 20) < (20, 15, 10)) # Output: True print((7, 8, 9) <= (7, 8, 9)) # Output: True","title":"Comparing Tuples"},{"location":"Python/1.2_Tuples_Advanced/#common-traps","text":"When creating a one-item tuple, don't forget the trailing comma. items = (99,) print(type(items)) # Output: <class 'tuple'> Tuples containing mutable objects, like lists, can't be used as dictionary keys. # This will raise an error cities_info = { (\"Mumbai\", [18.975, 72.8258]): \"Financial Capital\", } # Output: TypeError: unhashable type: 'list'","title":"Common Traps"},{"location":"Python/1.3_List/","text":"Understanding Python's sort() Method Let's start with a surprising example: numbers = [3, 2, 4, 5, 1] sorted_numbers = numbers.sort() # THIS WILL OUTPUT None print(sorted_numbers) # Output: None In this example, the output is None . Surprised and expected [1,2,3,4,5]?. Let's find out what happens in the backend The sort() method sorts a list without creating a new list. The sort() method sorts the elements of a list in place, meaning it modifies the original list directly without creating a new list. It does not return a new sorted list, but rather updates the existing list and returns None . This is the key difference between the sort() method and the sorted() function. The sorted() function returns a new list that is sorted, while the sort() method sorts the list it is called on and does not create a new list. Let's see some examples Example 1: Using sort() method numbers = [3, 2, 4, 5, 1] numbers.sort() print(numbers) # Output: [1, 2, 3, 4, 5] Example 2: Using sorted() function numbers = [3, 2, 4, 5, 1] sorted_numbers = sorted(numbers) print(sorted_numbers) # Output: [1, 2, 3, 4, 5] print(numbers) # Output: [3, 2, 4, 5, 1] Sorting in reverse order sorted_numbers = sorted(numbers, reverse=True)","title":"1.3 List"},{"location":"Python/1.3_List/#understanding-pythons-sort-method","text":"Let's start with a surprising example: numbers = [3, 2, 4, 5, 1] sorted_numbers = numbers.sort() # THIS WILL OUTPUT None print(sorted_numbers) # Output: None In this example, the output is None . Surprised and expected [1,2,3,4,5]?.","title":"Understanding Python's sort() Method"},{"location":"Python/1.3_List/#lets-find-out-what-happens-in-the-backend","text":"The sort() method sorts a list without creating a new list. The sort() method sorts the elements of a list in place, meaning it modifies the original list directly without creating a new list. It does not return a new sorted list, but rather updates the existing list and returns None . This is the key difference between the sort() method and the sorted() function. The sorted() function returns a new list that is sorted, while the sort() method sorts the list it is called on and does not create a new list.","title":"Let's find out what happens in the backend"},{"location":"Python/1.3_List/#lets-see-some-examples","text":"Example 1: Using sort() method numbers = [3, 2, 4, 5, 1] numbers.sort() print(numbers) # Output: [1, 2, 3, 4, 5] Example 2: Using sorted() function numbers = [3, 2, 4, 5, 1] sorted_numbers = sorted(numbers) print(sorted_numbers) # Output: [1, 2, 3, 4, 5] print(numbers) # Output: [3, 2, 4, 5, 1]","title":"Let's see some examples"},{"location":"Python/1.3_List/#sorting-in-reverse-order","text":"sorted_numbers = sorted(numbers, reverse=True)","title":"Sorting in reverse order"},{"location":"Python/1.4_Dictionaries/","text":"Dictionaries are an Ordered Collection of Key-Value Pairs (Python 3.7 and Later) Starting from Python 3.7, dictionaries maintain the insertion order of items. This means the order in which you add key-value pairs is preserved when you iterate over the dictionary. Let's look at examples # Creating a dictionary person = { \"name\": \"Alice\", \"age\": 30, \"city\": \"Wonderland\" } # Adding a new key-value pair person[\"email\"] = \"alice@example.com\" # Iterating over the dictionary for key, value in person.items(): print(key, value) Output: name Alice age 30 city Wonderland email alice@example.com In this example, the order of key-value pairs is the same as the order in which they were added to the dictionary.","title":"1.4 Dictionaries"},{"location":"Python/1.4_Dictionaries/#dictionaries-are-an-ordered-collection-of-key-value-pairs-python-37-and-later","text":"Starting from Python 3.7, dictionaries maintain the insertion order of items. This means the order in which you add key-value pairs is preserved when you iterate over the dictionary.","title":"Dictionaries are an Ordered Collection of Key-Value Pairs (Python 3.7 and Later)"},{"location":"Python/1.4_Dictionaries/#lets-look-at-examples","text":"# Creating a dictionary person = { \"name\": \"Alice\", \"age\": 30, \"city\": \"Wonderland\" } # Adding a new key-value pair person[\"email\"] = \"alice@example.com\" # Iterating over the dictionary for key, value in person.items(): print(key, value) Output: name Alice age 30 city Wonderland email alice@example.com In this example, the order of key-value pairs is the same as the order in which they were added to the dictionary.","title":"Let's look at examples"},{"location":"Python/1.5_Lamda_Functions/","text":"Python Lamda Functions. No-name. One-line functions Lamda function is a small function having just one expression and written in just one line. They don't need any function name. They are very handy when you need a small one-line code. Lamda functions always start with lamda What is Lambda? A lambda function is a small function you write in one line, without even naming it. Here is how a normal function and its lamda counterpart would look like: Where to Use Lambda Lambda functions are handy for small, quick tasks where you don\u2019t need a full function. Here are a few places where they are commonly used: 1. Sorting with Custom Rules Let\u2019s say you have a list of names and you want to sort them by length instead of alphabetically. Normally, you would write a function, but with a lambda, it\u2019s easy: names = ['Rahul', 'Amit', 'Zara', 'Pooja'] sorted_names = sorted(names, key=lambda x: len(x)) Use lambda for sorting lists in a custom way. 2. Filtering Lists If you have a list of numbers and you only want the even ones, lambda makes it easy: numbers = [1, 2, 3, 4, 5, 6, 7, 8] even_numbers = list(filter(lambda x: x % 2 == 0, numbers)) Lambda functions are perfect for filtering lists. 3. Mapping Data Let\u2019s say you want to double every number in a list. Instead of writing a loop, you can use map() with a lambda: numbers = [1, 2, 3, 4] doubled = list(map(lambda x: x * 2, numbers)) Use lambda to quickly transform data in lists. 4. Filtering with Lambda If you need to filter out items from a list based on a condition, filter with a lambda function is a quick solution: numbers = [5, 12, 17, 24, 29] filtered_numbers = list(filter(lambda x: x > 15, numbers)) Lambda is great for filtering lists based on specific conditions. 5. Applying Lambda to a DataFrame When working with data, you can use apply with a lambda function to perform operations on each element in a DataFrame column: df['new_column'] = df['existing_column'].apply(lambda x: x + 10) Apply lambda functions to DataFrame columns for quick data manipulation. 6. Combining Lambda with List Comprehensions Sometimes, you might want to use a lambda function within a list comprehension for more complex operations: result = [(lambda x: x * 2)(x) for x in range(5)] Combine lambda functions with list comprehensions. 7. Lamda with IF-ELIF-ELSE You can use a lambda function to quickly check if a number is odd or even: odd_even_lambda = lambda i: 'Number is Even' if i % 2 == 0 else 'Number is Odd' You can use lamda with if IF-ELIF-ELSE logic Let's recap Don\u2019t expect lambda functions to shrink your entire code. They\u2019re only meant for small, one-line tasks where you don\u2019t need to name the function.","title":"Lamda Functions"},{"location":"Python/1.5_Lamda_Functions/#python-lamda-functions-no-name-one-line-functions","text":"Lamda function is a small function having just one expression and written in just one line. They don't need any function name. They are very handy when you need a small one-line code. Lamda functions always start with lamda","title":"Python Lamda Functions. No-name. One-line functions"},{"location":"Python/1.5_Lamda_Functions/#what-is-lambda","text":"A lambda function is a small function you write in one line, without even naming it. Here is how a normal function and its lamda counterpart would look like:","title":"What is Lambda?"},{"location":"Python/1.5_Lamda_Functions/#where-to-use-lambda","text":"Lambda functions are handy for small, quick tasks where you don\u2019t need a full function. Here are a few places where they are commonly used:","title":"Where to Use Lambda"},{"location":"Python/1.5_Lamda_Functions/#1-sorting-with-custom-rules","text":"Let\u2019s say you have a list of names and you want to sort them by length instead of alphabetically. Normally, you would write a function, but with a lambda, it\u2019s easy: names = ['Rahul', 'Amit', 'Zara', 'Pooja'] sorted_names = sorted(names, key=lambda x: len(x)) Use lambda for sorting lists in a custom way.","title":"1. Sorting with Custom Rules"},{"location":"Python/1.5_Lamda_Functions/#2-filtering-lists","text":"If you have a list of numbers and you only want the even ones, lambda makes it easy: numbers = [1, 2, 3, 4, 5, 6, 7, 8] even_numbers = list(filter(lambda x: x % 2 == 0, numbers)) Lambda functions are perfect for filtering lists.","title":"2. Filtering Lists"},{"location":"Python/1.5_Lamda_Functions/#3-mapping-data","text":"Let\u2019s say you want to double every number in a list. Instead of writing a loop, you can use map() with a lambda: numbers = [1, 2, 3, 4] doubled = list(map(lambda x: x * 2, numbers)) Use lambda to quickly transform data in lists.","title":"3. Mapping Data"},{"location":"Python/1.5_Lamda_Functions/#4-filtering-with-lambda","text":"If you need to filter out items from a list based on a condition, filter with a lambda function is a quick solution: numbers = [5, 12, 17, 24, 29] filtered_numbers = list(filter(lambda x: x > 15, numbers)) Lambda is great for filtering lists based on specific conditions.","title":"4. Filtering with Lambda"},{"location":"Python/1.5_Lamda_Functions/#5-applying-lambda-to-a-dataframe","text":"When working with data, you can use apply with a lambda function to perform operations on each element in a DataFrame column: df['new_column'] = df['existing_column'].apply(lambda x: x + 10) Apply lambda functions to DataFrame columns for quick data manipulation.","title":"5. Applying Lambda to a DataFrame"},{"location":"Python/1.5_Lamda_Functions/#6-combining-lambda-with-list-comprehensions","text":"Sometimes, you might want to use a lambda function within a list comprehension for more complex operations: result = [(lambda x: x * 2)(x) for x in range(5)] Combine lambda functions with list comprehensions.","title":"6. Combining Lambda with List Comprehensions"},{"location":"Python/1.5_Lamda_Functions/#7-lamda-with-if-elif-else","text":"You can use a lambda function to quickly check if a number is odd or even: odd_even_lambda = lambda i: 'Number is Even' if i % 2 == 0 else 'Number is Odd' You can use lamda with if IF-ELIF-ELSE logic","title":"7. Lamda with IF-ELIF-ELSE"},{"location":"Python/1.5_Lamda_Functions/#lets-recap","text":"Don\u2019t expect lambda functions to shrink your entire code. They\u2019re only meant for small, one-line tasks where you don\u2019t need to name the function.","title":"Let's recap"},{"location":"Python/1.9_Func_Modl_Lib/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Python Functions, Modules, and Libraries Let's clear the confusion about Python libraries, modules, functions & methods. Functions and Methods Functions A function is a block of code inside def . Functions are stanalone and indepandant. Here\u2019s a simple function: def greet(name): return f\"Hello, {name}!\" print(greet(\"Alice\")) # Output: Hello, Alice! Methods A methods are like functions, but they are connected to an object; Methods are present inside a class and when you call them you need to give them live objects of that class. Here's an example of a method: class Greeter: def __init__(self, name): self.name = name def greet(self): return f\"Hello, {self.name}!\" greeter = Greeter(\"Alice\") # Creating a live object of a class print(greeter.greet()) # Giving the method a live object Summary Functions : Standalone blocks of code inside a def: . E.g. math.sqrt() , os.path.exists() , json.loads() Methods : Functions that you write inside a class. They should always be inside a class and usually include an __init__ method for initialization. E.g. list.append() , str.upper() , dict.items() Modules A python module is a .py file. E.g. ikea.py . It contains classes, functions and other code. You import it using import ikea and get all the funcs clases in your new code. Usually the module name and its filename is same. math module is math.py Example of a Module: Create a file named ikea.py : # ikea.py def add(a, b): return a + b def subtract(a, b): return a - b ikea.py becomes ikea module . You can import it now to use it: # main.py import ikea ikea.add(1,2) # modulename.function import moduleName vs from moduleName import funcName import module : ```python import ikea #Imports the full module ikea.add() #To use functions. Use moduleName.funcName `` - ** from moduleName import funcName`** python from ikea import add # Imports only a function(s) from that module add() #To use. Directly use the function. No ModuleName required. import module vs from module import * import module : Imports the entire module with the packing : Module is imported as a complete package(with the cover). Access : To use the modules functions you need to add moduleName. . Example : python import math print(math.sqrt(16)) # Output: 4.0 Benefits : Keeps the module's namespace separate, which helps avoid naming conflicts. It also makes it clear where each function, class, or variable comes from. from module import * : Imports all functions, classes etc. but without the package(cover) directly into the current namespace : This means each function, class, and variable in the module is individually imported into your current namespace(cover is not imported). Access : You can use the imported items directly without the module name prefix. Example : python from math import * print(sqrt(16)) # Output: 4.0 Issues : Can create naming conflict. Say, you created your own sqrt function, Python won't know which sqrt to use(math's or yours?). This makes it hard to trace where functions, classes, or variables come from, especially in large programs or many modules. Libraries A library is a collection of modules. Libraries contain similar module for similar tasks. Example: Using the pandas Library The pandas library has many modules for data manipulation and analysis. When you import pandas , you get access to all its tools. import pandas as pd # Importing the entire library! data = { 'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35] } df = pd.DataFrame(data) # Here, `DataFrame` is a module in the `pandas` library print(df) Conclusion Python Universe : The entire Python ecosystem. Galaxies : Libraries (e.g., NumPy, Pandas). Solar Systems : Modules within libraries. Planets : Classes within modules. Moons : Methods within classes. Satellites : Functions within modules. Asteroids/Comets : Variables within functions or classes. Installing Libraries in Python To install libraries in Python, you typically use the `pip` command from the command prompt or terminal. `pip` is a package manager for Python that allows you to install, upgrade, and manage libraries. Installing Libraries from the Command Prompt To install a library, open your command prompt or terminal and type: pip install libraryname For example, to install the pandas library, you would type: pip install pandas Installing Libraries Inside Code You can also install libraries from within your Python code using the os module to run shell commands: import os os.system('pip install pandas') However, it is more common to use the command prompt for installing libraries to avoid unnecessary overhead in your scripts. Sources to Find Libraries You can find Python libraries on the Python Package Index (PyPI) website, pypi.org . PyPI is the official repository for Python packages where you can search for and learn about different libraries. Managing libraries When Sharing Python Code When you write a Python script that relies on external libraries, you need to ensure those libraries are installed on any system where the script will run. Here\u2019s how you can manage dependencies using a `requirements.txt` file. Creating a Python Script with Dependencies Suppose you have a Python script named data_analysis.py that uses the pandas library: # data_analysis.py import pandas as pd data = { 'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35] } df = pd.DataFrame(data) print(df) Creating a requirements.txt File To manage dependencies, create a requirements.txt file that lists all the libraries your script needs. You can generate this file automatically if the libraries are already installed in your environment: pip freeze > requirements.txt This command will create a requirements.txt file with contents similar to: pandas==1.3.3 Installing Dependencies on Another System When someone else wants to run your data_analysis.py script on another system, they should follow these steps: Clone or Copy the Project : Get the script and the requirements.txt file onto their system. Create a Virtual Environment (Optional but Recommended) : bash python -m venv myenv source myenv/bin/activate # On Windows use `myenv\\Scripts\\activate` Newer systems(linux versions) now makes it mandatory to install venv. Install Dependencies : bash pip install -r requirements.txt This command reads the requirements.txt file and installs all the listed libraries. This way the script will not run into can't find xxx error. Example Workflow Let me explain it with a sample workflow: On the Development System : ```bash # Create the Python script echo \" import pandas as pd data = { 'Name': ['Donald', 'Biden', 'Mia Khalifa'], 'Age': [25, 30, 35] } df = pd.DataFrame(data) print(df) \" > data_analysis.py # Install pandas if not already installed pip install pandas # Generate requirements.txt pip freeze > requirements.txt ``` On the Target System : Using a terminal or CMD. ```bash # Clone project or copy files scp user@development_system:/path/to/project/* /path/to/local/directory/ # Go to the project directory cd /path/to/local/directory # (Optional) Create venv(virtual env) python -m venv myenv source myenv/bin/activate # If you use windows you may have to use myenv\\Scripts\\activate # Install required libraries pip install -r requirements.txt # Run the script python data_analysis.py ``` This is how you ensure that all necessary dependancies are available on the other machine where the script is re-run.","title":"Library-Modules-Funcs"},{"location":"Python/1.9_Func_Modl_Lib/#python-functions-modules-and-libraries","text":"Let's clear the confusion about Python libraries, modules, functions & methods.","title":"Python Functions, Modules, and Libraries"},{"location":"Python/1.9_Func_Modl_Lib/#functions-and-methods","text":"","title":"Functions and Methods"},{"location":"Python/1.9_Func_Modl_Lib/#functions","text":"A function is a block of code inside def . Functions are stanalone and indepandant. Here\u2019s a simple function: def greet(name): return f\"Hello, {name}!\" print(greet(\"Alice\")) # Output: Hello, Alice!","title":"Functions"},{"location":"Python/1.9_Func_Modl_Lib/#methods","text":"A methods are like functions, but they are connected to an object; Methods are present inside a class and when you call them you need to give them live objects of that class. Here's an example of a method: class Greeter: def __init__(self, name): self.name = name def greet(self): return f\"Hello, {self.name}!\" greeter = Greeter(\"Alice\") # Creating a live object of a class print(greeter.greet()) # Giving the method a live object Summary Functions : Standalone blocks of code inside a def: . E.g. math.sqrt() , os.path.exists() , json.loads() Methods : Functions that you write inside a class. They should always be inside a class and usually include an __init__ method for initialization. E.g. list.append() , str.upper() , dict.items()","title":"Methods"},{"location":"Python/1.9_Func_Modl_Lib/#modules","text":"A python module is a .py file. E.g. ikea.py . It contains classes, functions and other code. You import it using import ikea and get all the funcs clases in your new code. Usually the module name and its filename is same. math module is math.py Example of a Module: Create a file named ikea.py : # ikea.py def add(a, b): return a + b def subtract(a, b): return a - b ikea.py becomes ikea module . You can import it now to use it: # main.py import ikea ikea.add(1,2) # modulename.function","title":"Modules"},{"location":"Python/1.9_Func_Modl_Lib/#import-modulename-vs-from-modulename-import-funcname","text":"import module : ```python import ikea #Imports the full module ikea.add() #To use functions. Use moduleName.funcName `` - ** from moduleName import funcName`** python from ikea import add # Imports only a function(s) from that module add() #To use. Directly use the function. No ModuleName required.","title":"import moduleName vs from moduleName import funcName"},{"location":"Python/1.9_Func_Modl_Lib/#import-module-vs-from-module-import","text":"import module : Imports the entire module with the packing : Module is imported as a complete package(with the cover). Access : To use the modules functions you need to add moduleName. . Example : python import math print(math.sqrt(16)) # Output: 4.0 Benefits : Keeps the module's namespace separate, which helps avoid naming conflicts. It also makes it clear where each function, class, or variable comes from. from module import * : Imports all functions, classes etc. but without the package(cover) directly into the current namespace : This means each function, class, and variable in the module is individually imported into your current namespace(cover is not imported). Access : You can use the imported items directly without the module name prefix. Example : python from math import * print(sqrt(16)) # Output: 4.0 Issues : Can create naming conflict. Say, you created your own sqrt function, Python won't know which sqrt to use(math's or yours?). This makes it hard to trace where functions, classes, or variables come from, especially in large programs or many modules.","title":"import module vs from module import *"},{"location":"Python/1.9_Func_Modl_Lib/#libraries","text":"A library is a collection of modules. Libraries contain similar module for similar tasks.","title":"Libraries"},{"location":"Python/1.9_Func_Modl_Lib/#example-using-the-pandas-library","text":"The pandas library has many modules for data manipulation and analysis. When you import pandas , you get access to all its tools. import pandas as pd # Importing the entire library! data = { 'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35] } df = pd.DataFrame(data) # Here, `DataFrame` is a module in the `pandas` library print(df)","title":"Example: Using the pandas Library"},{"location":"Python/1.9_Func_Modl_Lib/#conclusion","text":"Python Universe : The entire Python ecosystem. Galaxies : Libraries (e.g., NumPy, Pandas). Solar Systems : Modules within libraries. Planets : Classes within modules. Moons : Methods within classes. Satellites : Functions within modules. Asteroids/Comets : Variables within functions or classes.","title":"Conclusion"},{"location":"Python/1.9_Func_Modl_Lib/#installing-libraries-in-python","text":"To install libraries in Python, you typically use the `pip` command from the command prompt or terminal. `pip` is a package manager for Python that allows you to install, upgrade, and manage libraries.","title":"Installing Libraries in Python"},{"location":"Python/1.9_Func_Modl_Lib/#installing-libraries-from-the-command-prompt","text":"To install a library, open your command prompt or terminal and type: pip install libraryname For example, to install the pandas library, you would type: pip install pandas","title":"Installing Libraries from the Command Prompt"},{"location":"Python/1.9_Func_Modl_Lib/#installing-libraries-inside-code","text":"You can also install libraries from within your Python code using the os module to run shell commands: import os os.system('pip install pandas') However, it is more common to use the command prompt for installing libraries to avoid unnecessary overhead in your scripts.","title":"Installing Libraries Inside Code"},{"location":"Python/1.9_Func_Modl_Lib/#sources-to-find-libraries","text":"You can find Python libraries on the Python Package Index (PyPI) website, pypi.org . PyPI is the official repository for Python packages where you can search for and learn about different libraries.","title":"Sources to Find Libraries"},{"location":"Python/1.9_Func_Modl_Lib/#managing-libraries-when-sharing-python-code","text":"When you write a Python script that relies on external libraries, you need to ensure those libraries are installed on any system where the script will run. Here\u2019s how you can manage dependencies using a `requirements.txt` file.","title":"Managing libraries When Sharing Python Code"},{"location":"Python/1.9_Func_Modl_Lib/#creating-a-python-script-with-dependencies","text":"Suppose you have a Python script named data_analysis.py that uses the pandas library: # data_analysis.py import pandas as pd data = { 'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35] } df = pd.DataFrame(data) print(df)","title":"Creating a Python Script with Dependencies"},{"location":"Python/1.9_Func_Modl_Lib/#creating-a-requirementstxt-file","text":"To manage dependencies, create a requirements.txt file that lists all the libraries your script needs. You can generate this file automatically if the libraries are already installed in your environment: pip freeze > requirements.txt This command will create a requirements.txt file with contents similar to: pandas==1.3.3","title":"Creating a requirements.txt File"},{"location":"Python/1.9_Func_Modl_Lib/#installing-dependencies-on-another-system","text":"When someone else wants to run your data_analysis.py script on another system, they should follow these steps: Clone or Copy the Project : Get the script and the requirements.txt file onto their system. Create a Virtual Environment (Optional but Recommended) : bash python -m venv myenv source myenv/bin/activate # On Windows use `myenv\\Scripts\\activate` Newer systems(linux versions) now makes it mandatory to install venv. Install Dependencies : bash pip install -r requirements.txt This command reads the requirements.txt file and installs all the listed libraries. This way the script will not run into can't find xxx error.","title":"Installing Dependencies on Another System"},{"location":"Python/1.9_Func_Modl_Lib/#example-workflow","text":"Let me explain it with a sample workflow: On the Development System : ```bash # Create the Python script echo \" import pandas as pd data = { 'Name': ['Donald', 'Biden', 'Mia Khalifa'], 'Age': [25, 30, 35] } df = pd.DataFrame(data) print(df) \" > data_analysis.py # Install pandas if not already installed pip install pandas # Generate requirements.txt pip freeze > requirements.txt ``` On the Target System : Using a terminal or CMD. ```bash # Clone project or copy files scp user@development_system:/path/to/project/* /path/to/local/directory/ # Go to the project directory cd /path/to/local/directory # (Optional) Create venv(virtual env) python -m venv myenv source myenv/bin/activate # If you use windows you may have to use myenv\\Scripts\\activate # Install required libraries pip install -r requirements.txt # Run the script python data_analysis.py ``` This is how you ensure that all necessary dependancies are available on the other machine where the script is re-run.","title":"Example Workflow"},{"location":"Python/1_Python/","text":"Python version - python -V Variables Python Comments Python list For item in list While in list Python functions Python Snippets Get Current User info Populat MSSQL Tables with random data Python Gotchas NameError in Python Python Object-Oriented Programming (OOP) 1. Class 2. Object 3. Encapsulation 4. Inheritance 5. Polymorphism 6. Abstraction Key Benefits of OOP: Python version - python -V To check python version use run in terminal/CMD Remember : V in CAPS Variables Python variables don't have a type. They are just names. If you have a python list the variable holding it doesn't need to know what is the type. It just knows its a list and it has some collection. List in Python my look like arrays. But, they are more. They are full blown Python collection objects. This means, they come with ready-to-use list methods. Python Comments Python list List are like Arrays v2 Access list items with the Square Bracket [] First few important list methods print() pop() extend() insert() For item in list While in list Python functions Python function introduces two new keywords: def and return Python doesn't force you to provide type of arguments and return value You can send objects as arguments and get objects as return. You don't have to provide the type of the object Python Snippets Get Current User info import os import getpass print(\"Current user:\", getpass.getuser()) # Directory to check directory_to_check = \"/data/spark-warehouse/test_db.db/films\" # Get the current user running the Spark application current_user = getpass.getuser() print(f\"Current user: {current_user}\") # Check read, write, and execute permissions has_read_permission = os.access(directory_to_check, os.R_OK) has_write_permission = os.access(directory_to_check, os.W_OK) has_execute_permission = os.access(directory_to_check, os.X_OK) print(f\"Read permission: {has_read_permission}\") print(f\"Write permission: {has_write_permission}\") print(f\"Execute permission: {has_execute_permission}\") Populat MSSQL Tables with random data This Python script populates a SQL Server database table, dbo.student , with random data. The dbo.student table should have: id (integer): A unique identifier for each student. Name (varchar): A randomly generated student's first name. Age (integer): A randomly generated age between 18 and 30. Here's a step-by-step breakdown: Database Connection : Connects to the Oxford SQL Server database using Windows Authentication ( Trusted_Connection=yes ). Row Count : The num_rows variable determines how many rows of data to insert. Adjust this to change the number of rows added. Random Data with Faker : Uses the Faker library to produce random student names. Get Last ID Value : Checks the highest id in dbo.student to determine the next available ID. If no records are present, it starts from 0. Create INSERT Commands : The script creates individual INSERT commands for each new student record. Run INSERT Commands : Executes each INSERT command to add new rows to dbo.student . Commit and Disconnect : Saves the changes and ends the database connection. For the script to work, ensure the table exists. It's optimal to execute the script from Azure Data Studio on the server itself. Modify connection details if running from a different location. # Faker was not found hence we installed the faker library using pip. Alternatively you can the command in terminal(mac) or command prompt(windows) # Dont run it if faker already installed pip install faker import pyodbc from faker import Faker import random # Establish a connection to your SQL Server conn = pyodbc.connect('Driver={SQL Server};' 'Server=.;' 'Database=Oxford;' 'Trusted_Connection=yes;') # Define the number of rows to insert num_rows = 1000 # Change this to the desired number of rows # Create a Faker instance for generating random names fake = Faker() # Get the last primary key value in the table cursor = conn.cursor() cursor.execute(\"SELECT MAX(id) FROM dbo.student\") last_id = cursor.fetchone()[0] if last_id is None: last_id = 0 # Generate and execute the INSERT statements for i in range(1, num_rows + 1): last_id += 1 # Increment the primary key value student_name = fake.first_name() # Generate a random first name student_age = random.randint(18, 30) # Generate a random age between 18 and 30 sql = f\"INSERT INTO dbo.student VALUES ({last_id}, '{student_name}', {student_age})\" cursor.execute(sql) print(\"Executed succesfully. Rows updated: \" + str(i)) # Commit the changes and close the connection conn.commit() conn.close() Python Gotchas NameError in Python If you don't create a variable an use it. You will get NameError in Python. for i in range(5): print(x) To avoid, simple, create the variable and warm it up. x = 10 # Create and warm up for i in range(5): print(x) # Then use Note for code like for i in range(x) the initializatio happens. So, don't take such examples and start using variables without warming them up. Python Concepts: What are Python's built-in data types? Python's built-in data types include: Numbers : int , float , complex Sequences : list , tuple , range Text : str Sets : set , frozenset Mappings : dict Booleans : bool Binary Types : bytes , bytearray , memoryview Explain the difference between a list and a tuple in Python. A list is a mutable sequence, meaning its elements can be changed after creation. A tuple is an immutable sequence, meaning its elements cannot be changed after creation. What is a dictionary in Python, and how is it different from a list? A dictionary is an unordered collection of key-value pairs, where each key is unique. A list is an ordered collection of elements that can be of any type and can contain duplicates. What are functions in Python, and why are they used? Functions in Python are blocks of reusable code that perform a specific task. They help in organizing code, avoiding repetition, and improving readability. Explain the concept of list comprehensions in Python. List comprehensions provide a concise way to create lists by specifying an expression followed by a for clause. They are more compact and faster than traditional for loops. What is the purpose of the self keyword in Python classes? The self keyword in Python is used to represent the instance of the class. It allows access to the attributes and methods of the class within its own scope. What are decorators in Python? Decorators are functions that modify the behavior of other functions or methods. They are used to add functionality to existing code in a clean and readable way. Explain the difference between shallow copy and deep copy in Python. A shallow copy creates a new object but does not create copies of nested objects; it references the original nested objects. A deep copy creates a new object and recursively copies all objects found within the original, ensuring no references to the original objects. What is exception handling in Python? Exception handling in Python is done using try , except , else , and finally blocks. It allows programmers to handle runtime errors gracefully without crashing the program. Python Object-Oriented Programming (OOP) Object-Oriented Programming (OOP) is a programming paradigm based on the concept of objects that represent real-world entities. These objects encapsulate data (attributes or properties) and behavior (methods or functions). Below are the core concepts of OOP: 1. Class A blueprint or template for creating objects. Defines the structure (attributes) and behavior (methods) that the objects of the class will have. Example: class Car: def __init__(self, brand, model): self.brand = brand # Attribute self.model = model # Attribute def drive(self): # Method print(f\"The {self.brand} {self.model} is driving.\") 2. Object An instance of a class. Objects are created using the class blueprint. Each object can have unique values for its attributes. Example: car1 = Car(\"Toyota\", \"Camry\") car1.drive() # Output: The Toyota Camry is driving. 3. Encapsulation Restricts direct access to some of an object's data and methods, ensuring controlled interaction. Attributes can be made private by prefixing them with an underscore ( _ ) or double underscore ( __ ). Example: class BankAccount: def __init__(self, balance): self.__balance = balance # Private attribute def deposit(self, amount): self.__balance += amount # Controlled access def get_balance(self): return self.__balance # Controlled retrieval 4. Inheritance Allows a class (child) to inherit attributes and methods from another class (parent). Promotes code reuse. Example: class Vehicle: def __init__(self, brand): self.brand = brand class Car(Vehicle): def __init__(self, brand, model): super().__init__(brand) self.model = model 5. Polymorphism Allows objects to take on multiple forms. Enables methods to have the same name but behave differently based on the object calling them. Example: class Animal: def speak(self): print(\"Animal speaks\") class Dog(Animal): def speak(self): # Overriding the parent class method print(\"Dog barks\") class Cat(Animal): def speak(self): # Overriding the parent class method print(\"Cat meows\") animals = [Dog(), Cat()] for animal in animals: animal.speak() 6. Abstraction Hides complex implementation details and shows only the necessary features. Achieved using abstract classes or interfaces. Example: from abc import ABC, abstractmethod class Shape(ABC): @abstractmethod def area(self): pass class Rectangle(Shape): def __init__(self, width, height): self.width = width self.height = height def area(self): # Implementation of the abstract method return self.width * self.height Key Benefits of OOP: Code Reusability : Inheritance allows reusing code in new contexts. Modularity : Encapsulation and classes make code easier to manage. Scalability : Easier to extend functionality without altering existing code. Maintenance : Organized and readable structure simplifies debugging and updates.","title":"Python"},{"location":"Python/1_Python/#python-version-python-v","text":"To check python version use run in terminal/CMD Remember : V in CAPS","title":"Python version - python -V"},{"location":"Python/1_Python/#variables","text":"Python variables don't have a type. They are just names. If you have a python list the variable holding it doesn't need to know what is the type. It just knows its a list and it has some collection. List in Python my look like arrays. But, they are more. They are full blown Python collection objects. This means, they come with ready-to-use list methods.","title":"Variables"},{"location":"Python/1_Python/#python-comments","text":"","title":"Python Comments"},{"location":"Python/1_Python/#python-list","text":"List are like Arrays v2 Access list items with the Square Bracket [] First few important list methods print() pop() extend() insert()","title":"Python list"},{"location":"Python/1_Python/#for-item-in-list","text":"","title":"For item in list"},{"location":"Python/1_Python/#while-in-list","text":"","title":"While in list"},{"location":"Python/1_Python/#python-functions","text":"Python function introduces two new keywords: def and return Python doesn't force you to provide type of arguments and return value You can send objects as arguments and get objects as return. You don't have to provide the type of the object","title":"Python functions"},{"location":"Python/1_Python/#python-snippets","text":"","title":"Python Snippets"},{"location":"Python/1_Python/#get-current-user-info","text":"import os import getpass print(\"Current user:\", getpass.getuser()) # Directory to check directory_to_check = \"/data/spark-warehouse/test_db.db/films\" # Get the current user running the Spark application current_user = getpass.getuser() print(f\"Current user: {current_user}\") # Check read, write, and execute permissions has_read_permission = os.access(directory_to_check, os.R_OK) has_write_permission = os.access(directory_to_check, os.W_OK) has_execute_permission = os.access(directory_to_check, os.X_OK) print(f\"Read permission: {has_read_permission}\") print(f\"Write permission: {has_write_permission}\") print(f\"Execute permission: {has_execute_permission}\")","title":"Get Current User info"},{"location":"Python/1_Python/#populat-mssql-tables-with-random-data","text":"This Python script populates a SQL Server database table, dbo.student , with random data. The dbo.student table should have: id (integer): A unique identifier for each student. Name (varchar): A randomly generated student's first name. Age (integer): A randomly generated age between 18 and 30. Here's a step-by-step breakdown: Database Connection : Connects to the Oxford SQL Server database using Windows Authentication ( Trusted_Connection=yes ). Row Count : The num_rows variable determines how many rows of data to insert. Adjust this to change the number of rows added. Random Data with Faker : Uses the Faker library to produce random student names. Get Last ID Value : Checks the highest id in dbo.student to determine the next available ID. If no records are present, it starts from 0. Create INSERT Commands : The script creates individual INSERT commands for each new student record. Run INSERT Commands : Executes each INSERT command to add new rows to dbo.student . Commit and Disconnect : Saves the changes and ends the database connection. For the script to work, ensure the table exists. It's optimal to execute the script from Azure Data Studio on the server itself. Modify connection details if running from a different location. # Faker was not found hence we installed the faker library using pip. Alternatively you can the command in terminal(mac) or command prompt(windows) # Dont run it if faker already installed pip install faker import pyodbc from faker import Faker import random # Establish a connection to your SQL Server conn = pyodbc.connect('Driver={SQL Server};' 'Server=.;' 'Database=Oxford;' 'Trusted_Connection=yes;') # Define the number of rows to insert num_rows = 1000 # Change this to the desired number of rows # Create a Faker instance for generating random names fake = Faker() # Get the last primary key value in the table cursor = conn.cursor() cursor.execute(\"SELECT MAX(id) FROM dbo.student\") last_id = cursor.fetchone()[0] if last_id is None: last_id = 0 # Generate and execute the INSERT statements for i in range(1, num_rows + 1): last_id += 1 # Increment the primary key value student_name = fake.first_name() # Generate a random first name student_age = random.randint(18, 30) # Generate a random age between 18 and 30 sql = f\"INSERT INTO dbo.student VALUES ({last_id}, '{student_name}', {student_age})\" cursor.execute(sql) print(\"Executed succesfully. Rows updated: \" + str(i)) # Commit the changes and close the connection conn.commit() conn.close()","title":"Populat MSSQL Tables with random data"},{"location":"Python/1_Python/#python-gotchas","text":"","title":"Python Gotchas"},{"location":"Python/1_Python/#nameerror-in-python","text":"If you don't create a variable an use it. You will get NameError in Python. for i in range(5): print(x) To avoid, simple, create the variable and warm it up. x = 10 # Create and warm up for i in range(5): print(x) # Then use Note for code like for i in range(x) the initializatio happens. So, don't take such examples and start using variables without warming them up. Python Concepts: What are Python's built-in data types? Python's built-in data types include: Numbers : int , float , complex Sequences : list , tuple , range Text : str Sets : set , frozenset Mappings : dict Booleans : bool Binary Types : bytes , bytearray , memoryview Explain the difference between a list and a tuple in Python. A list is a mutable sequence, meaning its elements can be changed after creation. A tuple is an immutable sequence, meaning its elements cannot be changed after creation. What is a dictionary in Python, and how is it different from a list? A dictionary is an unordered collection of key-value pairs, where each key is unique. A list is an ordered collection of elements that can be of any type and can contain duplicates. What are functions in Python, and why are they used? Functions in Python are blocks of reusable code that perform a specific task. They help in organizing code, avoiding repetition, and improving readability. Explain the concept of list comprehensions in Python. List comprehensions provide a concise way to create lists by specifying an expression followed by a for clause. They are more compact and faster than traditional for loops. What is the purpose of the self keyword in Python classes? The self keyword in Python is used to represent the instance of the class. It allows access to the attributes and methods of the class within its own scope. What are decorators in Python? Decorators are functions that modify the behavior of other functions or methods. They are used to add functionality to existing code in a clean and readable way. Explain the difference between shallow copy and deep copy in Python. A shallow copy creates a new object but does not create copies of nested objects; it references the original nested objects. A deep copy creates a new object and recursively copies all objects found within the original, ensuring no references to the original objects. What is exception handling in Python? Exception handling in Python is done using try , except , else , and finally blocks. It allows programmers to handle runtime errors gracefully without crashing the program.","title":"NameError in Python"},{"location":"Python/1_Python/#python-object-oriented-programming-oop","text":"Object-Oriented Programming (OOP) is a programming paradigm based on the concept of objects that represent real-world entities. These objects encapsulate data (attributes or properties) and behavior (methods or functions). Below are the core concepts of OOP:","title":"Python Object-Oriented Programming (OOP)"},{"location":"Python/1_Python/#1-class","text":"A blueprint or template for creating objects. Defines the structure (attributes) and behavior (methods) that the objects of the class will have. Example: class Car: def __init__(self, brand, model): self.brand = brand # Attribute self.model = model # Attribute def drive(self): # Method print(f\"The {self.brand} {self.model} is driving.\")","title":"1. Class"},{"location":"Python/1_Python/#2-object","text":"An instance of a class. Objects are created using the class blueprint. Each object can have unique values for its attributes. Example: car1 = Car(\"Toyota\", \"Camry\") car1.drive() # Output: The Toyota Camry is driving.","title":"2. Object"},{"location":"Python/1_Python/#3-encapsulation","text":"Restricts direct access to some of an object's data and methods, ensuring controlled interaction. Attributes can be made private by prefixing them with an underscore ( _ ) or double underscore ( __ ). Example: class BankAccount: def __init__(self, balance): self.__balance = balance # Private attribute def deposit(self, amount): self.__balance += amount # Controlled access def get_balance(self): return self.__balance # Controlled retrieval","title":"3. Encapsulation"},{"location":"Python/1_Python/#4-inheritance","text":"Allows a class (child) to inherit attributes and methods from another class (parent). Promotes code reuse. Example: class Vehicle: def __init__(self, brand): self.brand = brand class Car(Vehicle): def __init__(self, brand, model): super().__init__(brand) self.model = model","title":"4. Inheritance"},{"location":"Python/1_Python/#5-polymorphism","text":"Allows objects to take on multiple forms. Enables methods to have the same name but behave differently based on the object calling them. Example: class Animal: def speak(self): print(\"Animal speaks\") class Dog(Animal): def speak(self): # Overriding the parent class method print(\"Dog barks\") class Cat(Animal): def speak(self): # Overriding the parent class method print(\"Cat meows\") animals = [Dog(), Cat()] for animal in animals: animal.speak()","title":"5. Polymorphism"},{"location":"Python/1_Python/#6-abstraction","text":"Hides complex implementation details and shows only the necessary features. Achieved using abstract classes or interfaces. Example: from abc import ABC, abstractmethod class Shape(ABC): @abstractmethod def area(self): pass class Rectangle(Shape): def __init__(self, width, height): self.width = width self.height = height def area(self): # Implementation of the abstract method return self.width * self.height","title":"6. Abstraction"},{"location":"Python/1_Python/#key-benefits-of-oop","text":"Code Reusability : Inheritance allows reusing code in new contexts. Modularity : Encapsulation and classes make code easier to manage. Scalability : Easier to extend functionality without altering existing code. Maintenance : Organized and readable structure simplifies debugging and updates.","title":"Key Benefits of OOP:"},{"location":"Python/Linux/","text":"Some essential unix commands Command Description Example id Displays user and group information for the current user. id dwdas sudo su Switches to the root user. sudo su ls -la Lists all files and directories, including hidden ones, with detailed information. ls -la chown Changes the ownership of files and directories. chown -R dwdas:dwdas /user/hive rm -rf Removes everything inside a specified directory but leaves the directory itself intact. rm -rf /home/dwdas/* mkdir -p Creates directories, including parent directories if they don't exist. mkdir -p /user/hive/warehouse whoami Displays the current username. whoami printenv Displays environment variables. printenv SPARK_HOME cd Changes the current directory. cd /user/hive pwd Prints the current working directory. pwd cp Copies files or directories. cp source_file destination_file mv Moves or renames files or directories. mv old_name new_name chmod Changes the permissions of files or directories. chmod 755 script.sh top Displays real-time system information and process details. top ps Displays currently running processes. ps aux kill Terminates a process. kill -9 process_id grep Searches for patterns within files. grep \"search_term\" filename find Searches for files and directories. find /path -name filename df Displays disk space usage. df -h du Displays disk usage of files and directories. du -sh directory tar Archives files. tar -cvf archive_name.tar directory curl Transfers data from or to a server. curl -O http://example.com/file wget Downloads files from the internet. wget http://example.com/file nano A simple text editor. nano filename vim A more advanced text editor. vim filename ssh Connects to a remote machine via SSH. ssh user@hostname scp Copies files between hosts over SSH. scp local_file user@remote_host:/remote_path docker Manages Docker containers. docker ps systemctl Manages system services. systemctl status service_name service Manages system services (older systems). service service_name start alias Creates a shortcut for a command. alias ll='ls -la' history Shows the command history. history","title":"Essential Unix Commands"},{"location":"Python/Linux/#some-essential-unix-commands","text":"Command Description Example id Displays user and group information for the current user. id dwdas sudo su Switches to the root user. sudo su ls -la Lists all files and directories, including hidden ones, with detailed information. ls -la chown Changes the ownership of files and directories. chown -R dwdas:dwdas /user/hive rm -rf Removes everything inside a specified directory but leaves the directory itself intact. rm -rf /home/dwdas/* mkdir -p Creates directories, including parent directories if they don't exist. mkdir -p /user/hive/warehouse whoami Displays the current username. whoami printenv Displays environment variables. printenv SPARK_HOME cd Changes the current directory. cd /user/hive pwd Prints the current working directory. pwd cp Copies files or directories. cp source_file destination_file mv Moves or renames files or directories. mv old_name new_name chmod Changes the permissions of files or directories. chmod 755 script.sh top Displays real-time system information and process details. top ps Displays currently running processes. ps aux kill Terminates a process. kill -9 process_id grep Searches for patterns within files. grep \"search_term\" filename find Searches for files and directories. find /path -name filename df Displays disk space usage. df -h du Displays disk usage of files and directories. du -sh directory tar Archives files. tar -cvf archive_name.tar directory curl Transfers data from or to a server. curl -O http://example.com/file wget Downloads files from the internet. wget http://example.com/file nano A simple text editor. nano filename vim A more advanced text editor. vim filename ssh Connects to a remote machine via SSH. ssh user@hostname scp Copies files between hosts over SSH. scp local_file user@remote_host:/remote_path docker Manages Docker containers. docker ps systemctl Manages system services. systemctl status service_name service Manages system services (older systems). service service_name start alias Creates a shortcut for a command. alias ll='ls -la' history Shows the command history. history","title":"Some essential unix commands"},{"location":"Python/Pyspark/","text":"Get all session information Analyse a dataframe Common df operations - Part 1 Common df operations - Part 2 withColumn and withColumnRenamed in PySpark Get all session information import os import sys from pyspark.sql import SparkSession # Initialize a Spark session spark = SparkSession.builder.appName(\"ExampleApp\").getOrCreate() # Print Spark information in a presentable format print(f\"Spark version: {spark.version}\") print(f\"Application name: {spark.sparkContext.appName}\") print(f\"Master URL: {spark.sparkContext.master}\") print(f\"Application ID: {spark.sparkContext.applicationId}\") print(f\"Number of executors: {len(spark.sparkContext._jsc.sc().statusTracker().getExecutorInfos())}\") print(f\"Cores per executor: {spark.sparkContext.defaultParallelism}\") # Try to get the memory per executor configuration try: executor_memory = spark.conf.get(\"spark.executor.memory\") print(f\"Memory per executor: {executor_memory}\") except Exception as e: print(\"Memory per executor: Configuration not found.\") # Print all configuration properties in a formatted way print(\"\\nAll configuration properties:\") for conf_key, conf_value in spark.sparkContext.getConf().getAll(): print(f\" {conf_key}: {conf_value}\") # Print active jobs and stages using correct methods print(f\"\\nActive jobs: {spark.sparkContext.statusTracker().getActiveJobIds()}\") print(f\"Active stages: {spark.sparkContext.statusTracker().getActiveStageIds()}\") # Print additional environment information print(f\"\\nSpark home: {os.getenv('SPARK_HOME')}\") print(f\"Java home: {os.getenv('JAVA_HOME')}\") print(f\"Python version: {sys.version}\") print(f\"Python executable: {sys.executable}\") print(f\"Current working directory: {os.getcwd()}\") print(f\"User home directory: {os.path.expanduser('~')}\") print(f\"System PATH: {os.getenv('PATH')}\") Analyse a dataframe # 1. Print the Schema print(\"# 1. Schema of the DataFrame:\") df.printSchema() # Displays the schema (column names, data types, nullability) # 2. Show the First Few Rows print(\"\\n# 2. First 10 Rows of the DataFrame:\") df.show(10, truncate=False) # Displays the first 10 rows without truncating columns # 3. Describe the Data print(\"\\n# 3. Summary Statistics for Numeric Columns:\") df.describe().show() # Provides summary statistics for numeric columns # 4. Count the Number of Rows row_count = df.count() # Returns the total number of rows print(f\"\\n# 4. Total Number of Rows: {row_count}\") # 5. Check for Null Values from pyspark.sql.functions import isnull, when, count print(\"\\n# 5. Number of Null Values in Each Column:\") df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show() # Counts null values in each column # 6. Get Column Names columns = df.columns # Returns a list of column names print(f\"\\n# 6. Column Names: {columns}\") # 7. Summary Statistics for All Columns print(\"\\n# 7. Summary Statistics for All Columns:\") df.summary().show() # Provides summary statistics for all columns # 8. Distinct Values in a Column print(\"\\n# 8. Distinct Values in the 'Invoice' Column:\") df.select(\"Invoice\").distinct().show() # Shows all distinct values in the \"Invoice\" column # 9. Data Types of Columns print(\"\\n# 9. Data Types of Each Column:\") print(df.dtypes) # Returns a list of tuples with each column\u2019s name and data type # 10. Display Summary Statistics for Specific Columns print(\"\\n# 10. Summary Statistics for 'Quantity' and 'Price' Columns:\") df.describe(\"Quantity\", \"Price\").show() # Summary stats for \"Quantity\" and \"Price\" columns # 11. Group and Aggregate Data print(\"\\n# 11. Group and Count by 'Country':\") df.groupBy(\"Country\").count().show() # Groups data by \"Country\" and counts occurrences # 12. Sample Data print(\"\\n# 12. Random 10% Sample of the DataFrame:\") df.sample(0.1).show() # Returns a 10% random sample of the DataFrame # 13. Check DataFrame Size row_count = df.count() column_count = len(df.columns) print(f\"\\n# 13. Number of Rows: {row_count}, Number of Columns: {column_count}\") # 14. Drop Duplicates print(\"\\n# 14. DataFrame After Dropping Duplicates Based on 'InvoiceNo':\") df.dropDuplicates([\"InvoiceNo\"]).show() # Removes duplicate rows based on \"InvoiceNo\" # 15. Check DataFrame Memory Usage memory_usage = df.rdd.map(lambda row: len(str(row))).reduce(lambda a, b: a + b) print(f\"\\n# 15. Estimated Memory Usage: {memory_usage} bytes\") Common df operations - Part 1 Operation Description Example select() Selects a subset of columns. df.select(\"col1\", \"col2\") filter() Filters rows based on a condition. df.filter(df[\"col\"] > 10) withColumn() Adds or replaces a column. df.withColumn(\"new_col\", df[\"col\"] * 2) drop() Drops one or more columns. df.drop(\"col1\", \"col2\") groupBy() Groups rows by specified columns. df.groupBy(\"col\").count() agg() Aggregates data based on a specified column or expression. df.groupBy(\"col\").agg({\"col2\": \"sum\"}) orderBy() Sorts the DataFrame by specified columns. df.orderBy(\"col\") join() Joins two DataFrames. df1.join(df2, df1[\"col1\"] == df2[\"col2\"], \"inner\") union() Unions two DataFrames. df1.union(df2) distinct() Removes duplicate rows. df.distinct() dropDuplicates() Drops duplicate rows based on specified columns. df.dropDuplicates([\"col1\", \"col2\"]) sample() Returns a random sample of the DataFrame. df.sample(0.1) count() Returns the number of rows in the DataFrame. df.count() show() Displays the first n rows of the DataFrame. df.show(5) collect() Returns all the rows as a list of Row objects. df.collect() repartition() Repartitions the DataFrame into the specified number of partitions. df.repartition(10) coalesce() Reduces the number of partitions in the DataFrame. df.coalesce(1) cache() Caches the DataFrame in memory. df.cache() persist() Persists the DataFrame with the specified storage level. df.persist(StorageLevel.DISK_ONLY) createOrReplaceTempView() Creates or replaces a temporary view with the DataFrame. df.createOrReplaceTempView(\"temp_view\") write Writes the DataFrame to a specified format (e.g., Parquet, CSV). df.write.format(\"parquet\").save(\"path/to/save\") printSchema() Prints the schema of the DataFrame. df.printSchema() dtypes Returns a list of tuples with column names and data types. df.dtypes schema Returns the schema of the DataFrame as a StructType . df.schema describe() Computes basic statistics for numeric and string columns. df.describe().show() explain() Prints the physical plan to execute the DataFrame query. df.explain() head() Returns the first n rows as a list of Row objects. df.head(5) first() Returns the first row as a Row object. df.first() withColumnRenamed() Renames an existing column. df.withColumnRenamed(\"old_name\", \"new_name\") cast() Converts the data type of a DataFrame column. df.withColumn(\"age\", col(\"age\").cast(\"integer\")) alias() Assigns an alias to a DataFrame column or DataFrame. df.select(col(\"col1\").alias(\"new_col1\")) Common df operations - Part 2 Operation Description Example select() Selects a subset of columns. df.select(\"col1\", \"col2\") filter() Filters rows based on a condition. df.filter(df[\"col\"] > 10) withColumn() Adds or replaces a column. df.withColumn(\"new_col\", df[\"col\"] * 2) drop() Drops one or more columns. df.drop(\"col1\", \"col2\") groupBy() Groups rows by specified columns. df.groupBy(\"col\").count() agg() Aggregates data based on a specified column or expression. df.groupBy(\"col\").agg({\"col2\": \"sum\"}) orderBy() Sorts the DataFrame by specified columns. df.orderBy(\"col\") join() Joins two DataFrames. df1.join(df2, df1[\"col1\"] == df2[\"col2\"], \"inner\") union() Unions two DataFrames. df1.union(df2) distinct() Removes duplicate rows. df.distinct() dropDuplicates() Drops duplicate rows based on specified columns. df.dropDuplicates([\"col1\", \"col2\"]) sample() Returns a random sample of the DataFrame. df.sample(0.1) count() Returns the number of rows in the DataFrame. df.count() show() Displays the first n rows of the DataFrame. df.show(5) collect() Returns all the rows as a list of Row objects. df.collect() repartition() Repartitions the DataFrame into the specified number of partitions. df.repartition(10) coalesce() Reduces the number of partitions in the DataFrame. df.coalesce(1) cache() Caches the DataFrame in memory. df.cache() persist() Persists the DataFrame with the specified storage level. df.persist(StorageLevel.DISK_ONLY) createOrReplaceTempView() Creates or replaces a temporary view with the DataFrame. df.createOrReplaceTempView(\"temp_view\") write Writes the DataFrame to a specified format (e.g., Parquet, CSV). df.write.format(\"parquet\").save(\"path/to/save\") printSchema() Prints the schema of the DataFrame. df.printSchema() dtypes Returns a list of tuples with column names and data types. df.dtypes schema Returns the schema of the DataFrame as a StructType . df.schema describe() Computes basic statistics for numeric and string columns. df.describe().show() explain() Prints the physical plan to execute the DataFrame query. df.explain() head() Returns the first n rows as a list of Row objects. df.head(5) first() Returns the first row as a Row object. df.first() withColumnRenamed() Renames an existing column. df.withColumnRenamed(\"old_name\", \"new_name\") cast() Converts the data type of a DataFrame column. df.withColumn(\"age\", col(\"age\").cast(\"integer\")) alias() Assigns an alias to a DataFrame column or DataFrame. df.select(col(\"col1\").alias(\"new_col1\")) withColumn and withColumnRenamed in PySpark Technique Description Example Code Add New Column Add a new column with a constant value storesDF.withColumn(\"constantColumn\", lit(1)) Rename Column Rename an existing column storesDF.withColumnRenamed(\"oldColumnName\", \"newColumnName\") Modify Existing Column Modify an existing column storesDF.withColumn(\"existingColumn\", col(\"existingColumn\") * 2) Split Column into Multiple Split a column into multiple columns storesDF.withColumn(\"part1\", split(col(\"compositeColumn\"), \"_\").getItem(0)).withColumn(\"part2\", split(col(\"compositeColumn\"), \"_\").getItem(1)) Cast Column to New Type Cast a column to a new type storesDF.withColumn(\"castedColumn\", col(\"columnToCast\").cast(\"Integer\")) Apply UDF Apply a user-defined function (UDF) storesDF.withColumn(\"newColumn\", udfFunction(col(\"existingColumn\"))) Conditional Column Add a column based on a condition storesDF.withColumn(\"newColumn\", when(col(\"conditionColumn\") > 0, \"Positive\").otherwise(\"Negative\")) Add Column from Expression Add a column from an expression storesDF.withColumn(\"expressionColumn\", col(\"columnA\") + col(\"columnB\")) Drop Column Drop a column storesDF.drop(\"columnToDrop\") Add Column with SQL Expression Add a column using SQL expression storesDF.withColumn(\"newColumn\", expr(\"existingColumn + 1\"))","title":"PySpark"},{"location":"Python/Pyspark/#get-all-session-information","text":"import os import sys from pyspark.sql import SparkSession # Initialize a Spark session spark = SparkSession.builder.appName(\"ExampleApp\").getOrCreate() # Print Spark information in a presentable format print(f\"Spark version: {spark.version}\") print(f\"Application name: {spark.sparkContext.appName}\") print(f\"Master URL: {spark.sparkContext.master}\") print(f\"Application ID: {spark.sparkContext.applicationId}\") print(f\"Number of executors: {len(spark.sparkContext._jsc.sc().statusTracker().getExecutorInfos())}\") print(f\"Cores per executor: {spark.sparkContext.defaultParallelism}\") # Try to get the memory per executor configuration try: executor_memory = spark.conf.get(\"spark.executor.memory\") print(f\"Memory per executor: {executor_memory}\") except Exception as e: print(\"Memory per executor: Configuration not found.\") # Print all configuration properties in a formatted way print(\"\\nAll configuration properties:\") for conf_key, conf_value in spark.sparkContext.getConf().getAll(): print(f\" {conf_key}: {conf_value}\") # Print active jobs and stages using correct methods print(f\"\\nActive jobs: {spark.sparkContext.statusTracker().getActiveJobIds()}\") print(f\"Active stages: {spark.sparkContext.statusTracker().getActiveStageIds()}\") # Print additional environment information print(f\"\\nSpark home: {os.getenv('SPARK_HOME')}\") print(f\"Java home: {os.getenv('JAVA_HOME')}\") print(f\"Python version: {sys.version}\") print(f\"Python executable: {sys.executable}\") print(f\"Current working directory: {os.getcwd()}\") print(f\"User home directory: {os.path.expanduser('~')}\") print(f\"System PATH: {os.getenv('PATH')}\")","title":"Get all session information"},{"location":"Python/Pyspark/#analyse-a-dataframe","text":"# 1. Print the Schema print(\"# 1. Schema of the DataFrame:\") df.printSchema() # Displays the schema (column names, data types, nullability) # 2. Show the First Few Rows print(\"\\n# 2. First 10 Rows of the DataFrame:\") df.show(10, truncate=False) # Displays the first 10 rows without truncating columns # 3. Describe the Data print(\"\\n# 3. Summary Statistics for Numeric Columns:\") df.describe().show() # Provides summary statistics for numeric columns # 4. Count the Number of Rows row_count = df.count() # Returns the total number of rows print(f\"\\n# 4. Total Number of Rows: {row_count}\") # 5. Check for Null Values from pyspark.sql.functions import isnull, when, count print(\"\\n# 5. Number of Null Values in Each Column:\") df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show() # Counts null values in each column # 6. Get Column Names columns = df.columns # Returns a list of column names print(f\"\\n# 6. Column Names: {columns}\") # 7. Summary Statistics for All Columns print(\"\\n# 7. Summary Statistics for All Columns:\") df.summary().show() # Provides summary statistics for all columns # 8. Distinct Values in a Column print(\"\\n# 8. Distinct Values in the 'Invoice' Column:\") df.select(\"Invoice\").distinct().show() # Shows all distinct values in the \"Invoice\" column # 9. Data Types of Columns print(\"\\n# 9. Data Types of Each Column:\") print(df.dtypes) # Returns a list of tuples with each column\u2019s name and data type # 10. Display Summary Statistics for Specific Columns print(\"\\n# 10. Summary Statistics for 'Quantity' and 'Price' Columns:\") df.describe(\"Quantity\", \"Price\").show() # Summary stats for \"Quantity\" and \"Price\" columns # 11. Group and Aggregate Data print(\"\\n# 11. Group and Count by 'Country':\") df.groupBy(\"Country\").count().show() # Groups data by \"Country\" and counts occurrences # 12. Sample Data print(\"\\n# 12. Random 10% Sample of the DataFrame:\") df.sample(0.1).show() # Returns a 10% random sample of the DataFrame # 13. Check DataFrame Size row_count = df.count() column_count = len(df.columns) print(f\"\\n# 13. Number of Rows: {row_count}, Number of Columns: {column_count}\") # 14. Drop Duplicates print(\"\\n# 14. DataFrame After Dropping Duplicates Based on 'InvoiceNo':\") df.dropDuplicates([\"InvoiceNo\"]).show() # Removes duplicate rows based on \"InvoiceNo\" # 15. Check DataFrame Memory Usage memory_usage = df.rdd.map(lambda row: len(str(row))).reduce(lambda a, b: a + b) print(f\"\\n# 15. Estimated Memory Usage: {memory_usage} bytes\")","title":"Analyse a dataframe"},{"location":"Python/Pyspark/#common-df-operations-part-1","text":"Operation Description Example select() Selects a subset of columns. df.select(\"col1\", \"col2\") filter() Filters rows based on a condition. df.filter(df[\"col\"] > 10) withColumn() Adds or replaces a column. df.withColumn(\"new_col\", df[\"col\"] * 2) drop() Drops one or more columns. df.drop(\"col1\", \"col2\") groupBy() Groups rows by specified columns. df.groupBy(\"col\").count() agg() Aggregates data based on a specified column or expression. df.groupBy(\"col\").agg({\"col2\": \"sum\"}) orderBy() Sorts the DataFrame by specified columns. df.orderBy(\"col\") join() Joins two DataFrames. df1.join(df2, df1[\"col1\"] == df2[\"col2\"], \"inner\") union() Unions two DataFrames. df1.union(df2) distinct() Removes duplicate rows. df.distinct() dropDuplicates() Drops duplicate rows based on specified columns. df.dropDuplicates([\"col1\", \"col2\"]) sample() Returns a random sample of the DataFrame. df.sample(0.1) count() Returns the number of rows in the DataFrame. df.count() show() Displays the first n rows of the DataFrame. df.show(5) collect() Returns all the rows as a list of Row objects. df.collect() repartition() Repartitions the DataFrame into the specified number of partitions. df.repartition(10) coalesce() Reduces the number of partitions in the DataFrame. df.coalesce(1) cache() Caches the DataFrame in memory. df.cache() persist() Persists the DataFrame with the specified storage level. df.persist(StorageLevel.DISK_ONLY) createOrReplaceTempView() Creates or replaces a temporary view with the DataFrame. df.createOrReplaceTempView(\"temp_view\") write Writes the DataFrame to a specified format (e.g., Parquet, CSV). df.write.format(\"parquet\").save(\"path/to/save\") printSchema() Prints the schema of the DataFrame. df.printSchema() dtypes Returns a list of tuples with column names and data types. df.dtypes schema Returns the schema of the DataFrame as a StructType . df.schema describe() Computes basic statistics for numeric and string columns. df.describe().show() explain() Prints the physical plan to execute the DataFrame query. df.explain() head() Returns the first n rows as a list of Row objects. df.head(5) first() Returns the first row as a Row object. df.first() withColumnRenamed() Renames an existing column. df.withColumnRenamed(\"old_name\", \"new_name\") cast() Converts the data type of a DataFrame column. df.withColumn(\"age\", col(\"age\").cast(\"integer\")) alias() Assigns an alias to a DataFrame column or DataFrame. df.select(col(\"col1\").alias(\"new_col1\"))","title":"Common df operations - Part 1"},{"location":"Python/Pyspark/#common-df-operations-part-2","text":"Operation Description Example select() Selects a subset of columns. df.select(\"col1\", \"col2\") filter() Filters rows based on a condition. df.filter(df[\"col\"] > 10) withColumn() Adds or replaces a column. df.withColumn(\"new_col\", df[\"col\"] * 2) drop() Drops one or more columns. df.drop(\"col1\", \"col2\") groupBy() Groups rows by specified columns. df.groupBy(\"col\").count() agg() Aggregates data based on a specified column or expression. df.groupBy(\"col\").agg({\"col2\": \"sum\"}) orderBy() Sorts the DataFrame by specified columns. df.orderBy(\"col\") join() Joins two DataFrames. df1.join(df2, df1[\"col1\"] == df2[\"col2\"], \"inner\") union() Unions two DataFrames. df1.union(df2) distinct() Removes duplicate rows. df.distinct() dropDuplicates() Drops duplicate rows based on specified columns. df.dropDuplicates([\"col1\", \"col2\"]) sample() Returns a random sample of the DataFrame. df.sample(0.1) count() Returns the number of rows in the DataFrame. df.count() show() Displays the first n rows of the DataFrame. df.show(5) collect() Returns all the rows as a list of Row objects. df.collect() repartition() Repartitions the DataFrame into the specified number of partitions. df.repartition(10) coalesce() Reduces the number of partitions in the DataFrame. df.coalesce(1) cache() Caches the DataFrame in memory. df.cache() persist() Persists the DataFrame with the specified storage level. df.persist(StorageLevel.DISK_ONLY) createOrReplaceTempView() Creates or replaces a temporary view with the DataFrame. df.createOrReplaceTempView(\"temp_view\") write Writes the DataFrame to a specified format (e.g., Parquet, CSV). df.write.format(\"parquet\").save(\"path/to/save\") printSchema() Prints the schema of the DataFrame. df.printSchema() dtypes Returns a list of tuples with column names and data types. df.dtypes schema Returns the schema of the DataFrame as a StructType . df.schema describe() Computes basic statistics for numeric and string columns. df.describe().show() explain() Prints the physical plan to execute the DataFrame query. df.explain() head() Returns the first n rows as a list of Row objects. df.head(5) first() Returns the first row as a Row object. df.first() withColumnRenamed() Renames an existing column. df.withColumnRenamed(\"old_name\", \"new_name\") cast() Converts the data type of a DataFrame column. df.withColumn(\"age\", col(\"age\").cast(\"integer\")) alias() Assigns an alias to a DataFrame column or DataFrame. df.select(col(\"col1\").alias(\"new_col1\"))","title":"Common df operations - Part 2"},{"location":"Python/Pyspark/#withcolumn-and-withcolumnrenamed-in-pyspark","text":"Technique Description Example Code Add New Column Add a new column with a constant value storesDF.withColumn(\"constantColumn\", lit(1)) Rename Column Rename an existing column storesDF.withColumnRenamed(\"oldColumnName\", \"newColumnName\") Modify Existing Column Modify an existing column storesDF.withColumn(\"existingColumn\", col(\"existingColumn\") * 2) Split Column into Multiple Split a column into multiple columns storesDF.withColumn(\"part1\", split(col(\"compositeColumn\"), \"_\").getItem(0)).withColumn(\"part2\", split(col(\"compositeColumn\"), \"_\").getItem(1)) Cast Column to New Type Cast a column to a new type storesDF.withColumn(\"castedColumn\", col(\"columnToCast\").cast(\"Integer\")) Apply UDF Apply a user-defined function (UDF) storesDF.withColumn(\"newColumn\", udfFunction(col(\"existingColumn\"))) Conditional Column Add a column based on a condition storesDF.withColumn(\"newColumn\", when(col(\"conditionColumn\") > 0, \"Positive\").otherwise(\"Negative\")) Add Column from Expression Add a column from an expression storesDF.withColumn(\"expressionColumn\", col(\"columnA\") + col(\"columnB\")) Drop Column Drop a column storesDF.drop(\"columnToDrop\") Add Column with SQL Expression Add a column using SQL expression storesDF.withColumn(\"newColumn\", expr(\"existingColumn + 1\"))","title":"withColumn and withColumnRenamed in PySpark"},{"location":"Python/PythonScripts/","text":"Csv files creator with random data Employee CSV creator Here's a Python script to create CSV files with random values for specific fields. You can decide how many files to create and how many rows each file should have. This script uses the Faker library to generate realistic random data. So, before running the script, you must install Faker , or you will get ModuleNotFoundError: No module named 'faker' . pip install faker Then, run the script from Jupyter notebook or save it as a .py file and run it from the terminal: import csv import os from faker import Faker # Initialize Faker. This is the non-sense value creator. Haha fake = Faker() def generate_random_data(num_rows): data = [] for _ in range(num_rows): row = { 'id': fake.unique.random_int(min=1, max=1000000), 'first_name': fake.first_name(), 'last_name': fake.last_name(), 'passport_number': fake.unique.bothify(text='???########'), 'country': fake.country(), 'date_of_birth': fake.date_of_birth(minimum_age=18, maximum_age=80), 'profession': fake.job() } data.append(row) return data def create_csv_file(file_name, num_rows): data = generate_random_data(num_rows) with open(file_name, mode='w', newline='') as file: writer = csv.DictWriter(file, fieldnames=data[0].keys()) writer.writeheader() writer.writerows(data) def create_multiple_files(num_files, num_rows): for i in range(num_files): file_name = f'file_{i+1}.csv' create_csv_file(file_name, num_rows) print(f'Created {file_name}') # Provide the number of files and no of rows per file num_files = 5 # Change this to the number of files you want to create num_rows = 100 # Change this to the number of rows per file create_multiple_files(num_files, num_rows) Note: The files are outputted to the current working directory. To know use this command: python import os os.getcwd() Output Information The script creates CSV files with the following columns: - id : A unique identifier for each row - first_name : A randomly generated first name - last_name : A randomly generated last name - passport_number : A randomly generated passport number - country : A randomly generated country name - date_of_birth : A randomly generated date of birth - profession : A randomly generated profession Sample Row Here is an example of what a row in the CSV file might look like: id,first_name,last_name,passport_number,country,date_of_birth,profession 123456,John,Doe,ABC123456789,United States,1980-05-15,Software Developer Customization To change the number of files created, modify the num_files variable. To change the number of rows in each file, modify the num_rows variable. For example, to create 10 files with 50 rows each, set: num_files = 10 num_rows = 50 Then, run the script, and it will generate the specified number of CSV files with the desired number of rows. Insurance - Policy Issuance Process Python script to generate CSV files specifically for the Policy Issuance Process. import csv import os from faker import Faker from datetime import datetime # Initialize Faker fake = Faker() def generate_random_data(num_rows, start_index): data = [] for i in range(num_rows): # Use timestamp and start_index to generate a unique policy number policy_number = f'{start_index + i}_{int(datetime.now().timestamp() * 1000)}' row = { 'policy_number': policy_number, 'policyholder_name': fake.name(), 'insured_name': fake.name(), 'policy_type': fake.random_element(elements=('Life', 'Health', 'Auto', 'Home')), 'effective_date': fake.date_this_decade(), 'expiration_date': fake.date_this_decade(), 'premium_amount': round(fake.random_number(digits=5), 2), 'beneficiary_name': fake.name(), 'beneficiary_contact': fake.phone_number(), 'agent_name': fake.name(), 'agent_contact': fake.phone_number(), 'coverage_details': fake.sentence(nb_words=6), 'endorsements_riders': fake.sentence(nb_words=4), 'underwriter_name': fake.name() } data.append(row) return data def create_csv_file(file_name, num_rows, start_index): data = generate_random_data(num_rows, start_index) with open(file_name, mode='w', newline='') as file: writer = csv.DictWriter(file, fieldnames=data[0].keys()) writer.writeheader() writer.writerows(data) def create_multiple_files(num_files, num_rows): for i in range(num_files): file_name = f'policy_issuance_file_{i+1}.csv' create_csv_file(file_name, num_rows, i * num_rows) print(f'Created {file_name}') # Specify the number of files and rows per file num_files = 5 # Change this to the number of files you want to create num_rows = 100 # Change this to the number of rows per file create_multiple_files(num_files, num_rows) Explanation: Fields : policy_number : A unique identifier for the policy. policyholder_name : The name of the person holding the policy. insured_name : The name of the person covered by the policy. policy_type : The type of insurance (e.g., Life, Health, Auto, Home). effective_date : The start date of the policy. expiration_date : The end date of the policy. premium_amount : The cost of the policy. beneficiary_name : The name of the beneficiary. beneficiary_contact : The contact details of the beneficiary. agent_name : The name of the agent. agent_contact : The contact details of the agent. coverage_details : The specific coverage information. endorsements_riders : Any additional terms or coverage. underwriter_name : The name of the underwriter. generate_random_data(num_rows) : Generates a list of dictionaries with random data for the specified number of rows. create_csv_file(file_name, num_rows) : Creates a CSV file with the given file name and writes the generated random data into it. create_multiple_files(num_files, num_rows) : Creates the specified number of CSV files, each with the specified number of rows. To customize, you can change num_files to the number of files you want to create and num_rows to the number of rows per file. Save this script as generate_policy_issuance_csv.py and run it to generate your files.","title":"Python Sample Scripts"},{"location":"Python/PythonScripts/#csv-files-creator-with-random-data","text":"","title":"Csv files creator with random data"},{"location":"Python/PythonScripts/#employee-csv-creator","text":"Here's a Python script to create CSV files with random values for specific fields. You can decide how many files to create and how many rows each file should have. This script uses the Faker library to generate realistic random data. So, before running the script, you must install Faker , or you will get ModuleNotFoundError: No module named 'faker' . pip install faker Then, run the script from Jupyter notebook or save it as a .py file and run it from the terminal: import csv import os from faker import Faker # Initialize Faker. This is the non-sense value creator. Haha fake = Faker() def generate_random_data(num_rows): data = [] for _ in range(num_rows): row = { 'id': fake.unique.random_int(min=1, max=1000000), 'first_name': fake.first_name(), 'last_name': fake.last_name(), 'passport_number': fake.unique.bothify(text='???########'), 'country': fake.country(), 'date_of_birth': fake.date_of_birth(minimum_age=18, maximum_age=80), 'profession': fake.job() } data.append(row) return data def create_csv_file(file_name, num_rows): data = generate_random_data(num_rows) with open(file_name, mode='w', newline='') as file: writer = csv.DictWriter(file, fieldnames=data[0].keys()) writer.writeheader() writer.writerows(data) def create_multiple_files(num_files, num_rows): for i in range(num_files): file_name = f'file_{i+1}.csv' create_csv_file(file_name, num_rows) print(f'Created {file_name}') # Provide the number of files and no of rows per file num_files = 5 # Change this to the number of files you want to create num_rows = 100 # Change this to the number of rows per file create_multiple_files(num_files, num_rows) Note: The files are outputted to the current working directory. To know use this command: python import os os.getcwd()","title":"Employee CSV creator"},{"location":"Python/PythonScripts/#output-information","text":"The script creates CSV files with the following columns: - id : A unique identifier for each row - first_name : A randomly generated first name - last_name : A randomly generated last name - passport_number : A randomly generated passport number - country : A randomly generated country name - date_of_birth : A randomly generated date of birth - profession : A randomly generated profession","title":"Output Information"},{"location":"Python/PythonScripts/#sample-row","text":"Here is an example of what a row in the CSV file might look like: id,first_name,last_name,passport_number,country,date_of_birth,profession 123456,John,Doe,ABC123456789,United States,1980-05-15,Software Developer","title":"Sample Row"},{"location":"Python/PythonScripts/#customization","text":"To change the number of files created, modify the num_files variable. To change the number of rows in each file, modify the num_rows variable. For example, to create 10 files with 50 rows each, set: num_files = 10 num_rows = 50 Then, run the script, and it will generate the specified number of CSV files with the desired number of rows.","title":"Customization"},{"location":"Python/PythonScripts/#insurance-policy-issuance-process","text":"Python script to generate CSV files specifically for the Policy Issuance Process. import csv import os from faker import Faker from datetime import datetime # Initialize Faker fake = Faker() def generate_random_data(num_rows, start_index): data = [] for i in range(num_rows): # Use timestamp and start_index to generate a unique policy number policy_number = f'{start_index + i}_{int(datetime.now().timestamp() * 1000)}' row = { 'policy_number': policy_number, 'policyholder_name': fake.name(), 'insured_name': fake.name(), 'policy_type': fake.random_element(elements=('Life', 'Health', 'Auto', 'Home')), 'effective_date': fake.date_this_decade(), 'expiration_date': fake.date_this_decade(), 'premium_amount': round(fake.random_number(digits=5), 2), 'beneficiary_name': fake.name(), 'beneficiary_contact': fake.phone_number(), 'agent_name': fake.name(), 'agent_contact': fake.phone_number(), 'coverage_details': fake.sentence(nb_words=6), 'endorsements_riders': fake.sentence(nb_words=4), 'underwriter_name': fake.name() } data.append(row) return data def create_csv_file(file_name, num_rows, start_index): data = generate_random_data(num_rows, start_index) with open(file_name, mode='w', newline='') as file: writer = csv.DictWriter(file, fieldnames=data[0].keys()) writer.writeheader() writer.writerows(data) def create_multiple_files(num_files, num_rows): for i in range(num_files): file_name = f'policy_issuance_file_{i+1}.csv' create_csv_file(file_name, num_rows, i * num_rows) print(f'Created {file_name}') # Specify the number of files and rows per file num_files = 5 # Change this to the number of files you want to create num_rows = 100 # Change this to the number of rows per file create_multiple_files(num_files, num_rows)","title":"Insurance - Policy Issuance Process"},{"location":"Python/PythonScripts/#explanation","text":"Fields : policy_number : A unique identifier for the policy. policyholder_name : The name of the person holding the policy. insured_name : The name of the person covered by the policy. policy_type : The type of insurance (e.g., Life, Health, Auto, Home). effective_date : The start date of the policy. expiration_date : The end date of the policy. premium_amount : The cost of the policy. beneficiary_name : The name of the beneficiary. beneficiary_contact : The contact details of the beneficiary. agent_name : The name of the agent. agent_contact : The contact details of the agent. coverage_details : The specific coverage information. endorsements_riders : Any additional terms or coverage. underwriter_name : The name of the underwriter. generate_random_data(num_rows) : Generates a list of dictionaries with random data for the specified number of rows. create_csv_file(file_name, num_rows) : Creates a CSV file with the given file name and writes the generated random data into it. create_multiple_files(num_files, num_rows) : Creates the specified number of CSV files, each with the specified number of rows. To customize, you can change num_files to the number of files you want to create and num_rows to the number of rows per file. Save this script as generate_policy_issuance_csv.py and run it to generate your files.","title":"Explanation:"},{"location":"Python/GraphAPIJupyter/GraphAPIUsingJuputer/","text":"Running Microsoft Graph quick start Python code from a jupyter notebook Background Getting Started 1. Download the Sample Code 2. Setting Up the Notebook 3. Adapting the Code for Jupyter 4. Finalizing the Setup Cnclusion Running Microsoft Graph quick start Python code from a jupyter notebook Background In this guide I will show you how to run Microsoft's example code in a Jupyter notebook. Usually, this code is meant to be run in a terminal, but with a few small changes, you can run it in a notebook instead. Many people like this way because it's more familiar and easy to use. We'll focus on how to make these changes so the code works in Jupyter, without getting into more details about the code. Getting Started 1. Download the Sample Code Navigate to the Microsoft Graph Quick Start page. 2. Select Python as your language of choice. 3. Click on Get a client ID and log in using your personal, work, or school account. Note: An Outlook account is necessary for this step. 4. Upon successful login, a client ID will be presented to you. Make sure to save this ID. 5. Click on Download the code sample and save the msgraph-training-python.zip file. Unzip its contents into a directory where you have permission to run Python code, such as a Visual Studio Code workspace. 2. Setting Up the Notebook Open a Jupyter notebook in Visual Studio Code (VS Code) and navigate ( cd ) to the graphtutorial folder (e.g., %cd <Path to>\\graphtutorial ). Install the required dependencies by running pip install -r requirements.txt in a notebook cell. This process may take about a minute. 3. Adapting the Code for Jupyter Attempt to run the main.py file directly in a notebook cell with the command %run main.py . This will likely result in an error due to a conflict between Jupyter's and asyncio's event loops. 2. To resolve this, install the nest_asyncio package with pip install nest_asyncio . This package allows for the nesting of asyncio's event loop, facilitating the running of asynchronous code in Jupyter. 3. In a new cell, copy and paste the entire code from main.py and add the following lines just before the main() function: ```python import nest_asyncio nest_asyncio.apply() ``` Comment out the last line of the script to prevent it from executing immediately. 5. Run the modified cell. You should not see any output yet. 6. In a new cell, execute await main() to run the main function asynchronously. This should produce the expected output. 4. Finalizing the Setup Follow the on-screen instructions and navigate to Microsoft Device Login to enter the provided code. Complete the login process as prompted. 2. A selection box or palette will appear at the top of VS Code. Enter your choice (e.g., 2 to view emails) and press Enter. 3. Once you've completed your tasks, enter 0 at the top of the input box and press Enter to conclude the session. Cnclusion I have put the juputer notebook vrsion of the code here . All you have to do is place it in your \\msgraph-training-python\\graphtutorial folder. Open it and run cell by cell.","title":"Graph API - Juputer"},{"location":"Python/GraphAPIJupyter/GraphAPIUsingJuputer/#running-microsoft-graph-quick-start-python-code-from-a-jupyter-notebook","text":"","title":"Running Microsoft Graph quick start Python code from a jupyter notebook"},{"location":"Python/GraphAPIJupyter/GraphAPIUsingJuputer/#background","text":"In this guide I will show you how to run Microsoft's example code in a Jupyter notebook. Usually, this code is meant to be run in a terminal, but with a few small changes, you can run it in a notebook instead. Many people like this way because it's more familiar and easy to use. We'll focus on how to make these changes so the code works in Jupyter, without getting into more details about the code.","title":"Background"},{"location":"Python/GraphAPIJupyter/GraphAPIUsingJuputer/#getting-started","text":"","title":"Getting Started"},{"location":"Python/GraphAPIJupyter/GraphAPIUsingJuputer/#1-download-the-sample-code","text":"Navigate to the Microsoft Graph Quick Start page. 2. Select Python as your language of choice. 3. Click on Get a client ID and log in using your personal, work, or school account. Note: An Outlook account is necessary for this step. 4. Upon successful login, a client ID will be presented to you. Make sure to save this ID. 5. Click on Download the code sample and save the msgraph-training-python.zip file. Unzip its contents into a directory where you have permission to run Python code, such as a Visual Studio Code workspace.","title":"1. Download the Sample Code"},{"location":"Python/GraphAPIJupyter/GraphAPIUsingJuputer/#2-setting-up-the-notebook","text":"Open a Jupyter notebook in Visual Studio Code (VS Code) and navigate ( cd ) to the graphtutorial folder (e.g., %cd <Path to>\\graphtutorial ). Install the required dependencies by running pip install -r requirements.txt in a notebook cell. This process may take about a minute.","title":"2. Setting Up the Notebook"},{"location":"Python/GraphAPIJupyter/GraphAPIUsingJuputer/#3-adapting-the-code-for-jupyter","text":"Attempt to run the main.py file directly in a notebook cell with the command %run main.py . This will likely result in an error due to a conflict between Jupyter's and asyncio's event loops. 2. To resolve this, install the nest_asyncio package with pip install nest_asyncio . This package allows for the nesting of asyncio's event loop, facilitating the running of asynchronous code in Jupyter. 3. In a new cell, copy and paste the entire code from main.py and add the following lines just before the main() function: ```python import nest_asyncio nest_asyncio.apply() ``` Comment out the last line of the script to prevent it from executing immediately. 5. Run the modified cell. You should not see any output yet. 6. In a new cell, execute await main() to run the main function asynchronously. This should produce the expected output.","title":"3. Adapting the Code for Jupyter"},{"location":"Python/GraphAPIJupyter/GraphAPIUsingJuputer/#4-finalizing-the-setup","text":"Follow the on-screen instructions and navigate to Microsoft Device Login to enter the provided code. Complete the login process as prompted. 2. A selection box or palette will appear at the top of VS Code. Enter your choice (e.g., 2 to view emails) and press Enter. 3. Once you've completed your tasks, enter 0 at the top of the input box and press Enter to conclude the session.","title":"4. Finalizing the Setup"},{"location":"Python/GraphAPIJupyter/GraphAPIUsingJuputer/#cnclusion","text":"I have put the juputer notebook vrsion of the code here . All you have to do is place it in your \\msgraph-training-python\\graphtutorial folder. Open it and run cell by cell.","title":"Cnclusion"},{"location":"SQL/1.0.0_dbt/","text":"What is dbt? dbt stands for data built tool. It's a tool to run SQL as workflows. It's almost all-SQL. dbt is the T in ELT. When you use dbt the data is already present in the final destination. Why dbt? Opensource. Free. Development-Testing-Deployment-Documentation-DataLineage So easily. Integrates so well with CI/CD. Resusage code with Macros and Jinja? So why not just use a SQL notebook in databricks? Have you seen how notebooks run, one cell after another. Do you think workflows are always like a train? Long and linear? Workflows have branches like trees. dbt is far more than what databricks sql notebook's one cell at a time offers. So, to summarize, with dbt, you can run sql on your data which is already inside like enjoying in house like lakehouse, beachhouse, warehouse etc. Explain more With dbt you can put your complex sql into small small chunks. All your coding good practices like modular kitchen, git, CI/ABCD you can do with dbt. How to install dbt? Is it local or on cloud? DBT has two ways to install: Local(dbt core): Here you just install a command line tool. It conneccts with databases etc with 'adapters' Cloud(dbt cloud): This is not just cloud. It offers all fancy stuffs like: User interface, job scheduling, CI/CD, hosting documentation, monitoring, alerting, integrated IDE, CLI(to connect from local) Steps Install dbt locally on Windows Install python on your system. Install VS Code Install python and dbt extensions in VS Code Awesome links DBT Guides DBT Guides","title":"dbt"},{"location":"SQL/1.0.0_dbt/#what-is-dbt","text":"dbt stands for data built tool. It's a tool to run SQL as workflows. It's almost all-SQL. dbt is the T in ELT. When you use dbt the data is already present in the final destination.","title":"What is dbt?"},{"location":"SQL/1.0.0_dbt/#why-dbt","text":"Opensource. Free. Development-Testing-Deployment-Documentation-DataLineage So easily. Integrates so well with CI/CD. Resusage code with Macros and Jinja?","title":"Why dbt?"},{"location":"SQL/1.0.0_dbt/#so-why-not-just-use-a-sql-notebook-in-databricks","text":"Have you seen how notebooks run, one cell after another. Do you think workflows are always like a train? Long and linear? Workflows have branches like trees. dbt is far more than what databricks sql notebook's one cell at a time offers. So, to summarize, with dbt, you can run sql on your data which is already inside like enjoying in house like lakehouse, beachhouse, warehouse etc.","title":"So why not just use a SQL notebook in databricks?"},{"location":"SQL/1.0.0_dbt/#explain-more","text":"With dbt you can put your complex sql into small small chunks. All your coding good practices like modular kitchen, git, CI/ABCD you can do with dbt.","title":"Explain more"},{"location":"SQL/1.0.0_dbt/#how-to-install-dbt-is-it-local-or-on-cloud","text":"DBT has two ways to install: Local(dbt core): Here you just install a command line tool. It conneccts with databases etc with 'adapters' Cloud(dbt cloud): This is not just cloud. It offers all fancy stuffs like: User interface, job scheduling, CI/CD, hosting documentation, monitoring, alerting, integrated IDE, CLI(to connect from local)","title":"How to install dbt? Is it local or on cloud?"},{"location":"SQL/1.0.0_dbt/#steps-install-dbt-locally-on-windows","text":"Install python on your system. Install VS Code Install python and dbt extensions in VS Code","title":"Steps Install dbt locally on Windows"},{"location":"SQL/1.0.0_dbt/#awesome-links","text":"DBT Guides DBT Guides","title":"Awesome links"},{"location":"SQL/1.0.1_setup_simple_dbt_project/","text":"Background Here, I will show you how to setup a simple dbt project setup from scratch. You will just need a windows laptop for this as I have done it on my Windows machine. Google console pre-requisite setup Step 1: Create a Google Cloud Project Go to the Google Cloud Console : Google Cloud Console . Create a New Project : In the top left corner, click on the Project dropdown and select New Project . Enter a Project Name (e.g., my-dbt-project ). Note down the Project ID ; you\u2019ll need this later. This is typically something like my-dbt-project-123456 . Set the Project as Active : Click on the Project dropdown again and select your newly created project to make it active. Step 2: Enable BigQuery API Enable the BigQuery API : In the Google Cloud Console, navigate to APIs & Services > Library . Search for \"BigQuery API\" and click Enable . Step 3: Create a Service Account Create a Service Account : Go to IAM & Admin > Service Accounts in the Google Cloud Console. Click + CREATE SERVICE ACCOUNT . Enter a Service Account Name (e.g., dbt-service-account ). Click Create and Continue . Grant the Service Account Permissions : Under Role , select BigQuery Admin . This gives the service account full access to BigQuery. Click Continue and then Done . Create and Download the JSON Key : In the Service Accounts list, find your new service account and click on it. Go to the Keys tab, click Add Key > Create New Key . Select JSON and click Create . The JSON key file will be downloaded to your computer. This is the key file you will use in dbt. Step 4: Set Up Google Cloud SDK (Optional, but Recommended) Install Google Cloud SDK : If you don\u2019t already have it installed, you can download it from Google Cloud SDK . Follow the instructions to install it on your machine. Authenticate with the SDK : Open a terminal and run: bash gcloud auth login This will open a browser window where you can log in with your Google account. Set the Active Project : Run the following command to set your project as the active project: bash gcloud config set project your_project_id Replace your_project_id with the actual Project ID you noted earlier. Reference: https://www.youtube.com/watch?v=DzxtCxi4YaA https://robust-dinosaur-2ef.notion.site/PUBLIC-Retail-Project-af398809b643495e851042fa293ffe5b Step 5: Create a BigQuery Dataset Navigate to BigQuery in the Google Cloud Console. Create a Dataset : Click on your project name to expand it, then click Create Dataset . Enter a Dataset ID (e.g., my_dataset ). Configure location and other settings as needed, then click Create Dataset . Step 6: Note Down the Information Make sure you have the following information handy: - Project ID : Your Google Cloud Project ID (e.g., my-dbt-project-123456 ). - Service Account JSON Key File : The path to the JSON file you downloaded (e.g., /path/to/your-service-account-file.json ). - Dataset ID : The ID of the dataset you created (e.g., my_dataset ). Step 7: Start Creating Your dbt Project Now you\u2019re ready to create your dbt project using the above information. When dbt asks for: - BigQuery : Choose BigQuery as your data warehouse. - Key File Path : Provide the path to the JSON key file. - Project ID : Enter the Google Cloud Project ID. - Dataset : Enter the Dataset ID you created in BigQuery. Step 1: Install dbt and BigQuery Adapter Just create a folder , say, dbt-bigquery, CD to it and run python -m venv dbt-venv . You will see a folder created dbt-venv Activate the dbt-venv by running dbt-venv\\Scripts\\activate Run the following commands to install dbt and the BigQuery adapter: bash pip install dbt-core dbt-bigquery This will install dbt and the necessary adapter to connect to BigQuery. Step 3: Initialize a dbt Project In your terminal inside Visual Studio Code, run: bash dbt init your_project_name Replace your_project_name with the name you want for your dbt project. Navigate to the newly created project directory: bash cd your_project_name Step 4: Configure Your dbt Project for BigQuery Open the profiles.yml file : If you don't have one, you can create it in ~/.dbt/ . The file should look like this: yaml your_project_name: target: dev outputs: dev: type: bigquery method: service-account project: your-gcp-project-id dataset: your_dataset_name threads: 1 keyfile: /path/to/your-service-account-file.json timeout_seconds: 300 location: your_location Replace your_project_name , your-gcp-project-id , your_dataset_name , and /path/to/your-service-account-file.json with your actual project details. Save the file . Step 5: Create Models and Start Developing Open the models folder in your dbt project. Create a new .sql file for each model you want to define. For example: sql -- my_first_model.sql SELECT * FROM `your_gcp_project.your_dataset.your_table` Replace your_gcp_project , your_dataset , and your_table with the actual names. Step 6: Run Your dbt Models In the terminal, inside your dbt project directory, run: bash dbt run This will execute your models and create the tables or views in BigQuery. Step 7: Check Your Work Check BigQuery to see if your tables or views have been created as expected. Step 8: Version Control (Optional) Initialize a Git repository in your project directory: bash git init Add and commit your files : bash git add . git commit -m \"Initial commit\" Final Tips Always test your models by running dbt run regularly to ensure everything is working. Use dbt docs to generate documentation with dbt docs generate . That's it! You've set up a dbt project for BigQuery using Visual Studio Code. Happy developing!","title":"1.0.1 setup simple dbt project"},{"location":"SQL/1.0.1_setup_simple_dbt_project/#background","text":"Here, I will show you how to setup a simple dbt project setup from scratch. You will just need a windows laptop for this as I have done it on my Windows machine.","title":"Background"},{"location":"SQL/1.0.1_setup_simple_dbt_project/#google-console-pre-requisite-setup","text":"","title":"Google console pre-requisite setup"},{"location":"SQL/1.0.1_setup_simple_dbt_project/#step-1-create-a-google-cloud-project","text":"Go to the Google Cloud Console : Google Cloud Console . Create a New Project : In the top left corner, click on the Project dropdown and select New Project . Enter a Project Name (e.g., my-dbt-project ). Note down the Project ID ; you\u2019ll need this later. This is typically something like my-dbt-project-123456 . Set the Project as Active : Click on the Project dropdown again and select your newly created project to make it active.","title":"Step 1: Create a Google Cloud Project"},{"location":"SQL/1.0.1_setup_simple_dbt_project/#step-2-enable-bigquery-api","text":"Enable the BigQuery API : In the Google Cloud Console, navigate to APIs & Services > Library . Search for \"BigQuery API\" and click Enable .","title":"Step 2: Enable BigQuery API"},{"location":"SQL/1.0.1_setup_simple_dbt_project/#step-3-create-a-service-account","text":"Create a Service Account : Go to IAM & Admin > Service Accounts in the Google Cloud Console. Click + CREATE SERVICE ACCOUNT . Enter a Service Account Name (e.g., dbt-service-account ). Click Create and Continue . Grant the Service Account Permissions : Under Role , select BigQuery Admin . This gives the service account full access to BigQuery. Click Continue and then Done . Create and Download the JSON Key : In the Service Accounts list, find your new service account and click on it. Go to the Keys tab, click Add Key > Create New Key . Select JSON and click Create . The JSON key file will be downloaded to your computer. This is the key file you will use in dbt.","title":"Step 3: Create a Service Account"},{"location":"SQL/1.0.1_setup_simple_dbt_project/#step-4-set-up-google-cloud-sdk-optional-but-recommended","text":"Install Google Cloud SDK : If you don\u2019t already have it installed, you can download it from Google Cloud SDK . Follow the instructions to install it on your machine. Authenticate with the SDK : Open a terminal and run: bash gcloud auth login This will open a browser window where you can log in with your Google account. Set the Active Project : Run the following command to set your project as the active project: bash gcloud config set project your_project_id Replace your_project_id with the actual Project ID you noted earlier. Reference: https://www.youtube.com/watch?v=DzxtCxi4YaA https://robust-dinosaur-2ef.notion.site/PUBLIC-Retail-Project-af398809b643495e851042fa293ffe5b","title":"Step 4: Set Up Google Cloud SDK (Optional, but Recommended)"},{"location":"SQL/1.0.1_setup_simple_dbt_project/#step-5-create-a-bigquery-dataset","text":"Navigate to BigQuery in the Google Cloud Console. Create a Dataset : Click on your project name to expand it, then click Create Dataset . Enter a Dataset ID (e.g., my_dataset ). Configure location and other settings as needed, then click Create Dataset .","title":"Step 5: Create a BigQuery Dataset"},{"location":"SQL/1.0.1_setup_simple_dbt_project/#step-6-note-down-the-information","text":"Make sure you have the following information handy: - Project ID : Your Google Cloud Project ID (e.g., my-dbt-project-123456 ). - Service Account JSON Key File : The path to the JSON file you downloaded (e.g., /path/to/your-service-account-file.json ). - Dataset ID : The ID of the dataset you created (e.g., my_dataset ).","title":"Step 6: Note Down the Information"},{"location":"SQL/1.0.1_setup_simple_dbt_project/#step-7-start-creating-your-dbt-project","text":"Now you\u2019re ready to create your dbt project using the above information. When dbt asks for: - BigQuery : Choose BigQuery as your data warehouse. - Key File Path : Provide the path to the JSON key file. - Project ID : Enter the Google Cloud Project ID. - Dataset : Enter the Dataset ID you created in BigQuery.","title":"Step 7: Start Creating Your dbt Project"},{"location":"SQL/1.0.1_setup_simple_dbt_project/#step-1-install-dbt-and-bigquery-adapter","text":"Just create a folder , say, dbt-bigquery, CD to it and run python -m venv dbt-venv . You will see a folder created dbt-venv Activate the dbt-venv by running dbt-venv\\Scripts\\activate Run the following commands to install dbt and the BigQuery adapter: bash pip install dbt-core dbt-bigquery This will install dbt and the necessary adapter to connect to BigQuery.","title":"Step 1: Install dbt and BigQuery Adapter"},{"location":"SQL/1.0.1_setup_simple_dbt_project/#step-3-initialize-a-dbt-project","text":"In your terminal inside Visual Studio Code, run: bash dbt init your_project_name Replace your_project_name with the name you want for your dbt project. Navigate to the newly created project directory: bash cd your_project_name","title":"Step 3: Initialize a dbt Project"},{"location":"SQL/1.0.1_setup_simple_dbt_project/#step-4-configure-your-dbt-project-for-bigquery","text":"Open the profiles.yml file : If you don't have one, you can create it in ~/.dbt/ . The file should look like this: yaml your_project_name: target: dev outputs: dev: type: bigquery method: service-account project: your-gcp-project-id dataset: your_dataset_name threads: 1 keyfile: /path/to/your-service-account-file.json timeout_seconds: 300 location: your_location Replace your_project_name , your-gcp-project-id , your_dataset_name , and /path/to/your-service-account-file.json with your actual project details. Save the file .","title":"Step 4: Configure Your dbt Project for BigQuery"},{"location":"SQL/1.0.1_setup_simple_dbt_project/#step-5-create-models-and-start-developing","text":"Open the models folder in your dbt project. Create a new .sql file for each model you want to define. For example: sql -- my_first_model.sql SELECT * FROM `your_gcp_project.your_dataset.your_table` Replace your_gcp_project , your_dataset , and your_table with the actual names.","title":"Step 5: Create Models and Start Developing"},{"location":"SQL/1.0.1_setup_simple_dbt_project/#step-6-run-your-dbt-models","text":"In the terminal, inside your dbt project directory, run: bash dbt run This will execute your models and create the tables or views in BigQuery.","title":"Step 6: Run Your dbt Models"},{"location":"SQL/1.0.1_setup_simple_dbt_project/#step-7-check-your-work","text":"Check BigQuery to see if your tables or views have been created as expected.","title":"Step 7: Check Your Work"},{"location":"SQL/1.0.1_setup_simple_dbt_project/#step-8-version-control-optional","text":"Initialize a Git repository in your project directory: bash git init Add and commit your files : bash git add . git commit -m \"Initial commit\"","title":"Step 8: Version Control (Optional)"},{"location":"SQL/1.0.1_setup_simple_dbt_project/#final-tips","text":"Always test your models by running dbt run regularly to ensure everything is working. Use dbt docs to generate documentation with dbt docs generate . That's it! You've set up a dbt project for BigQuery using Visual Studio Code. Happy developing!","title":"Final Tips"},{"location":"SQL/FlatFileSoure/","text":"Flat File Source","title":"FlatFileSoure"},{"location":"SQL/InputAndOutputProperties/","text":"Understanding External Columns and Output Columns in SSIS In SSIS (SQL Server Integration Services), \"External Columns\" and \"Output Columns\" play crucial roles in managing how data moves through the data flow. External Columns What It Is : External columns represent how the data appears in your source file (like a CSV file). These columns reflect the structure and data types as defined in the source. Why It Matters : They tell SSIS what kind of data is in each column, such as whether it\u2019s text, a number, or a date, and how long the text can be. Example : If your CSV file has a column called coverage_details and the longest text in this column is 200 characters, you need to set the length of this column to 200 in External Columns. This tells SSIS to expect up to 200 characters of text in that column. Output Columns What It Is : Output columns are the columns that SSIS will pass on to the next step in your data flow. These are the columns that will be used by other components like Data Conversion or SQL Server Destination. Why It Matters : They define how the data is passed from the Flat File Source to other components in your data flow. The data types and lengths must match or be compatible with the next step\u2019s requirements. Example : The coverage_details column in Output Columns should be set to the same length (200 characters in this case) as in External Columns. This ensures that the data from the source file is correctly transferred to the next component without errors. Where to Find External Columns and Output Columns External Columns and Output Columns is mainly found in components that handle data sources and transformations in the Data Flow task (like Flat File Source, OLE DB Source, Excel Source, or Data Conversion Transformation). How to See Them : 1. Double-click the component in the Data Flow task. 2. Click on the \"Advanced Editor\u2026\" button. 3. Go to the \"Input and Output Properties\" tab. 4. You will see sections for \"External Columns\" and \"Output Columns\". Example for Flat File Source : Example for OLE DB Source : This way, you can easily find and manage External Columns and Output Columns in SSIS to ensure your data flows correctly. Common errors related to External Columns and Output Columns If External Columns and Output Columns are not set correctly, various types of errors can occur: Truncation Errors If coverage_details is set to 50 characters in External Columns but the actual data is 200 characters, SSIS will try to fit long text into a smaller space, causing a truncation error. The error message will say, \"Text was truncated or one or more characters had no match in the target code page.\" Data Conversion Errors If coverage_details is defined as a numeric column in SSIS but the source file contains text data, a conversion error will occur. The error message will say, \"Data conversion failed. The data conversion for column 'coverage_details' returned status value 2 and status text 'The value could not be converted because of a potential loss of data.'\" Mapping Errors happen when the source column and destination column are not compatible in terms of data types or lengths. For example, if coverage_details is set to 200 characters in SSIS but the SQL Server table defines it as 100 characters, SSIS will not be able to map the data correctly. The error message will say, \"Cannot convert between Unicode and non-Unicode string data types.\"","title":"Understanding External Columns and Output Columns in SSIS"},{"location":"SQL/InputAndOutputProperties/#understanding-external-columns-and-output-columns-in-ssis","text":"In SSIS (SQL Server Integration Services), \"External Columns\" and \"Output Columns\" play crucial roles in managing how data moves through the data flow.","title":"Understanding External Columns and Output Columns in SSIS"},{"location":"SQL/InputAndOutputProperties/#external-columns","text":"What It Is : External columns represent how the data appears in your source file (like a CSV file). These columns reflect the structure and data types as defined in the source. Why It Matters : They tell SSIS what kind of data is in each column, such as whether it\u2019s text, a number, or a date, and how long the text can be. Example : If your CSV file has a column called coverage_details and the longest text in this column is 200 characters, you need to set the length of this column to 200 in External Columns. This tells SSIS to expect up to 200 characters of text in that column.","title":"External Columns"},{"location":"SQL/InputAndOutputProperties/#output-columns","text":"What It Is : Output columns are the columns that SSIS will pass on to the next step in your data flow. These are the columns that will be used by other components like Data Conversion or SQL Server Destination. Why It Matters : They define how the data is passed from the Flat File Source to other components in your data flow. The data types and lengths must match or be compatible with the next step\u2019s requirements. Example : The coverage_details column in Output Columns should be set to the same length (200 characters in this case) as in External Columns. This ensures that the data from the source file is correctly transferred to the next component without errors.","title":"Output Columns"},{"location":"SQL/InputAndOutputProperties/#where-to-find-external-columns-and-output-columns","text":"External Columns and Output Columns is mainly found in components that handle data sources and transformations in the Data Flow task (like Flat File Source, OLE DB Source, Excel Source, or Data Conversion Transformation). How to See Them : 1. Double-click the component in the Data Flow task. 2. Click on the \"Advanced Editor\u2026\" button. 3. Go to the \"Input and Output Properties\" tab. 4. You will see sections for \"External Columns\" and \"Output Columns\". Example for Flat File Source : Example for OLE DB Source : This way, you can easily find and manage External Columns and Output Columns in SSIS to ensure your data flows correctly.","title":"Where to Find External Columns and Output Columns"},{"location":"SQL/InputAndOutputProperties/#common-errors-related-to-external-columns-and-output-columns","text":"If External Columns and Output Columns are not set correctly, various types of errors can occur: Truncation Errors If coverage_details is set to 50 characters in External Columns but the actual data is 200 characters, SSIS will try to fit long text into a smaller space, causing a truncation error. The error message will say, \"Text was truncated or one or more characters had no match in the target code page.\" Data Conversion Errors If coverage_details is defined as a numeric column in SSIS but the source file contains text data, a conversion error will occur. The error message will say, \"Data conversion failed. The data conversion for column 'coverage_details' returned status value 2 and status text 'The value could not be converted because of a potential loss of data.'\" Mapping Errors happen when the source column and destination column are not compatible in terms of data types or lengths. For example, if coverage_details is set to 200 characters in SSIS but the SQL Server table defines it as 100 characters, SSIS will not be able to map the data correctly. The error message will say, \"Cannot convert between Unicode and non-Unicode string data types.\"","title":"Common errors related to External Columns and Output Columns"},{"location":"SQL/MSSQL_Versions/","text":"SQL Server Versions Product Release Date RTM Version Support End Date Notes SQL Server 2022 2022-11 16.0.1000.6 2028-01 No more Service Packs SQL Server 2019 2019-11 15.0.2000.5 2025-01 No more Service Packs SQL Server 2017 2017-10 14.0.1000.169 2022-10 No more Service Packs SQL Server 2016 2016-06 13.0.1601.5 2021-07 SQL Server 2014 2014-04 12.0.2000.8 2019-07 Obsolete versions \u2013 out of support SQL Server 2012 2012-03 11.0.2100.60 2017-07 Obsolete versions \u2013 out of support SQL Server 2008 R2 2010-04 10.50.1600.1 2014-07 Obsolete versions \u2013 out of support SQL Server 2008 2008-08 10.0.1600.22 2014-07 Obsolete versions \u2013 out of support SQL Server 2005 2005-11 9.0.1399.06 2011-04 Obsolete versions \u2013 out of support SQL Server 2000 2000-11 8.0.194 2008-04 Obsolete versions \u2013 out of support SQL Server 7.0 1998-11 7.0.623 2005-12 Obsolete versions \u2013 out of support SQL Server 6.5 1996-06 6.50.201 2002-01 Obsolete versions \u2013 out of support SQL Server 6.0 1995-06 6.00.121 1999-03 Obsolete versions \u2013 out of support","title":"SQL Server Versions"},{"location":"SQL/MSSQL_Versions/#sql-server-versions","text":"Product Release Date RTM Version Support End Date Notes SQL Server 2022 2022-11 16.0.1000.6 2028-01 No more Service Packs SQL Server 2019 2019-11 15.0.2000.5 2025-01 No more Service Packs SQL Server 2017 2017-10 14.0.1000.169 2022-10 No more Service Packs SQL Server 2016 2016-06 13.0.1601.5 2021-07 SQL Server 2014 2014-04 12.0.2000.8 2019-07 Obsolete versions \u2013 out of support SQL Server 2012 2012-03 11.0.2100.60 2017-07 Obsolete versions \u2013 out of support SQL Server 2008 R2 2010-04 10.50.1600.1 2014-07 Obsolete versions \u2013 out of support SQL Server 2008 2008-08 10.0.1600.22 2014-07 Obsolete versions \u2013 out of support SQL Server 2005 2005-11 9.0.1399.06 2011-04 Obsolete versions \u2013 out of support SQL Server 2000 2000-11 8.0.194 2008-04 Obsolete versions \u2013 out of support SQL Server 7.0 1998-11 7.0.623 2005-12 Obsolete versions \u2013 out of support SQL Server 6.5 1996-06 6.50.201 2002-01 Obsolete versions \u2013 out of support SQL Server 6.0 1995-06 6.00.121 1999-03 Obsolete versions \u2013 out of support","title":"SQL Server Versions"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Project 1 - ETL Flat Files to MSSQL This project is a small version of a real-time project often used in the banking and insurance sectors. Here, data is received from different sources as flat files (like .csv or .xml). In banking, this file is sometimes called a control file. The file may come from Dynamics 365,right-fax servers, through web services, etc. This data needs to be ingested into SQL Server. The data comes from various branches or field offices throughout the day, and the workflow needs to run at a regular time each day to process all the files. This kind of ETL is best suited for handling large volumes of data in batch mode, such as end-of-day processing for banks etc. Project Overview Data Source : A folder containing .csv files with the following data: policy_number,policyholder_name,insured_name,policy_type,effective_date,expiration_date,premium_amount,beneficiary_name,beneficiary_contact,agent_name,agent_contact,coverage_details,endorsements_riders,underwriter_name 0_1722756562687,Pamela Bennett,Steven Dunn,Home,2021-08-24,2021-02-03,83572,Dana Hampton,366-752-7514x4594,Troy Young,+1-818-775-4416x2930,Set like go entire ground.,Drive generation major.,Candice Williams File Naming Convention : The CSV file names will be in the format policy_issuance_file_x.csv where x is 1, 2, 3... SSIS Workflow : The workflow will contain a Foreach Loop container. Inside the Foreach Loop, there will be a Data Flow Task. The Data Flow Task will contain: Flat File Source -> Data Conversion -> OLE DB Destination. Setup Required : You can develop this easily on a Windows machine. You will need MSSQL server. Visual Studio(Community edition will do). SSDT(to create the SSIS package in VS). Step 1: Create the Required Database in MSSQL Open SQL Server Management Studio (SSMS) and run the following SQL script to create the database and table. -- Create the database CREATE DATABASE InsuranceDB; GO -- Use the newly created database USE InsuranceDB; GO -- Create the table CREATE TABLE PolicyIssuance ( policy_number NVARCHAR(50) PRIMARY KEY, policyholder_name NVARCHAR(100), insured_name NVARCHAR(100), policy_type NVARCHAR(50), effective_date DATE, expiration_date DATE, premium_amount DECIMAL(10, 2), beneficiary_name NVARCHAR(100), beneficiary_contact NVARCHAR(50), agent_name NVARCHAR(100), agent_contact NVARCHAR(50), coverage_details NVARCHAR(255), endorsements_riders NVARCHAR(255), underwriter_name NVARCHAR(100) ); GO -- Verify the table creation SELECT * FROM PolicyIssuance; GO To check the details of the table created, use: sp_help PolicyIssuance; Step 2: Create the SSIS Package Configure the Foreach Loop Container Open SQL Server Data Tools (SSDT) or Visual Studio with SSIS extension installed. Create a new SSIS project. Create a variable var_FilePath (String) to store the full path of the current file being processed. Drag a Foreach Loop Container onto the Control Flow tab. Double-click the Foreach Loop Container to open the editor. In the Collection tab: Set Enumerator to Foreach File Enumerator . Set Folder to C:\\Users\\dwaip\\Desktop\\sample_csvs\\ . Set Files to policy_issuance_file_*.csv to process all files matching this pattern. In the Variable Mappings tab: Add the FilePath variable and set the Index to 0. Drag a Data Flow Task into the Foreach Loop Container. Configure the Data Flow Task Double-click the Data Flow Task to open the Data Flow tab. Add a Flat File Source to the Data Flow. Double-click the Flat File Source to configure it. Configure Flat File Connection Manager In the Flat File Source Editor, click New to create a new Flat File Connection Manager. Set up the connection manager using any one of the sample CSV files (e.g., policy_issuance_file_1.csv ) to configure the columns. Once configured, click OK . Set Connection String to Dynamic In the Flat File Connection Manager: Go to Properties and find Expressions . Click the ellipsis ( ... ) next to Expressions . In the Property dropdown, select ConnectionString . Set the expression to use the var_FilePath variable: @[User::var_FilePath] . If the setting is correct you will see this: Add Data Conversion Transformation Add a Data Conversion Transformation . Configure it to convert the necessary columns : policyholder_name (original) converted to Copy of policyholder_name insured_name (original) converted to Copy of insured_name Other columns.. Add SQL Server Destination Add an OLE DB Destination to the Data Flow. Connect the Data Conversion Transformation to the OLE DB Destination. Configure the OLE DB Destination to map the columns correctly to the PolicyIssuance table. Map Columns in OLE DB Destination Open the SQL Server Destination Editor. Go to the Mappings Tab. Map the converted columns: Copy of policyholder_name to policyholder_name Copy of insured_name to insured_name Copy of beneficiary_name to beneficiary_name Similarly, map other converted columns as needed. Test Run the Package Save the package. Right-click the package and select Execute Package. Check the MSSQL table to verify the data has been loaded correctly. Step 3: Deploy and Schedule the SSIS Package in MSSQL Server To deploy your SSIS package and schedule it to run once a day, follow these steps: Deploy the SSIS Package Deploy the Package : Right-click the project in the Solution Explorer and select \"Build\" to ensure there are no errors. Right-click the project and select \"Deploy\". The Integration Services Deployment Wizard will open. Follow the steps: Select SSIS in SQL Server as the deployment target. Provide the server name and path where the package will be deployed. Finally, click Deploy to deploy the package. If everything goes well, your package will be deployed. Schedule the Package using SQL Server Agent Create a New SQL Server Agent Job Connect to your SQL Server instance. Expand the SQL Server Agent node. Right-click Jobs and select New Job . In the \"New Job\" window, provide a name for the job (e.g., \"Daily SSIS Package Run\"). Set Job Steps In the Steps page, click New to create a new step. Set the Step name (e.g., \"Run SSIS Package\"). Set Type to SQL Server Integration Services Package . Set \"Package source\" to \"SSIS Catalog\". Select the server and the package you deployed. Click \"OK\" to save the step. Set Job Schedule In the \"Schedules\" page, click \"New\" to create a new schedule. Provide a name for the schedule (e.g., \"Daily Schedule\"). Set the frequency to \"Daily\". Set the time of day you want the package to run. Click \"OK\" to save the schedule. Set Notifications (Optional) In the \"Notifications\" page, set up notifications to receive alerts in case of job failure or success. Save the Job Click \"OK\" to save the job. Common Errors and Their Solutions The most common errors will be related to size mismatch and type mismatch. There will also be Unicode-non-Unicode type mismatch issues. For text files, choose Unicode string [DT_WSTR] as the output which can be mapped to MSSQL server NVARCHAR variables. I was able to ingest data from .csv files correctly using the DT_STR format. This means the data can come in as DT_STR, but it needs to go into MSSQL as DT_WSTR. The coverage_details field was the largest in the .csv file and caused the most errors. So, I will explain the errors using this field as an example. Input and Output Properties Tab - Common place for error troubleshooting Go to the Input and Output Properties in the Advanced Editor (right-click the step). Truncation Errors If coverage_details is set to 50 characters in External Columns but the actual data is 200 characters, SSIS will try to fit long text into a smaller space, causing a truncation error. The error message will say, \"Text was truncated or one or more characters had no match in the target code page.\" Data Conversion Errors If coverage_details is defined as a numeric column in SSIS but the source file contains text data, a conversion error will occur. The error message will say, \"Data conversion failed. The data conversion for column 'coverage_details' returned status value 2 and status text 'The value could not be converted because of a potential loss of data.'\" Mapping Errors These happen when the source column and destination column are not compatible in terms of data types or lengths. For example, if coverage_details is set to 200 characters in SSIS but the SQL Server table defines it as 100 characters, SSIS will not be able to map the data correctly. The error message will say, \"Cannot convert between Unicode and non-Unicode string data types.\" SSIS toolbox not visible In the newer version of VS sometimes the SSIS Toolbox might be absent. You just have to right-click and select SSIS Toolbox. That's it. SSISDB not found An Integration Services catalog (SSISDB) was not found on this server instance. To deploy a project to this server, you must create the SSISDB catalog. Open the Create Catalog dialog box from the Integration Services Catalogs node. Resolution Open SSMS and connect to your SQL Server instance. In the Object Explorer, expand the \"Integration Services Catalogs\" node. Right-click and select \"Create Catalog\u2026\". In the \"Create Catalog\" dialog box, check the \"Enable CLR Integration\" checkbox. Enter a password for the SSISDB database master key. This password will be used to secure the catalog. Click \"OK\" to create the catalog. You will see a SSISDB created inside the Integration Services Catalogs folder. Now, you can deploy your projects here.","title":"Project 1 - ETL Flat Files to MSSQL"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#project-1-etl-flat-files-to-mssql","text":"This project is a small version of a real-time project often used in the banking and insurance sectors. Here, data is received from different sources as flat files (like .csv or .xml). In banking, this file is sometimes called a control file. The file may come from Dynamics 365,right-fax servers, through web services, etc. This data needs to be ingested into SQL Server. The data comes from various branches or field offices throughout the day, and the workflow needs to run at a regular time each day to process all the files. This kind of ETL is best suited for handling large volumes of data in batch mode, such as end-of-day processing for banks etc.","title":"Project 1 - ETL Flat Files to MSSQL"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#project-overview","text":"Data Source : A folder containing .csv files with the following data: policy_number,policyholder_name,insured_name,policy_type,effective_date,expiration_date,premium_amount,beneficiary_name,beneficiary_contact,agent_name,agent_contact,coverage_details,endorsements_riders,underwriter_name 0_1722756562687,Pamela Bennett,Steven Dunn,Home,2021-08-24,2021-02-03,83572,Dana Hampton,366-752-7514x4594,Troy Young,+1-818-775-4416x2930,Set like go entire ground.,Drive generation major.,Candice Williams File Naming Convention : The CSV file names will be in the format policy_issuance_file_x.csv where x is 1, 2, 3... SSIS Workflow : The workflow will contain a Foreach Loop container. Inside the Foreach Loop, there will be a Data Flow Task. The Data Flow Task will contain: Flat File Source -> Data Conversion -> OLE DB Destination. Setup Required : You can develop this easily on a Windows machine. You will need MSSQL server. Visual Studio(Community edition will do). SSDT(to create the SSIS package in VS).","title":"Project Overview"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#step-1-create-the-required-database-in-mssql","text":"Open SQL Server Management Studio (SSMS) and run the following SQL script to create the database and table. -- Create the database CREATE DATABASE InsuranceDB; GO -- Use the newly created database USE InsuranceDB; GO -- Create the table CREATE TABLE PolicyIssuance ( policy_number NVARCHAR(50) PRIMARY KEY, policyholder_name NVARCHAR(100), insured_name NVARCHAR(100), policy_type NVARCHAR(50), effective_date DATE, expiration_date DATE, premium_amount DECIMAL(10, 2), beneficiary_name NVARCHAR(100), beneficiary_contact NVARCHAR(50), agent_name NVARCHAR(100), agent_contact NVARCHAR(50), coverage_details NVARCHAR(255), endorsements_riders NVARCHAR(255), underwriter_name NVARCHAR(100) ); GO -- Verify the table creation SELECT * FROM PolicyIssuance; GO To check the details of the table created, use: sp_help PolicyIssuance;","title":"Step 1: Create the Required Database in MSSQL"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#step-2-create-the-ssis-package","text":"","title":"Step 2: Create the SSIS Package"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#configure-the-foreach-loop-container","text":"Open SQL Server Data Tools (SSDT) or Visual Studio with SSIS extension installed. Create a new SSIS project. Create a variable var_FilePath (String) to store the full path of the current file being processed. Drag a Foreach Loop Container onto the Control Flow tab. Double-click the Foreach Loop Container to open the editor. In the Collection tab: Set Enumerator to Foreach File Enumerator . Set Folder to C:\\Users\\dwaip\\Desktop\\sample_csvs\\ . Set Files to policy_issuance_file_*.csv to process all files matching this pattern. In the Variable Mappings tab: Add the FilePath variable and set the Index to 0. Drag a Data Flow Task into the Foreach Loop Container.","title":"Configure the Foreach Loop Container"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#configure-the-data-flow-task","text":"Double-click the Data Flow Task to open the Data Flow tab. Add a Flat File Source to the Data Flow. Double-click the Flat File Source to configure it.","title":"Configure the Data Flow Task"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#configure-flat-file-connection-manager","text":"In the Flat File Source Editor, click New to create a new Flat File Connection Manager. Set up the connection manager using any one of the sample CSV files (e.g., policy_issuance_file_1.csv ) to configure the columns. Once configured, click OK .","title":"Configure Flat File Connection Manager"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#set-connection-string-to-dynamic","text":"In the Flat File Connection Manager: Go to Properties and find Expressions . Click the ellipsis ( ... ) next to Expressions . In the Property dropdown, select ConnectionString . Set the expression to use the var_FilePath variable: @[User::var_FilePath] . If the setting is correct you will see this:","title":"Set Connection String to Dynamic"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#add-data-conversion-transformation","text":"Add a Data Conversion Transformation . Configure it to convert the necessary columns : policyholder_name (original) converted to Copy of policyholder_name insured_name (original) converted to Copy of insured_name Other columns..","title":"Add Data Conversion Transformation"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#add-sql-server-destination","text":"Add an OLE DB Destination to the Data Flow. Connect the Data Conversion Transformation to the OLE DB Destination. Configure the OLE DB Destination to map the columns correctly to the PolicyIssuance table.","title":"Add SQL Server Destination"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#map-columns-in-ole-db-destination","text":"Open the SQL Server Destination Editor. Go to the Mappings Tab. Map the converted columns: Copy of policyholder_name to policyholder_name Copy of insured_name to insured_name Copy of beneficiary_name to beneficiary_name Similarly, map other converted columns as needed.","title":"Map Columns in OLE DB Destination"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#test-run-the-package","text":"Save the package. Right-click the package and select Execute Package. Check the MSSQL table to verify the data has been loaded correctly.","title":"Test Run the Package"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#step-3-deploy-and-schedule-the-ssis-package-in-mssql-server","text":"To deploy your SSIS package and schedule it to run once a day, follow these steps:","title":"Step 3: Deploy and Schedule the SSIS Package in MSSQL Server"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#deploy-the-ssis-package","text":"Deploy the Package : Right-click the project in the Solution Explorer and select \"Build\" to ensure there are no errors. Right-click the project and select \"Deploy\". The Integration Services Deployment Wizard will open. Follow the steps: Select SSIS in SQL Server as the deployment target. Provide the server name and path where the package will be deployed. Finally, click Deploy to deploy the package. If everything goes well, your package will be deployed.","title":"Deploy the SSIS Package"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#schedule-the-package-using-sql-server-agent","text":"","title":"Schedule the Package using SQL Server Agent"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#create-a-new-sql-server-agent-job","text":"Connect to your SQL Server instance. Expand the SQL Server Agent node. Right-click Jobs and select New Job . In the \"New Job\" window, provide a name for the job (e.g., \"Daily SSIS Package Run\").","title":"Create a New SQL Server Agent Job"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#set-job-steps","text":"In the Steps page, click New to create a new step. Set the Step name (e.g., \"Run SSIS Package\"). Set Type to SQL Server Integration Services Package . Set \"Package source\" to \"SSIS Catalog\". Select the server and the package you deployed. Click \"OK\" to save the step.","title":"Set Job Steps"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#set-job-schedule","text":"In the \"Schedules\" page, click \"New\" to create a new schedule. Provide a name for the schedule (e.g., \"Daily Schedule\"). Set the frequency to \"Daily\". Set the time of day you want the package to run. Click \"OK\" to save the schedule.","title":"Set Job Schedule"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#set-notifications-optional","text":"In the \"Notifications\" page, set up notifications to receive alerts in case of job failure or success.","title":"Set Notifications (Optional)"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#save-the-job","text":"Click \"OK\" to save the job.","title":"Save the Job"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#common-errors-and-their-solutions","text":"The most common errors will be related to size mismatch and type mismatch. There will also be Unicode-non-Unicode type mismatch issues. For text files, choose Unicode string [DT_WSTR] as the output which can be mapped to MSSQL server NVARCHAR variables. I was able to ingest data from .csv files correctly using the DT_STR format. This means the data can come in as DT_STR, but it needs to go into MSSQL as DT_WSTR. The coverage_details field was the largest in the .csv file and caused the most errors. So, I will explain the errors using this field as an example.","title":"Common Errors and Their Solutions"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#input-and-output-properties-tab-common-place-for-error-troubleshooting","text":"Go to the Input and Output Properties in the Advanced Editor (right-click the step).","title":"Input and Output Properties Tab - Common place for error troubleshooting"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#truncation-errors","text":"If coverage_details is set to 50 characters in External Columns but the actual data is 200 characters, SSIS will try to fit long text into a smaller space, causing a truncation error. The error message will say, \"Text was truncated or one or more characters had no match in the target code page.\"","title":"Truncation Errors"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#data-conversion-errors","text":"If coverage_details is defined as a numeric column in SSIS but the source file contains text data, a conversion error will occur. The error message will say, \"Data conversion failed. The data conversion for column 'coverage_details' returned status value 2 and status text 'The value could not be converted because of a potential loss of data.'\"","title":"Data Conversion Errors"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#mapping-errors","text":"These happen when the source column and destination column are not compatible in terms of data types or lengths. For example, if coverage_details is set to 200 characters in SSIS but the SQL Server table defines it as 100 characters, SSIS will not be able to map the data correctly. The error message will say, \"Cannot convert between Unicode and non-Unicode string data types.\"","title":"Mapping Errors"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#ssis-toolbox-not-visible","text":"In the newer version of VS sometimes the SSIS Toolbox might be absent. You just have to right-click and select SSIS Toolbox. That's it.","title":"SSIS toolbox not visible"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#ssisdb-not-found","text":"An Integration Services catalog (SSISDB) was not found on this server instance. To deploy a project to this server, you must create the SSISDB catalog. Open the Create Catalog dialog box from the Integration Services Catalogs node.","title":"SSISDB not found"},{"location":"SQL/Project_1-ETL-CSV-MSSQL/#resolution","text":"Open SSMS and connect to your SQL Server instance. In the Object Explorer, expand the \"Integration Services Catalogs\" node. Right-click and select \"Create Catalog\u2026\". In the \"Create Catalog\" dialog box, check the \"Enable CLR Integration\" checkbox. Enter a password for the SSISDB database master key. This password will be used to secure the catalog. Click \"OK\" to create the catalog. You will see a SSISDB created inside the Integration Services Catalogs folder. Now, you can deploy your projects here.","title":"Resolution"},{"location":"SQL/Project_2-UsingWebServicesInSSIS/","text":"Table of contents {: .text-delta } 1. TOC {:toc} How to Use Web Services in SSIS Script Task Introduction Using web services in SSIS (SQL Server Integration Services) can be quite helpful when you need to get data from an external system. Here\u2019s a simple guide on how to do this using the Script Task in SSIS. Steps to Use Web Services in SSIS Script Task Create a Proxy Class for the Web Service First, you need to create a Proxy Class using WSDL (Web Services Description Language). This will help your SSIS package understand how to communicate with the web service. Open the .NET command prompt and run this command: sh wsdl /language:VB http://localhost:8080/WebService/Service1.asmx?WSDL /out:C:\\WebService1.vb This command creates a file called WebService1.vb which is your Proxy Class. By default, it uses C Sharp if you don\u2019t specify the language. In SSIS Script Task 2005, you can only use Visual Basic .NET, so generate the Proxy Class with VB if you are using SSIS 2005. For those unfamiliar with WSDL.EXE, it's a tool to generate code for XML Web Services and clients using ASP.NET from WSDL files, XSD schemas, and .discomap documents. It can be used with disco.exe. Publish Your Web Service to IIS In your web service project, right-click and select \"Publish\". Choose the web site where you want to publish your web service. Add the Proxy Class to Your SSIS Project Open your SSIS package. Drag and drop a Script Task onto the Control Flow area. Right-click on the Script Task and select \"Edit\". In the Script Task Editor, go to the \"Script\" section and click on \"Design Script\". In the Script Editor, open Project Explorer (if it\u2019s not visible, go to View -> Project Explorer). Add your Proxy Class ( WebService1.vb ) to the project: Right-click on your project in Project Explorer. Select \"Add\" -> \"Existing Item\" and add the WebService1.vb file. Edit the Script in the Script Task Ensure that you include necessary namespaces in your script. Add these at the top of your Proxy Class file: vb Imports System.Web.Services Imports System.Xml.Serialization In the ScriptMain class, create an object of the Proxy Class and call the web service methods. Here\u2019s an example: vb Public Sub Main() Dim ws As New Service1 MsgBox(\"Square of 2: \" & ws.Square(2)) Dts.TaskResult = Dts.Results.Success End Sub Handle Authentication Errors If you get an HTTP 401 Unauthorized error, add credentials to your web service call. Modify your Main method like this: vb Public Sub Main() Dim ws As New Service1 ws.Credentials = New System.Net.NetworkCredential(\"username\", \"password\", \"domain\") MsgBox(\"Square of 2: \" & ws.Square(2)) Dts.TaskResult = Dts.Results.Success End Sub","title":"Project 2 - Web Service SSIS Script Task"},{"location":"SQL/Project_2-UsingWebServicesInSSIS/#how-to-use-web-services-in-ssis-script-task","text":"","title":"How to Use Web Services in SSIS Script Task"},{"location":"SQL/Project_2-UsingWebServicesInSSIS/#introduction","text":"Using web services in SSIS (SQL Server Integration Services) can be quite helpful when you need to get data from an external system. Here\u2019s a simple guide on how to do this using the Script Task in SSIS.","title":"Introduction"},{"location":"SQL/Project_2-UsingWebServicesInSSIS/#steps-to-use-web-services-in-ssis-script-task","text":"Create a Proxy Class for the Web Service First, you need to create a Proxy Class using WSDL (Web Services Description Language). This will help your SSIS package understand how to communicate with the web service. Open the .NET command prompt and run this command: sh wsdl /language:VB http://localhost:8080/WebService/Service1.asmx?WSDL /out:C:\\WebService1.vb This command creates a file called WebService1.vb which is your Proxy Class. By default, it uses C Sharp if you don\u2019t specify the language. In SSIS Script Task 2005, you can only use Visual Basic .NET, so generate the Proxy Class with VB if you are using SSIS 2005. For those unfamiliar with WSDL.EXE, it's a tool to generate code for XML Web Services and clients using ASP.NET from WSDL files, XSD schemas, and .discomap documents. It can be used with disco.exe. Publish Your Web Service to IIS In your web service project, right-click and select \"Publish\". Choose the web site where you want to publish your web service. Add the Proxy Class to Your SSIS Project Open your SSIS package. Drag and drop a Script Task onto the Control Flow area. Right-click on the Script Task and select \"Edit\". In the Script Task Editor, go to the \"Script\" section and click on \"Design Script\". In the Script Editor, open Project Explorer (if it\u2019s not visible, go to View -> Project Explorer). Add your Proxy Class ( WebService1.vb ) to the project: Right-click on your project in Project Explorer. Select \"Add\" -> \"Existing Item\" and add the WebService1.vb file. Edit the Script in the Script Task Ensure that you include necessary namespaces in your script. Add these at the top of your Proxy Class file: vb Imports System.Web.Services Imports System.Xml.Serialization In the ScriptMain class, create an object of the Proxy Class and call the web service methods. Here\u2019s an example: vb Public Sub Main() Dim ws As New Service1 MsgBox(\"Square of 2: \" & ws.Square(2)) Dts.TaskResult = Dts.Results.Success End Sub Handle Authentication Errors If you get an HTTP 401 Unauthorized error, add credentials to your web service call. Modify your Main method like this: vb Public Sub Main() Dim ws As New Service1 ws.Credentials = New System.Net.NetworkCredential(\"username\", \"password\", \"domain\") MsgBox(\"Square of 2: \" & ws.Square(2)) Dts.TaskResult = Dts.Results.Success End Sub","title":"Steps to Use Web Services in SSIS Script Task"},{"location":"SQL/SQL/","text":"Essential SparkSQL commands OPENROWSET - The Powerful Transact-SQL for Data Engineering. What is OPENROWSET? Practical scenarios where OPENROWSET is preferred Syntax Key Options Options for CSV Examples What other options we have? APPROX_COUNT_DISTINCT when COUNT(DISTINCT ColName) runs forever How to run it? Some Fun facts about this T-SQL function Practice Joins with MSSQL AdventureWorksLT2016 Table Descriptions Questions Question 1: List Customers with Their Orders Question 2: List Products and Their Categories Question 3: Find Customers and Their Addresses Question 4: List All Products and Their Order Details Question 5: Products with No Orders Question 6: Calculate Total Sales per Customer Using CTE Comprehensive List of SQL Joins 1. INNER JOIN (also known as JOIN ) 2. LEFT JOIN (also known as LEFT OUTER JOIN ) 3. RIGHT JOIN (also known as RIGHT OUTER JOIN ) 4. FULL JOIN (also known as FULL OUTER JOIN ) 5. CROSS JOIN 6. SELF JOIN 7. NATURAL JOIN 8. ANTI JOIN (also known as LEFT ANTI JOIN ) 9. SEMI JOIN (also known as LEFT SEMI JOIN or RIGHT SEMI JOIN ) 10. EXCEPT JOIN 11. OUTER APPLY 12. INNER APPLY Summary Table SQL Keys Essential SparkSQL commands Command Description Example SET -v Shows all Spark SQL settings. SET -v; SHOW DATABASES Lists all databases. SHOW DATABASES; CREATE DATABASE Creates a new database. CREATE DATABASE db_name; DROP DATABASE Drops an existing database. DROP DATABASE db_name; USE Sets the current database. USE db_name; SHOW TABLES Lists all tables in the current database. SHOW TABLES; DESCRIBE TABLE Provides the schema of a table. DESCRIBE TABLE table_name; DESCRIBE EXTENDED Provides detailed information about a table, including metadata. DESCRIBE EXTENDED table_name; CREATE TABLE Creates a new table. CREATE TABLE table_name (col1 INT, ...); CREATE EXTERNAL TABLE Creates an external table. CREATE EXTERNAL TABLE table_name ...; DROP TABLE Drops an existing table. DROP TABLE table_name; INSERT INTO Inserts data into a table. INSERT INTO table_name VALUES (...); SELECT Queries data from tables. SELECT * FROM table_name; CREATE VIEW Creates a view based on a query. CREATE VIEW view_name AS SELECT ...; DROP VIEW Drops an existing view. DROP VIEW view_name; CACHE TABLE Caches a table in memory. CACHE TABLE table_name; UNCACHE TABLE Removes a table from the in-memory cache. UNCACHE TABLE table_name; SHOW COLUMNS Lists all columns of a table. SHOW COLUMNS FROM table_name; ALTER TABLE Changes the structure of an existing table. ALTER TABLE table_name ADD COLUMNS (...); TRUNCATE TABLE Removes all rows from a table without deleting the table. TRUNCATE TABLE table_name; MSCK REPAIR TABLE Recovers partitions of a table. MSCK REPAIR TABLE table_name; SHOW PARTITIONS Lists all partitions of a table. SHOW PARTITIONS table_name; EXPLAIN Provides a detailed execution plan for a query. EXPLAIN SELECT * FROM table_name; SHOW CREATE TABLE Displays the CREATE TABLE statement for an existing table. SHOW CREATE TABLE table_name; LOAD DATA Loads data into a table from a file. LOAD DATA INPATH 'path' INTO TABLE ...; SET Sets a Spark SQL configuration property. SET property_name=value; SHOW FUNCTIONS Lists all functions available in the Spark SQL environment. SHOW FUNCTIONS; DESCRIBE FUNCTION Provides information about a function. DESCRIBE FUNCTION function_name; OPENROWSET - The Powerful Transact-SQL for Data Engineering. What is OPENROWSET? OPENROWSET is a Transact-SQL function. Using this, you can create a select statement from external data sources (CSV, Parquet, JSON, Delta, ADLS, Blob) as if it were a local database. You don't need to create a table or linked service to run this. Hence, it's fast and requires fewer lines of code. It is used in SQL Server, Azure SQL DB, Synapse Analytics, Fabric, and Databricks. For `OPENROWSET` to work, you need to ensure that the Synapse workspace or the query has the appropriate authentication to access the external data source. Practical scenarios where OPENROWSET is preferred Ad-Hoc Data Retrieval: Scenario: A business analyst needs to quickly retrieve data from an Excel file for an ad-hoc report. Reason: OPENROWSET allows for on-the-fly access to the Excel file without the need to import it into SQL Server first, providing immediate data retrieval for analysis. Cross-Database Queries: Scenario: A developer needs to run a query that joins tables from different SQL Server instances. Reason: OPENROWSET can be used to run distributed queries across different servers, avoiding the need to set up linked servers which can be more complex and require additional configuration and permissions. Accessing External Data Sources: Scenario: A company needs to integrate data from a remote Oracle database for a one-time data migration task. Reason: OPENROWSET provides a quick way to query data from the Oracle database using an OLE DB provider, bypassing the need for more permanent and complex solutions like linked servers or SSIS packages. Temporary Data Access: Scenario: During a data validation process, a data engineer needs to access data from a CSV file provided by a client. Reason: OPENROWSET allows the engineer to read the CSV file directly into a SQL query, facilitating temporary data access without the overhead of creating permanent database tables or ETL processes. Data Import for Development and Testing: Scenario: Developers need to import data from various formats (e.g., Excel, CSV) into a development environment for testing purposes. Reason: OPENROWSET can be used to import data directly into the development environment without the need for pre-configured data import processes, speeding up the testing and development cycles. Syntax -- General Syntax OPENROWSET( BULK 'file_path', FORMAT = 'file_format', [DATA_SOURCE = 'data_source_name'], [WITH ( column_name column_type [ 'column_ordinal' | 'json_path' ] )] ) AS alias_name -- Example for CSV SELECT * FROM OPENROWSET( BULK 'https://storageaccount.blob.core.windows.net/container/file.csv', FORMAT = 'CSV', FIRSTROW = 2, FIELDTERMINATOR = ',', ROWTERMINATOR = '\\n' ) AS [result] -- Example for Parquet SELECT * FROM OPENROWSET( BULK 'https://storageaccount.blob.core.windows.net/container/file.parquet', FORMAT = 'PARQUET' ) AS [result] -- Example for Delta Lake SELECT * FROM OPENROWSET( BULK 'https://storageaccount.blob.core.windows.net/container/file.delta', FORMAT = 'DELTA' ) AS [result] Key Options BULK : Specifies the path to the file(s). FORMAT : Specifies the file format ( CSV , PARQUET , or DELTA ). DATA_SOURCE : (Optional) Specifies the external data source. WITH : Defines the schema of the columns to be read. Options for CSV FIELDTERMINATOR : Character used to separate fields (default is , ). ROWTERMINATOR : Character used to separate rows (default is \\n ). FIRSTROW : Specifies the first row to read (default is 1). HEADER_ROW : Indicates if the CSV file contains a header row ( TRUE or FALSE ). Examples Reading a CSV file without specifying a schema: sql SELECT * FROM OPENROWSET( BULK 'https://storageaccount.blob.core.windows.net/container/data.csv', FORMAT = 'CSV' ) AS [data] Reading specific columns from a CSV file: sql SELECT * FROM OPENROWSET( BULK 'https://storageaccount.blob.core.windows.net/container/data.csv', FORMAT = 'CSV', FIELDTERMINATOR = ',', ROWTERMINATOR = '\\n', FIRSTROW = 2 ) WITH ( col1 VARCHAR(50), col2 INT, col3 DATE ) AS [data] Reading a Parquet file: sql SELECT * FROM OPENROWSET( BULK 'https://storageaccount.blob.core.windows.net/container/data.parquet', FORMAT = 'PARQUET' ) AS [data] Reading a Delta Lake file: sql SELECT * FROM OPENROWSET( BULK 'https://storageaccount.blob.core.windows.net/container/data.delta', FORMAT = 'DELTA' ) AS [data] What other options we have? Alternative Usage Scenario Efficiency Microsoft Recommendation BULK INSERT Loading large volumes of data from a file into a SQL Server table. High for large files Use when importing large data volumes directly. PolyBase Querying external data in Hadoop, Azure Blob Storage, and Azure Data Lake. High Use for big data environments and distributed systems. EXTERNAL TABLE Accessing external data sources seamlessly as if they are regular SQL tables. Moderate to High Recommended for persistent access to external data. Linked Servers Connecting SQL Server to other data sources, like another SQL Server or an OLE DB data source. Varies Use for diverse data sources and quick data joins. Databricks Reading data from various sources using Spark SQL (not native to Databricks but works with Synapse). High for big data Use for Spark SQL integrations and large-scale analytics. APPROX_COUNT_DISTINCT when COUNT(DISTINCT ColName) runs forever Imagine your table has 1 trillion rows. You want to count distinct customer names. Do you think COUNT(DISTINCT CustomerName) will show you results in your lifetime? No, for such scenarios, use APPROX_COUNT_DISTINCT(ColName) . It will give you around 97% accuracy. How to run it? Pretty simple and straightforward. Normal Count SELECT COUNT(DISTINCT CustomerName) FROM Customers; Approximate Count SELECT APPROX_COUNT_DISTINCT(CustomerName) FROM Customers; Some Fun facts about this T-SQL function In MS SQL Server APPROX_COUNT_DISTINCT it first came in SQL Server 2019 . It is available in Synapse SQL . These other brands support this function: Google BigQuery Amazon Redshift : It offers the APPROXIMATE COUNT(DISTINCT column) function, which serves a similar purpose. PostgreSQL : The approx_count_distinct extension is available through the HLL (HyperLogLog) extension, which allows for approximate distinct counting. Apache Spark : The approx_count_distinct function is available in Spark SQL, which uses HyperLogLog for approximate counting. Practice Joins with MSSQL AdventureWorksLT2016 Download MSSQL Sample database AdventureWorksLT2016.bak and restore it. It will give you a full enviornment. https://github.com/Microsoft/sql-server-samples/releases/download/adventureworks/AdventureWorksLT2016.bak Table Descriptions Here is a brief description of some of the key tables in the AdventureWorksLT2016 database: SalesLT.Customer : Contains customer information such as CustomerID, FirstName, LastName, EmailAddress, etc. SalesLT.SalesOrderHeader : Contains sales order information including SalesOrderID, OrderDate, CustomerID, TotalDue, etc. SalesLT.SalesOrderDetail : Contains details of each sales order, such as SalesOrderID, ProductID, OrderQty, UnitPrice, etc. SalesLT.Product : Contains product information including ProductID, Name, ProductNumber, Color, ListPrice, etc. SalesLT.ProductCategory : Contains information about product categories including ProductCategoryID and Name. SalesLT.ProductModel : Contains information about product models including ProductModelID and Name. SalesLT.Address : Contains address information including AddressID, AddressLine1, City, StateProvince, etc. SalesLT.CustomerAddress : A junction table linking customers to addresses. Questions Question 1: List Customers with Their Orders Question : Retrieve a list of customers along with their order details. Include the customer's first name, last name, and order date. Only include customers who have placed at least one order. Hint : - Use SalesLT.Customer to get customer information. - Use SalesLT.SalesOrderHeader to get order information. - Use an INNER JOIN to connect these tables. Question 2: List Products and Their Categories Question : Retrieve a list of products along with their category names. Include all products, even those that do not belong to any category. Hint : - Use SalesLT.Product to get product information. - Use SalesLT.ProductCategory to get category information. - Use a LEFT JOIN to include all products. Question 3: Find Customers and Their Addresses Question : Retrieve a list of customers and their addresses. Include all customers, even if they do not have an address listed. Hint : - Use SalesLT.Customer to get customer information. - Use SalesLT.CustomerAddress to get the relationship between customers and addresses. - Use SalesLT.Address to get address information. - Use a LEFT JOIN to include all customers. Question 4: List All Products and Their Order Details Question : Retrieve a list of all products and their order details. Include all products and orders, even if they do not match. Hint : - Use SalesLT.Product to get product information. - Use SalesLT.SalesOrderDetail to get order details. - Use a FULL OUTER JOIN to include all products and orders. Question 5: Products with No Orders Question : Find products that have not been ordered by any customer. Hint : - Use SalesLT.Product to get product information. - Use SalesLT.SalesOrderDetail to get order details. - Use a LEFT JOIN and filter to find products with no matching orders. Question 6: Calculate Total Sales per Customer Using CTE Question : Use a Common Table Expression (CTE) to calculate the total sales amount for each customer. Hint : - Use SalesLT.Customer to get customer information. - Use SalesLT.SalesOrderHeader to get order information. - Use SUM function to calculate total sales. - Use a CTE to organize the query. Comprehensive List of SQL Joins 1. INNER JOIN (also known as JOIN ) Description : Returns only the rows where there is a match in both tables. Alternative Names : None Use Case : Used when you only want the matching rows between two tables. Common in relational queries. Example : sql SELECT * FROM employees INNER JOIN departments ON employees.department_id = departments.id; 2. LEFT JOIN (also known as LEFT OUTER JOIN ) Description : Returns all rows from the left table, and the matching rows from the right table. If no match, NULL values are returned for the right table. Alternative Names : LEFT OUTER JOIN Use Case : Used when you want all rows from the left table, even if there's no match in the right table. Example : sql SELECT * FROM employees LEFT JOIN departments ON employees.department_id = departments.id; 3. RIGHT JOIN (also known as RIGHT OUTER JOIN ) Description : Returns all rows from the right table, and the matching rows from the left table. If no match, NULL values are returned for the left table. Alternative Names : RIGHT OUTER JOIN Use Case : Used when you want all rows from the right table, even if there's no match in the left table. Example : sql SELECT * FROM employees RIGHT JOIN departments ON employees.department_id = departments.id; 4. FULL JOIN (also known as FULL OUTER JOIN ) Description : Returns all rows from both tables, with matching rows from both sides. Non-matching rows are returned as NULL. Alternative Names : FULL OUTER JOIN Use Case : Used when you need to include all rows from both tables, even if no matches exist. Example : sql SELECT * FROM employees FULL JOIN departments ON employees.department_id = departments.id; 5. CROSS JOIN Description : Returns the Cartesian product of both tables. Each row from the first table is combined with each row from the second table. Alternative Names : None Use Case : Used when you want to combine every row from one table with every row from another table. Typically used for generating combinations or testing. Example : sql SELECT * FROM products CROSS JOIN suppliers; 6. SELF JOIN Description : A table is joined with itself to compare rows within the same table. Alternative Names : None Use Case : Used for hierarchical relationships (e.g., finding managers for employees in the same table). Example : sql SELECT e1.name AS Employee, e2.name AS Manager FROM employees e1 LEFT JOIN employees e2 ON e1.manager_id = e2.id; 7. NATURAL JOIN Description : Automatically joins tables based on columns with the same name in both tables. No need to explicitly specify the join condition. Alternative Names : None Use Case : Used when both tables have the same column names and you want an automatic join on those columns. Example : sql SELECT * FROM employees NATURAL JOIN departments; 8. ANTI JOIN (also known as LEFT ANTI JOIN ) Description : Returns rows from the left table where no matching rows exist in the right table. Alternative Names : LEFT ANTI JOIN Use Case : Used when you need rows from the left table that do not have a corresponding match in the right table. Example (using LEFT JOIN and filtering NULL ): sql SELECT employees.name FROM employees LEFT JOIN departments ON employees.department_id = departments.id WHERE departments.id IS NULL; 9. SEMI JOIN (also known as LEFT SEMI JOIN or RIGHT SEMI JOIN ) Description : Returns rows from the left table where a match exists in the right table, but only columns from the left table are returned. Alternative Names : LEFT SEMI JOIN , RIGHT SEMI JOIN Use Case : Used when you need to know which rows in the left table have a match in the right table but only need the left table's columns. Example (using EXISTS ): sql SELECT employees.name FROM employees WHERE EXISTS ( SELECT 1 FROM departments WHERE employees.department_id = departments.id ); 10. EXCEPT JOIN Description : Combines two result sets and returns rows from the left set that are not in the right set. Alternative Names : None Use Case : Used for set difference operations where you need to find rows in one table that don't exist in another. Example : sql SELECT * FROM employees EXCEPT SELECT * FROM employees_in_department; 11. OUTER APPLY Description : Similar to a left join but with subqueries evaluated per row in the left table. Alternative Names : None Use Case : Used when you need a subquery or table-valued function evaluated once per row in the left table. Example : sql SELECT employees.name, department_details.* FROM employees OUTER APPLY getDepartmentDetails(employees.department_id) AS department_details; 12. INNER APPLY Description : Similar to an inner join but with subqueries evaluated per row in the left table. Alternative Names : None Use Case : Used when you want to perform an inner join with a subquery or table-valued function evaluated per row. Example : sql SELECT employees.name, department_details.* FROM employees INNER APPLY getDepartmentDetails(employees.department_id) AS department_details; Summary Table Join Type Alternative Name(s) Description Use Case INNER JOIN JOIN Matches rows in both tables General use, finding common records LEFT JOIN LEFT OUTER JOIN All rows from left, matched from right When you need all rows from the left table RIGHT JOIN RIGHT OUTER JOIN All rows from right, matched from left When you need all rows from the right table FULL JOIN FULL OUTER JOIN All rows from both tables, with NULLs for non-matches When you need complete data from both tables CROSS JOIN None Cartesian product of both tables Generating combinations or large result sets SELF JOIN None Joining a table with itself For hierarchical relationships within a table NATURAL JOIN None Automatically joins on matching columns When tables have common column names ANTI JOIN LEFT ANTI JOIN Rows from the left table without a match in the right When you need records that don't match SEMI JOIN LEFT SEMI JOIN, RIGHT SEMI JOIN Rows from left with matching right rows, only left columns When you need to check for existence in the right table EXCEPT JOIN None Rows from left that are not in right For set difference operations OUTER APPLY None Subqueries evaluated once per row in the left table Using subqueries for each row INNER APPLY None Subqueries evaluated once per row in the left table, like an inner join Inner joins with subqueries SQL Keys Key Type Description Key Feature Primary Key Uniquely identifies each row in a table Ensures uniqueness and no NULL values Foreign Key Refers to a primary key in another table Creates relationships between tables Unique Key Ensures unique values in a column Allows one NULL value Candidate Key Potential primary key Can uniquely identify a row Composite Key Made of multiple columns Combines columns to create uniqueness Alternate Key Any candidate key not chosen as the primary Alternative unique identifier Superkey A set of columns that can uniquely identify a row May include extra columns Natural Key A key formed from real-world data Represents real-world attributes Surrogate Key An artificial key generated by the system No inherent meaning outside the database SQL Concepts: What is the difference between WHERE and HAVING clauses in SQL? The WHERE clause filters rows before any grouping is performed, while the HAVING clause filters groups after the GROUP BY operation. Explain the purpose of the GROUP BY clause. The GROUP BY clause groups rows that have the same values into summary rows, often used with aggregate functions like COUNT , SUM , AVG , MAX , or MIN . What are aggregate functions in SQL? Can you name a few? Aggregate functions perform a calculation on a set of values and return a single value. Examples include COUNT , SUM , AVG , MAX , and MIN . What is a primary key, and why is it important? A primary key is a unique identifier for a record in a table. It ensures that each record can be uniquely identified and prevents duplicate records. What is a foreign key, and how does it relate to a primary key? A foreign key is a field in one table that uniquely identifies a row of another table. It establishes a link between the two tables, ensuring referential integrity. Describe the different types of joins in SQL. INNER JOIN : Returns records that have matching values in both tables. LEFT JOIN (or LEFT OUTER JOIN) : Returns all records from the left table and the matched records from the right table; if no match, NULL values are returned for columns from the right table. RIGHT JOIN (or RIGHT OUTER JOIN) : Returns all records from the right table and the matched records from the left table; if no match, NULL values are returned for columns from the left table. FULL JOIN (or FULL OUTER JOIN) : Returns all records when there is a match in either left or right table; if no match, NULL values are returned for columns from the table without a match. What is normalization in database design? Normalization is the process of organizing data to reduce redundancy and dependency by dividing large tables into smaller ones and defining relationships between them. What are indexes in SQL, and how do they improve query performance? Indexes are database objects that improve the speed of data retrieval operations on a table at the cost of additional space and decreased performance on data modification operations. They function similarly to an index in a book, allowing the database to find data more quickly. Explain the concept of a transaction in SQL. A transaction is a sequence of one or more SQL operations executed as a single unit. Transactions ensure data integrity and are governed by the ACID properties: Atomicity, Consistency, Isolation, and Durability. What is the purpose of the DISTINCT keyword? The DISTINCT keyword is used to return only distinct (different) values in the result set, eliminating duplicate records.","title":"Spark-SQL"},{"location":"SQL/SQL/#essential-sparksql-commands","text":"Command Description Example SET -v Shows all Spark SQL settings. SET -v; SHOW DATABASES Lists all databases. SHOW DATABASES; CREATE DATABASE Creates a new database. CREATE DATABASE db_name; DROP DATABASE Drops an existing database. DROP DATABASE db_name; USE Sets the current database. USE db_name; SHOW TABLES Lists all tables in the current database. SHOW TABLES; DESCRIBE TABLE Provides the schema of a table. DESCRIBE TABLE table_name; DESCRIBE EXTENDED Provides detailed information about a table, including metadata. DESCRIBE EXTENDED table_name; CREATE TABLE Creates a new table. CREATE TABLE table_name (col1 INT, ...); CREATE EXTERNAL TABLE Creates an external table. CREATE EXTERNAL TABLE table_name ...; DROP TABLE Drops an existing table. DROP TABLE table_name; INSERT INTO Inserts data into a table. INSERT INTO table_name VALUES (...); SELECT Queries data from tables. SELECT * FROM table_name; CREATE VIEW Creates a view based on a query. CREATE VIEW view_name AS SELECT ...; DROP VIEW Drops an existing view. DROP VIEW view_name; CACHE TABLE Caches a table in memory. CACHE TABLE table_name; UNCACHE TABLE Removes a table from the in-memory cache. UNCACHE TABLE table_name; SHOW COLUMNS Lists all columns of a table. SHOW COLUMNS FROM table_name; ALTER TABLE Changes the structure of an existing table. ALTER TABLE table_name ADD COLUMNS (...); TRUNCATE TABLE Removes all rows from a table without deleting the table. TRUNCATE TABLE table_name; MSCK REPAIR TABLE Recovers partitions of a table. MSCK REPAIR TABLE table_name; SHOW PARTITIONS Lists all partitions of a table. SHOW PARTITIONS table_name; EXPLAIN Provides a detailed execution plan for a query. EXPLAIN SELECT * FROM table_name; SHOW CREATE TABLE Displays the CREATE TABLE statement for an existing table. SHOW CREATE TABLE table_name; LOAD DATA Loads data into a table from a file. LOAD DATA INPATH 'path' INTO TABLE ...; SET Sets a Spark SQL configuration property. SET property_name=value; SHOW FUNCTIONS Lists all functions available in the Spark SQL environment. SHOW FUNCTIONS; DESCRIBE FUNCTION Provides information about a function. DESCRIBE FUNCTION function_name;","title":"Essential SparkSQL commands"},{"location":"SQL/SQL/#openrowset-the-powerful-transact-sql-for-data-engineering","text":"","title":"OPENROWSET - The Powerful Transact-SQL for Data Engineering."},{"location":"SQL/SQL/#what-is-openrowset","text":"OPENROWSET is a Transact-SQL function. Using this, you can create a select statement from external data sources (CSV, Parquet, JSON, Delta, ADLS, Blob) as if it were a local database. You don't need to create a table or linked service to run this. Hence, it's fast and requires fewer lines of code. It is used in SQL Server, Azure SQL DB, Synapse Analytics, Fabric, and Databricks. For `OPENROWSET` to work, you need to ensure that the Synapse workspace or the query has the appropriate authentication to access the external data source.","title":"What is OPENROWSET?"},{"location":"SQL/SQL/#practical-scenarios-where-openrowset-is-preferred","text":"Ad-Hoc Data Retrieval: Scenario: A business analyst needs to quickly retrieve data from an Excel file for an ad-hoc report. Reason: OPENROWSET allows for on-the-fly access to the Excel file without the need to import it into SQL Server first, providing immediate data retrieval for analysis. Cross-Database Queries: Scenario: A developer needs to run a query that joins tables from different SQL Server instances. Reason: OPENROWSET can be used to run distributed queries across different servers, avoiding the need to set up linked servers which can be more complex and require additional configuration and permissions. Accessing External Data Sources: Scenario: A company needs to integrate data from a remote Oracle database for a one-time data migration task. Reason: OPENROWSET provides a quick way to query data from the Oracle database using an OLE DB provider, bypassing the need for more permanent and complex solutions like linked servers or SSIS packages. Temporary Data Access: Scenario: During a data validation process, a data engineer needs to access data from a CSV file provided by a client. Reason: OPENROWSET allows the engineer to read the CSV file directly into a SQL query, facilitating temporary data access without the overhead of creating permanent database tables or ETL processes. Data Import for Development and Testing: Scenario: Developers need to import data from various formats (e.g., Excel, CSV) into a development environment for testing purposes. Reason: OPENROWSET can be used to import data directly into the development environment without the need for pre-configured data import processes, speeding up the testing and development cycles.","title":"Practical scenarios where OPENROWSET is preferred"},{"location":"SQL/SQL/#syntax","text":"-- General Syntax OPENROWSET( BULK 'file_path', FORMAT = 'file_format', [DATA_SOURCE = 'data_source_name'], [WITH ( column_name column_type [ 'column_ordinal' | 'json_path' ] )] ) AS alias_name -- Example for CSV SELECT * FROM OPENROWSET( BULK 'https://storageaccount.blob.core.windows.net/container/file.csv', FORMAT = 'CSV', FIRSTROW = 2, FIELDTERMINATOR = ',', ROWTERMINATOR = '\\n' ) AS [result] -- Example for Parquet SELECT * FROM OPENROWSET( BULK 'https://storageaccount.blob.core.windows.net/container/file.parquet', FORMAT = 'PARQUET' ) AS [result] -- Example for Delta Lake SELECT * FROM OPENROWSET( BULK 'https://storageaccount.blob.core.windows.net/container/file.delta', FORMAT = 'DELTA' ) AS [result]","title":"Syntax"},{"location":"SQL/SQL/#key-options","text":"BULK : Specifies the path to the file(s). FORMAT : Specifies the file format ( CSV , PARQUET , or DELTA ). DATA_SOURCE : (Optional) Specifies the external data source. WITH : Defines the schema of the columns to be read.","title":"Key Options"},{"location":"SQL/SQL/#options-for-csv","text":"FIELDTERMINATOR : Character used to separate fields (default is , ). ROWTERMINATOR : Character used to separate rows (default is \\n ). FIRSTROW : Specifies the first row to read (default is 1). HEADER_ROW : Indicates if the CSV file contains a header row ( TRUE or FALSE ).","title":"Options for CSV"},{"location":"SQL/SQL/#examples","text":"Reading a CSV file without specifying a schema: sql SELECT * FROM OPENROWSET( BULK 'https://storageaccount.blob.core.windows.net/container/data.csv', FORMAT = 'CSV' ) AS [data] Reading specific columns from a CSV file: sql SELECT * FROM OPENROWSET( BULK 'https://storageaccount.blob.core.windows.net/container/data.csv', FORMAT = 'CSV', FIELDTERMINATOR = ',', ROWTERMINATOR = '\\n', FIRSTROW = 2 ) WITH ( col1 VARCHAR(50), col2 INT, col3 DATE ) AS [data] Reading a Parquet file: sql SELECT * FROM OPENROWSET( BULK 'https://storageaccount.blob.core.windows.net/container/data.parquet', FORMAT = 'PARQUET' ) AS [data] Reading a Delta Lake file: sql SELECT * FROM OPENROWSET( BULK 'https://storageaccount.blob.core.windows.net/container/data.delta', FORMAT = 'DELTA' ) AS [data]","title":"Examples"},{"location":"SQL/SQL/#what-other-options-we-have","text":"Alternative Usage Scenario Efficiency Microsoft Recommendation BULK INSERT Loading large volumes of data from a file into a SQL Server table. High for large files Use when importing large data volumes directly. PolyBase Querying external data in Hadoop, Azure Blob Storage, and Azure Data Lake. High Use for big data environments and distributed systems. EXTERNAL TABLE Accessing external data sources seamlessly as if they are regular SQL tables. Moderate to High Recommended for persistent access to external data. Linked Servers Connecting SQL Server to other data sources, like another SQL Server or an OLE DB data source. Varies Use for diverse data sources and quick data joins. Databricks Reading data from various sources using Spark SQL (not native to Databricks but works with Synapse). High for big data Use for Spark SQL integrations and large-scale analytics.","title":"What other options we have?"},{"location":"SQL/SQL/#approx_count_distinct-when-countdistinct-colname-runs-forever","text":"Imagine your table has 1 trillion rows. You want to count distinct customer names. Do you think COUNT(DISTINCT CustomerName) will show you results in your lifetime? No, for such scenarios, use APPROX_COUNT_DISTINCT(ColName) . It will give you around 97% accuracy.","title":"APPROX_COUNT_DISTINCT when COUNT(DISTINCT ColName) runs forever"},{"location":"SQL/SQL/#how-to-run-it","text":"Pretty simple and straightforward. Normal Count SELECT COUNT(DISTINCT CustomerName) FROM Customers; Approximate Count SELECT APPROX_COUNT_DISTINCT(CustomerName) FROM Customers;","title":"How to run it?"},{"location":"SQL/SQL/#some-fun-facts-about-this-t-sql-function","text":"In MS SQL Server APPROX_COUNT_DISTINCT it first came in SQL Server 2019 . It is available in Synapse SQL . These other brands support this function: Google BigQuery Amazon Redshift : It offers the APPROXIMATE COUNT(DISTINCT column) function, which serves a similar purpose. PostgreSQL : The approx_count_distinct extension is available through the HLL (HyperLogLog) extension, which allows for approximate distinct counting. Apache Spark : The approx_count_distinct function is available in Spark SQL, which uses HyperLogLog for approximate counting.","title":"Some Fun facts about this T-SQL function"},{"location":"SQL/SQL/#practice-joins-with-mssql-adventureworkslt2016","text":"Download MSSQL Sample database AdventureWorksLT2016.bak and restore it. It will give you a full enviornment. https://github.com/Microsoft/sql-server-samples/releases/download/adventureworks/AdventureWorksLT2016.bak","title":"Practice Joins with MSSQL AdventureWorksLT2016"},{"location":"SQL/SQL/#table-descriptions","text":"Here is a brief description of some of the key tables in the AdventureWorksLT2016 database: SalesLT.Customer : Contains customer information such as CustomerID, FirstName, LastName, EmailAddress, etc. SalesLT.SalesOrderHeader : Contains sales order information including SalesOrderID, OrderDate, CustomerID, TotalDue, etc. SalesLT.SalesOrderDetail : Contains details of each sales order, such as SalesOrderID, ProductID, OrderQty, UnitPrice, etc. SalesLT.Product : Contains product information including ProductID, Name, ProductNumber, Color, ListPrice, etc. SalesLT.ProductCategory : Contains information about product categories including ProductCategoryID and Name. SalesLT.ProductModel : Contains information about product models including ProductModelID and Name. SalesLT.Address : Contains address information including AddressID, AddressLine1, City, StateProvince, etc. SalesLT.CustomerAddress : A junction table linking customers to addresses.","title":"Table Descriptions"},{"location":"SQL/SQL/#questions","text":"","title":"Questions"},{"location":"SQL/SQL/#question-1-list-customers-with-their-orders","text":"Question : Retrieve a list of customers along with their order details. Include the customer's first name, last name, and order date. Only include customers who have placed at least one order. Hint : - Use SalesLT.Customer to get customer information. - Use SalesLT.SalesOrderHeader to get order information. - Use an INNER JOIN to connect these tables.","title":"Question 1: List Customers with Their Orders"},{"location":"SQL/SQL/#question-2-list-products-and-their-categories","text":"Question : Retrieve a list of products along with their category names. Include all products, even those that do not belong to any category. Hint : - Use SalesLT.Product to get product information. - Use SalesLT.ProductCategory to get category information. - Use a LEFT JOIN to include all products.","title":"Question 2: List Products and Their Categories"},{"location":"SQL/SQL/#question-3-find-customers-and-their-addresses","text":"Question : Retrieve a list of customers and their addresses. Include all customers, even if they do not have an address listed. Hint : - Use SalesLT.Customer to get customer information. - Use SalesLT.CustomerAddress to get the relationship between customers and addresses. - Use SalesLT.Address to get address information. - Use a LEFT JOIN to include all customers.","title":"Question 3: Find Customers and Their Addresses"},{"location":"SQL/SQL/#question-4-list-all-products-and-their-order-details","text":"Question : Retrieve a list of all products and their order details. Include all products and orders, even if they do not match. Hint : - Use SalesLT.Product to get product information. - Use SalesLT.SalesOrderDetail to get order details. - Use a FULL OUTER JOIN to include all products and orders.","title":"Question 4: List All Products and Their Order Details"},{"location":"SQL/SQL/#question-5-products-with-no-orders","text":"Question : Find products that have not been ordered by any customer. Hint : - Use SalesLT.Product to get product information. - Use SalesLT.SalesOrderDetail to get order details. - Use a LEFT JOIN and filter to find products with no matching orders.","title":"Question 5: Products with No Orders"},{"location":"SQL/SQL/#question-6-calculate-total-sales-per-customer-using-cte","text":"Question : Use a Common Table Expression (CTE) to calculate the total sales amount for each customer. Hint : - Use SalesLT.Customer to get customer information. - Use SalesLT.SalesOrderHeader to get order information. - Use SUM function to calculate total sales. - Use a CTE to organize the query.","title":"Question 6: Calculate Total Sales per Customer Using CTE"},{"location":"SQL/SQL/#comprehensive-list-of-sql-joins","text":"","title":"Comprehensive List of SQL Joins"},{"location":"SQL/SQL/#1-inner-join-also-known-as-join","text":"Description : Returns only the rows where there is a match in both tables. Alternative Names : None Use Case : Used when you only want the matching rows between two tables. Common in relational queries. Example : sql SELECT * FROM employees INNER JOIN departments ON employees.department_id = departments.id;","title":"1. INNER JOIN (also known as JOIN)"},{"location":"SQL/SQL/#2-left-join-also-known-as-left-outer-join","text":"Description : Returns all rows from the left table, and the matching rows from the right table. If no match, NULL values are returned for the right table. Alternative Names : LEFT OUTER JOIN Use Case : Used when you want all rows from the left table, even if there's no match in the right table. Example : sql SELECT * FROM employees LEFT JOIN departments ON employees.department_id = departments.id;","title":"2. LEFT JOIN (also known as LEFT OUTER JOIN)"},{"location":"SQL/SQL/#3-right-join-also-known-as-right-outer-join","text":"Description : Returns all rows from the right table, and the matching rows from the left table. If no match, NULL values are returned for the left table. Alternative Names : RIGHT OUTER JOIN Use Case : Used when you want all rows from the right table, even if there's no match in the left table. Example : sql SELECT * FROM employees RIGHT JOIN departments ON employees.department_id = departments.id;","title":"3. RIGHT JOIN (also known as RIGHT OUTER JOIN)"},{"location":"SQL/SQL/#4-full-join-also-known-as-full-outer-join","text":"Description : Returns all rows from both tables, with matching rows from both sides. Non-matching rows are returned as NULL. Alternative Names : FULL OUTER JOIN Use Case : Used when you need to include all rows from both tables, even if no matches exist. Example : sql SELECT * FROM employees FULL JOIN departments ON employees.department_id = departments.id;","title":"4. FULL JOIN (also known as FULL OUTER JOIN)"},{"location":"SQL/SQL/#5-cross-join","text":"Description : Returns the Cartesian product of both tables. Each row from the first table is combined with each row from the second table. Alternative Names : None Use Case : Used when you want to combine every row from one table with every row from another table. Typically used for generating combinations or testing. Example : sql SELECT * FROM products CROSS JOIN suppliers;","title":"5. CROSS JOIN"},{"location":"SQL/SQL/#6-self-join","text":"Description : A table is joined with itself to compare rows within the same table. Alternative Names : None Use Case : Used for hierarchical relationships (e.g., finding managers for employees in the same table). Example : sql SELECT e1.name AS Employee, e2.name AS Manager FROM employees e1 LEFT JOIN employees e2 ON e1.manager_id = e2.id;","title":"6. SELF JOIN"},{"location":"SQL/SQL/#7-natural-join","text":"Description : Automatically joins tables based on columns with the same name in both tables. No need to explicitly specify the join condition. Alternative Names : None Use Case : Used when both tables have the same column names and you want an automatic join on those columns. Example : sql SELECT * FROM employees NATURAL JOIN departments;","title":"7. NATURAL JOIN"},{"location":"SQL/SQL/#8-anti-join-also-known-as-left-anti-join","text":"Description : Returns rows from the left table where no matching rows exist in the right table. Alternative Names : LEFT ANTI JOIN Use Case : Used when you need rows from the left table that do not have a corresponding match in the right table. Example (using LEFT JOIN and filtering NULL ): sql SELECT employees.name FROM employees LEFT JOIN departments ON employees.department_id = departments.id WHERE departments.id IS NULL;","title":"8. ANTI JOIN (also known as LEFT ANTI JOIN)"},{"location":"SQL/SQL/#9-semi-join-also-known-as-left-semi-join-or-right-semi-join","text":"Description : Returns rows from the left table where a match exists in the right table, but only columns from the left table are returned. Alternative Names : LEFT SEMI JOIN , RIGHT SEMI JOIN Use Case : Used when you need to know which rows in the left table have a match in the right table but only need the left table's columns. Example (using EXISTS ): sql SELECT employees.name FROM employees WHERE EXISTS ( SELECT 1 FROM departments WHERE employees.department_id = departments.id );","title":"9. SEMI JOIN (also known as LEFT SEMI JOIN or RIGHT SEMI JOIN)"},{"location":"SQL/SQL/#10-except-join","text":"Description : Combines two result sets and returns rows from the left set that are not in the right set. Alternative Names : None Use Case : Used for set difference operations where you need to find rows in one table that don't exist in another. Example : sql SELECT * FROM employees EXCEPT SELECT * FROM employees_in_department;","title":"10. EXCEPT JOIN"},{"location":"SQL/SQL/#11-outer-apply","text":"Description : Similar to a left join but with subqueries evaluated per row in the left table. Alternative Names : None Use Case : Used when you need a subquery or table-valued function evaluated once per row in the left table. Example : sql SELECT employees.name, department_details.* FROM employees OUTER APPLY getDepartmentDetails(employees.department_id) AS department_details;","title":"11. OUTER APPLY"},{"location":"SQL/SQL/#12-inner-apply","text":"Description : Similar to an inner join but with subqueries evaluated per row in the left table. Alternative Names : None Use Case : Used when you want to perform an inner join with a subquery or table-valued function evaluated per row. Example : sql SELECT employees.name, department_details.* FROM employees INNER APPLY getDepartmentDetails(employees.department_id) AS department_details;","title":"12. INNER APPLY"},{"location":"SQL/SQL/#summary-table","text":"Join Type Alternative Name(s) Description Use Case INNER JOIN JOIN Matches rows in both tables General use, finding common records LEFT JOIN LEFT OUTER JOIN All rows from left, matched from right When you need all rows from the left table RIGHT JOIN RIGHT OUTER JOIN All rows from right, matched from left When you need all rows from the right table FULL JOIN FULL OUTER JOIN All rows from both tables, with NULLs for non-matches When you need complete data from both tables CROSS JOIN None Cartesian product of both tables Generating combinations or large result sets SELF JOIN None Joining a table with itself For hierarchical relationships within a table NATURAL JOIN None Automatically joins on matching columns When tables have common column names ANTI JOIN LEFT ANTI JOIN Rows from the left table without a match in the right When you need records that don't match SEMI JOIN LEFT SEMI JOIN, RIGHT SEMI JOIN Rows from left with matching right rows, only left columns When you need to check for existence in the right table EXCEPT JOIN None Rows from left that are not in right For set difference operations OUTER APPLY None Subqueries evaluated once per row in the left table Using subqueries for each row INNER APPLY None Subqueries evaluated once per row in the left table, like an inner join Inner joins with subqueries","title":"Summary Table"},{"location":"SQL/SQL/#sql-keys","text":"Key Type Description Key Feature Primary Key Uniquely identifies each row in a table Ensures uniqueness and no NULL values Foreign Key Refers to a primary key in another table Creates relationships between tables Unique Key Ensures unique values in a column Allows one NULL value Candidate Key Potential primary key Can uniquely identify a row Composite Key Made of multiple columns Combines columns to create uniqueness Alternate Key Any candidate key not chosen as the primary Alternative unique identifier Superkey A set of columns that can uniquely identify a row May include extra columns Natural Key A key formed from real-world data Represents real-world attributes Surrogate Key An artificial key generated by the system No inherent meaning outside the database SQL Concepts: What is the difference between WHERE and HAVING clauses in SQL? The WHERE clause filters rows before any grouping is performed, while the HAVING clause filters groups after the GROUP BY operation. Explain the purpose of the GROUP BY clause. The GROUP BY clause groups rows that have the same values into summary rows, often used with aggregate functions like COUNT , SUM , AVG , MAX , or MIN . What are aggregate functions in SQL? Can you name a few? Aggregate functions perform a calculation on a set of values and return a single value. Examples include COUNT , SUM , AVG , MAX , and MIN . What is a primary key, and why is it important? A primary key is a unique identifier for a record in a table. It ensures that each record can be uniquely identified and prevents duplicate records. What is a foreign key, and how does it relate to a primary key? A foreign key is a field in one table that uniquely identifies a row of another table. It establishes a link between the two tables, ensuring referential integrity. Describe the different types of joins in SQL. INNER JOIN : Returns records that have matching values in both tables. LEFT JOIN (or LEFT OUTER JOIN) : Returns all records from the left table and the matched records from the right table; if no match, NULL values are returned for columns from the right table. RIGHT JOIN (or RIGHT OUTER JOIN) : Returns all records from the right table and the matched records from the left table; if no match, NULL values are returned for columns from the left table. FULL JOIN (or FULL OUTER JOIN) : Returns all records when there is a match in either left or right table; if no match, NULL values are returned for columns from the table without a match. What is normalization in database design? Normalization is the process of organizing data to reduce redundancy and dependency by dividing large tables into smaller ones and defining relationships between them. What are indexes in SQL, and how do they improve query performance? Indexes are database objects that improve the speed of data retrieval operations on a table at the cost of additional space and decreased performance on data modification operations. They function similarly to an index in a book, allowing the database to find data more quickly. Explain the concept of a transaction in SQL. A transaction is a sequence of one or more SQL operations executed as a single unit. Transactions ensure data integrity and are governed by the ACID properties: Atomicity, Consistency, Isolation, and Durability. What is the purpose of the DISTINCT keyword? The DISTINCT keyword is used to return only distinct (different) values in the result set, eliminating duplicate records.","title":"SQL Keys"},{"location":"SQL/SQL_AdvancedTopics/","text":"1. Normalization and Denormalization Normalization is the process of organizing data to reduce redundancy and improve integrity. It involves dividing large tables into smaller, manageable ones and using foreign keys to link them. First Normal Form (1NF) : Each column contains atomic (indivisible) values, and each record is unique. Second Normal Form (2NF) : Achieved by removing partial dependency, i.e., all non-key attributes must be fully dependent on the primary key. Third Normal Form (3NF) : No transitive dependency, meaning non-key attributes cannot depend on other non-key attributes. Denormalization : This involves combining tables to reduce the need for complex joins, improving query performance at the expense of redundancy. 2. Joins Inner Join : Returns only rows where there is a match in both tables. Left Join (or Left Outer Join) : Returns all rows from the left table, and matched rows from the right table; if no match, NULL values are returned. Right Join (or Right Outer Join) : Similar to Left Join but returns all rows from the right table. Full Outer Join : Returns rows when there is a match in one of the tables. Cross Join : Returns the Cartesian product of the two tables, i.e., all combinations of rows. Self Join : A table is joined with itself, often using aliases. 3. Subqueries Scalar Subquery : Returns a single value, and is often used in SELECT , WHERE , or HAVING clauses. sql SELECT name FROM employees WHERE salary > (SELECT AVG(salary) FROM employees); Correlated Subquery : A subquery that refers to columns from the outer query. sql SELECT e.name FROM employees e WHERE e.salary > (SELECT AVG(salary) FROM employees WHERE department_id = e.department_id); IN vs. EXISTS : IN is used for checking if a value is in a set. EXISTS is used when the query checks for the presence of rows. 4. Indexes Clustered Index : This type of index determines the physical order of data rows in the table. A table can have only one clustered index. Non-Clustered Index : This does not change the physical order of data. It creates a separate structure to speed up retrieval. Composite Index : Involves multiple columns. It can be used when queries commonly filter by more than one column. 5. Transactions ACID Properties : Ensures the database transactions are processed reliably. Atomicity : A transaction is either fully completed or not executed at all. Consistency : The database remains in a valid state before and after the transaction. Isolation : Transactions are isolated from each other. Durability : Once a transaction is committed, it is permanent. Transaction Control : BEGIN TRANSACTION : Starts a transaction. COMMIT : Saves the changes. ROLLBACK : Reverts the changes if there is an error. 6. Window Functions ROW_NUMBER() : Assigns a unique row number to each row in a result set. sql SELECT name, ROW_NUMBER() OVER (ORDER BY salary DESC) AS rank FROM employees; RANK() and DENSE_RANK() : Similar to ROW_NUMBER() , but handles ties differently. PARTITION BY : Divides the result set into partitions and performs operations on each partition. 7. Triggers and Stored Procedures Triggers : Automatically executed or fired when certain events occur on a table or view (e.g., INSERT , UPDATE , DELETE ). sql CREATE TRIGGER update_salary AFTER UPDATE ON employees FOR EACH ROW BEGIN -- trigger logic here END; Stored Procedures : A set of SQL statements stored in the database that can be executed as a program. sql CREATE PROCEDURE GetEmployeeSalary(IN emp_id INT) BEGIN SELECT salary FROM employees WHERE employee_id = emp_id; END; Functions : Similar to stored procedures, but typically return a value. 8. Partitioning Range Partitioning : Dividing a table into partitions based on a range of values (e.g., dates). List Partitioning : Dividing a table into partitions based on a list of values. Hash Partitioning : Dividing the data into partitions based on a hash function. 9. Advanced SQL Clauses WITH Clause (Common Table Expressions or CTEs) : Makes a subquery easier to reference. sql WITH DepartmentAvgSalary AS ( SELECT department_id, AVG(salary) AS avg_salary FROM employees GROUP BY department_id ) SELECT e.name, e.salary FROM employees e JOIN DepartmentAvgSalary das ON e.department_id = das.department_id WHERE e.salary > das.avg_salary; GROUP BY ROLLUP and CUBE : Useful for generating subtotals and grand totals. sql SELECT department, SUM(salary) FROM employees GROUP BY ROLLUP (department); 10. Data Warehousing Concepts Star Schema : A central fact table surrounded by dimension tables. Snowflake Schema : A more normalized form of the star schema, where dimension tables are further split into related tables. Fact Tables and Dimension Tables : Fact tables hold quantitative data, while dimension tables contain descriptive data (e.g., products , dates ). ETL Process : Extract, Transform, Load; used for moving data from operational databases to a data warehouse. 11. NoSQL Databases For databases like MongoDB, Cassandra, or Redis, understanding data models (document, key-value, column-family) and how they differ from relational databases is important. Aggregation Pipeline (MongoDB) : A framework for performing data transformations within MongoDB. Cassandra Queries : Using CQL (Cassandra Query Language) for column-family based databases. 12. Optimizing Queries EXPLAIN Plan : Analyzing the execution plan of a query can help identify bottlenecks. Query Hints : Directing the query optimizer to use specific indexes or join strategies. Materialized Views : Precomputed results that can be used for complex queries to improve performance. Partition Pruning : Ensuring that only relevant partitions are scanned to improve performance.","title":"SQL Advanced Topics"},{"location":"SQL/SQL_AdvancedTopics/#1-normalization-and-denormalization","text":"Normalization is the process of organizing data to reduce redundancy and improve integrity. It involves dividing large tables into smaller, manageable ones and using foreign keys to link them. First Normal Form (1NF) : Each column contains atomic (indivisible) values, and each record is unique. Second Normal Form (2NF) : Achieved by removing partial dependency, i.e., all non-key attributes must be fully dependent on the primary key. Third Normal Form (3NF) : No transitive dependency, meaning non-key attributes cannot depend on other non-key attributes. Denormalization : This involves combining tables to reduce the need for complex joins, improving query performance at the expense of redundancy.","title":"1. Normalization and Denormalization"},{"location":"SQL/SQL_AdvancedTopics/#2-joins","text":"Inner Join : Returns only rows where there is a match in both tables. Left Join (or Left Outer Join) : Returns all rows from the left table, and matched rows from the right table; if no match, NULL values are returned. Right Join (or Right Outer Join) : Similar to Left Join but returns all rows from the right table. Full Outer Join : Returns rows when there is a match in one of the tables. Cross Join : Returns the Cartesian product of the two tables, i.e., all combinations of rows. Self Join : A table is joined with itself, often using aliases.","title":"2. Joins"},{"location":"SQL/SQL_AdvancedTopics/#3-subqueries","text":"Scalar Subquery : Returns a single value, and is often used in SELECT , WHERE , or HAVING clauses. sql SELECT name FROM employees WHERE salary > (SELECT AVG(salary) FROM employees); Correlated Subquery : A subquery that refers to columns from the outer query. sql SELECT e.name FROM employees e WHERE e.salary > (SELECT AVG(salary) FROM employees WHERE department_id = e.department_id); IN vs. EXISTS : IN is used for checking if a value is in a set. EXISTS is used when the query checks for the presence of rows.","title":"3. Subqueries"},{"location":"SQL/SQL_AdvancedTopics/#4-indexes","text":"Clustered Index : This type of index determines the physical order of data rows in the table. A table can have only one clustered index. Non-Clustered Index : This does not change the physical order of data. It creates a separate structure to speed up retrieval. Composite Index : Involves multiple columns. It can be used when queries commonly filter by more than one column.","title":"4. Indexes"},{"location":"SQL/SQL_AdvancedTopics/#5-transactions","text":"ACID Properties : Ensures the database transactions are processed reliably. Atomicity : A transaction is either fully completed or not executed at all. Consistency : The database remains in a valid state before and after the transaction. Isolation : Transactions are isolated from each other. Durability : Once a transaction is committed, it is permanent. Transaction Control : BEGIN TRANSACTION : Starts a transaction. COMMIT : Saves the changes. ROLLBACK : Reverts the changes if there is an error.","title":"5. Transactions"},{"location":"SQL/SQL_AdvancedTopics/#6-window-functions","text":"ROW_NUMBER() : Assigns a unique row number to each row in a result set. sql SELECT name, ROW_NUMBER() OVER (ORDER BY salary DESC) AS rank FROM employees; RANK() and DENSE_RANK() : Similar to ROW_NUMBER() , but handles ties differently. PARTITION BY : Divides the result set into partitions and performs operations on each partition.","title":"6. Window Functions"},{"location":"SQL/SQL_AdvancedTopics/#7-triggers-and-stored-procedures","text":"Triggers : Automatically executed or fired when certain events occur on a table or view (e.g., INSERT , UPDATE , DELETE ). sql CREATE TRIGGER update_salary AFTER UPDATE ON employees FOR EACH ROW BEGIN -- trigger logic here END; Stored Procedures : A set of SQL statements stored in the database that can be executed as a program. sql CREATE PROCEDURE GetEmployeeSalary(IN emp_id INT) BEGIN SELECT salary FROM employees WHERE employee_id = emp_id; END; Functions : Similar to stored procedures, but typically return a value.","title":"7. Triggers and Stored Procedures"},{"location":"SQL/SQL_AdvancedTopics/#8-partitioning","text":"Range Partitioning : Dividing a table into partitions based on a range of values (e.g., dates). List Partitioning : Dividing a table into partitions based on a list of values. Hash Partitioning : Dividing the data into partitions based on a hash function.","title":"8. Partitioning"},{"location":"SQL/SQL_AdvancedTopics/#9-advanced-sql-clauses","text":"WITH Clause (Common Table Expressions or CTEs) : Makes a subquery easier to reference. sql WITH DepartmentAvgSalary AS ( SELECT department_id, AVG(salary) AS avg_salary FROM employees GROUP BY department_id ) SELECT e.name, e.salary FROM employees e JOIN DepartmentAvgSalary das ON e.department_id = das.department_id WHERE e.salary > das.avg_salary; GROUP BY ROLLUP and CUBE : Useful for generating subtotals and grand totals. sql SELECT department, SUM(salary) FROM employees GROUP BY ROLLUP (department);","title":"9. Advanced SQL Clauses"},{"location":"SQL/SQL_AdvancedTopics/#10-data-warehousing-concepts","text":"Star Schema : A central fact table surrounded by dimension tables. Snowflake Schema : A more normalized form of the star schema, where dimension tables are further split into related tables. Fact Tables and Dimension Tables : Fact tables hold quantitative data, while dimension tables contain descriptive data (e.g., products , dates ). ETL Process : Extract, Transform, Load; used for moving data from operational databases to a data warehouse.","title":"10. Data Warehousing Concepts"},{"location":"SQL/SQL_AdvancedTopics/#11-nosql-databases","text":"For databases like MongoDB, Cassandra, or Redis, understanding data models (document, key-value, column-family) and how they differ from relational databases is important. Aggregation Pipeline (MongoDB) : A framework for performing data transformations within MongoDB. Cassandra Queries : Using CQL (Cassandra Query Language) for column-family based databases.","title":"11. NoSQL Databases"},{"location":"SQL/SQL_AdvancedTopics/#12-optimizing-queries","text":"EXPLAIN Plan : Analyzing the execution plan of a query can help identify bottlenecks. Query Hints : Directing the query optimizer to use specific indexes or join strategies. Materialized Views : Precomputed results that can be used for complex queries to improve performance. Partition Pruning : Ensuring that only relevant partitions are scanned to improve performance.","title":"12. Optimizing Queries"},{"location":"SQL/SSIS/","text":"SQL Server Integration Services To put simply SSIS is the ETL tool for MSSQL Server. Before Azure Data Factory this was the ETL tool for Microosft ecosystem. Common activity with this tool is extracting data from XML, Csvs, .txt files and load into MSSQL Server. How to install SSIS SSIS is installed as an option during MSSQL installation. There is no separate installation of this. Control Flow and Data Flow The SSIS ETL workflow is made up two main components: Control Flow: This is the 'roadmap/blueprint/overall plan/sequence of steps' of the ETL workflow. It tell what steps to perform in which order. E.g. First check if the file exists, Then read the data from it, Then, put the data to SQL server. Control flow contains tasks and containers(for loop etc) Data flow It is the actual transformation and movement of data. It contains Source, transformation and destination. Oversimplified summary: Control flow is the Workflow and Dataflow is the transformation. Remember, control flow is the workflow. Dataflow is the transformation. Dataflow task is a task in the control flow. Key Differences Feature Control Flow Data Flow Basic Unit Task Transformation Focus Workflow and execution order Data movement and transformation Processing Sequential Parallel Examples Execute SQL, send email, file transfer Extract from database, clean data, load to data warehouse Relationship between Control Flow and Data Flow The control flow initiates and controls the data flow. A Data Flow Task is a type of task in the control flow. Multiple data flows can be executed within a single SSIS package. SSIS SQL Destination Types To send data from an SSIS workflow to an MSSQL server, we have three options (called Data Flow Destinations). Here\u2019s a simple guide to help you decide which destination to choose for your SQL Server. In SSIS, a destination is a component(Module) that sends data into a database, etc. Destination Type What It Does When to Use Pros Cons ADO.NET Destination Loads data into various databases using ADO.NET connections. For MySQL, Oracle, and other ADO.NET compatible databases. Supports complex data types and handles large data well. Sometimes slower than OLE DB in some cases. OLE DB Destination Loads data into databases using OLE DB connections. Mainly for SQL Server or other OLE DB compatible databases. Generally faster for SQL Server, widely used in ETL processes. Less flexible with some modern data types compared to ADO.NET. SQL Server Destination Loads data directly into SQL Server databases. When you need the best performance for SQL Server. Fastest performance for loading data into SQL Server. Only works with SQL Server and needs local execution. What to choose for MSSQL Server? The SQL Server Destination is the fastest but needs the package to run on the same server as SQL Server and usually requires higher permissions. So, OLE DB Destination could be the best choice for most cases. SQL Server Management Studio SQL Server Management Studio (SSMS) is a tool for managing SQL Server and Azure SQL Database. With SSMS, you can set up, check, and control your SQL Server and databases. You can use it to deploy, monitor, and upgrade your application's data parts, and to create queries and scripts. SSMS lets you query, design, and manage your databases and data warehouses, whether they are on your local computer or in the cloud. SSMS Installation SSMS can be freely downloaded from microsoft site and installed on your windows system. You can install it almost any version of WIndows. It doesn't require windwos server to be instaslled.","title":"SQL Server Integration Services"},{"location":"SQL/SSIS/#sql-server-integration-services","text":"To put simply SSIS is the ETL tool for MSSQL Server. Before Azure Data Factory this was the ETL tool for Microosft ecosystem. Common activity with this tool is extracting data from XML, Csvs, .txt files and load into MSSQL Server.","title":"SQL Server Integration Services"},{"location":"SQL/SSIS/#how-to-install-ssis","text":"SSIS is installed as an option during MSSQL installation. There is no separate installation of this.","title":"How to install SSIS"},{"location":"SQL/SSIS/#control-flow-and-data-flow","text":"The SSIS ETL workflow is made up two main components: Control Flow: This is the 'roadmap/blueprint/overall plan/sequence of steps' of the ETL workflow. It tell what steps to perform in which order. E.g. First check if the file exists, Then read the data from it, Then, put the data to SQL server. Control flow contains tasks and containers(for loop etc) Data flow It is the actual transformation and movement of data. It contains Source, transformation and destination. Oversimplified summary: Control flow is the Workflow and Dataflow is the transformation. Remember, control flow is the workflow. Dataflow is the transformation. Dataflow task is a task in the control flow.","title":"Control Flow and Data Flow"},{"location":"SQL/SSIS/#key-differences","text":"Feature Control Flow Data Flow Basic Unit Task Transformation Focus Workflow and execution order Data movement and transformation Processing Sequential Parallel Examples Execute SQL, send email, file transfer Extract from database, clean data, load to data warehouse","title":"Key Differences"},{"location":"SQL/SSIS/#relationship-between-control-flow-and-data-flow","text":"The control flow initiates and controls the data flow. A Data Flow Task is a type of task in the control flow. Multiple data flows can be executed within a single SSIS package.","title":"Relationship between Control Flow and Data Flow"},{"location":"SQL/SSIS/#ssis-sql-destination-types","text":"To send data from an SSIS workflow to an MSSQL server, we have three options (called Data Flow Destinations). Here\u2019s a simple guide to help you decide which destination to choose for your SQL Server. In SSIS, a destination is a component(Module) that sends data into a database, etc. Destination Type What It Does When to Use Pros Cons ADO.NET Destination Loads data into various databases using ADO.NET connections. For MySQL, Oracle, and other ADO.NET compatible databases. Supports complex data types and handles large data well. Sometimes slower than OLE DB in some cases. OLE DB Destination Loads data into databases using OLE DB connections. Mainly for SQL Server or other OLE DB compatible databases. Generally faster for SQL Server, widely used in ETL processes. Less flexible with some modern data types compared to ADO.NET. SQL Server Destination Loads data directly into SQL Server databases. When you need the best performance for SQL Server. Fastest performance for loading data into SQL Server. Only works with SQL Server and needs local execution.","title":"SSIS SQL Destination Types"},{"location":"SQL/SSIS/#what-to-choose-for-mssql-server","text":"The SQL Server Destination is the fastest but needs the package to run on the same server as SQL Server and usually requires higher permissions. So, OLE DB Destination could be the best choice for most cases.","title":"What to choose for MSSQL Server?"},{"location":"SQL/SSIS/#sql-server-management-studio","text":"SQL Server Management Studio (SSMS) is a tool for managing SQL Server and Azure SQL Database. With SSMS, you can set up, check, and control your SQL Server and databases. You can use it to deploy, monitor, and upgrade your application's data parts, and to create queries and scripts. SSMS lets you query, design, and manage your databases and data warehouses, whether they are on your local computer or in the cloud.","title":"SQL Server Management Studio"},{"location":"SQL/SSIS/#ssms-installation","text":"SSMS can be freely downloaded from microsoft site and installed on your windows system. You can install it almost any version of WIndows. It doesn't require windwos server to be instaslled.","title":"SSMS Installation"},{"location":"SQL/SSRS/","text":"[TBD]Overview Reporting Services Components Reporting Services Windows Service Report Designer Installing Reporting Services Using BIDS Installing the Sample Database Creating a Simple Report with the Wizard Next Steps Creating a Shared Data Source Explanation Design Query Step Explanation Select Report Type Step Explanation Design Table Step Explanation Button Descriptions: Choose Table Layout Step Explanation Choose Table Style Step Explanation Completing the Wizard Step Explanation Creating a Report from Scratch Using Report Designer Adding a New Report to Our Project Explanation Creating a Shared Data Source Explanation Creating a Data Set Explanation [TBD]Overview SQL Server Reporting Services 2008 (SSRS) is a tool in SQL Server 2008 for designing, developing, testing, and deploying reports. It works with the Business Intelligence Development Studio (BIDS) for all tasks related to report authoring and deployment. BIDS is included with SQL Server 2008. In this tutorial, we will cover the following topics to help you build a report successfully: Reporting Services Components Install Reporting Services Business Intelligence Development Studio (BIDS) Install Sample Database Create a Simple Report with the Wizard Create a Simple Report with the Report Designer Deploy Reports Configure Report Manager Security Reporting Services Components SSRS uses two main databases: ReportServer and ReportServerTempDB. ReportServer stores reports, data sources, snapshots, subscriptions, etc., while ReportServerTempDB is for temporary storage. These databases are created automatically during the SQL Server installation if you choose the default setup. You can also create them manually using the Reporting Services Configuration Manager. Reporting Services Windows Service SSRS functionality is implemented as a Windows service. Understanding this service isn't essential for using SSRS but can be useful. The service provides several functions, as shown in the diagram below: Key points about the Windows service: HTTP Listener : This is a new feature in SSRS 2008, removing the need for Internet Information Server (IIS). Report Manager : An ASP.NET application providing a browser-based interface for managing SSRS. Web Service : An ASP.NET application offering a programmatic interface to SSRS, used by Report Manager and can be used for custom implementations. Background Processing : Handles core services for SSRS. Report Designer The Report Designer in BIDS allows you to design, develop, test, and deploy reports. BIDS, included with SQL Server 2008, has a user-friendly interface and new visualizations. Non-developers can use Report Builder, but it is not covered in this tutorial. Installing Reporting Services To install SSRS, run the SQL Server 2008 SETUP.EXE and follow the steps, selecting Reporting Services in the Feature Selection dialog: Ensure to check all items under Shared Features, especially BIDS, as it is used for designing, developing, testing, and deploying reports. In the Reporting Services Configuration dialog, select \"Install the native mode default configuration\": This installs and configures SSRS automatically. After installation, you can start using SSRS immediately. Using BIDS BIDS helps in developing reports with an intuitive interface based on Microsoft's Visual Studio. To install BIDS, select the Business Intelligence Development Studio checkbox in the Shared Features section during the SQL Server installation. BIDS uses the concept of projects and solutions to organize reports. To create a new project: 1. Launch BIDS from the Microsoft SQL Server 2008 program group. 2. Click File > New Project. 3. Select Visual Studio Solutions under Project Types and Blank Solution under Visual Studio installed templates. Enter the name and location: Click OK to create a blank solution. Next, add a project to the solution: 1. Click File > Add > New Project. 2. Select Report Server Project and fill in the details: We are now ready to install the sample database to use in our reports. Installing the Sample Database We will use the AdventureWorksLT sample database for our reports. This database is chosen because it is small and easy to set up. Download the AdventureWorksLT database from the Microsoft SQL Server Product Samples page on CodePlex. Run the installer (.msi file) and choose the folder for the database and log files. For a default SQL Server installation, use: C:\\Program Files\\Microsoft SQL Server\\MSSQL10.MSSQLSERVER\\MSSQL\\DATA After installation, attach the database to your SQL Server using SQL Server Management Studio: In the Attach Databases dialog, click Add and navigate to the AdventureWorksLT_Data.mdf file: Click OK to attach the database. Creating a Simple Report with the Wizard The Report Wizard in BIDS helps you create a simple report quickly. Open your solution in BIDS. Right-click the Reports node and select Add New Report to launch the Report Wizard: Follow these steps in the wizard: Create a Shared Data Source : Define the data source your report will use. Design a Query : Specify the SQL query to fetch the data for your report. Select a Report Type : Choose the layout type (e.g., tabular or matrix). Design a Table : Define the structure of the report. Choose the Table Layout : Select how the table should be displayed. Complete the Wizard : Finalize the report setup. Next Steps Now that you've created a simple report using the wizard, you can further customize it using the Report Designer in BIDS. The Report Designer allows you to define every aspect of your report, giving you more control and flexibility. Once you're comfortable with the basics, you can start building more sophisticated reports and dashboards. Creating a Shared Data Source A Data Source contains the information needed to fetch the data for your report. SSRS can access data from relational databases, OLAP databases, and almost any data source with an ODBC or OLE DB driver. When creating a Data Source, you can specify it as shared, meaning it can be used by any report in the same project. It is generally a good idea to create Shared Data Sources. If a Data Source is not shared, its definition is stored inside the report and cannot be shared with other reports. In this section, we will go through the steps to create a Shared Data Source. Explanation After launching the Report Wizard, you will see the Select the Data Source dialog as shown below: Since our project does not have any Shared Data Sources yet, we need to define a new Data Source. Here are the details you need to provide: Name : Choose a descriptive name for the Data Source, like AdventureWorksLT. Avoid spaces in the name to prevent errors. Type : Select the appropriate type from the dropdown list. The default value, Microsoft SQL Server, is correct for our AdventureWorksLT database. Connection String : Enter the connection string for your Data Source. It is usually easier to click the Edit button to enter the details and have the connection string created for you. Edit button : Click this to open the Connection Properties dialog where you can enter the server name and select the database. The connection string will be created for you. Credentials button : Click this to open the Data Source Credentials dialog where you can specify the credentials to use when connecting to the Data Source. Make this a shared data source checkbox : Check this box to create a Shared Data Source, so any report in the same project can use it. Click the Edit button to open the Connection Properties dialog. Enter your server name and select the AdventureWorksLT database as shown below: The Server name is where your SQL Server database is deployed. If you are running a named instance of SQL Server, specify it as SERVERNAME\\INSTANCENAME. If SQL Server is running locally, you can use localhost instead of SERVERNAME. Click the Test Connection button to verify the connection, then click OK to close the dialog. Next, click the Credentials button to open the Data Source Credentials dialog as shown below: The default selection, Use Windows Authentication (Integrated Security), is fine for our purposes. This means Reporting Services will connect to the Data Source using the Windows credentials of the person running the report. When you deploy the report and Data Source for others to use, you can select a different option if needed. For now, we'll stick with the default. After completing these steps, the Select the Data Source dialog will look like this: Click Next to proceed to the Design the Query dialog, which we will cover in the next section. Design Query Step The Design Query step of the Report Wizard allows us to specify what data we want to retrieve from our Data Source and display in our report. In this section, we will explain how to define a query to fetch the data for our report. Explanation The Design Query step in the Report Wizard displays the following dialog: You can click the Query Builder button to build your query graphically or type it directly into the Query string textbox. Here is an example query you can use: SELECT c.ParentProductCategoryName, c.ProductCategoryName, SUM(d.LineTotal) AS Sales FROM SalesLT.Product p JOIN SalesLT.vGetAllCategories c ON c.ProductCategoryID = p.ProductCategoryID JOIN SalesLT.SalesOrderDetail d ON d.ProductID = p.ProductID GROUP BY c.ParentProductCategoryName, c.ProductCategoryName ORDER BY c.ParentProductCategoryName, c.ProductCategoryName This query provides a sales summary broken down by product category. Copy and paste this query into the Query string textbox in the Design Query dialog. Alternatively, you can click the Query Builder button and design the query graphically. Click Next to move to the Select Report Type dialog. Select Report Type Step The Select Report Type step of the Report Wizard lets us choose between a Tabular or Matrix type of report. Here are the details of these report types. Explanation The Select Report Type step in the Report Wizard shows the following dialog: A tabular report has page headings, column headings, and subtotals running down the page. A matrix report allows defining fields on columns and rows and provides interactive drilldown capabilities. We will create a tabular report as it is simple and familiar. Click Next to move to the Design the Table dialog. Design Table Step The Design Table step of the Report Wizard allows us to layout the available fields on our report, choosing between Page, Group, and Details. Explanation The Design Table step in the Report Wizard displays the following dialog: The Available fields list is populated based on the query you defined in the previous step. Click on a field, then click the appropriate button to place that field. Fill in the dialog as shown below: Button Descriptions: Page : Use this to start a new page when the field value changes, e.g., each ParentProductCategory on a different page. Group : Group the fields in this list. Details : Fields in this list appear in each row of the report. Click Next to move to the Choose Table Layout dialog. Choose Table Layout Step The Choose Table Layout step of the Report Wizard lets us choose a stepped or blocked layout and whether to include subtotals and enable drilldown. Explanation The Choose Table Layout step in the Report Wizard displays the following dialog: The default Stepped layout shows the groupings as above. Block layout saves space but disables drilldown. Including Subtotals provides intermediate totals based on groupings. Enabling drilldown initially hides details and allows expanding with a click on the plus icon. Fill in the dialog as shown below: Click Next to move to the Choose Table Style dialog. Choose Table Style Step The Choose Table Style step of the Report Wizard allows us to select from different styles. This is purely cosmetic, offering different color schemes. Explanation The Choose Table Style step in the Report Wizard displays the following dialog: Choose a style from the list and click Next to move to the Completing the Wizard dialog. Completing the Wizard Step The Completing the Wizard step of the Report Wizard summarizes your choices from the previous dialogs. Explanation The Completing the Wizard step in the Report Wizard displays the following dialog: Provide a descriptive name for your report in the Report Name textbox, e.g., ReportWizardExample. You can check the Preview report checkbox to see what your report will look like. Review your choices in the summary. If you need to change anything, click the Back button to revisit the previous dialogs. Click the Finish button to generate your report. Your report will appear in the Solution Explorer as shown below: The report will also be displayed in the Report Designer. Click the Preview tab to render your report. A portion of the report is shown below: We will make a few changes to the report. Click the Design tab; you will see the following: We'll add spaces in the heading, widen the columns, and format the sales numbers. Here are the steps: Add spaces in the heading: Click between the 't' and 'W', and between 'd' and 'E'. Widen the columns: Click in the ParentProductCategory cell. An Excel-like grid appears. Hover between the cells at the top, click, and drag to widen them. Format sales numbers: Click inside the [Sum(Sales)] column, locate Format in the Properties window, and type C0 to format the cell as currency with no decimals. Repeat for the [Sales] column. After making these changes, the report design should look like this: Click the Preview tab to display the report: Click the + icon to the left of the Parent Product Category Names to drill down to Product Category Name details. This completes our tutorial section on the Report Wizard. Creating a Report from Scratch Using Report Designer In the prior section, we created a report using the Report Wizard in Business Intelligence Development Studio (BIDS). In this section, we will create a report from scratch using the Report Designer in BIDS. With the Report Designer, you start with an empty canvas and define every aspect of the report yourself, allowing you to create sophisticated reports and dashboards. We will complete the following steps to build a simple report: Add a new report to our project Create a shared data source Create a Dataset Configure a Table The following screenshot shows the report we will build as rendered in the Report Manager: This report is based on the same query used in the earlier Report Wizard section. The plus sign icon to the left of the value in the Parent Product Category column allows us to drill down to the Product Category details. Now, let's start creating our report. Adding a New Report to Our Project The first step in creating a report is to add a new report to our project. Explanation In the earlier section on Projects and Solutions, we created a blank solution and added a Report Server project to the solution. In the previous section, we added a new report by stepping through the Report Wizard. The BIDS Solution Explorer shows our Reports project along with the Shared Data Source and ReportWizardExample created in the previous section: Right-click on the Reports node, then select Add > New Item, which will display the Add New Item - Reports dialog. Fill in the dialog as shown below: Click the Add button to add a new report to your project. Your new report will be displayed in the Report Designer. Let's review the Report Designer before we continue creating our report from scratch. There are three parts of the Report Designer: Design Surface : The palette where you lay out your report. Report Data : Allows you to define Data Sources, Datasets, Parameters, and Images. You can access built-in fields like Report Name and Page Number and drag and drop items onto the design surface. Toolbox : Contains the Report Items that you drag and drop onto the design surface, such as Table, Matrix, Rectangle, List, etc. When you add or open a report, the design surface will be displayed. After adding a report, you will see the following blank design surface: You can display the Report Data and Toolbox areas by selecting them from the top-level View menu if they aren't shown. I prefer to position them to the left of the designer. The Report Data area is shown below: In the screenshot above, Report Data and the Toolbox share the same area of the screen; click on the tab at the bottom to switch between them. The Toolbox contains the following elements that you will drag and drop onto the design surface: Note the push pin icon in the heading of Report Data and the Toolbox. Clicking this toggles between showing the tab and hiding it, putting a button you can hover over to display the tab. You can customize what you see in the report designer and position it however you like. Click on the Report Data or Toolbox heading and drag it around to position it. Now, let's continue to the next section and create a Shared Data Source. Creating a Shared Data Source We discussed the Shared Data Source in the earlier section on using the Report Wizard to create a new report. The Data Source contains the information that Reporting Services needs to retrieve the data for our report. A Shared Data Source can be used by any report in the same project. In this section, we will create a Shared Data Source. Explanation To create a Shared Data Source, click the New button in the Report Data area, then select Data Source from the menu as shown below: The Data Source Properties dialog will be displayed as shown below: First, provide a name; enter AdventureWorksLT in the Name textbox. Since we already defined a Shared Data Source in the earlier section on using the Report Wizard, click the Use shared data source reference radio button and select AdventureWorksLT from the dropdown list. The Data Source Properties dialog is shown below: At this point, we are done. If you need to create a new Shared Data Source, click the New button and complete the Shared Data Source Properties dialog. This is essentially the same as what we did in the Report Wizard section. We can now see our Shared Data Source in the Report Data area as shown below: We are now ready to continue to the next section and create a Data Set. Creating a Data Set A Data Set contains a query that Reporting Services uses to retrieve the data for our report. This query could be a SQL statement like we used in the Design the Query step of the Report Wizard section; it could also be a stored procedure that we execute. In this section, we will define a new Data Set using the same query from the Report Wizard section. Explanation To create a Data Set, right-click on the AdventureWorksLT Shared Data Source that we created in the previous section and select Add Dataset from the menu as shown below: The Dataset Properties dialog will be displayed as shown below: First, provide a name; enter Main in the Name textbox. Since we only have one Shared Data Source in our project, it will be selected automatically in the Data source dropdown. To define our query, you could click the Query Designer button and do it graphically or type in the query as we did in the Report Wizard section. Instead, click the Import button, which will initially display the familiar Open File dialog. Navigate to the report we created earlier in the Report Wizard section of the tutorial as shown below: Click OK to display the Import Query dialog as shown below: The above dialog displays the Datasets and their queries from the report. Our earlier report has only one Dataset, so just click the Import button. If the report had multiple Datasets, you could choose the Dataset from the list on the left. The Report Data area now shows our new Dataset and the list of available fields as shown below: We are now ready to continue to the next section to configure a Table for our report layout.","title":"SSRS"},{"location":"SQL/SSRS/#tbdoverview","text":"SQL Server Reporting Services 2008 (SSRS) is a tool in SQL Server 2008 for designing, developing, testing, and deploying reports. It works with the Business Intelligence Development Studio (BIDS) for all tasks related to report authoring and deployment. BIDS is included with SQL Server 2008. In this tutorial, we will cover the following topics to help you build a report successfully: Reporting Services Components Install Reporting Services Business Intelligence Development Studio (BIDS) Install Sample Database Create a Simple Report with the Wizard Create a Simple Report with the Report Designer Deploy Reports Configure Report Manager Security","title":"[TBD]Overview"},{"location":"SQL/SSRS/#reporting-services-components","text":"SSRS uses two main databases: ReportServer and ReportServerTempDB. ReportServer stores reports, data sources, snapshots, subscriptions, etc., while ReportServerTempDB is for temporary storage. These databases are created automatically during the SQL Server installation if you choose the default setup. You can also create them manually using the Reporting Services Configuration Manager.","title":"Reporting Services Components"},{"location":"SQL/SSRS/#reporting-services-windows-service","text":"SSRS functionality is implemented as a Windows service. Understanding this service isn't essential for using SSRS but can be useful. The service provides several functions, as shown in the diagram below: Key points about the Windows service: HTTP Listener : This is a new feature in SSRS 2008, removing the need for Internet Information Server (IIS). Report Manager : An ASP.NET application providing a browser-based interface for managing SSRS. Web Service : An ASP.NET application offering a programmatic interface to SSRS, used by Report Manager and can be used for custom implementations. Background Processing : Handles core services for SSRS.","title":"Reporting Services Windows Service"},{"location":"SQL/SSRS/#report-designer","text":"The Report Designer in BIDS allows you to design, develop, test, and deploy reports. BIDS, included with SQL Server 2008, has a user-friendly interface and new visualizations. Non-developers can use Report Builder, but it is not covered in this tutorial.","title":"Report Designer"},{"location":"SQL/SSRS/#installing-reporting-services","text":"To install SSRS, run the SQL Server 2008 SETUP.EXE and follow the steps, selecting Reporting Services in the Feature Selection dialog: Ensure to check all items under Shared Features, especially BIDS, as it is used for designing, developing, testing, and deploying reports. In the Reporting Services Configuration dialog, select \"Install the native mode default configuration\": This installs and configures SSRS automatically. After installation, you can start using SSRS immediately.","title":"Installing Reporting Services"},{"location":"SQL/SSRS/#using-bids","text":"BIDS helps in developing reports with an intuitive interface based on Microsoft's Visual Studio. To install BIDS, select the Business Intelligence Development Studio checkbox in the Shared Features section during the SQL Server installation. BIDS uses the concept of projects and solutions to organize reports. To create a new project: 1. Launch BIDS from the Microsoft SQL Server 2008 program group. 2. Click File > New Project. 3. Select Visual Studio Solutions under Project Types and Blank Solution under Visual Studio installed templates. Enter the name and location: Click OK to create a blank solution. Next, add a project to the solution: 1. Click File > Add > New Project. 2. Select Report Server Project and fill in the details: We are now ready to install the sample database to use in our reports.","title":"Using BIDS"},{"location":"SQL/SSRS/#installing-the-sample-database","text":"We will use the AdventureWorksLT sample database for our reports. This database is chosen because it is small and easy to set up. Download the AdventureWorksLT database from the Microsoft SQL Server Product Samples page on CodePlex. Run the installer (.msi file) and choose the folder for the database and log files. For a default SQL Server installation, use: C:\\Program Files\\Microsoft SQL Server\\MSSQL10.MSSQLSERVER\\MSSQL\\DATA After installation, attach the database to your SQL Server using SQL Server Management Studio: In the Attach Databases dialog, click Add and navigate to the AdventureWorksLT_Data.mdf file: Click OK to attach the database.","title":"Installing the Sample Database"},{"location":"SQL/SSRS/#creating-a-simple-report-with-the-wizard","text":"The Report Wizard in BIDS helps you create a simple report quickly. Open your solution in BIDS. Right-click the Reports node and select Add New Report to launch the Report Wizard: Follow these steps in the wizard: Create a Shared Data Source : Define the data source your report will use. Design a Query : Specify the SQL query to fetch the data for your report. Select a Report Type : Choose the layout type (e.g., tabular or matrix). Design a Table : Define the structure of the report. Choose the Table Layout : Select how the table should be displayed. Complete the Wizard : Finalize the report setup.","title":"Creating a Simple Report with the Wizard"},{"location":"SQL/SSRS/#next-steps","text":"Now that you've created a simple report using the wizard, you can further customize it using the Report Designer in BIDS. The Report Designer allows you to define every aspect of your report, giving you more control and flexibility. Once you're comfortable with the basics, you can start building more sophisticated reports and dashboards.","title":"Next Steps"},{"location":"SQL/SSRS/#creating-a-shared-data-source","text":"A Data Source contains the information needed to fetch the data for your report. SSRS can access data from relational databases, OLAP databases, and almost any data source with an ODBC or OLE DB driver. When creating a Data Source, you can specify it as shared, meaning it can be used by any report in the same project. It is generally a good idea to create Shared Data Sources. If a Data Source is not shared, its definition is stored inside the report and cannot be shared with other reports. In this section, we will go through the steps to create a Shared Data Source.","title":"Creating a Shared Data Source"},{"location":"SQL/SSRS/#explanation","text":"After launching the Report Wizard, you will see the Select the Data Source dialog as shown below: Since our project does not have any Shared Data Sources yet, we need to define a new Data Source. Here are the details you need to provide: Name : Choose a descriptive name for the Data Source, like AdventureWorksLT. Avoid spaces in the name to prevent errors. Type : Select the appropriate type from the dropdown list. The default value, Microsoft SQL Server, is correct for our AdventureWorksLT database. Connection String : Enter the connection string for your Data Source. It is usually easier to click the Edit button to enter the details and have the connection string created for you. Edit button : Click this to open the Connection Properties dialog where you can enter the server name and select the database. The connection string will be created for you. Credentials button : Click this to open the Data Source Credentials dialog where you can specify the credentials to use when connecting to the Data Source. Make this a shared data source checkbox : Check this box to create a Shared Data Source, so any report in the same project can use it. Click the Edit button to open the Connection Properties dialog. Enter your server name and select the AdventureWorksLT database as shown below: The Server name is where your SQL Server database is deployed. If you are running a named instance of SQL Server, specify it as SERVERNAME\\INSTANCENAME. If SQL Server is running locally, you can use localhost instead of SERVERNAME. Click the Test Connection button to verify the connection, then click OK to close the dialog. Next, click the Credentials button to open the Data Source Credentials dialog as shown below: The default selection, Use Windows Authentication (Integrated Security), is fine for our purposes. This means Reporting Services will connect to the Data Source using the Windows credentials of the person running the report. When you deploy the report and Data Source for others to use, you can select a different option if needed. For now, we'll stick with the default. After completing these steps, the Select the Data Source dialog will look like this: Click Next to proceed to the Design the Query dialog, which we will cover in the next section.","title":"Explanation"},{"location":"SQL/SSRS/#design-query-step","text":"The Design Query step of the Report Wizard allows us to specify what data we want to retrieve from our Data Source and display in our report. In this section, we will explain how to define a query to fetch the data for our report.","title":"Design Query Step"},{"location":"SQL/SSRS/#explanation_1","text":"The Design Query step in the Report Wizard displays the following dialog: You can click the Query Builder button to build your query graphically or type it directly into the Query string textbox. Here is an example query you can use: SELECT c.ParentProductCategoryName, c.ProductCategoryName, SUM(d.LineTotal) AS Sales FROM SalesLT.Product p JOIN SalesLT.vGetAllCategories c ON c.ProductCategoryID = p.ProductCategoryID JOIN SalesLT.SalesOrderDetail d ON d.ProductID = p.ProductID GROUP BY c.ParentProductCategoryName, c.ProductCategoryName ORDER BY c.ParentProductCategoryName, c.ProductCategoryName This query provides a sales summary broken down by product category. Copy and paste this query into the Query string textbox in the Design Query dialog. Alternatively, you can click the Query Builder button and design the query graphically. Click Next to move to the Select Report Type dialog.","title":"Explanation"},{"location":"SQL/SSRS/#select-report-type-step","text":"The Select Report Type step of the Report Wizard lets us choose between a Tabular or Matrix type of report. Here are the details of these report types.","title":"Select Report Type Step"},{"location":"SQL/SSRS/#explanation_2","text":"The Select Report Type step in the Report Wizard shows the following dialog: A tabular report has page headings, column headings, and subtotals running down the page. A matrix report allows defining fields on columns and rows and provides interactive drilldown capabilities. We will create a tabular report as it is simple and familiar. Click Next to move to the Design the Table dialog.","title":"Explanation"},{"location":"SQL/SSRS/#design-table-step","text":"The Design Table step of the Report Wizard allows us to layout the available fields on our report, choosing between Page, Group, and Details.","title":"Design Table Step"},{"location":"SQL/SSRS/#explanation_3","text":"The Design Table step in the Report Wizard displays the following dialog: The Available fields list is populated based on the query you defined in the previous step. Click on a field, then click the appropriate button to place that field. Fill in the dialog as shown below:","title":"Explanation"},{"location":"SQL/SSRS/#button-descriptions","text":"Page : Use this to start a new page when the field value changes, e.g., each ParentProductCategory on a different page. Group : Group the fields in this list. Details : Fields in this list appear in each row of the report. Click Next to move to the Choose Table Layout dialog.","title":"Button Descriptions:"},{"location":"SQL/SSRS/#choose-table-layout-step","text":"The Choose Table Layout step of the Report Wizard lets us choose a stepped or blocked layout and whether to include subtotals and enable drilldown.","title":"Choose Table Layout Step"},{"location":"SQL/SSRS/#explanation_4","text":"The Choose Table Layout step in the Report Wizard displays the following dialog: The default Stepped layout shows the groupings as above. Block layout saves space but disables drilldown. Including Subtotals provides intermediate totals based on groupings. Enabling drilldown initially hides details and allows expanding with a click on the plus icon. Fill in the dialog as shown below: Click Next to move to the Choose Table Style dialog.","title":"Explanation"},{"location":"SQL/SSRS/#choose-table-style-step","text":"The Choose Table Style step of the Report Wizard allows us to select from different styles. This is purely cosmetic, offering different color schemes.","title":"Choose Table Style Step"},{"location":"SQL/SSRS/#explanation_5","text":"The Choose Table Style step in the Report Wizard displays the following dialog: Choose a style from the list and click Next to move to the Completing the Wizard dialog.","title":"Explanation"},{"location":"SQL/SSRS/#completing-the-wizard-step","text":"The Completing the Wizard step of the Report Wizard summarizes your choices from the previous dialogs.","title":"Completing the Wizard Step"},{"location":"SQL/SSRS/#explanation_6","text":"The Completing the Wizard step in the Report Wizard displays the following dialog: Provide a descriptive name for your report in the Report Name textbox, e.g., ReportWizardExample. You can check the Preview report checkbox to see what your report will look like. Review your choices in the summary. If you need to change anything, click the Back button to revisit the previous dialogs. Click the Finish button to generate your report. Your report will appear in the Solution Explorer as shown below: The report will also be displayed in the Report Designer. Click the Preview tab to render your report. A portion of the report is shown below: We will make a few changes to the report. Click the Design tab; you will see the following: We'll add spaces in the heading, widen the columns, and format the sales numbers. Here are the steps: Add spaces in the heading: Click between the 't' and 'W', and between 'd' and 'E'. Widen the columns: Click in the ParentProductCategory cell. An Excel-like grid appears. Hover between the cells at the top, click, and drag to widen them. Format sales numbers: Click inside the [Sum(Sales)] column, locate Format in the Properties window, and type C0 to format the cell as currency with no decimals. Repeat for the [Sales] column. After making these changes, the report design should look like this: Click the Preview tab to display the report: Click the + icon to the left of the Parent Product Category Names to drill down to Product Category Name details. This completes our tutorial section on the Report Wizard.","title":"Explanation"},{"location":"SQL/SSRS/#creating-a-report-from-scratch-using-report-designer","text":"In the prior section, we created a report using the Report Wizard in Business Intelligence Development Studio (BIDS). In this section, we will create a report from scratch using the Report Designer in BIDS. With the Report Designer, you start with an empty canvas and define every aspect of the report yourself, allowing you to create sophisticated reports and dashboards. We will complete the following steps to build a simple report: Add a new report to our project Create a shared data source Create a Dataset Configure a Table The following screenshot shows the report we will build as rendered in the Report Manager: This report is based on the same query used in the earlier Report Wizard section. The plus sign icon to the left of the value in the Parent Product Category column allows us to drill down to the Product Category details. Now, let's start creating our report.","title":"Creating a Report from Scratch Using Report Designer"},{"location":"SQL/SSRS/#adding-a-new-report-to-our-project","text":"The first step in creating a report is to add a new report to our project.","title":"Adding a New Report to Our Project"},{"location":"SQL/SSRS/#explanation_7","text":"In the earlier section on Projects and Solutions, we created a blank solution and added a Report Server project to the solution. In the previous section, we added a new report by stepping through the Report Wizard. The BIDS Solution Explorer shows our Reports project along with the Shared Data Source and ReportWizardExample created in the previous section: Right-click on the Reports node, then select Add > New Item, which will display the Add New Item - Reports dialog. Fill in the dialog as shown below: Click the Add button to add a new report to your project. Your new report will be displayed in the Report Designer. Let's review the Report Designer before we continue creating our report from scratch. There are three parts of the Report Designer: Design Surface : The palette where you lay out your report. Report Data : Allows you to define Data Sources, Datasets, Parameters, and Images. You can access built-in fields like Report Name and Page Number and drag and drop items onto the design surface. Toolbox : Contains the Report Items that you drag and drop onto the design surface, such as Table, Matrix, Rectangle, List, etc. When you add or open a report, the design surface will be displayed. After adding a report, you will see the following blank design surface: You can display the Report Data and Toolbox areas by selecting them from the top-level View menu if they aren't shown. I prefer to position them to the left of the designer. The Report Data area is shown below: In the screenshot above, Report Data and the Toolbox share the same area of the screen; click on the tab at the bottom to switch between them. The Toolbox contains the following elements that you will drag and drop onto the design surface: Note the push pin icon in the heading of Report Data and the Toolbox. Clicking this toggles between showing the tab and hiding it, putting a button you can hover over to display the tab. You can customize what you see in the report designer and position it however you like. Click on the Report Data or Toolbox heading and drag it around to position it. Now, let's continue to the next section and create a Shared Data Source.","title":"Explanation"},{"location":"SQL/SSRS/#creating-a-shared-data-source_1","text":"We discussed the Shared Data Source in the earlier section on using the Report Wizard to create a new report. The Data Source contains the information that Reporting Services needs to retrieve the data for our report. A Shared Data Source can be used by any report in the same project. In this section, we will create a Shared Data Source.","title":"Creating a Shared Data Source"},{"location":"SQL/SSRS/#explanation_8","text":"To create a Shared Data Source, click the New button in the Report Data area, then select Data Source from the menu as shown below: The Data Source Properties dialog will be displayed as shown below: First, provide a name; enter AdventureWorksLT in the Name textbox. Since we already defined a Shared Data Source in the earlier section on using the Report Wizard, click the Use shared data source reference radio button and select AdventureWorksLT from the dropdown list. The Data Source Properties dialog is shown below: At this point, we are done. If you need to create a new Shared Data Source, click the New button and complete the Shared Data Source Properties dialog. This is essentially the same as what we did in the Report Wizard section. We can now see our Shared Data Source in the Report Data area as shown below: We are now ready to continue to the next section and create a Data Set.","title":"Explanation"},{"location":"SQL/SSRS/#creating-a-data-set","text":"A Data Set contains a query that Reporting Services uses to retrieve the data for our report. This query could be a SQL statement like we used in the Design the Query step of the Report Wizard section; it could also be a stored procedure that we execute. In this section, we will define a new Data Set using the same query from the Report Wizard section.","title":"Creating a Data Set"},{"location":"SQL/SSRS/#explanation_9","text":"To create a Data Set, right-click on the AdventureWorksLT Shared Data Source that we created in the previous section and select Add Dataset from the menu as shown below: The Dataset Properties dialog will be displayed as shown below: First, provide a name; enter Main in the Name textbox. Since we only have one Shared Data Source in our project, it will be selected automatically in the Data source dropdown. To define our query, you could click the Query Designer button and do it graphically or type in the query as we did in the Report Wizard section. Instead, click the Import button, which will initially display the familiar Open File dialog. Navigate to the report we created earlier in the Report Wizard section of the tutorial as shown below: Click OK to display the Import Query dialog as shown below: The above dialog displays the Datasets and their queries from the report. Our earlier report has only one Dataset, so just click the Import button. If the report had multiple Datasets, you could choose the Dataset from the list on the left. The Report Data area now shows our new Dataset and the list of available fields as shown below: We are now ready to continue to the next section to configure a Table for our report layout.","title":"Explanation"},{"location":"SQL/Windows_Functions/","text":"Beginner\u2019s Guide to Window Functions in SQL What if you could perform calculations on rows of data but still see every row? Meet Window Functions! Imagine you're running a marathon, and you\u2019re curious not just about who won, but also how you rank within your age group, your city, or among your friends. Window Functions are like the \"rank tracker\" for your SQL data\u2014helping you zoom in on specific slices of your data while keeping the big picture in view. Think of it this way: Aggregate functions (like SUM, AVG) summarize data by squashing rows together. Window Functions, on the other hand, let you see both the summary and the individual details. It\u2019s like having your cake and eating it too! What Are Window Functions? The Cheat Code for Advanced Data Analysis Window Functions are SQL\u2019s way of saying, \u201cHey, let\u2019s analyze data row-by-row, but also calculate cool stuff based on groups or sequences.\u201d The magic happens in the OVER clause\u2014that\u2019s where you define the \"window\" of rows you\u2019re analyzing. Here\u2019s the anatomy of a Window Function: function_name(column_name) OVER (PARTITION BY column_name ORDER BY column_name) Function Name : This is the action (e.g., SUM, AVG, ROW_NUMBER). OVER Clause : The heart of the operation\u2014it tells SQL how to group and order the data. PARTITION BY : Defines groups (e.g., all employees in the same department). ORDER BY : Specifies the sequence within each group. Meet the Cast: Types of Window Functions Let\u2019s dive into the superstar Window Functions\u2014each has its own talent for tackling data challenges. 1. Aggregate Functions with OVER Clause \"Summing Things Up Without Losing the Details\" Use these to calculate totals, averages, and more within defined windows. Perfect for seeing the forest and the trees. Scenario : You\u2019re managing a team and want to know each person\u2019s salary along with the total salary of their department. sql SELECT department, employee_name, salary, SUM(salary) OVER (PARTITION BY department) AS total_salary FROM employees; Output: | Department | Employee Name | Salary | Total Salary | |------------|---------------|--------|--------------| | Sales | Alice | 5000 | 15000 | | Sales | Bob | 7000 | 15000 | | Sales | Charlie | 3000 | 15000 | 2. Ranking Functions \"Who\u2019s on Top?\" When you need to assign ranks or positions to rows, these functions are your go-to tools. ROW_NUMBER() : The Uniquely Numbered MVP Scenario : Rank employees by salary within each department. sql SELECT department, employee_name, salary, ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) AS rank FROM employees; PROD Scenario : Here is a real-query used in a prod envirnment: This query uses the ROW_NUMBER() window function to deduplicate data based on the integration_key column. Here\u2019s how it works step by step: ##### Breakdown of ROW_NUMBER and RANKING Logic: Purpose of ROW_NUMBER() : It assigns a unique row number to each record within a partition defined by the PARTITION BY clause. The numbering starts at 1 for each partition, based on the ordering specified in the ORDER BY clause. PARTITION BY integration_key : Divides the data into partitions, where each partition corresponds to a unique integration_key . ORDER BY versionnumber DESC, w_business_dt DESC : Within each partition, the rows are sorted by: versionnumber in descending order (higher version numbers appear first). If there are ties in versionnumber , they are further sorted by w_business_dt in descending order (more recent dates appear first). Alias rn : The ROW_NUMBER() function assigns a row number to each row in the partition. The row with the highest priority (based on the ORDER BY conditions) gets rn = 1 . ##### Final Selection: WHERE data.rn = 1 : This filters out all rows except the one with rn = 1 , effectively keeping only the \"best\" row (highest versionnumber and latest date) for each integration_key . SELECT DISTINCT data.* : Ensures that there are no duplicate rows in the output. ##### What Happens in the Query: Deduplication : The CTE ( WITH deduplicated_data ) ensures each integration_key has exactly one row based on the ranking logic. Extraction : The main query fetches only the top-ranked rows ( rn = 1 ) for each integration_key . This approach is commonly used in SQL to handle deduplication when there\u2019s a clear ranking or prioritization logic. Let me know if you need additional examples or variations! RANK() : Sharing the Spotlight Assigns the same rank to rows with identical values, but skips the next rank. Scenario : Similar to ROW_NUMBER() but with ties. DENSE_RANK() : Closing the Gaps No gaps in ranks, even with ties. NTILE(n) : Divide and Conquer Divides rows into n equal parts\u2014great for quartiles or deciles. 3. Distribution Functions \"How Do You Stack Up?\" Use these to see how a value compares to others within a window. CUME_DIST() : Track Your Progress Scenario : See the cumulative distribution of salaries. sql SELECT employee_name, salary, CUME_DIST() OVER (ORDER BY salary ASC) AS cumulative_distribution FROM employees; PERCENT_RANK() : Relative Ranking Made Easy Similar to CUME_DIST() , but calculates the rank percentage. Playtime: Real-Life Scenarios 1. Pagination: Scrolling Through Results Like a Pro When you need to split results into pages, ROW_NUMBER() is your best friend. SELECT * FROM ( SELECT employee_name, ROW_NUMBER() OVER (ORDER BY employee_id) AS row_num FROM employees ) subquery WHERE row_num BETWEEN 11 AND 20; 2. Running Totals: Tracking Progress Step by Step Great for financial or time-series data. SELECT order_id, amount, SUM(amount) OVER (ORDER BY order_date) AS running_total FROM orders; Cheat Sheet: Quick Reference Table Function Superpower Handles Ties Gaps in Rank ROW_NUMBER() Unique numbering of rows No N/A RANK() Rank with ties, skips next Yes Yes DENSE_RANK() Rank with ties, no gaps Yes No NTILE(n) Split rows into n groups N/A N/A Your Turn: Practice Makes Perfect Here\u2019s a fun challenge to flex your SQL muscles. Given this table: Region Salesperson Sales East John 5000 East Jane 7000 West Jack 8000 West Jill 3000 Challenge: Rank salespeople by their sales within each region. Calculate the cumulative sales for each region.","title":"Window Functions"},{"location":"SQL/Windows_Functions/#beginners-guide-to-window-functions-in-sql","text":"","title":"Beginner\u2019s Guide to Window Functions in SQL"},{"location":"SQL/Windows_Functions/#what-if-you-could-perform-calculations-on-rows-of-data-but-still-see-every-row-meet-window-functions","text":"Imagine you're running a marathon, and you\u2019re curious not just about who won, but also how you rank within your age group, your city, or among your friends. Window Functions are like the \"rank tracker\" for your SQL data\u2014helping you zoom in on specific slices of your data while keeping the big picture in view. Think of it this way: Aggregate functions (like SUM, AVG) summarize data by squashing rows together. Window Functions, on the other hand, let you see both the summary and the individual details. It\u2019s like having your cake and eating it too!","title":"What if you could perform calculations on rows of data but still see every row? Meet Window Functions!"},{"location":"SQL/Windows_Functions/#what-are-window-functions","text":"","title":"What Are Window Functions?"},{"location":"SQL/Windows_Functions/#the-cheat-code-for-advanced-data-analysis","text":"Window Functions are SQL\u2019s way of saying, \u201cHey, let\u2019s analyze data row-by-row, but also calculate cool stuff based on groups or sequences.\u201d The magic happens in the OVER clause\u2014that\u2019s where you define the \"window\" of rows you\u2019re analyzing. Here\u2019s the anatomy of a Window Function: function_name(column_name) OVER (PARTITION BY column_name ORDER BY column_name) Function Name : This is the action (e.g., SUM, AVG, ROW_NUMBER). OVER Clause : The heart of the operation\u2014it tells SQL how to group and order the data. PARTITION BY : Defines groups (e.g., all employees in the same department). ORDER BY : Specifies the sequence within each group.","title":"The Cheat Code for Advanced Data Analysis"},{"location":"SQL/Windows_Functions/#meet-the-cast-types-of-window-functions","text":"Let\u2019s dive into the superstar Window Functions\u2014each has its own talent for tackling data challenges.","title":"Meet the Cast: Types of Window Functions"},{"location":"SQL/Windows_Functions/#1-aggregate-functions-with-over-clause","text":"","title":"1. Aggregate Functions with OVER Clause"},{"location":"SQL/Windows_Functions/#summing-things-up-without-losing-the-details","text":"Use these to calculate totals, averages, and more within defined windows. Perfect for seeing the forest and the trees. Scenario : You\u2019re managing a team and want to know each person\u2019s salary along with the total salary of their department. sql SELECT department, employee_name, salary, SUM(salary) OVER (PARTITION BY department) AS total_salary FROM employees; Output: | Department | Employee Name | Salary | Total Salary | |------------|---------------|--------|--------------| | Sales | Alice | 5000 | 15000 | | Sales | Bob | 7000 | 15000 | | Sales | Charlie | 3000 | 15000 |","title":"\"Summing Things Up Without Losing the Details\""},{"location":"SQL/Windows_Functions/#2-ranking-functions","text":"","title":"2. Ranking Functions"},{"location":"SQL/Windows_Functions/#whos-on-top","text":"When you need to assign ranks or positions to rows, these functions are your go-to tools.","title":"\"Who\u2019s on Top?\""},{"location":"SQL/Windows_Functions/#row_number-the-uniquely-numbered-mvp","text":"Scenario : Rank employees by salary within each department. sql SELECT department, employee_name, salary, ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) AS rank FROM employees; PROD Scenario : Here is a real-query used in a prod envirnment: This query uses the ROW_NUMBER() window function to deduplicate data based on the integration_key column. Here\u2019s how it works step by step: ##### Breakdown of ROW_NUMBER and RANKING Logic: Purpose of ROW_NUMBER() : It assigns a unique row number to each record within a partition defined by the PARTITION BY clause. The numbering starts at 1 for each partition, based on the ordering specified in the ORDER BY clause. PARTITION BY integration_key : Divides the data into partitions, where each partition corresponds to a unique integration_key . ORDER BY versionnumber DESC, w_business_dt DESC : Within each partition, the rows are sorted by: versionnumber in descending order (higher version numbers appear first). If there are ties in versionnumber , they are further sorted by w_business_dt in descending order (more recent dates appear first). Alias rn : The ROW_NUMBER() function assigns a row number to each row in the partition. The row with the highest priority (based on the ORDER BY conditions) gets rn = 1 . ##### Final Selection: WHERE data.rn = 1 : This filters out all rows except the one with rn = 1 , effectively keeping only the \"best\" row (highest versionnumber and latest date) for each integration_key . SELECT DISTINCT data.* : Ensures that there are no duplicate rows in the output. ##### What Happens in the Query: Deduplication : The CTE ( WITH deduplicated_data ) ensures each integration_key has exactly one row based on the ranking logic. Extraction : The main query fetches only the top-ranked rows ( rn = 1 ) for each integration_key . This approach is commonly used in SQL to handle deduplication when there\u2019s a clear ranking or prioritization logic. Let me know if you need additional examples or variations!","title":"ROW_NUMBER(): The Uniquely Numbered MVP"},{"location":"SQL/Windows_Functions/#rank-sharing-the-spotlight","text":"Assigns the same rank to rows with identical values, but skips the next rank. Scenario : Similar to ROW_NUMBER() but with ties.","title":"RANK(): Sharing the Spotlight"},{"location":"SQL/Windows_Functions/#dense_rank-closing-the-gaps","text":"No gaps in ranks, even with ties.","title":"DENSE_RANK(): Closing the Gaps"},{"location":"SQL/Windows_Functions/#ntilen-divide-and-conquer","text":"Divides rows into n equal parts\u2014great for quartiles or deciles.","title":"NTILE(n): Divide and Conquer"},{"location":"SQL/Windows_Functions/#3-distribution-functions","text":"","title":"3. Distribution Functions"},{"location":"SQL/Windows_Functions/#how-do-you-stack-up","text":"Use these to see how a value compares to others within a window.","title":"\"How Do You Stack Up?\""},{"location":"SQL/Windows_Functions/#cume_dist-track-your-progress","text":"Scenario : See the cumulative distribution of salaries. sql SELECT employee_name, salary, CUME_DIST() OVER (ORDER BY salary ASC) AS cumulative_distribution FROM employees;","title":"CUME_DIST(): Track Your Progress"},{"location":"SQL/Windows_Functions/#percent_rank-relative-ranking-made-easy","text":"Similar to CUME_DIST() , but calculates the rank percentage.","title":"PERCENT_RANK(): Relative Ranking Made Easy"},{"location":"SQL/Windows_Functions/#playtime-real-life-scenarios","text":"","title":"Playtime: Real-Life Scenarios"},{"location":"SQL/Windows_Functions/#1-pagination-scrolling-through-results-like-a-pro","text":"When you need to split results into pages, ROW_NUMBER() is your best friend. SELECT * FROM ( SELECT employee_name, ROW_NUMBER() OVER (ORDER BY employee_id) AS row_num FROM employees ) subquery WHERE row_num BETWEEN 11 AND 20;","title":"1. Pagination: Scrolling Through Results Like a Pro"},{"location":"SQL/Windows_Functions/#2-running-totals-tracking-progress-step-by-step","text":"Great for financial or time-series data. SELECT order_id, amount, SUM(amount) OVER (ORDER BY order_date) AS running_total FROM orders;","title":"2. Running Totals: Tracking Progress Step by Step"},{"location":"SQL/Windows_Functions/#cheat-sheet-quick-reference-table","text":"Function Superpower Handles Ties Gaps in Rank ROW_NUMBER() Unique numbering of rows No N/A RANK() Rank with ties, skips next Yes Yes DENSE_RANK() Rank with ties, no gaps Yes No NTILE(n) Split rows into n groups N/A N/A","title":"Cheat Sheet: Quick Reference Table"},{"location":"SQL/Windows_Functions/#your-turn-practice-makes-perfect","text":"Here\u2019s a fun challenge to flex your SQL muscles. Given this table: Region Salesperson Sales East John 5000 East Jane 7000 West Jack 8000 West Jill 3000","title":"Your Turn: Practice Makes Perfect"},{"location":"SQL/Windows_Functions/#challenge","text":"Rank salespeople by their sales within each region. Calculate the cumulative sales for each region.","title":"Challenge:"},{"location":"SQL/connecting-with-dbt/","text":"Connect Local dbt with MSSQL Server This guide will take you through the steps to create a project with dbt and connect it to a Microsoft SQL Server (MSSQL) database. Steps to follow Create .dbt Folder : Create a .dbt folder in your user directory, e.g., C:\\Users\\dwaip\\.dbt . Add profiles.yml : Inside the .dbt folder, place a profiles.yml file with the following content: ```yml # Das: This profiles.yml configuration is tested for: # - SQL Server 2022 build 16.0.1121.4 # - Python 3.12.3 # - Registered adapter: sqlserver=1.7.4 # - dbt version: 1.7.18 # - Authentication: Windows Login # Profile name should match what's in your dbt_project.yml hello_mssql: # 'target' specifies the environment (e.g., dev, prod) target: dev # 'outputs' define configurations for environments outputs: # Configuration for the 'dev' environment dev: # Database type type: sqlserver # ODBC driver for SQL Server driver: 'ODBC Driver 17 for SQL Server' # SQL Server name or IP server: 'MOMO' # Default port for SQL Server port: 1433 database: 'InsuranceDB' schema: 'dbo' # Use Windows login credentials trusted_connection: true # Enable encryption for data transmission encrypt: true # Trust the server's SSL certificate (useful for self-signed certificates) trust_cert: true ``` Create Project Folder : Create a new folder on your laptop, e.g., dbt_projects , and navigate (CD) into it. Set Up Virtual Environment : Inside your project folder, create a virtual environment by running python -m venv dbt_venv . Activate Virtual Environment : Activate the virtual environment by running .\\Scripts\\activate . The activate.bat or activate.ps1 file is inside the Scripts folder. Install dbt and Adapter : Install dbt core and the SQL Server adapter by running pip install dbt-core dbt-sqlserver . Replace dbt-sqlserver with the appropriate adapter name if needed. Initialize dbt Project : Initialize your dbt project by running dbt init [project_name] , e.g., dbt init hello_mssql . This will create a folder named hello_mssql . Ensure the project name matches the one in your profiles.yml . Run dbt Debug : Navigate (CD) into the hello_mssql folder and run dbt debug . Ensure you run dbt debug from inside the project folder. Connect Local dbt with Databricks Set Up .dbt Folder : Create a .dbt folder in your user directory, e.g., C:\\Users\\dwaip\\.dbt . Add profiles.yml : Inside the .dbt folder, place a profiles.yml file with the following content: yaml databricks: outputs: dev: type: databricks server_host: [databricks_host_url] http_path: [http_path] token: [access_token] schema: [schema_name] catalog: [catalog_name] # optional warehouse: [warehouse_name] # optional target: dev Replace [databricks_host_url] , [http_path] , [access_token] , [schema_name] , [catalog_name] , and [warehouse_name] with your specific details. Create Project Folder : Create a new folder on your laptop, e.g., dbt_projects , and navigate (CD) into it. Set Up Virtual Environment : Inside your project folder, create a virtual environment by running python -m venv dbt_venv . Activate Virtual Environment : Activate the virtual environment by running .\\Scripts\\activate . The activate.bat or activate.ps1 file is inside the Scripts folder. Install dbt and Adapter : Install dbt core and the Databricks adapter by running pip install dbt-core dbt-databricks . Initialize dbt Project : Initialize your dbt project by running dbt init [project_name] , e.g., dbt init hello_databricks . This will create a folder named hello_databricks . Ensure the project name matches the one in your profiles.yml . Run dbt Debug : Navigate (CD) into the hello_databricks folder and run dbt debug . Ensure you run dbt debug from inside the project folder. Errors 1. Profiles.yml File is Invalid Error: When running dbt debug , an error appeared saying the profiles.yml file was invalid, with the message: 'yes' is not valid under any of the given schemas . Resolution: Change the value of trusted_connection from 'yes' to true (without quotes) in the profiles.yml file. This ensures dbt correctly recognizes the trusted connection setting. 2. dbt_project.yml File Not Found Error: The dbt debug command failed because the dbt_project.yml file was not found in the project directory. Resolution: Create a dbt_project.yml file in the project directory and ensure it correctly references the profile name used in the profiles.yml file. 3. Could Not Find Profile Named 'hello_mssql' Error: An error occurred because dbt couldn't find a profile named hello_mssql , even though the profile was set up in the profiles.yml . Resolution: Ensure that the profile in dbt_project.yml matches the profile name in profiles.yml . For example, if the profile name is mssql_server in profiles.yml , make sure dbt_project.yml references mssql_server . 4. SSL Certificate Not Trusted Error: The connection to SQL Server failed with the error: The certificate chain was issued by an authority that is not trusted . Resolution: To resolve this, add trust_cert: true to your profiles.yml file. This will bypass the SSL certificate validation error and allow dbt to connect to SQL Server. Connect Local dbt with spark Further reading https://github.com/dbt-labs/dbt-spark","title":"Connect Local dbt with MSSQL Server"},{"location":"SQL/connecting-with-dbt/#connect-local-dbt-with-mssql-server","text":"This guide will take you through the steps to create a project with dbt and connect it to a Microsoft SQL Server (MSSQL) database.","title":"Connect Local dbt with MSSQL Server"},{"location":"SQL/connecting-with-dbt/#steps-to-follow","text":"Create .dbt Folder : Create a .dbt folder in your user directory, e.g., C:\\Users\\dwaip\\.dbt . Add profiles.yml : Inside the .dbt folder, place a profiles.yml file with the following content: ```yml # Das: This profiles.yml configuration is tested for: # - SQL Server 2022 build 16.0.1121.4 # - Python 3.12.3 # - Registered adapter: sqlserver=1.7.4 # - dbt version: 1.7.18 # - Authentication: Windows Login # Profile name should match what's in your dbt_project.yml hello_mssql: # 'target' specifies the environment (e.g., dev, prod) target: dev # 'outputs' define configurations for environments outputs: # Configuration for the 'dev' environment dev: # Database type type: sqlserver # ODBC driver for SQL Server driver: 'ODBC Driver 17 for SQL Server' # SQL Server name or IP server: 'MOMO' # Default port for SQL Server port: 1433 database: 'InsuranceDB' schema: 'dbo' # Use Windows login credentials trusted_connection: true # Enable encryption for data transmission encrypt: true # Trust the server's SSL certificate (useful for self-signed certificates) trust_cert: true ``` Create Project Folder : Create a new folder on your laptop, e.g., dbt_projects , and navigate (CD) into it. Set Up Virtual Environment : Inside your project folder, create a virtual environment by running python -m venv dbt_venv . Activate Virtual Environment : Activate the virtual environment by running .\\Scripts\\activate . The activate.bat or activate.ps1 file is inside the Scripts folder. Install dbt and Adapter : Install dbt core and the SQL Server adapter by running pip install dbt-core dbt-sqlserver . Replace dbt-sqlserver with the appropriate adapter name if needed. Initialize dbt Project : Initialize your dbt project by running dbt init [project_name] , e.g., dbt init hello_mssql . This will create a folder named hello_mssql . Ensure the project name matches the one in your profiles.yml . Run dbt Debug : Navigate (CD) into the hello_mssql folder and run dbt debug . Ensure you run dbt debug from inside the project folder.","title":"Steps to follow"},{"location":"SQL/connecting-with-dbt/#connect-local-dbt-with-databricks","text":"Set Up .dbt Folder : Create a .dbt folder in your user directory, e.g., C:\\Users\\dwaip\\.dbt . Add profiles.yml : Inside the .dbt folder, place a profiles.yml file with the following content: yaml databricks: outputs: dev: type: databricks server_host: [databricks_host_url] http_path: [http_path] token: [access_token] schema: [schema_name] catalog: [catalog_name] # optional warehouse: [warehouse_name] # optional target: dev Replace [databricks_host_url] , [http_path] , [access_token] , [schema_name] , [catalog_name] , and [warehouse_name] with your specific details. Create Project Folder : Create a new folder on your laptop, e.g., dbt_projects , and navigate (CD) into it. Set Up Virtual Environment : Inside your project folder, create a virtual environment by running python -m venv dbt_venv . Activate Virtual Environment : Activate the virtual environment by running .\\Scripts\\activate . The activate.bat or activate.ps1 file is inside the Scripts folder. Install dbt and Adapter : Install dbt core and the Databricks adapter by running pip install dbt-core dbt-databricks . Initialize dbt Project : Initialize your dbt project by running dbt init [project_name] , e.g., dbt init hello_databricks . This will create a folder named hello_databricks . Ensure the project name matches the one in your profiles.yml . Run dbt Debug : Navigate (CD) into the hello_databricks folder and run dbt debug . Ensure you run dbt debug from inside the project folder.","title":"Connect Local dbt with Databricks"},{"location":"SQL/connecting-with-dbt/#errors","text":"","title":"Errors"},{"location":"SQL/connecting-with-dbt/#1-profilesyml-file-is-invalid","text":"","title":"1. Profiles.yml File is Invalid"},{"location":"SQL/connecting-with-dbt/#error","text":"When running dbt debug , an error appeared saying the profiles.yml file was invalid, with the message: 'yes' is not valid under any of the given schemas .","title":"Error:"},{"location":"SQL/connecting-with-dbt/#resolution","text":"Change the value of trusted_connection from 'yes' to true (without quotes) in the profiles.yml file. This ensures dbt correctly recognizes the trusted connection setting.","title":"Resolution:"},{"location":"SQL/connecting-with-dbt/#2-dbt_projectyml-file-not-found","text":"","title":"2. dbt_project.yml File Not Found"},{"location":"SQL/connecting-with-dbt/#error_1","text":"The dbt debug command failed because the dbt_project.yml file was not found in the project directory.","title":"Error:"},{"location":"SQL/connecting-with-dbt/#resolution_1","text":"Create a dbt_project.yml file in the project directory and ensure it correctly references the profile name used in the profiles.yml file.","title":"Resolution:"},{"location":"SQL/connecting-with-dbt/#3-could-not-find-profile-named-hello_mssql","text":"","title":"3. Could Not Find Profile Named 'hello_mssql'"},{"location":"SQL/connecting-with-dbt/#error_2","text":"An error occurred because dbt couldn't find a profile named hello_mssql , even though the profile was set up in the profiles.yml .","title":"Error:"},{"location":"SQL/connecting-with-dbt/#resolution_2","text":"Ensure that the profile in dbt_project.yml matches the profile name in profiles.yml . For example, if the profile name is mssql_server in profiles.yml , make sure dbt_project.yml references mssql_server .","title":"Resolution:"},{"location":"SQL/connecting-with-dbt/#4-ssl-certificate-not-trusted","text":"","title":"4. SSL Certificate Not Trusted"},{"location":"SQL/connecting-with-dbt/#error_3","text":"The connection to SQL Server failed with the error: The certificate chain was issued by an authority that is not trusted .","title":"Error:"},{"location":"SQL/connecting-with-dbt/#resolution_3","text":"To resolve this, add trust_cert: true to your profiles.yml file. This will bypass the SSL certificate validation error and allow dbt to connect to SQL Server.","title":"Resolution:"},{"location":"SQL/connecting-with-dbt/#connect-local-dbt-with-spark","text":"Further reading https://github.com/dbt-labs/dbt-spark","title":"Connect Local dbt with spark"},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Overview What is Spark? Its mainly a processing engine more like Mapreduce v2. It has no internal storage sytem like HDFS, but uses external file system lie ADLS, HDFS, S3 etc. Its processing engine is called Spark Core. It has standalone resource manager by default to manage its clusters. Spark Architecture Spark Lifecycle What is Apache Hive? Apache Hive is a database like MSSQL Server. Its actually a data warehouse. It stores data in Hadoop File system(HDFS) as tables. Hive's query language is called HiveQL , similar to SQL. he Hive metastore stores information about Hive tables. Developed by Facebook, Hive later became an Apache project. What is Hadoop? Hadoop is a distributed file system and processing framework similar to an MSSQL cluster but designed for handling large-scale file systems and big data. Here are its main components: Hadoop Distributed File System (HDFS) : The file system for big data, akin to NTFS or FAT in traditional systems. MapReduce : The older generation of big data processing, similar to Spark. YARN (Yet Another Resource Negotiator) : The cluster manager. Hadoop Common : The set of shared libraries and utilities. Is Spark replacing MapReduce? Yes, Spark is like the next version of MapReduce. Is branded Spark (Synapse/Databricks) replacing traditional Spark, Hadoop, and Hive? Imagine your company is new to data engineering and needs to process a lot of data. How long will it take to set up with Azure compared to using free open-source products on bare metal? With Azure, you just sign up and create the setup with a few clicks. If the company has the budget, the entire setup takes about an hour. On the other hand, using traditional Spark, Hive, and Hadoop, setting up servers, networks, installation, configuration, and connectivity can become a year-long project. That\u2019s the difference between open-source and paid services. Open-source is free but risky. Paid services cost money but are easy, fast, accountable, and well-maintained. Never heard of Hive. We use only Spark and Synapse Analytics. If you always use branded products like Synapse and Databricks, you might not use Hive much. However, Hive catalogs are still used in Databricks. Just run a command like DESCRIBE EXTENDED TableName , and you'll see where Hive is involved. But you don't have to worry about the setup. Spark RDDs Spark In-memory computing When we talk about in-memory its not just conventioal caching. Its actually storing data in RAM, processing in RAM etc. So, spark uses it and thats why its faster than mapreduce.","title":"Spark"},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts/#overview","text":"","title":"Overview"},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts/#what-is-spark","text":"Its mainly a processing engine more like Mapreduce v2. It has no internal storage sytem like HDFS, but uses external file system lie ADLS, HDFS, S3 etc. Its processing engine is called Spark Core. It has standalone resource manager by default to manage its clusters.","title":"What is Spark?"},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts/#spark-architecture","text":"","title":"Spark Architecture"},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts/#spark-lifecycle","text":"","title":"Spark Lifecycle"},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts/#what-is-apache-hive","text":"Apache Hive is a database like MSSQL Server. Its actually a data warehouse. It stores data in Hadoop File system(HDFS) as tables. Hive's query language is called HiveQL , similar to SQL. he Hive metastore stores information about Hive tables. Developed by Facebook, Hive later became an Apache project.","title":"What is Apache Hive?"},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts/#what-is-hadoop","text":"Hadoop is a distributed file system and processing framework similar to an MSSQL cluster but designed for handling large-scale file systems and big data. Here are its main components: Hadoop Distributed File System (HDFS) : The file system for big data, akin to NTFS or FAT in traditional systems. MapReduce : The older generation of big data processing, similar to Spark. YARN (Yet Another Resource Negotiator) : The cluster manager. Hadoop Common : The set of shared libraries and utilities.","title":"What is Hadoop?"},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts/#is-spark-replacing-mapreduce","text":"Yes, Spark is like the next version of MapReduce.","title":"Is Spark replacing MapReduce?"},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts/#is-branded-spark-synapsedatabricks-replacing-traditional-spark-hadoop-and-hive","text":"Imagine your company is new to data engineering and needs to process a lot of data. How long will it take to set up with Azure compared to using free open-source products on bare metal? With Azure, you just sign up and create the setup with a few clicks. If the company has the budget, the entire setup takes about an hour. On the other hand, using traditional Spark, Hive, and Hadoop, setting up servers, networks, installation, configuration, and connectivity can become a year-long project. That\u2019s the difference between open-source and paid services. Open-source is free but risky. Paid services cost money but are easy, fast, accountable, and well-maintained.","title":"Is branded Spark (Synapse/Databricks) replacing traditional Spark, Hadoop, and Hive?"},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts/#never-heard-of-hive-we-use-only-spark-and-synapse-analytics","text":"If you always use branded products like Synapse and Databricks, you might not use Hive much. However, Hive catalogs are still used in Databricks. Just run a command like DESCRIBE EXTENDED TableName , and you'll see where Hive is involved. But you don't have to worry about the setup.","title":"Never heard of Hive. We use only Spark and Synapse Analytics."},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts/#spark-rdds","text":"","title":"Spark RDDs"},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts/#spark-in-memory-computing","text":"When we talk about in-memory its not just conventioal caching. Its actually storing data in RAM, processing in RAM etc. So, spark uses it and thats why its faster than mapreduce.","title":"Spark In-memory computing"},{"location":"Spark-DataBricks/1.0_Spark/1.10_Scala_Cheatsheet/","text":"Scala Cheatsheet for Spark Category Operation Code Snippet Basic Operations Variable Declaration val x: Int = 10 // Immutable var y: Int = 20 // Mutable Collections val list = List(1, 2, 3, 4, 5) val array = Array(1, 2, 3, 4, 5) val map = Map(\"a\" -> 1, \"b\" -> 2, \"c\" -> 3) Spark Setup Initialize Spark Session import org.apache.spark.sql.SparkSession val spark = SparkSession.builder.appName(\"Spark App\").config(\"spark.master\", \"local\").getOrCreate() RDD Operations Create RDD val rdd = spark.sparkContext.parallelize(Seq(1, 2, 3, 4, 5)) Transformations val mappedRDD = rdd.map(_ * 2) val filteredRDD = rdd.filter(_ > 2) Actions val collected = rdd.collect() val count = rdd.count() val firstElement = rdd.first() DataFrame Operations Create DataFrame import spark.implicits._ val df = Seq((1, \"a\"), (2, \"b\"), (3, \"c\")).toDF(\"id\", \"value\") Show DataFrame df.show() DataFrame Transformations val filteredDF = df.filter($\"id\" > 1) val selectedDF = df.select(\"value\") val withColumnDF = df.withColumn(\"new_column\", $\"id\" * 2) SQL Queries df.createOrReplaceTempView(\"table\") val sqlDF = spark.sql(\"SELECT * FROM table WHERE id > 1\") Dataset Operations Create Dataset case class Record(id: Int, value: String) val ds = Seq(Record(1, \"a\"), Record(2, \"b\"), Record(3, \"c\")).toDS() Dataset Transformations val filteredDS = ds.filter(_.id > 1) val mappedDS = ds.map(record => record.copy(value = record.value.toUpperCase)) Conversions RDD to DataFrame val rddToDF = rdd.toDF(\"numbers\") DataFrame to RDD val dfToRDD = df.rdd DataFrame to Dataset val dfToDS = df.as[Record] Dataset to DataFrame val dsToDF = ds.toDF() Reading and Writing Data Read CSV val csvDF = spark.read.option(\"header\", \"true\").csv(\"path/to/file.csv\") Write CSV df.write.option(\"header\", \"true\").csv(\"path/to/save\") Read Parquet val parquetDF = spark.read.parquet(\"path/to/file.parquet\") Write Parquet df.write.parquet(\"path/to/save\") Common Data Engineering Functions GroupBy and Aggregations val groupedDF = df.groupBy(\"id\").count() val aggregatedDF = df.groupBy(\"id\").agg(sum(\"value\")) Join Operations val df1 = Seq((1, \"a\"), (2, \"b\")).toDF(\"id\", \"value1\") val df2 = Seq((1, \"x\"), (2, \"y\")).toDF(\"id\", \"value2\") val joinedDF = df1.join(df2, \"id\") Window Functions import org.apache.spark.sql.expressions.Window import org.apache.spark.sql.functions._ val windowSpec = Window.partitionBy(\"id\").orderBy(\"value\") val windowedDF = df.withColumn(\"rank\", rank().over(windowSpec)) UDFs (User-Defined Functions) import org.apache.spark.sql.functions.udf val addOne = udf((x: Int) => x + 1) val dfWithUDF = df.withColumn(\"new_value\", addOne($\"id\"))","title":"Scala Cheatsheet"},{"location":"Spark-DataBricks/1.0_Spark/1.10_Scala_Cheatsheet/#scala-cheatsheet-for-spark","text":"Category Operation Code Snippet Basic Operations Variable Declaration val x: Int = 10 // Immutable var y: Int = 20 // Mutable Collections val list = List(1, 2, 3, 4, 5) val array = Array(1, 2, 3, 4, 5) val map = Map(\"a\" -> 1, \"b\" -> 2, \"c\" -> 3) Spark Setup Initialize Spark Session import org.apache.spark.sql.SparkSession val spark = SparkSession.builder.appName(\"Spark App\").config(\"spark.master\", \"local\").getOrCreate() RDD Operations Create RDD val rdd = spark.sparkContext.parallelize(Seq(1, 2, 3, 4, 5)) Transformations val mappedRDD = rdd.map(_ * 2) val filteredRDD = rdd.filter(_ > 2) Actions val collected = rdd.collect() val count = rdd.count() val firstElement = rdd.first() DataFrame Operations Create DataFrame import spark.implicits._ val df = Seq((1, \"a\"), (2, \"b\"), (3, \"c\")).toDF(\"id\", \"value\") Show DataFrame df.show() DataFrame Transformations val filteredDF = df.filter($\"id\" > 1) val selectedDF = df.select(\"value\") val withColumnDF = df.withColumn(\"new_column\", $\"id\" * 2) SQL Queries df.createOrReplaceTempView(\"table\") val sqlDF = spark.sql(\"SELECT * FROM table WHERE id > 1\") Dataset Operations Create Dataset case class Record(id: Int, value: String) val ds = Seq(Record(1, \"a\"), Record(2, \"b\"), Record(3, \"c\")).toDS() Dataset Transformations val filteredDS = ds.filter(_.id > 1) val mappedDS = ds.map(record => record.copy(value = record.value.toUpperCase)) Conversions RDD to DataFrame val rddToDF = rdd.toDF(\"numbers\") DataFrame to RDD val dfToRDD = df.rdd DataFrame to Dataset val dfToDS = df.as[Record] Dataset to DataFrame val dsToDF = ds.toDF() Reading and Writing Data Read CSV val csvDF = spark.read.option(\"header\", \"true\").csv(\"path/to/file.csv\") Write CSV df.write.option(\"header\", \"true\").csv(\"path/to/save\") Read Parquet val parquetDF = spark.read.parquet(\"path/to/file.parquet\") Write Parquet df.write.parquet(\"path/to/save\") Common Data Engineering Functions GroupBy and Aggregations val groupedDF = df.groupBy(\"id\").count() val aggregatedDF = df.groupBy(\"id\").agg(sum(\"value\")) Join Operations val df1 = Seq((1, \"a\"), (2, \"b\")).toDF(\"id\", \"value1\") val df2 = Seq((1, \"x\"), (2, \"y\")).toDF(\"id\", \"value2\") val joinedDF = df1.join(df2, \"id\") Window Functions import org.apache.spark.sql.expressions.Window import org.apache.spark.sql.functions._ val windowSpec = Window.partitionBy(\"id\").orderBy(\"value\") val windowedDF = df.withColumn(\"rank\", rank().over(windowSpec)) UDFs (User-Defined Functions) import org.apache.spark.sql.functions.udf val addOne = udf((x: Int) => x + 1) val dfWithUDF = df.withColumn(\"new_value\", addOne($\"id\"))","title":"Scala Cheatsheet for Spark"},{"location":"Spark-DataBricks/1.0_Spark/1.11_Spark_Interview_Questions/","text":"Common Spark Concepts which may be asked in an interview Types of join strateries Types of Joins Common Spark optimization techniques Different phases of Spark-SQL engine Common reasons for analysis exception in Spark Flow of how Spark works internally Explain DAG in Spark Explain spark.sql.shuffle.partitions Variable Common Spark Concepts which may be asked in an interview In this article I will try to put some common Spark concepts in a tabular format(So that its compact). These are good concepts to remember. Also, they may be asked in Interview questions. Types of join strateries Join Strategy Description Use Case Example Shuffle Hash Join Both DataFrames are shuffled based on the join keys, and then a hash join is performed. Useful when both DataFrames are large and have a good distribution of keys. spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"false\") df1.join(df2, \"key\") Broadcast Hash Join One of the DataFrames is small enough to fit in memory and is broadcasted to all worker nodes. A hash join is then performed. Efficient when one DataFrame is much smaller than the other. val broadcastDF = broadcast(df2) df1.join(broadcastDF, \"key\") Sort-Merge Join Both DataFrames are sorted on the join keys and then merged. This requires a shuffle if the data is not already sorted. Suitable for large DataFrames when the join keys are sorted or can be sorted efficiently. spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"true\") df1.join(df2, \"key\") Cartesian Join (Cross Join) Every row of one DataFrame is paired with every row of the other DataFrame. Generally not recommended due to its computational expense, but can be used for generating combinations of all rows. df1.crossJoin(df2) Broadcast Nested Loop Join The smaller DataFrame is broadcasted, and a nested loop join is performed. Used when there are no join keys or the join condition is complex and cannot be optimized with hash or sort-merge joins. val broadcastDF = broadcast(df2) df1.join(broadcastDF) Shuffle-and-Replicate Nested Loop Join Both DataFrames are shuffled and replicated to perform a nested loop join. Used for complex join conditions that cannot be handled by other join strategies. df1.join(df2, expr(\"complex_condition\")) Types of Joins Join Type Description Example Inner Join Returns rows that have matching values in both DataFrames. df1.join(df2, \"key\") Outer Join Returns all rows when there is a match in either DataFrame. Missing values are filled with nulls. df1.join(df2, Seq(\"key\"), \"outer\") Left Outer Join Returns all rows from the left DataFrame, and matched rows from the right DataFrame. df1.join(df2, Seq(\"key\"), \"left_outer\") Right Outer Join Returns all rows from the right DataFrame, and matched rows from the left DataFrame. df1.join(df2, Seq(\"key\"), \"right_outer\") Left Semi Join Returns only the rows from the left DataFrame that have a match in the right DataFrame. df1.join(df2, Seq(\"key\"), \"left_semi\") Left Anti Join Returns only the rows from the left DataFrame that do not have a match in the right DataFrame. df1.join(df2, Seq(\"key\"), \"left_anti\") Cross Join Returns the Cartesian product of both DataFrames. Every row in the left DataFrame will be combined with every row in the right DataFrame. df1.crossJoin(df2) Self Join A join in which a DataFrame is joined with itself. This can be an inner, outer, left, or right join. df.join(df, df(\"key1\") === df(\"key2\")) Common Spark optimization techniques Technique What it is When to use Example Caching and Persistence Storing data in memory for quick access When you need to reuse the same data multiple times scala val cachedData = df.cache() Broadcast Variables Sending a small dataset to all worker nodes When one dataset is much smaller than the other scala val broadcastData = spark.broadcast(smallDF) largeDF.join(broadcastData.value, \"key\") Partitioning Dividing data into smaller, manageable chunks When dealing with large datasets to improve parallel processing scala val partitionedData = df.repartition(10, $\"key\") Avoiding Shuffles Reducing the movement of data between nodes To improve performance by minimizing network overhead Use mapPartitions instead of groupBy when possible Coalesce Reducing the number of partitions When the data has become sparse after a transformation scala val coalescedData = df.coalesce(1) Predicate Pushdown Filtering data as early as possible in the processing To reduce the amount of data read and processed scala val filteredData = df.filter($\"column\" > 10) Using the Right Join Strategy Choosing the most efficient way to join two datasets Based on the size and distribution of data Prefer broadcast joins for small datasets Tuning Spark Configurations Adjusting settings to optimize resource usage To match the workload and cluster resources scala spark.conf.set(\"spark.executor.memory\", \"4g\") Using DataFrames/Datasets API Leveraging the high-level APIs for optimizations To benefit from Catalyst optimizer and Tungsten execution engine scala val df = spark.read.csv(\"data.csv\") df.groupBy(\"column\").count() Vectorized Query Execution Processing multiple rows of data at a time For high-performance operations on large datasets Use built-in SQL functions and DataFrame methods Different phases of Spark-SQL engine Phase Description Details Example Parsing Converting SQL queries into a logical plan. The SQL query is parsed into an abstract syntax tree (AST). Converting SELECT * FROM table WHERE id = 1 into an internal format. Analysis Resolving references and verifying the logical plan. Resolves column names, table names, and function names; checks for errors. Ensuring that the table table and the column id exist in the database. Optimization Improving the logical plan for better performance. Transforms the logical plan using various optimization techniques; applies rules via Catalyst optimizer. Reordering filters to reduce the amount of data processed early on. Physical Planning Converting the logical plan into a physical plan. Converts the optimized logical plan into one or more physical plans; selects the most efficient plan. Deciding whether to use a hash join or a sort-merge join. Code Generation Generating executable code from the physical plan. Generates Java bytecode to execute the physical plan; this code runs on Spark executors. Creating code to perform join operations, filter data, and compute results. Execution Running the generated code on the Spark cluster. Distributes generated code across the Spark cluster; executed by Spark executors; results collected and returned. Running join and filter operations on different nodes in the cluster and aggregating results. Common reasons for analysis exception in Spark Here are some common reasons why you might encounter an AnalysisException in Spark: Reason Description Example Non-Existent Column or Table Column or table specified does not exist. Referring to a non-existent column id . Ambiguous Column Reference Same column name exists in multiple tables without qualification. Joining two DataFrames with the same column name id . Invalid SQL Syntax SQL query has syntax errors. Using incorrect SQL syntax like SELCT . Unsupported Operations Using an operation that Spark SQL does not support. Using an unsupported function. Schema Mismatch Schema of the DataFrame does not match the expected schema. Inserting data with different column types. Missing File or Directory Specified file or directory does not exist. Referring to a non-existent CSV file. Incorrect Data Type Operations expecting a specific data type are given the wrong type. Performing a math operation on a string column. Flow of how Spark works internally Component/Step Role/Process Function/Example Driver Program Entry point for the Spark application - Manages application lifecycle - Defines RDD transformations and actions SparkContext Acts as the master of the Spark application - Connects to cluster manager - Coordinates tasks Cluster Manager Manages the cluster of machines - Allocates resources to Spark applications - Examples: YARN, Mesos, standalone Executors Worker nodes that run tasks and store data - Execute assigned code - Return results to the driver - Cache data in memory for quick access Spark Application Submission Submitting the driver program to the cluster manager - Example: Submitting a job using spark-submit SparkContext Initialization Driver program initializes SparkContext - Example: val sc = new SparkContext(conf) Job Scheduling Driver program defines transformations and actions on RDDs/DataFrames - Example: val rdd = sc.textFile(\"data.txt\").map(line => line.split(\" \")) DAG (Directed Acyclic Graph) Creation Constructing a DAG of stages for the job - Stages are sets of tasks that can be executed in parallel - Example: A series of map and filter transformations create a DAG Task Execution Dividing the DAG into stages, creating tasks, and sending them to executors - Tasks are distributed across executors - Each executor processes a partition of the data Data Shuffling Exchanging data between nodes during operations like reduceByKey - Data is grouped by key across nodes - Example: Shuffling data for aggregations Result Collection Executors process the tasks and send the results back to the driver program - Example: Final results of collect or count are returned to the driver Job Completion Driver program completes the execution - Example: Driver terminates after executing sc.stop() Explain DAG in Spark Topic Description Details Example DAG in Spark DAG stands for Directed Acyclic Graph. - Series of steps representing the operations on data. - Directed : Operations flow in one direction. - Acyclic : No cycles or loops. N/A Why We Need DAG Optimizes Execution, Fault Tolerance, and Parallel Processing. - Optimizes Execution : Spark can optimize operations. - Fault Tolerance : Recomputes lost data if a node fails. - Parallel Processing : Divides tasks into stages for parallel execution. N/A Without DAG No Optimization, No Fault Tolerance, and Less Parallelism. - No Optimization : Operations would run as written, slower performance. - No Fault Tolerance : Inefficient data recomputation. - Less Parallelism : Harder to parallelize tasks. N/A Example Example of a Spark job and DAG construction. - Read Data : sc.textFile(\"file.txt\") - Split Lines into Words : data.flatMap(...) - Map Words to Key-Value Pairs : words.map(...) - Reduce by Key : wordCounts.reduceByKey(...) - Collect Results : wordCounts.collect() scala val data = sc.textFile(\"file.txt\") val words = data.flatMap(line => line.split(\" \")) val wordCounts = words.map(word => (word, 1)).reduceByKey(_ + _) wordCounts.collect() Explain spark.sql.shuffle.partitions Variable Topic Description Details Example spark.sql.shuffle.partitions Configuration setting for shuffle partitions - Default Value : 200 partitions - Defines the default number of partitions used when shuffling data for wide transformations scala spark.conf.set(\"spark.sql.shuffle.partitions\", \"number_of_partitions\") Purpose Optimize Performance and Control Data Distribution - Optimize Performance : Balances workload across the cluster - Control Data Distribution : Manages how data is distributed and processed during shuffle operations N/A When It's Used Wide Transformations and SQL Queries - Wide Transformations : reduceByKey , groupByKey , join , etc. - SQL Queries : Operations involving shuffling data like joins and aggregations N/A How to Set It Setting via Configuration and spark-submit - Configuration : spark.conf.set(\"spark.sql.shuffle.partitions\", \"number_of_partitions\") - spark-submit : spark-submit --conf spark.sql.shuffle.partitions=number_of_partitions ... N/A Example Default and Custom Settings - Default Setting : scala val spark = SparkSession.builder.appName(\"Example\").getOrCreate() println(spark.conf.get(\"spark.sql.shuffle.partitions\")) // Output: 200 - Custom Setting : scala val spark = SparkSession.builder.appName(\"Example\").getOrCreate() spark.conf.set(\"spark.sql.shuffle.partitions\", \"50\") val df = spark.read.json(\"data.json\") df.groupBy(\"column\").count().show() scala val spark = SparkSession.builder.appName(\"Example\").getOrCreate() println(spark.conf.get(\"spark.sql.shuffle.partitions\")) // Output: 200 Why Adjust This Setting? Small and Large Datasets - Small Datasets : Reduce the number of partitions to avoid too many small tasks, leading to overhead - Large Datasets : Increase the number of partitions to distribute data evenly and avoid large partitions that slow down processing N/A","title":"Spark Interview Questions"},{"location":"Spark-DataBricks/1.0_Spark/1.11_Spark_Interview_Questions/#common-spark-concepts-which-may-be-asked-in-an-interview","text":"In this article I will try to put some common Spark concepts in a tabular format(So that its compact). These are good concepts to remember. Also, they may be asked in Interview questions.","title":"Common Spark Concepts which may be asked in an interview"},{"location":"Spark-DataBricks/1.0_Spark/1.11_Spark_Interview_Questions/#types-of-join-strateries","text":"Join Strategy Description Use Case Example Shuffle Hash Join Both DataFrames are shuffled based on the join keys, and then a hash join is performed. Useful when both DataFrames are large and have a good distribution of keys. spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"false\") df1.join(df2, \"key\") Broadcast Hash Join One of the DataFrames is small enough to fit in memory and is broadcasted to all worker nodes. A hash join is then performed. Efficient when one DataFrame is much smaller than the other. val broadcastDF = broadcast(df2) df1.join(broadcastDF, \"key\") Sort-Merge Join Both DataFrames are sorted on the join keys and then merged. This requires a shuffle if the data is not already sorted. Suitable for large DataFrames when the join keys are sorted or can be sorted efficiently. spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"true\") df1.join(df2, \"key\") Cartesian Join (Cross Join) Every row of one DataFrame is paired with every row of the other DataFrame. Generally not recommended due to its computational expense, but can be used for generating combinations of all rows. df1.crossJoin(df2) Broadcast Nested Loop Join The smaller DataFrame is broadcasted, and a nested loop join is performed. Used when there are no join keys or the join condition is complex and cannot be optimized with hash or sort-merge joins. val broadcastDF = broadcast(df2) df1.join(broadcastDF) Shuffle-and-Replicate Nested Loop Join Both DataFrames are shuffled and replicated to perform a nested loop join. Used for complex join conditions that cannot be handled by other join strategies. df1.join(df2, expr(\"complex_condition\"))","title":"Types of join strateries"},{"location":"Spark-DataBricks/1.0_Spark/1.11_Spark_Interview_Questions/#types-of-joins","text":"Join Type Description Example Inner Join Returns rows that have matching values in both DataFrames. df1.join(df2, \"key\") Outer Join Returns all rows when there is a match in either DataFrame. Missing values are filled with nulls. df1.join(df2, Seq(\"key\"), \"outer\") Left Outer Join Returns all rows from the left DataFrame, and matched rows from the right DataFrame. df1.join(df2, Seq(\"key\"), \"left_outer\") Right Outer Join Returns all rows from the right DataFrame, and matched rows from the left DataFrame. df1.join(df2, Seq(\"key\"), \"right_outer\") Left Semi Join Returns only the rows from the left DataFrame that have a match in the right DataFrame. df1.join(df2, Seq(\"key\"), \"left_semi\") Left Anti Join Returns only the rows from the left DataFrame that do not have a match in the right DataFrame. df1.join(df2, Seq(\"key\"), \"left_anti\") Cross Join Returns the Cartesian product of both DataFrames. Every row in the left DataFrame will be combined with every row in the right DataFrame. df1.crossJoin(df2) Self Join A join in which a DataFrame is joined with itself. This can be an inner, outer, left, or right join. df.join(df, df(\"key1\") === df(\"key2\"))","title":"Types of Joins"},{"location":"Spark-DataBricks/1.0_Spark/1.11_Spark_Interview_Questions/#common-spark-optimization-techniques","text":"Technique What it is When to use Example Caching and Persistence Storing data in memory for quick access When you need to reuse the same data multiple times scala val cachedData = df.cache() Broadcast Variables Sending a small dataset to all worker nodes When one dataset is much smaller than the other scala val broadcastData = spark.broadcast(smallDF) largeDF.join(broadcastData.value, \"key\") Partitioning Dividing data into smaller, manageable chunks When dealing with large datasets to improve parallel processing scala val partitionedData = df.repartition(10, $\"key\") Avoiding Shuffles Reducing the movement of data between nodes To improve performance by minimizing network overhead Use mapPartitions instead of groupBy when possible Coalesce Reducing the number of partitions When the data has become sparse after a transformation scala val coalescedData = df.coalesce(1) Predicate Pushdown Filtering data as early as possible in the processing To reduce the amount of data read and processed scala val filteredData = df.filter($\"column\" > 10) Using the Right Join Strategy Choosing the most efficient way to join two datasets Based on the size and distribution of data Prefer broadcast joins for small datasets Tuning Spark Configurations Adjusting settings to optimize resource usage To match the workload and cluster resources scala spark.conf.set(\"spark.executor.memory\", \"4g\") Using DataFrames/Datasets API Leveraging the high-level APIs for optimizations To benefit from Catalyst optimizer and Tungsten execution engine scala val df = spark.read.csv(\"data.csv\") df.groupBy(\"column\").count() Vectorized Query Execution Processing multiple rows of data at a time For high-performance operations on large datasets Use built-in SQL functions and DataFrame methods","title":"Common Spark optimization techniques"},{"location":"Spark-DataBricks/1.0_Spark/1.11_Spark_Interview_Questions/#different-phases-of-spark-sql-engine","text":"Phase Description Details Example Parsing Converting SQL queries into a logical plan. The SQL query is parsed into an abstract syntax tree (AST). Converting SELECT * FROM table WHERE id = 1 into an internal format. Analysis Resolving references and verifying the logical plan. Resolves column names, table names, and function names; checks for errors. Ensuring that the table table and the column id exist in the database. Optimization Improving the logical plan for better performance. Transforms the logical plan using various optimization techniques; applies rules via Catalyst optimizer. Reordering filters to reduce the amount of data processed early on. Physical Planning Converting the logical plan into a physical plan. Converts the optimized logical plan into one or more physical plans; selects the most efficient plan. Deciding whether to use a hash join or a sort-merge join. Code Generation Generating executable code from the physical plan. Generates Java bytecode to execute the physical plan; this code runs on Spark executors. Creating code to perform join operations, filter data, and compute results. Execution Running the generated code on the Spark cluster. Distributes generated code across the Spark cluster; executed by Spark executors; results collected and returned. Running join and filter operations on different nodes in the cluster and aggregating results.","title":"Different phases of Spark-SQL engine"},{"location":"Spark-DataBricks/1.0_Spark/1.11_Spark_Interview_Questions/#common-reasons-for-analysis-exception-in-spark","text":"Here are some common reasons why you might encounter an AnalysisException in Spark: Reason Description Example Non-Existent Column or Table Column or table specified does not exist. Referring to a non-existent column id . Ambiguous Column Reference Same column name exists in multiple tables without qualification. Joining two DataFrames with the same column name id . Invalid SQL Syntax SQL query has syntax errors. Using incorrect SQL syntax like SELCT . Unsupported Operations Using an operation that Spark SQL does not support. Using an unsupported function. Schema Mismatch Schema of the DataFrame does not match the expected schema. Inserting data with different column types. Missing File or Directory Specified file or directory does not exist. Referring to a non-existent CSV file. Incorrect Data Type Operations expecting a specific data type are given the wrong type. Performing a math operation on a string column.","title":"Common reasons for analysis exception in Spark"},{"location":"Spark-DataBricks/1.0_Spark/1.11_Spark_Interview_Questions/#flow-of-how-spark-works-internally","text":"Component/Step Role/Process Function/Example Driver Program Entry point for the Spark application - Manages application lifecycle - Defines RDD transformations and actions SparkContext Acts as the master of the Spark application - Connects to cluster manager - Coordinates tasks Cluster Manager Manages the cluster of machines - Allocates resources to Spark applications - Examples: YARN, Mesos, standalone Executors Worker nodes that run tasks and store data - Execute assigned code - Return results to the driver - Cache data in memory for quick access Spark Application Submission Submitting the driver program to the cluster manager - Example: Submitting a job using spark-submit SparkContext Initialization Driver program initializes SparkContext - Example: val sc = new SparkContext(conf) Job Scheduling Driver program defines transformations and actions on RDDs/DataFrames - Example: val rdd = sc.textFile(\"data.txt\").map(line => line.split(\" \")) DAG (Directed Acyclic Graph) Creation Constructing a DAG of stages for the job - Stages are sets of tasks that can be executed in parallel - Example: A series of map and filter transformations create a DAG Task Execution Dividing the DAG into stages, creating tasks, and sending them to executors - Tasks are distributed across executors - Each executor processes a partition of the data Data Shuffling Exchanging data between nodes during operations like reduceByKey - Data is grouped by key across nodes - Example: Shuffling data for aggregations Result Collection Executors process the tasks and send the results back to the driver program - Example: Final results of collect or count are returned to the driver Job Completion Driver program completes the execution - Example: Driver terminates after executing sc.stop()","title":"Flow of how Spark works internally"},{"location":"Spark-DataBricks/1.0_Spark/1.11_Spark_Interview_Questions/#explain-dag-in-spark","text":"Topic Description Details Example DAG in Spark DAG stands for Directed Acyclic Graph. - Series of steps representing the operations on data. - Directed : Operations flow in one direction. - Acyclic : No cycles or loops. N/A Why We Need DAG Optimizes Execution, Fault Tolerance, and Parallel Processing. - Optimizes Execution : Spark can optimize operations. - Fault Tolerance : Recomputes lost data if a node fails. - Parallel Processing : Divides tasks into stages for parallel execution. N/A Without DAG No Optimization, No Fault Tolerance, and Less Parallelism. - No Optimization : Operations would run as written, slower performance. - No Fault Tolerance : Inefficient data recomputation. - Less Parallelism : Harder to parallelize tasks. N/A Example Example of a Spark job and DAG construction. - Read Data : sc.textFile(\"file.txt\") - Split Lines into Words : data.flatMap(...) - Map Words to Key-Value Pairs : words.map(...) - Reduce by Key : wordCounts.reduceByKey(...) - Collect Results : wordCounts.collect() scala val data = sc.textFile(\"file.txt\") val words = data.flatMap(line => line.split(\" \")) val wordCounts = words.map(word => (word, 1)).reduceByKey(_ + _) wordCounts.collect()","title":"Explain DAG in Spark"},{"location":"Spark-DataBricks/1.0_Spark/1.11_Spark_Interview_Questions/#explain-sparksqlshufflepartitions-variable","text":"Topic Description Details Example spark.sql.shuffle.partitions Configuration setting for shuffle partitions - Default Value : 200 partitions - Defines the default number of partitions used when shuffling data for wide transformations scala spark.conf.set(\"spark.sql.shuffle.partitions\", \"number_of_partitions\") Purpose Optimize Performance and Control Data Distribution - Optimize Performance : Balances workload across the cluster - Control Data Distribution : Manages how data is distributed and processed during shuffle operations N/A When It's Used Wide Transformations and SQL Queries - Wide Transformations : reduceByKey , groupByKey , join , etc. - SQL Queries : Operations involving shuffling data like joins and aggregations N/A How to Set It Setting via Configuration and spark-submit - Configuration : spark.conf.set(\"spark.sql.shuffle.partitions\", \"number_of_partitions\") - spark-submit : spark-submit --conf spark.sql.shuffle.partitions=number_of_partitions ... N/A Example Default and Custom Settings - Default Setting : scala val spark = SparkSession.builder.appName(\"Example\").getOrCreate() println(spark.conf.get(\"spark.sql.shuffle.partitions\")) // Output: 200 - Custom Setting : scala val spark = SparkSession.builder.appName(\"Example\").getOrCreate() spark.conf.set(\"spark.sql.shuffle.partitions\", \"50\") val df = spark.read.json(\"data.json\") df.groupBy(\"column\").count().show() scala val spark = SparkSession.builder.appName(\"Example\").getOrCreate() println(spark.conf.get(\"spark.sql.shuffle.partitions\")) // Output: 200 Why Adjust This Setting? Small and Large Datasets - Small Datasets : Reduce the number of partitions to avoid too many small tasks, leading to overhead - Large Datasets : Increase the number of partitions to distribute data evenly and avoid large partitions that slow down processing N/A","title":"Explain spark.sql.shuffle.partitions Variable"},{"location":"Spark-DataBricks/1.0_Spark/1.12_Spark_Shuffle/","text":"Shuffle in Spark - Moving Data Between Partitions You know about partitions, where data is divided into different containers. But, suppose you are doing a GroupBy operation. You will need to bring the data together in one place. This process of bringing data from different partitions to one place is called shuffle. Shuffle happens during operations like GroupBy , Join , or ReduceByKey . However, shuffle is very expensive. Our goal should be to reduce it as much as possible. Here are some ways to reduce shuffle: Combine Operations to Reduce Shuffles Instead of doing separate operations that cause multiple shuffles, combine them into one. df1 = df.groupBy(\"id\").count() df2 = df1.filter(df1[\"count\"] > 1) # Combined operations to reduce shuffles: df_combined = df.groupBy(\"id\").count().filter(\"count > 1\") df_combined.show() Repartition Your Data Repartition your data to balance the load and optimize data distribution. # Example DataFrame df = spark.createDataFrame([(1, 'a'), (2, 'b'), (1, 'c'), (2, 'd')], [\"id\", \"value\"]) # Repartitioning to optimize data distribution df_repartitioned = df.repartition(\"id\") df_repartitioned.show() Cache Data for Reuse Cache data that you need to use multiple times to avoid repeated shuffling. # Example DataFrame df = spark.createDataFrame([(1, 'a'), (2, 'b'), (1, 'c'), (2, 'd')], [\"id\", \"value\"]) # Caching intermediate DataFrame df_cached = df.cache() # Using cached DataFrame multiple times df_cached.groupBy(\"id\").count().show() df_cached.filter(df_cached[\"id\"] == 1).show() So, you can operations, repartitioning data, and cahe frequently used data to reduce shuffle.","title":"Shuffle in Spark"},{"location":"Spark-DataBricks/1.0_Spark/1.12_Spark_Shuffle/#shuffle-in-spark-moving-data-between-partitions","text":"You know about partitions, where data is divided into different containers. But, suppose you are doing a GroupBy operation. You will need to bring the data together in one place. This process of bringing data from different partitions to one place is called shuffle. Shuffle happens during operations like GroupBy , Join , or ReduceByKey . However, shuffle is very expensive. Our goal should be to reduce it as much as possible. Here are some ways to reduce shuffle:","title":"Shuffle in Spark - Moving Data Between Partitions"},{"location":"Spark-DataBricks/1.0_Spark/1.12_Spark_Shuffle/#combine-operations-to-reduce-shuffles","text":"Instead of doing separate operations that cause multiple shuffles, combine them into one. df1 = df.groupBy(\"id\").count() df2 = df1.filter(df1[\"count\"] > 1) # Combined operations to reduce shuffles: df_combined = df.groupBy(\"id\").count().filter(\"count > 1\") df_combined.show()","title":"Combine Operations to Reduce Shuffles"},{"location":"Spark-DataBricks/1.0_Spark/1.12_Spark_Shuffle/#repartition-your-data","text":"Repartition your data to balance the load and optimize data distribution. # Example DataFrame df = spark.createDataFrame([(1, 'a'), (2, 'b'), (1, 'c'), (2, 'd')], [\"id\", \"value\"]) # Repartitioning to optimize data distribution df_repartitioned = df.repartition(\"id\") df_repartitioned.show()","title":"Repartition Your Data"},{"location":"Spark-DataBricks/1.0_Spark/1.12_Spark_Shuffle/#cache-data-for-reuse","text":"Cache data that you need to use multiple times to avoid repeated shuffling. # Example DataFrame df = spark.createDataFrame([(1, 'a'), (2, 'b'), (1, 'c'), (2, 'd')], [\"id\", \"value\"]) # Caching intermediate DataFrame df_cached = df.cache() # Using cached DataFrame multiple times df_cached.groupBy(\"id\").count().show() df_cached.filter(df_cached[\"id\"] == 1).show() So, you can operations, repartitioning data, and cahe frequently used data to reduce shuffle.","title":"Cache Data for Reuse"},{"location":"Spark-DataBricks/1.0_Spark/1.13_SparkDatabaseTablesCatalogsMetastore/","text":"Spark Database, Tables, Warehouse, Metastore & Catalogs A Spark Database is just a folder named databasename.db inside the spark-warehouse folder. A Managed/Internal/Spark-Metastore table is a subfolder within the databasename.db folder. Partitions are also stored as subfolders. The location of the warehouse folder is set by the spark.sql.warehouse.dir setting. If spark.sql.warehouse.dir is not set, Spark uses a default directory, usually a spark-warehouse folder in the current working directory of the application. You can find out the warehouse directory by running the command SET spark.sql.warehouse.dir in the spark-sql prompt. You can set the warehouse directory in your session with .config(\"spark.sql.warehouse.dir\", \"/path/to/your/warehouse\") . SPARK Managed Tables (AKA Internal / Spark-Metastore Tables) Using Spark-SQL Shell When you create tables in the spark-sql shell using commands like the one below, Spark will create a managed table. The table data will be stored in the spark-warehouse folder, and the Derby database ( metastore_db folder) will contain its metadata. CREATE TABLE Hollywood (name STRING); The table will be permanent, meaning you can query it even after restarting Spark. Here is an example output of DESCRIBE EXTENDED Hollywood in the spark-sql shell: Catalog : spark_catalog - Spark uses its own internal catalog to manage metadata. Database : default - The default database provided by Spark. Type : MANAGED - Indicates that Spark manages the table's lifecycle. Provider : hive - Refers to Spark's capability to handle Hive-compatible metadata. Serde Library : org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe - Serialization and deserialization library. InputFormat : org.apache.hadoop.mapred.TextInputFormat - Input format for reading the table data. OutputFormat : org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat - Output format for writing the table data. Location : file:/home/dwdas/spark-warehouse/hollywood - File path where the table data is stored. Partition Provider : Catalog - Indicates that the catalog manages partitions. Key Takeaways: Built-in Catalog : Even without a standalone Hive installation, Spark provides managed table functionality by leveraging its built-in catalog and Hive-compatible features. SQL-like Operations : You can use SQL-like operations to manage tables within Spark. Embedded Deployment Mode : By default, Spark SQL uses an embedded deployment mode of a Hive metastore with an Apache Derby database. Production Use : The default embedded deployment mode is not recommended for production use due to the limitation of only one active SparkSession at a time. Remember: This Derby-mini-Hive method that Spark uses to manage internal tables has a limitation: only one active session is allowed at a time. Attempting multiple `spark-sql` sessions will result in a Derby database exception. Would you take this to production? Metastore in Spark The metastore in Spark stores metadata about tables, like their names and the locations of their files (e.g., Parquet files). In Spark, the metastore is typically configured in one of two common ways, but there are also more advanced options available. Standalone Hive Metastore: You can install and configure a standalone Hive metastore server. This server would manage the metadata independently and communicate with your Spark application. Embedded Hive Metastore with Derby Spark includes a built-in metastore that uses an embedded Apache Derby database. This database starts in the application's working directory and stores data in the metastore_db folder. It's a convenient, pseudo-metastore suitable for small applications or datasets. To use this, simply enable Hive support in your Spark session with enableHiveSupport() : from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"EmbeddedMetastoreExample\") \\ .enableHiveSupport() \\ .getOrCreate() Again: By default, Hive uses an embedded Apache Derby database to store its metadata. While this is a convenient option for initial setup, it has limitations. Notably, Derby can only support one active user at a time. This makes it unsuitable for scenarios requiring multiple concurrent Hive sessions. So, the solution is to use a standard database like MySQL/Postgrees or MSSQL as the metastore DB. And, let the poor derby take some rest. Catalog types in Spark In-Memory Catalog: Default catalog. Stores data memory. Everything vanishes when session is closed. For some quick queries etc. Hive Catalog: A mini-version comes shipped with Spark. You need to enable hive support to use it. Data is stoerd permanently. Can have a full-fledged hivve as well. JDBC Catalog: When you want a full-database like MSSQL to store the catalog information. Can be like Hive+MSSQL. Custom Catalogs: Custom catalogs can be implemented using ExtendedCatalogInterface. AWS glue, Synapese databricks. Delta Lake Catalog: When using Delta Lake, it provides its own catalog implementation. What happens when you enter spark-sql on a freshly installed Spark server? Imagine you have a fresh standalone Spark server. You log into the server through the terminal, and your current directory is /home/dwdas . The moment you enter the spark-sql command, Spark starts an embedded Derby database and creates a metastore_db folder in your current directory. This folder serves as the root of the Derby database. Essentially, Spark \"boots a Derby instance on metastore_db \". By default, a fresh Spark setup uses its in-house Derby database as a 'metastore' to store the names and file locations of the Spark tables you create. This is Spark's basic way to manage its tables. Though you can upgrade it and use an external metastore and other advanced features. Remember: Your current directory is crucial because everything is created inside it. This often confuses new learners who forget this, making it hard to find their tables later. Remember: Spark SQL does not use a Hive metastore under the cover. By default, it uses in-memory catalogs if Hive support is not enabled. Spark-shell: By default, uses in-memory catalogs unless configured to use Hive metastore. Set spark.sql.catalogImplementation to hive or in-memory to control this. What Happens When You Create a Table in Spark-SQL? Let's open the spark-sql and enter this simple command: CREATE TABLE movies (title STRING, genre STRING); INSERT INTO movies VALUES ('Inception', 'Sci-Fi'); You will see logs like these: 24/06/20 07:44:39 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead. Explanation : - When you execute a CREATE TABLE statement without specifying a table provider, Spark\u2019s session catalog defaults to creating a Hive SerDe table. The session catalog interprets and resolves SQL commands. Note : To create a native Spark SQL table, set spark.sql.legacy.createHiveTableByDefault to false . In spark-defaults.conf : properties spark.sql.legacy.createHiveTableByDefault=false In Spark SQL shell: sh spark-sql --conf spark.sql.legacy.createHiveTableByDefault=false In PySpark session: ```python from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"example\") \\ .config(\"spark.sql.legacy.createHiveTableByDefault\", \"false\") \\ .enableHiveSupport() \\ .getOrCreate() ``` You might also see a warning like this: 24/06/20 07:44:40 WARN HiveMetaStore: Location: file:/home/dwdas/spark-warehouse/movies specified for non-external table: movies Explanation : This indicates the default storage location for non-external tables is spark-warehouse . Note : Change the default location by setting spark.sql.warehouse.dir . To change the warehouse directory: In spark-defaults.conf : properties spark.sql.warehouse.dir=/your/custom/path In Spark SQL shell: sh spark-sql --conf spark.sql.warehouse.dir=/your/custom/path In PySpark session: ```python from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"example\") \\ .config(\"spark.sql.warehouse.dir\", \"/your/custom/path\") \\ .getOrCreate() ``` Key Takeaway When you run spark-sql with default settings, it will start a Derby database and create a metastore_db folder inside your current directory. So, be mindful of your current directory. If you create a table in a PySpark session, Spark will create both a metastore_db and a spark-warehouse folder.","title":"Spark DB-Tables-Metastore-Catalogs"},{"location":"Spark-DataBricks/1.0_Spark/1.13_SparkDatabaseTablesCatalogsMetastore/#spark-database-tables-warehouse-metastore-catalogs","text":"A Spark Database is just a folder named databasename.db inside the spark-warehouse folder. A Managed/Internal/Spark-Metastore table is a subfolder within the databasename.db folder. Partitions are also stored as subfolders. The location of the warehouse folder is set by the spark.sql.warehouse.dir setting. If spark.sql.warehouse.dir is not set, Spark uses a default directory, usually a spark-warehouse folder in the current working directory of the application. You can find out the warehouse directory by running the command SET spark.sql.warehouse.dir in the spark-sql prompt. You can set the warehouse directory in your session with .config(\"spark.sql.warehouse.dir\", \"/path/to/your/warehouse\") .","title":"Spark Database, Tables, Warehouse, Metastore &amp; Catalogs"},{"location":"Spark-DataBricks/1.0_Spark/1.13_SparkDatabaseTablesCatalogsMetastore/#spark-managed-tables-aka-internal-spark-metastore-tables-using-spark-sql-shell","text":"When you create tables in the spark-sql shell using commands like the one below, Spark will create a managed table. The table data will be stored in the spark-warehouse folder, and the Derby database ( metastore_db folder) will contain its metadata. CREATE TABLE Hollywood (name STRING); The table will be permanent, meaning you can query it even after restarting Spark. Here is an example output of DESCRIBE EXTENDED Hollywood in the spark-sql shell: Catalog : spark_catalog - Spark uses its own internal catalog to manage metadata. Database : default - The default database provided by Spark. Type : MANAGED - Indicates that Spark manages the table's lifecycle. Provider : hive - Refers to Spark's capability to handle Hive-compatible metadata. Serde Library : org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe - Serialization and deserialization library. InputFormat : org.apache.hadoop.mapred.TextInputFormat - Input format for reading the table data. OutputFormat : org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat - Output format for writing the table data. Location : file:/home/dwdas/spark-warehouse/hollywood - File path where the table data is stored. Partition Provider : Catalog - Indicates that the catalog manages partitions.","title":"SPARK Managed Tables (AKA Internal / Spark-Metastore Tables) Using Spark-SQL Shell"},{"location":"Spark-DataBricks/1.0_Spark/1.13_SparkDatabaseTablesCatalogsMetastore/#key-takeaways","text":"Built-in Catalog : Even without a standalone Hive installation, Spark provides managed table functionality by leveraging its built-in catalog and Hive-compatible features. SQL-like Operations : You can use SQL-like operations to manage tables within Spark. Embedded Deployment Mode : By default, Spark SQL uses an embedded deployment mode of a Hive metastore with an Apache Derby database. Production Use : The default embedded deployment mode is not recommended for production use due to the limitation of only one active SparkSession at a time. Remember: This Derby-mini-Hive method that Spark uses to manage internal tables has a limitation: only one active session is allowed at a time. Attempting multiple `spark-sql` sessions will result in a Derby database exception. Would you take this to production?","title":"Key Takeaways:"},{"location":"Spark-DataBricks/1.0_Spark/1.13_SparkDatabaseTablesCatalogsMetastore/#metastore-in-spark","text":"The metastore in Spark stores metadata about tables, like their names and the locations of their files (e.g., Parquet files). In Spark, the metastore is typically configured in one of two common ways, but there are also more advanced options available. Standalone Hive Metastore: You can install and configure a standalone Hive metastore server. This server would manage the metadata independently and communicate with your Spark application. Embedded Hive Metastore with Derby Spark includes a built-in metastore that uses an embedded Apache Derby database. This database starts in the application's working directory and stores data in the metastore_db folder. It's a convenient, pseudo-metastore suitable for small applications or datasets. To use this, simply enable Hive support in your Spark session with enableHiveSupport() : from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"EmbeddedMetastoreExample\") \\ .enableHiveSupport() \\ .getOrCreate() Again: By default, Hive uses an embedded Apache Derby database to store its metadata. While this is a convenient option for initial setup, it has limitations. Notably, Derby can only support one active user at a time. This makes it unsuitable for scenarios requiring multiple concurrent Hive sessions. So, the solution is to use a standard database like MySQL/Postgrees or MSSQL as the metastore DB. And, let the poor derby take some rest.","title":"Metastore in Spark"},{"location":"Spark-DataBricks/1.0_Spark/1.13_SparkDatabaseTablesCatalogsMetastore/#catalog-types-in-spark","text":"In-Memory Catalog: Default catalog. Stores data memory. Everything vanishes when session is closed. For some quick queries etc. Hive Catalog: A mini-version comes shipped with Spark. You need to enable hive support to use it. Data is stoerd permanently. Can have a full-fledged hivve as well. JDBC Catalog: When you want a full-database like MSSQL to store the catalog information. Can be like Hive+MSSQL. Custom Catalogs: Custom catalogs can be implemented using ExtendedCatalogInterface. AWS glue, Synapese databricks. Delta Lake Catalog: When using Delta Lake, it provides its own catalog implementation.","title":"Catalog types in Spark"},{"location":"Spark-DataBricks/1.0_Spark/1.13_SparkDatabaseTablesCatalogsMetastore/#what-happens-when-you-enter-spark-sql-on-a-freshly-installed-spark-server","text":"Imagine you have a fresh standalone Spark server. You log into the server through the terminal, and your current directory is /home/dwdas . The moment you enter the spark-sql command, Spark starts an embedded Derby database and creates a metastore_db folder in your current directory. This folder serves as the root of the Derby database. Essentially, Spark \"boots a Derby instance on metastore_db \". By default, a fresh Spark setup uses its in-house Derby database as a 'metastore' to store the names and file locations of the Spark tables you create. This is Spark's basic way to manage its tables. Though you can upgrade it and use an external metastore and other advanced features. Remember: Your current directory is crucial because everything is created inside it. This often confuses new learners who forget this, making it hard to find their tables later. Remember: Spark SQL does not use a Hive metastore under the cover. By default, it uses in-memory catalogs if Hive support is not enabled. Spark-shell: By default, uses in-memory catalogs unless configured to use Hive metastore. Set spark.sql.catalogImplementation to hive or in-memory to control this.","title":"What happens when you enter spark-sql on a freshly installed Spark server?"},{"location":"Spark-DataBricks/1.0_Spark/1.13_SparkDatabaseTablesCatalogsMetastore/#what-happens-when-you-create-a-table-in-spark-sql","text":"Let's open the spark-sql and enter this simple command: CREATE TABLE movies (title STRING, genre STRING); INSERT INTO movies VALUES ('Inception', 'Sci-Fi'); You will see logs like these: 24/06/20 07:44:39 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead. Explanation : - When you execute a CREATE TABLE statement without specifying a table provider, Spark\u2019s session catalog defaults to creating a Hive SerDe table. The session catalog interprets and resolves SQL commands. Note : To create a native Spark SQL table, set spark.sql.legacy.createHiveTableByDefault to false . In spark-defaults.conf : properties spark.sql.legacy.createHiveTableByDefault=false In Spark SQL shell: sh spark-sql --conf spark.sql.legacy.createHiveTableByDefault=false In PySpark session: ```python from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"example\") \\ .config(\"spark.sql.legacy.createHiveTableByDefault\", \"false\") \\ .enableHiveSupport() \\ .getOrCreate() ``` You might also see a warning like this: 24/06/20 07:44:40 WARN HiveMetaStore: Location: file:/home/dwdas/spark-warehouse/movies specified for non-external table: movies Explanation : This indicates the default storage location for non-external tables is spark-warehouse . Note : Change the default location by setting spark.sql.warehouse.dir . To change the warehouse directory: In spark-defaults.conf : properties spark.sql.warehouse.dir=/your/custom/path In Spark SQL shell: sh spark-sql --conf spark.sql.warehouse.dir=/your/custom/path In PySpark session: ```python from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"example\") \\ .config(\"spark.sql.warehouse.dir\", \"/your/custom/path\") \\ .getOrCreate() ```","title":"What Happens When You Create a Table in Spark-SQL?"},{"location":"Spark-DataBricks/1.0_Spark/1.13_SparkDatabaseTablesCatalogsMetastore/#key-takeaway","text":"When you run spark-sql with default settings, it will start a Derby database and create a metastore_db folder inside your current directory. So, be mindful of your current directory. If you create a table in a PySpark session, Spark will create both a metastore_db and a spark-warehouse folder.","title":"Key Takeaway"},{"location":"Spark-DataBricks/1.0_Spark/1.14_Q%26A/","text":"Q & A Which of the following statements about the Spark driver is incorrect? A. The Spark driver is the node in which the Spark application's main method runs to coordinate the Spark application. B. The Spark driver is horizontally scaled to increase overall processing throughput. C. The Spark driver contains the SparkContext object. D. The Spark driver is responsible for scheduling the execution of data by various worker nodes in cluster mode. E. The Spark driver should be as close as possible to worker nodes for optimal performance. Answer: B. The Spark driver is horizontally scaled to increase overall processing throughput. Explanation: - A is correct because the Spark driver indeed runs the application's main method and coordinates the application. - B is incorrect because the Spark driver is a single entity and is not horizontally scalable. The driver manages and coordinates tasks but does not scale horizontally to increase throughput. - C is correct because the Spark driver contains the SparkContext object, which is the entry point for Spark functionality. - D is correct because the Spark driver schedules the execution of tasks on worker nodes. - E is correct because the driver should be close to the worker nodes to minimize latency and improve performance. Which of the following describes nodes in cluster-mode Spark? A. Nodes are the most granular level of execution in the Spark execution hierarchy. B. There is only one node and it hosts both the driver and executors. C. Nodes are another term for executors, so they are processing engine instances for performing computations. D. There are driver nodes and worker nodes, both of which can scale horizontally. E. Worker nodes are machines that host the executors responsible for the execution of tasks. Answer: E. Worker nodes are machines that host the executors responsible for the execution of tasks. Explanation: - A is incorrect because tasks, not nodes, are the most granular level of execution. - B is incorrect because in a cluster mode, there are multiple nodes, including separate driver and worker nodes. - C is incorrect because nodes and executors are not the same; nodes host executors. - D is incorrect because typically, only worker nodes scale horizontally, not driver nodes. - E is correct because worker nodes are indeed machines that host executors, which execute the tasks assigned by the driver. Which of the following statements about slots is true? A. There must be more slots than executors. B. There must be more tasks than slots. C. Slots are the most granular level of execution in the Spark execution hierarchy. D. Slots are not used in cluster mode. E. Slots are resources for parallelization within a Spark application. Answer: E. Slots are resources for parallelization within a Spark application. Explanation: - A is incorrect because there is no requirement for having more slots than executors. - B is incorrect because the number of tasks does not necessarily have to exceed the number of slots. - C is incorrect because tasks, not slots, are the most granular level of execution. - D is incorrect because slots are indeed used in cluster mode to enable parallel task execution. - E is correct because slots are resources that allow tasks to run in parallel, providing the means for concurrent execution. Which of the following is a combination of a block of data and a set of transformers that will run on a single executor? A. Executor B. Node C. Job D. Task E. Slot Answer: D. Task Explanation: - A is incorrect because an executor is a process running on a worker node that executes tasks. - B is incorrect because a node can host multiple executors. - C is incorrect because a job is a higher-level construct encompassing multiple stages and tasks. - D is correct because a task is the unit of work that includes a block of data and the computation to be performed on it. - E is incorrect because a slot is a resource for parallel task execution, not the unit of work itself. Which of the following is a group of tasks that can be executed in parallel to compute the same set of operations on potentially multiple machines? A. Job B. Slot C. Executor D. Task E. Stage Answer: E. Stage Explanation: A is incorrect because a job is an entire computation consisting of multiple stages. B is incorrect because a slot is a resource for parallel task execution. C is incorrect because an executor runs tasks on worker nodes. D is incorrect because a task is a single unit of work. E is correct because a stage is a set of tasks that can be executed in parallel to perform the same computation. Which of the following describes a shuffle? A. A shuffle is the process by which data is compared across partitions. B. A shuffle is the process by which data is compared across executors. C. A shuffle is the process by which partitions are allocated to tasks. D. A shuffle is the process by which partitions are ordered for write. E. A shuffle is the process by which tasks are ordered for execution. Answer: A. A shuffle is the process by which data is compared across partitions. Explanation: A is correct because shuffling involves redistributing data across partitions to align with the needs of downstream transformations. B is incorrect because shuffling happens across partitions, not specifically executors. C is incorrect because partition allocation to tasks is not the definition of a shuffle. D is incorrect because shuffling is not about ordering partitions for write operations. E is incorrect because shuffling does not involve ordering tasks for execution. DataFrame df is very large with a large number of partitions, more than there are executors in the cluster. Based on this situation, which of the following is incorrect? Assume there is one core per executor. A. Performance will be suboptimal because not all executors will be utilized at the same time. B. Performance will be suboptimal because not all data can be processed at the same time. C. There will be a large number of shuffle connections performed on DataFrame df when operations inducing a shuffle are called. D. There will be a lot of overhead associated with managing resources for data processing within each task. E. There might be risk of out-of-memory errors depending on the size of the executors in the cluster. Answer: A. Performance will be suboptimal because not all executors will be utilized at the same time. Explanation: A is incorrect because having more partitions than executors does not necessarily mean executors will be underutilized; they will process partitions sequentially. B is correct because more partitions than executors mean data processing cannot happen all at once, affecting performance. C is correct because a high number of partitions can lead to many shuffle operations. D is correct because managing many tasks increases overhead. E is correct because large data volumes can risk out-of-memory errors if executor memory is insufficient. Which of the following operations will trigger evaluation? A. DataFrame.filter() B. DataFrame.distinct() C. DataFrame.intersect() D. DataFrame.join() E. DataFrame.count() Answer: E. DataFrame.count() Explanation: - A is incorrect because DataFrame.filter() is a transformation, which defines a computation but does not trigger it. - B is incorrect because DataFrame.distinct() is also a transformation. - C is incorrect because DataFrame.intersect() is a transformation. - D is incorrect because DataFrame.join() is a transformation. - E is correct because DataFrame.count() is an action that triggers the actual execution of the computation. Which of the following describes the difference between transformations and actions? A. Transformations work on DataFrames/Datasets while actions are reserved for native language objects. B. There is no difference between actions and transformations. C. Actions are business logic operations that do not induce execution while transformations are execution triggers focused on returning results. D. Actions work on DataFrames/Datasets while transformations are reserved for native language objects. E. Transformations are business logic operations that do not induce execution while actions are execution triggers focused on returning results. Answer: E. Transformations are business logic operations that do not induce execution while actions are execution triggers focused on returning results. Explanation: - A is incorrect because both transformations and actions work on DataFrames/Datasets. - B is incorrect because there is a clear difference between transformations and actions. - C is incorrect because it incorrectly describes the role of transformations and actions. - D is incorrect because both transformations and actions work on DataFrames/Datasets. - E is correct because transformations define the computations without triggering them, while actions trigger the execution and return the results.","title":"Q&A"},{"location":"Spark-DataBricks/1.0_Spark/1.14_Q%26A/#q-a","text":"Which of the following statements about the Spark driver is incorrect? A. The Spark driver is the node in which the Spark application's main method runs to coordinate the Spark application. B. The Spark driver is horizontally scaled to increase overall processing throughput. C. The Spark driver contains the SparkContext object. D. The Spark driver is responsible for scheduling the execution of data by various worker nodes in cluster mode. E. The Spark driver should be as close as possible to worker nodes for optimal performance. Answer: B. The Spark driver is horizontally scaled to increase overall processing throughput. Explanation: - A is correct because the Spark driver indeed runs the application's main method and coordinates the application. - B is incorrect because the Spark driver is a single entity and is not horizontally scalable. The driver manages and coordinates tasks but does not scale horizontally to increase throughput. - C is correct because the Spark driver contains the SparkContext object, which is the entry point for Spark functionality. - D is correct because the Spark driver schedules the execution of tasks on worker nodes. - E is correct because the driver should be close to the worker nodes to minimize latency and improve performance. Which of the following describes nodes in cluster-mode Spark? A. Nodes are the most granular level of execution in the Spark execution hierarchy. B. There is only one node and it hosts both the driver and executors. C. Nodes are another term for executors, so they are processing engine instances for performing computations. D. There are driver nodes and worker nodes, both of which can scale horizontally. E. Worker nodes are machines that host the executors responsible for the execution of tasks. Answer: E. Worker nodes are machines that host the executors responsible for the execution of tasks. Explanation: - A is incorrect because tasks, not nodes, are the most granular level of execution. - B is incorrect because in a cluster mode, there are multiple nodes, including separate driver and worker nodes. - C is incorrect because nodes and executors are not the same; nodes host executors. - D is incorrect because typically, only worker nodes scale horizontally, not driver nodes. - E is correct because worker nodes are indeed machines that host executors, which execute the tasks assigned by the driver. Which of the following statements about slots is true? A. There must be more slots than executors. B. There must be more tasks than slots. C. Slots are the most granular level of execution in the Spark execution hierarchy. D. Slots are not used in cluster mode. E. Slots are resources for parallelization within a Spark application. Answer: E. Slots are resources for parallelization within a Spark application. Explanation: - A is incorrect because there is no requirement for having more slots than executors. - B is incorrect because the number of tasks does not necessarily have to exceed the number of slots. - C is incorrect because tasks, not slots, are the most granular level of execution. - D is incorrect because slots are indeed used in cluster mode to enable parallel task execution. - E is correct because slots are resources that allow tasks to run in parallel, providing the means for concurrent execution. Which of the following is a combination of a block of data and a set of transformers that will run on a single executor? A. Executor B. Node C. Job D. Task E. Slot Answer: D. Task Explanation: - A is incorrect because an executor is a process running on a worker node that executes tasks. - B is incorrect because a node can host multiple executors. - C is incorrect because a job is a higher-level construct encompassing multiple stages and tasks. - D is correct because a task is the unit of work that includes a block of data and the computation to be performed on it. - E is incorrect because a slot is a resource for parallel task execution, not the unit of work itself. Which of the following is a group of tasks that can be executed in parallel to compute the same set of operations on potentially multiple machines? A. Job B. Slot C. Executor D. Task E. Stage Answer: E. Stage Explanation: A is incorrect because a job is an entire computation consisting of multiple stages. B is incorrect because a slot is a resource for parallel task execution. C is incorrect because an executor runs tasks on worker nodes. D is incorrect because a task is a single unit of work. E is correct because a stage is a set of tasks that can be executed in parallel to perform the same computation. Which of the following describes a shuffle? A. A shuffle is the process by which data is compared across partitions. B. A shuffle is the process by which data is compared across executors. C. A shuffle is the process by which partitions are allocated to tasks. D. A shuffle is the process by which partitions are ordered for write. E. A shuffle is the process by which tasks are ordered for execution. Answer: A. A shuffle is the process by which data is compared across partitions. Explanation: A is correct because shuffling involves redistributing data across partitions to align with the needs of downstream transformations. B is incorrect because shuffling happens across partitions, not specifically executors. C is incorrect because partition allocation to tasks is not the definition of a shuffle. D is incorrect because shuffling is not about ordering partitions for write operations. E is incorrect because shuffling does not involve ordering tasks for execution. DataFrame df is very large with a large number of partitions, more than there are executors in the cluster. Based on this situation, which of the following is incorrect? Assume there is one core per executor. A. Performance will be suboptimal because not all executors will be utilized at the same time. B. Performance will be suboptimal because not all data can be processed at the same time. C. There will be a large number of shuffle connections performed on DataFrame df when operations inducing a shuffle are called. D. There will be a lot of overhead associated with managing resources for data processing within each task. E. There might be risk of out-of-memory errors depending on the size of the executors in the cluster. Answer: A. Performance will be suboptimal because not all executors will be utilized at the same time. Explanation: A is incorrect because having more partitions than executors does not necessarily mean executors will be underutilized; they will process partitions sequentially. B is correct because more partitions than executors mean data processing cannot happen all at once, affecting performance. C is correct because a high number of partitions can lead to many shuffle operations. D is correct because managing many tasks increases overhead. E is correct because large data volumes can risk out-of-memory errors if executor memory is insufficient. Which of the following operations will trigger evaluation? A. DataFrame.filter() B. DataFrame.distinct() C. DataFrame.intersect() D. DataFrame.join() E. DataFrame.count() Answer: E. DataFrame.count() Explanation: - A is incorrect because DataFrame.filter() is a transformation, which defines a computation but does not trigger it. - B is incorrect because DataFrame.distinct() is also a transformation. - C is incorrect because DataFrame.intersect() is a transformation. - D is incorrect because DataFrame.join() is a transformation. - E is correct because DataFrame.count() is an action that triggers the actual execution of the computation. Which of the following describes the difference between transformations and actions? A. Transformations work on DataFrames/Datasets while actions are reserved for native language objects. B. There is no difference between actions and transformations. C. Actions are business logic operations that do not induce execution while transformations are execution triggers focused on returning results. D. Actions work on DataFrames/Datasets while transformations are reserved for native language objects. E. Transformations are business logic operations that do not induce execution while actions are execution triggers focused on returning results. Answer: E. Transformations are business logic operations that do not induce execution while actions are execution triggers focused on returning results. Explanation: - A is incorrect because both transformations and actions work on DataFrames/Datasets. - B is incorrect because there is a clear difference between transformations and actions. - C is incorrect because it incorrectly describes the role of transformations and actions. - D is incorrect because both transformations and actions work on DataFrames/Datasets. - E is correct because transformations define the computations without triggering them, while actions trigger the execution and return the results.","title":"Q &amp; A"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics/","text":"Some common Pyspark Topics Just PySpark vs Real Spark Standalone Python vs. Anaconda Python Standalone Python Anaconda Python PySpark vs full Apache Spark Installation PySpark via pip Full Apache Spark Installation Conclusion Finding Spark Frequently asked Pyspark questions Some common Pyspark Topics Just PySpark vs Real Spark Here, I'll try to clear up the often-muddled area between PySpark and Full Spark installations. We will also touch upon different types of python installations available. Standalone Python vs. Anaconda Python Standalone Python Thi is the python you directly install from Python Software Foundation . Choose this for a lightweight setup, specific version control, and when using Python for general-purpose programming. Anaconda Python An open-source Python distribution for scientific computing and data science.Go for Anaconda for an easy-to-manage data science environment, especially when dealing with large datasets, machine learning, and analytics. PySpark vs full Apache Spark Installation PySpark via pip Many believe pip install pyspark installs the entire Apache Spark framework . No, it does not. When you install PySpark via pip, it installs the Python interface plus a minimal, standalone version of Apache Spark that can run locally on your machine. This standalone version of Spark is what allows the simulation of a Spark cluster environment on your single computer. Here's a breakdown: Apache Spark in PySpark : The PySpark package installed via pip includes a lightweight, standalone Spark installation. This isn't the full-fledged, distributed Spark system typically used in large-scale setups but a minimal version that can run on a single machine. When you execute PySpark code after installing it via pip, you're actually running this local version of Spark. Local Mode Execution : In this \"local mode,\" Spark operates as if it's a cluster but is actually just using the resources (like CPU and memory) of your single machine. It simulates the behavior of a Spark cluster, which in a full setup would distribute processing tasks across multiple nodes (machines). This mode is incredibly useful for development, testing, and learning because it lets you write and test code that would normally run on a Spark cluster, without the need for setting up multiple machines. The Spark Context : When your PySpark code runs, it initializes a \"Spark context\" within your Python script. This context is the primary connection to the Spark execution environment and allows your Python script to access Spark's functionalities. In the pip-installed PySpark environment, this Spark context talks to the local Spark instance included in the PySpark package, not a remote or distributed cluster. Full Apache Spark Installation Full Spark Installation involves setting up the complete Apache Spark framework, for building large-scale data processing applications, beyond the scope of PySpark alone. This is necessary for production-grade, large-scale data processing and when you need to harness the full power of Spark's distributed computing capabilities. Conclusion To sum it up, pip install pyspark actually installs both the Python interface to Spark (PySpark) and a minimal, local-only version of Apache Spark itself. This setup allows you to run Spark jobs as if you had a Spark cluster, but it's all happening within your own computer. The local mode is a simulation of a distributed Spark environment, suitable for learning, development, and processing small data sets. Finding Spark When you install Spark using a package manager like pip , it places the Spark binaries and libraries inside the Python's dist-packages or site-packages directory. This is a different installation approach than downloading and extracting Spark directly from the Apache Spark website. Here's what you need to know: SPARK_HOME for pip-installed PySpark : If you've installed Spark via pip , then the equivalent SPARK_HOME would be /usr/local/lib/python3.8/dist-packages/pyspark/ . This directory would contain all the necessary libraries and dependencies for Spark to run within a Python environment. Configuration and JARs : Configuration files, JARs, and other related resources would also be located within this directory structure. For instance, if you're looking for the location to place additional JAR files, it would likely be somewhere like /usr/local/lib/python3.8/dist-packages/pyspark/jars/ . Using spark-submit : The spark-submit script in this directory is tailored for the pip-installed Spark and ensures that it picks up the correct libraries and configurations from this location. In summary, if you've installed Spark using pip within your Docker container, then the /usr/local/lib/python3.8/dist-packages/pyspark/ directory is your primary point of reference for Spark configurations, binaries, and libraries. This is distinct from a standalone Spark installation, which would typically reside directly under /usr/local/ or another directory, separate from the Python packages directory. What if echo $SPARK_HOME or %SPARK_HOME% returns nothing and you are not sure where spark is installed? Even though you instaleld it? Let's try a different approach. Spark's binaries (like spark-submit and spark-shell ) are often a good hint as to where Spark might be installed. Search for Spark binaries : Execute the following command within your Docker container: bash find / -name \"spark-submit\" 2>/dev/null This command searches for the spark-submit binary, which is a common Spark binary. The directory containing this binary is likely your Spark installation directory. Once you've located the directory containing spark-submit , you'll typically find Spark's home directory one level up. For instance, if spark-submit is found in /path/to/spark/bin/spark-submit , then /path/to/spark is likely your SPARK_HOME . In my system it returned bash /usr/local/bin/spark-submit /usr/local/lib/python3.8/dist-packages/pyspark/bin/spark-submit How do I know where I have installed all the libraries or which is actually the right spark home? The path /usr/local/lib/python3.8/dist-packages/pyspark/bin/spark-submit suggests that this is the location where PySpark was installed as a Python package, likely via pip or a similar package manager. When you install Spark using a package manager like pip , it places the Spark binaries and libraries inside the Python's dist-packages or site-packages directory. This is a different installation approach than downloading and extracting Spark directly from the Apache Spark website. Here's what you need to know: SPARK_HOME for pip-installed PySpark : If you've installed Spark via pip , then the equivalent SPARK_HOME would be /usr/local/lib/python3.8/dist-packages/pyspark/ . This directory would contain all the necessary libraries and dependencies for Spark to run within a Python environment. Configuration and JARs : Configuration files, JARs, and other related resources would also be located within this directory structure. For instance, if you're looking for the location to place additional JAR files, it would likely be somewhere like /usr/local/lib/python3.8/dist-packages/pyspark/jars/ . Using spark-submit : The spark-submit script in this directory is tailored for the pip-installed Spark and ensures that it picks up the correct libraries and configurations from this location. In summary, if you've installed Spark using pip within your Docker container, then the /usr/local/lib/python3.8/dist-packages/pyspark/ directory is your primary point of reference for Spark configurations, binaries, and libraries. This is distinct from a standalone Spark installation, which would typically reside directly under /usr/local/ or another directory, separate from the Python packages directory. Frequently asked Pyspark questions In this section, I'll go over some common PySpark questions and their answers. These are basic questions that anyone using PySpark should be familiar with. So, let's get started :-) What is PySpark, how is different from Apache Spark? PySpark is the Python API for Apache Spark, allowing Python programmers to use Spark\u2019s large-scale data processing capabilities. Apache Spark is a unified analytics engine for large-scale data processing, originally written in Scala. PySpark provides a similar interface to Spark but allows for Python programming syntax and libraries. What's different between RDD, DataFrame, and Dataset in PySpark. RDD (Resilient Distributed Dataset) is the fundamental data structure of Spark, representing an immutable, distributed collection of objects that can be processed in parallel. DataFrame is a distributed collection of data organized into named columns, similar to a table in a relational database but with richer optimizations under the hood. Dataset is a type-safe version of DataFrame available in Scala and Java, offering the benefits of RDDs with the optimization benefits of DataFrames. How do you create a SparkSession in PySpark? You can create a SparkSession using the SparkSession.builder method, often initializing it with configurations such as appName to name your application and master to specify the cluster manager. For example: spark = SparkSession.builder.appName(\"MyApp\").getOrCreate() . What are the advantages of using PySpark over traditional Python libraries like Pandas? PySpark provides distributed data processing capabilities, allowing for processing of large datasets that do not fit into memory on a single machine. It offers high-level APIs and supports complex ETL operations, real-time processing, and machine learning, unlike Pandas, which is limited by the memory of a single machine. What do you understand by lazy evaluation in PySpark. Lazy evaluation in PySpark means that execution will not start until an action is performed. Transformations in PySpark are lazy, meaning they define a series of operations on data but do not compute anything until the user calls an action. This allows Spark to optimize the execution plan for efficiency. How can you read a CSV in PySpark? To read a CSV file using PySpark, you can use the spark.read.csv method, specifying the path to the CSV file. Options can be set for things like delimiter, header presence, and schema inference. For example: df = spark.read.csv(\"path/to/csv\", header=True, inferSchema=True) . Explain the actions and transformations in PySpark with examples. Transformations in PySpark create new RDDs, DataFrames, or Datasets from existing ones and are lazily evaluated. Examples include map , filter , and groupBy . Actions, on the other hand, trigger computation and return results. Examples include count , collect , and show . For instance, rdd.filter(lambda x: x > 10) is a transformation, while rdd.count() is an action What are the various ways to select columns in a PySpark DataFrame? Columns in a PySpark DataFrame can be selected using the select method by specifying column names directly or using the df[\"column_name\"] syntax. You can also use SQL expressions with the selectExpr method. How do you handle missing or null values in PySpark DataFrames? Missing or null values in PySpark DataFrames can be handled using methods like fillna to replace nulls with specified values, drop to remove rows with null values, or na.drop() and na.fill() for more nuanced control. Explain the difference between map() and flatMap() functions in PySpark. The map() function applies a function to each element of an RDD, returning a new RDD with the results. flatMap() , on the other hand, applies a function to each element and then flattens the results into a new RDD. Essentially, map() returns elements one-to-one, while flatMap() can return 0 or more elements for each input. How do you perform joins in PySpark DataFrames? Joins in PySpark DataFrames are performed using the join method, specifying another DataFrame to join with, the key or condition to join on, and the type of join (e.g., inner, outer, left, right). Explain the significance of caching in PySpark and how it's implemented. Caching in PySpark is significant for optimization, allowing intermediate results to be stored in memory for faster access in subsequent actions. It's implemented using the cache() or persist() methods on RDDs or DataFrames, which store the data in memory or more persistent storage levels. What are User Defined Functions (UDFs) in PySpark, and when would you use them? UDFs in PySpark allow you to extend the built-in functions by defining custom functions in Python, which can then be used in DataFrame operations. They are useful for applying complex transformations or business logic that are not covered by Spark\u2019s built-in functions. How do you aggregate data in PySpark? Data in PySpark can be aggregated using methods like groupBy followed by aggregation functions such as count , sum , avg , etc. For example, df.groupBy(\"column_name\").count() would count the number of rows for each unique value in the specified column. Explain window functions and their usage in PySpark. Window functions in PySpark operate on a group of rows (a window) while returning a value for each row in the dataset. They are useful for operations like running totals, moving averages, and ranking without having to group the dataset. They are defined using the Window class and applied with functions like rank , row_number , etc. What strategies would you employ for optimizing PySpark jobs? Strategies for optimizing PySpark jobs include broadcasting large lookup tables, partitioning data effectively, caching intermediate results, minimizing shuffles, and using efficient serialization formats. Adjusting Spark configurations to match the job's needs can also improve performance. How does partitioning impact performance in PySpark? Proper partitioning in PySpark can significantly impact performance by ensuring that data is distributed evenly across nodes, reducing shuffles and improving parallel processing efficiency. Poor partitioning can lead to data skew and bottlenecks. Explain broadcast variables and their role in PySpark optimization. Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They are used to optimize performance in PySpark, especially when you have a large dataset that needs to be used across multiple nodes. How do you handle skewed data in PySpark? Handling skewed data in PySpark can involve strategies such as salting keys to distribute the data more evenly, repartitioning or coalescing, and custom partitioning schemes to avoid data skew and ensure balanced workload across nodes. Discuss the concept of accumulators in PySpark. Accumulators in PySpark are variables that are only \u201cadded\u201d to through an associative and commutative operation and can be used to implement counters or sums. PySpark ensures they are updated correctly across tasks. How do you handle streaming in PySpark? Working with structured streaming involves defining a DataStreamReader or DataStreamWriter with a schema, reading streaming data from various sources (like Kafka, sockets, or files), applying transformations, and then writing the output to a sink (like a file system, console, or memory). How can you handle schema evolution in PySpark? Schema evolution in PySpark can be handled by using options like mergeSchema in data sources that support schema merging (e.g., Parquet). It allows for the automatic merging of differing schemas in data files over time, accommodating the addition of new columns or changes in data types. Explain the difference between persist() and cache() in PySpark. Both persist() and cache() in PySpark are used to store the computation results of an RDD, DataFrame, or Dataset so they can be reused in subsequent actions. The difference is that persist() allows the user to specify the storage level (memory, disk, etc.), whereas cache() uses the default storage level (MEMORY_ONLY). How do you work with nested JSON data in PySpark? Working with nested JSON data in PySpark involves reading the JSON file into a DataFrame and then using functions like explode to flatten nested structures or select and col for accessing nested fields. PySpark's built-in functions for dealing with complex data types are also useful here. What is the purpose of the PySpark MLlib library? The purpose of the PySpark MLlib library is to provide machine learning algorithms and utilities for classification, regression, clustering, collaborative filtering, dimensionality reduction, and underlying optimization primitives. It allows for scalable and efficient execution of ML tasks on big data. How do you integrate PySpark with other Python libraries like NumPy and Pandas? Integration of PySpark with other Python libraries like NumPy and Pandas can be achieved through the use of PySpark's ability to convert DataFrames to and from Pandas DataFrames ( toPandas and createDataFrame methods) and by using UDFs to apply functions that utilize these libraries on Spark DataFrames. Explain the process of deploying PySpark applications in a cluster. Deploying PySpark applications in a cluster involves packaging your application's code and dependencies, submitting the job to a cluster manager (like Spark Standalone, YARN, or Mesos) using the spark-submit script, and specifying configurations such as the number of executors, memory per executor, and the application's entry point. What are the best practices for writing efficient PySpark code? Best practices include using DataFrames for better optimization, avoiding UDFs when built-in functions are available, minimizing data shuffles, broadcasting large reference datasets, efficient data partitioning, and leveraging Spark's built-in functions for complex operations. How do you handle memory-related issues in PySpark? Handling memory-related issues involves optimizing Spark configurations such as executor memory, driver memory, and memory overhead. Tuning the size and number of partitions, avoiding large broadcast variables, and using disk storage when necessary can also help. Explain the significance of the Catalyst optimizer in PySpark. The Catalyst optimizer is a key component of Spark SQL that improves the performance of SQL and DataFrame queries. It optimizes query execution by analyzing query plans and applying optimization rules, such as predicate pushdown and constant folding, to generate an efficient physical plan. What are some common errors you've encountered while working with PySpark, and how did you resolve them? Common errors include out-of-memory errors, task serialization issues, and data skew. Resolving these issues typically involves tuning Spark configurations, ensuring efficient data partitioning, and applying strategies to handle large datasets and skewed data. How do you debug PySpark applications effectively? Effective debugging of PySpark applications involves checking Spark UI for detailed information on job execution, stages, and tasks, logging information at key points in the application, and using local mode for debugging simpler versions of the code. Explain the streaming capabilities of PySpark. PySpark supports structured streaming, a high-level API for stream processing that allows users to express streaming computations the same way they would express batch computations on static data. It supports event-time processing, window functions, and stateful operations. Can you explain model evaluation and hyperparameter tuning in PySpark. Model evaluation and hyperparameter tuning in PySpark can be performed using the MLlib library, which offers tools like CrossValidator for cross-validation and ParamGridBuilder for building a grid of parameters to search over. Evaluation metrics are available for assessing model performance. Name some common methods or tools do you use for testing PySpark code? Testing PySpark code can involve using the pyspark.sql.functions.col for column operations, the DataFrame API for data manipulation, and third-party libraries like PyTest for writing test cases. Mocking data and simulating Spark behavior in a local environment are also common practices. How do you ensure data quality and consistency in PySpark pipelines? Ensuring data quality and consistency involves implementing validation checks, using schema enforcement on DataFrames, employing data profiling and cleansing techniques, and maintaining data lineage and auditing processes. How do you perform machine learning tasks using PySpark MLlib? Performing machine learning tasks with PySpark MLlib involves using its DataFrame-based API for constructing ML pipelines, utilizing transformers and estimators for preprocessing and model training, and applying built-in algorithms for classification, regression, clustering, etc. How do you handle large-scale machine learning with PySpark? Handling large-scale machine learning involves leveraging the distributed computing capabilities of Spark and MLlib, using algorithms optimized for parallel processing, effectively partitioning data, and tuning Spark resources to balance the workload across the cluster. What are the challenges one faces while implementing machine learning algorithms using PySpark? Challenges include dealing with data skewness, selecting the right algorithms that scale efficiently, managing resource allocation in a distributed environment, ensuring data quality, and integrating with other systems for real-time predictions. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"PySpark Concepts I"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics/#some-common-pyspark-topics","text":"","title":"Some common Pyspark Topics"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics/#just-pyspark-vs-real-spark","text":"Here, I'll try to clear up the often-muddled area between PySpark and Full Spark installations. We will also touch upon different types of python installations available.","title":"Just PySpark vs Real Spark"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics/#standalone-python-vs-anaconda-python","text":"","title":"Standalone Python vs. Anaconda Python"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics/#standalone-python","text":"Thi is the python you directly install from Python Software Foundation . Choose this for a lightweight setup, specific version control, and when using Python for general-purpose programming.","title":"Standalone Python"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics/#anaconda-python","text":"An open-source Python distribution for scientific computing and data science.Go for Anaconda for an easy-to-manage data science environment, especially when dealing with large datasets, machine learning, and analytics.","title":"Anaconda Python"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics/#pyspark-vs-full-apache-spark-installation","text":"","title":"PySpark vs full Apache Spark Installation"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics/#pyspark-via-pip","text":"Many believe pip install pyspark installs the entire Apache Spark framework . No, it does not. When you install PySpark via pip, it installs the Python interface plus a minimal, standalone version of Apache Spark that can run locally on your machine. This standalone version of Spark is what allows the simulation of a Spark cluster environment on your single computer. Here's a breakdown: Apache Spark in PySpark : The PySpark package installed via pip includes a lightweight, standalone Spark installation. This isn't the full-fledged, distributed Spark system typically used in large-scale setups but a minimal version that can run on a single machine. When you execute PySpark code after installing it via pip, you're actually running this local version of Spark. Local Mode Execution : In this \"local mode,\" Spark operates as if it's a cluster but is actually just using the resources (like CPU and memory) of your single machine. It simulates the behavior of a Spark cluster, which in a full setup would distribute processing tasks across multiple nodes (machines). This mode is incredibly useful for development, testing, and learning because it lets you write and test code that would normally run on a Spark cluster, without the need for setting up multiple machines. The Spark Context : When your PySpark code runs, it initializes a \"Spark context\" within your Python script. This context is the primary connection to the Spark execution environment and allows your Python script to access Spark's functionalities. In the pip-installed PySpark environment, this Spark context talks to the local Spark instance included in the PySpark package, not a remote or distributed cluster.","title":"PySpark via pip"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics/#full-apache-spark-installation","text":"Full Spark Installation involves setting up the complete Apache Spark framework, for building large-scale data processing applications, beyond the scope of PySpark alone. This is necessary for production-grade, large-scale data processing and when you need to harness the full power of Spark's distributed computing capabilities.","title":"Full Apache Spark Installation"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics/#conclusion","text":"To sum it up, pip install pyspark actually installs both the Python interface to Spark (PySpark) and a minimal, local-only version of Apache Spark itself. This setup allows you to run Spark jobs as if you had a Spark cluster, but it's all happening within your own computer. The local mode is a simulation of a distributed Spark environment, suitable for learning, development, and processing small data sets.","title":"Conclusion"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics/#finding-spark","text":"When you install Spark using a package manager like pip , it places the Spark binaries and libraries inside the Python's dist-packages or site-packages directory. This is a different installation approach than downloading and extracting Spark directly from the Apache Spark website. Here's what you need to know: SPARK_HOME for pip-installed PySpark : If you've installed Spark via pip , then the equivalent SPARK_HOME would be /usr/local/lib/python3.8/dist-packages/pyspark/ . This directory would contain all the necessary libraries and dependencies for Spark to run within a Python environment. Configuration and JARs : Configuration files, JARs, and other related resources would also be located within this directory structure. For instance, if you're looking for the location to place additional JAR files, it would likely be somewhere like /usr/local/lib/python3.8/dist-packages/pyspark/jars/ . Using spark-submit : The spark-submit script in this directory is tailored for the pip-installed Spark and ensures that it picks up the correct libraries and configurations from this location. In summary, if you've installed Spark using pip within your Docker container, then the /usr/local/lib/python3.8/dist-packages/pyspark/ directory is your primary point of reference for Spark configurations, binaries, and libraries. This is distinct from a standalone Spark installation, which would typically reside directly under /usr/local/ or another directory, separate from the Python packages directory. What if echo $SPARK_HOME or %SPARK_HOME% returns nothing and you are not sure where spark is installed? Even though you instaleld it? Let's try a different approach. Spark's binaries (like spark-submit and spark-shell ) are often a good hint as to where Spark might be installed. Search for Spark binaries : Execute the following command within your Docker container: bash find / -name \"spark-submit\" 2>/dev/null This command searches for the spark-submit binary, which is a common Spark binary. The directory containing this binary is likely your Spark installation directory. Once you've located the directory containing spark-submit , you'll typically find Spark's home directory one level up. For instance, if spark-submit is found in /path/to/spark/bin/spark-submit , then /path/to/spark is likely your SPARK_HOME . In my system it returned bash /usr/local/bin/spark-submit /usr/local/lib/python3.8/dist-packages/pyspark/bin/spark-submit How do I know where I have installed all the libraries or which is actually the right spark home? The path /usr/local/lib/python3.8/dist-packages/pyspark/bin/spark-submit suggests that this is the location where PySpark was installed as a Python package, likely via pip or a similar package manager. When you install Spark using a package manager like pip , it places the Spark binaries and libraries inside the Python's dist-packages or site-packages directory. This is a different installation approach than downloading and extracting Spark directly from the Apache Spark website. Here's what you need to know: SPARK_HOME for pip-installed PySpark : If you've installed Spark via pip , then the equivalent SPARK_HOME would be /usr/local/lib/python3.8/dist-packages/pyspark/ . This directory would contain all the necessary libraries and dependencies for Spark to run within a Python environment. Configuration and JARs : Configuration files, JARs, and other related resources would also be located within this directory structure. For instance, if you're looking for the location to place additional JAR files, it would likely be somewhere like /usr/local/lib/python3.8/dist-packages/pyspark/jars/ . Using spark-submit : The spark-submit script in this directory is tailored for the pip-installed Spark and ensures that it picks up the correct libraries and configurations from this location. In summary, if you've installed Spark using pip within your Docker container, then the /usr/local/lib/python3.8/dist-packages/pyspark/ directory is your primary point of reference for Spark configurations, binaries, and libraries. This is distinct from a standalone Spark installation, which would typically reside directly under /usr/local/ or another directory, separate from the Python packages directory.","title":"Finding Spark"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics/#frequently-asked-pyspark-questions","text":"In this section, I'll go over some common PySpark questions and their answers. These are basic questions that anyone using PySpark should be familiar with. So, let's get started :-) What is PySpark, how is different from Apache Spark? PySpark is the Python API for Apache Spark, allowing Python programmers to use Spark\u2019s large-scale data processing capabilities. Apache Spark is a unified analytics engine for large-scale data processing, originally written in Scala. PySpark provides a similar interface to Spark but allows for Python programming syntax and libraries. What's different between RDD, DataFrame, and Dataset in PySpark. RDD (Resilient Distributed Dataset) is the fundamental data structure of Spark, representing an immutable, distributed collection of objects that can be processed in parallel. DataFrame is a distributed collection of data organized into named columns, similar to a table in a relational database but with richer optimizations under the hood. Dataset is a type-safe version of DataFrame available in Scala and Java, offering the benefits of RDDs with the optimization benefits of DataFrames. How do you create a SparkSession in PySpark? You can create a SparkSession using the SparkSession.builder method, often initializing it with configurations such as appName to name your application and master to specify the cluster manager. For example: spark = SparkSession.builder.appName(\"MyApp\").getOrCreate() . What are the advantages of using PySpark over traditional Python libraries like Pandas? PySpark provides distributed data processing capabilities, allowing for processing of large datasets that do not fit into memory on a single machine. It offers high-level APIs and supports complex ETL operations, real-time processing, and machine learning, unlike Pandas, which is limited by the memory of a single machine. What do you understand by lazy evaluation in PySpark. Lazy evaluation in PySpark means that execution will not start until an action is performed. Transformations in PySpark are lazy, meaning they define a series of operations on data but do not compute anything until the user calls an action. This allows Spark to optimize the execution plan for efficiency. How can you read a CSV in PySpark? To read a CSV file using PySpark, you can use the spark.read.csv method, specifying the path to the CSV file. Options can be set for things like delimiter, header presence, and schema inference. For example: df = spark.read.csv(\"path/to/csv\", header=True, inferSchema=True) . Explain the actions and transformations in PySpark with examples. Transformations in PySpark create new RDDs, DataFrames, or Datasets from existing ones and are lazily evaluated. Examples include map , filter , and groupBy . Actions, on the other hand, trigger computation and return results. Examples include count , collect , and show . For instance, rdd.filter(lambda x: x > 10) is a transformation, while rdd.count() is an action What are the various ways to select columns in a PySpark DataFrame? Columns in a PySpark DataFrame can be selected using the select method by specifying column names directly or using the df[\"column_name\"] syntax. You can also use SQL expressions with the selectExpr method. How do you handle missing or null values in PySpark DataFrames? Missing or null values in PySpark DataFrames can be handled using methods like fillna to replace nulls with specified values, drop to remove rows with null values, or na.drop() and na.fill() for more nuanced control. Explain the difference between map() and flatMap() functions in PySpark. The map() function applies a function to each element of an RDD, returning a new RDD with the results. flatMap() , on the other hand, applies a function to each element and then flattens the results into a new RDD. Essentially, map() returns elements one-to-one, while flatMap() can return 0 or more elements for each input. How do you perform joins in PySpark DataFrames? Joins in PySpark DataFrames are performed using the join method, specifying another DataFrame to join with, the key or condition to join on, and the type of join (e.g., inner, outer, left, right). Explain the significance of caching in PySpark and how it's implemented. Caching in PySpark is significant for optimization, allowing intermediate results to be stored in memory for faster access in subsequent actions. It's implemented using the cache() or persist() methods on RDDs or DataFrames, which store the data in memory or more persistent storage levels. What are User Defined Functions (UDFs) in PySpark, and when would you use them? UDFs in PySpark allow you to extend the built-in functions by defining custom functions in Python, which can then be used in DataFrame operations. They are useful for applying complex transformations or business logic that are not covered by Spark\u2019s built-in functions. How do you aggregate data in PySpark? Data in PySpark can be aggregated using methods like groupBy followed by aggregation functions such as count , sum , avg , etc. For example, df.groupBy(\"column_name\").count() would count the number of rows for each unique value in the specified column. Explain window functions and their usage in PySpark. Window functions in PySpark operate on a group of rows (a window) while returning a value for each row in the dataset. They are useful for operations like running totals, moving averages, and ranking without having to group the dataset. They are defined using the Window class and applied with functions like rank , row_number , etc. What strategies would you employ for optimizing PySpark jobs? Strategies for optimizing PySpark jobs include broadcasting large lookup tables, partitioning data effectively, caching intermediate results, minimizing shuffles, and using efficient serialization formats. Adjusting Spark configurations to match the job's needs can also improve performance. How does partitioning impact performance in PySpark? Proper partitioning in PySpark can significantly impact performance by ensuring that data is distributed evenly across nodes, reducing shuffles and improving parallel processing efficiency. Poor partitioning can lead to data skew and bottlenecks. Explain broadcast variables and their role in PySpark optimization. Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They are used to optimize performance in PySpark, especially when you have a large dataset that needs to be used across multiple nodes. How do you handle skewed data in PySpark? Handling skewed data in PySpark can involve strategies such as salting keys to distribute the data more evenly, repartitioning or coalescing, and custom partitioning schemes to avoid data skew and ensure balanced workload across nodes. Discuss the concept of accumulators in PySpark. Accumulators in PySpark are variables that are only \u201cadded\u201d to through an associative and commutative operation and can be used to implement counters or sums. PySpark ensures they are updated correctly across tasks. How do you handle streaming in PySpark? Working with structured streaming involves defining a DataStreamReader or DataStreamWriter with a schema, reading streaming data from various sources (like Kafka, sockets, or files), applying transformations, and then writing the output to a sink (like a file system, console, or memory). How can you handle schema evolution in PySpark? Schema evolution in PySpark can be handled by using options like mergeSchema in data sources that support schema merging (e.g., Parquet). It allows for the automatic merging of differing schemas in data files over time, accommodating the addition of new columns or changes in data types. Explain the difference between persist() and cache() in PySpark. Both persist() and cache() in PySpark are used to store the computation results of an RDD, DataFrame, or Dataset so they can be reused in subsequent actions. The difference is that persist() allows the user to specify the storage level (memory, disk, etc.), whereas cache() uses the default storage level (MEMORY_ONLY). How do you work with nested JSON data in PySpark? Working with nested JSON data in PySpark involves reading the JSON file into a DataFrame and then using functions like explode to flatten nested structures or select and col for accessing nested fields. PySpark's built-in functions for dealing with complex data types are also useful here. What is the purpose of the PySpark MLlib library? The purpose of the PySpark MLlib library is to provide machine learning algorithms and utilities for classification, regression, clustering, collaborative filtering, dimensionality reduction, and underlying optimization primitives. It allows for scalable and efficient execution of ML tasks on big data. How do you integrate PySpark with other Python libraries like NumPy and Pandas? Integration of PySpark with other Python libraries like NumPy and Pandas can be achieved through the use of PySpark's ability to convert DataFrames to and from Pandas DataFrames ( toPandas and createDataFrame methods) and by using UDFs to apply functions that utilize these libraries on Spark DataFrames. Explain the process of deploying PySpark applications in a cluster. Deploying PySpark applications in a cluster involves packaging your application's code and dependencies, submitting the job to a cluster manager (like Spark Standalone, YARN, or Mesos) using the spark-submit script, and specifying configurations such as the number of executors, memory per executor, and the application's entry point. What are the best practices for writing efficient PySpark code? Best practices include using DataFrames for better optimization, avoiding UDFs when built-in functions are available, minimizing data shuffles, broadcasting large reference datasets, efficient data partitioning, and leveraging Spark's built-in functions for complex operations. How do you handle memory-related issues in PySpark? Handling memory-related issues involves optimizing Spark configurations such as executor memory, driver memory, and memory overhead. Tuning the size and number of partitions, avoiding large broadcast variables, and using disk storage when necessary can also help. Explain the significance of the Catalyst optimizer in PySpark. The Catalyst optimizer is a key component of Spark SQL that improves the performance of SQL and DataFrame queries. It optimizes query execution by analyzing query plans and applying optimization rules, such as predicate pushdown and constant folding, to generate an efficient physical plan. What are some common errors you've encountered while working with PySpark, and how did you resolve them? Common errors include out-of-memory errors, task serialization issues, and data skew. Resolving these issues typically involves tuning Spark configurations, ensuring efficient data partitioning, and applying strategies to handle large datasets and skewed data. How do you debug PySpark applications effectively? Effective debugging of PySpark applications involves checking Spark UI for detailed information on job execution, stages, and tasks, logging information at key points in the application, and using local mode for debugging simpler versions of the code. Explain the streaming capabilities of PySpark. PySpark supports structured streaming, a high-level API for stream processing that allows users to express streaming computations the same way they would express batch computations on static data. It supports event-time processing, window functions, and stateful operations. Can you explain model evaluation and hyperparameter tuning in PySpark. Model evaluation and hyperparameter tuning in PySpark can be performed using the MLlib library, which offers tools like CrossValidator for cross-validation and ParamGridBuilder for building a grid of parameters to search over. Evaluation metrics are available for assessing model performance. Name some common methods or tools do you use for testing PySpark code? Testing PySpark code can involve using the pyspark.sql.functions.col for column operations, the DataFrame API for data manipulation, and third-party libraries like PyTest for writing test cases. Mocking data and simulating Spark behavior in a local environment are also common practices. How do you ensure data quality and consistency in PySpark pipelines? Ensuring data quality and consistency involves implementing validation checks, using schema enforcement on DataFrames, employing data profiling and cleansing techniques, and maintaining data lineage and auditing processes. How do you perform machine learning tasks using PySpark MLlib? Performing machine learning tasks with PySpark MLlib involves using its DataFrame-based API for constructing ML pipelines, utilizing transformers and estimators for preprocessing and model training, and applying built-in algorithms for classification, regression, clustering, etc. How do you handle large-scale machine learning with PySpark? Handling large-scale machine learning involves leveraging the distributed computing capabilities of Spark and MLlib, using algorithms optimized for parallel processing, effectively partitioning data, and tuning Spark resources to balance the workload across the cluster. What are the challenges one faces while implementing machine learning algorithms using PySpark? Challenges include dealing with data skewness, selecting the right algorithms that scale efficiently, managing resource allocation in a distributed environment, ensuring data quality, and integrating with other systems for real-time predictions. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Frequently asked Pyspark questions"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive/","text":"Creating Delta Tables in Hive with Spark Overview This article will explain two things: - How to create tables in Hive. - How to create Delta tables in Hive. The Setup: Spark : You can run this example on any Spark setup\u2014Spark clusters or PySpark. There's no special Spark setup required, meaning the code will work on any basic Spark environment. However, there is one important condition: the Spark version must be 3.2 . Don\u2019t use newer versions like 3.4 or 3.5, as they can cause compatibility issues. Also, the Delta package must be delta-core_2.12:1.2.1 . After many trials, this combination works smoothly. Hive : The Hive Metastore can be accessed using the address thrift://metastore:9083 . You need to make sure Spark can connect to this location. Apart from this, there's nothing else to configure. The Code: from pyspark.sql import SparkSession # Create a Spark session and load Delta Lake dependencies dynamically from Maven spark = SparkSession.builder \\ .appName(\"Spark Hive Delta Integration\") \\ .config(\"spark.sql.catalogImplementation\", \"hive\") \\ .config(\"hive.metastore.uris\", \"thrift://metastore:9083\") \\ .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\ .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\ .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.1\") \\ .enableHiveSupport() \\ .getOrCreate() # Sample data for Delta table data = [(\"Rambo\", 30), (\"Titanic\", 25)] df = spark.createDataFrame(data, [\"name\", \"release\"]) # Create a Delta table in Hive df.write.format(\"delta\").saveAsTable(\"default.movies_delta\") The Output: Showing the Table Data: # Read Delta table and show schema df = spark.read.format(\"delta\").table(\"default.movies_delta\") df.printSchema() Showing the History (A Delta Feature): from delta.tables import * # Load Delta table delta_table = DeltaTable.forName(spark, \"default.movies_delta\") # Show the history of the Delta table delta_table.history().show() Here is how the history will look: If You Don't Want to Use Hive: If you don\u2019t want to use Hive and just want to save data to any location, you can also do that. Here\u2019s the code: spark = SparkSession.builder \\ .appName(\"Spark Delta Integration\") \\ .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\ .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\ .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.1\") \\ .getOrCreate() # Sample data for Delta table data = [(\"Rambo\", 30), (\"Titanic\", 25)] df = spark.createDataFrame(data, [\"name\", \"release\"]) # Write the DataFrame to a Delta table df.write.format(\"delta\").save(\"/tmp/delta-table\") # Read back the Delta table delta_df = spark.read.format(\"delta\").load(\"/tmp/delta-table\") delta_df.show() Errors Encountered Here are some errors you might face due to incompatibility between Delta and Spark versions: Py4JJavaError: An error occurred while calling o79.save. java.lang.NoClassDefFoundError: org/apache/spark/sql/execution/datasources/FileFormatWriter$Empty2Null java.lang.NoSuchMethodError: 'org.apache.spark.sql.catalyst.TableIdentifier org.apache.spark.sql.catalyst.TableIdentifier.copy(java.lang.String, scala.Option)' Warning You might see a warning like this: WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table default.movies_delta into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive. Reason: Hive can handle many table types, but Delta Lake (a storage format by Databricks) is not supported by Hive directly. Spark uses its own format to save Delta tables, which doesn\u2019t match Hive\u2019s SerDe (Serializer/Deserializer). This warning comes up because Hive can\u2019t read Delta tables directly unless special Delta support is added or managed via Spark. Solution: Since Delta tables are built for Spark, the simplest way is to keep using Spark to query your Delta tables. Don\u2019t register them in Hive\u2019s metastore. Instead, manage them directly with Spark to avoid Hive trying to read the Delta format. Note: Hive doesn\u2019t natively support Delta, but you can use Delta Lake open-source libraries to connect Hive and Delta. However, this setup is complicated and not very reliable. JARs Download When you use the following configuration: .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.1\") It automatically downloads the necessary JARs from these locations: Delta Core 2.12 JAR Delta Storage JAR ANTLR Runtime JAR Seeing the table using beeline From any server, not hiveserver2, connect to the beeling in hiveserver2 using this command: beeline -u jdbc:hive2://hiveserver2:10000/default Issue a command DESCRIBE FORMATTED default.movies_delta; YOu will see an output like this:","title":"Spark-Hive-Delta Connection"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive/#creating-delta-tables-in-hive-with-spark","text":"","title":"Creating Delta Tables in Hive with Spark"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive/#overview","text":"This article will explain two things: - How to create tables in Hive. - How to create Delta tables in Hive.","title":"Overview"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive/#the-setup","text":"Spark : You can run this example on any Spark setup\u2014Spark clusters or PySpark. There's no special Spark setup required, meaning the code will work on any basic Spark environment. However, there is one important condition: the Spark version must be 3.2 . Don\u2019t use newer versions like 3.4 or 3.5, as they can cause compatibility issues. Also, the Delta package must be delta-core_2.12:1.2.1 . After many trials, this combination works smoothly. Hive : The Hive Metastore can be accessed using the address thrift://metastore:9083 . You need to make sure Spark can connect to this location. Apart from this, there's nothing else to configure.","title":"The Setup:"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive/#the-code","text":"from pyspark.sql import SparkSession # Create a Spark session and load Delta Lake dependencies dynamically from Maven spark = SparkSession.builder \\ .appName(\"Spark Hive Delta Integration\") \\ .config(\"spark.sql.catalogImplementation\", \"hive\") \\ .config(\"hive.metastore.uris\", \"thrift://metastore:9083\") \\ .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\ .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\ .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.1\") \\ .enableHiveSupport() \\ .getOrCreate() # Sample data for Delta table data = [(\"Rambo\", 30), (\"Titanic\", 25)] df = spark.createDataFrame(data, [\"name\", \"release\"]) # Create a Delta table in Hive df.write.format(\"delta\").saveAsTable(\"default.movies_delta\")","title":"The Code:"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive/#the-output","text":"","title":"The Output:"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive/#showing-the-table-data","text":"# Read Delta table and show schema df = spark.read.format(\"delta\").table(\"default.movies_delta\") df.printSchema()","title":"Showing the Table Data:"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive/#showing-the-history-a-delta-feature","text":"from delta.tables import * # Load Delta table delta_table = DeltaTable.forName(spark, \"default.movies_delta\") # Show the history of the Delta table delta_table.history().show() Here is how the history will look:","title":"Showing the History (A Delta Feature):"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive/#if-you-dont-want-to-use-hive","text":"If you don\u2019t want to use Hive and just want to save data to any location, you can also do that. Here\u2019s the code: spark = SparkSession.builder \\ .appName(\"Spark Delta Integration\") \\ .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\ .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\ .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.1\") \\ .getOrCreate() # Sample data for Delta table data = [(\"Rambo\", 30), (\"Titanic\", 25)] df = spark.createDataFrame(data, [\"name\", \"release\"]) # Write the DataFrame to a Delta table df.write.format(\"delta\").save(\"/tmp/delta-table\") # Read back the Delta table delta_df = spark.read.format(\"delta\").load(\"/tmp/delta-table\") delta_df.show()","title":"If You Don't Want to Use Hive:"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive/#errors-encountered","text":"Here are some errors you might face due to incompatibility between Delta and Spark versions: Py4JJavaError: An error occurred while calling o79.save. java.lang.NoClassDefFoundError: org/apache/spark/sql/execution/datasources/FileFormatWriter$Empty2Null java.lang.NoSuchMethodError: 'org.apache.spark.sql.catalyst.TableIdentifier org.apache.spark.sql.catalyst.TableIdentifier.copy(java.lang.String, scala.Option)' Warning You might see a warning like this: WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table default.movies_delta into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive. Reason: Hive can handle many table types, but Delta Lake (a storage format by Databricks) is not supported by Hive directly. Spark uses its own format to save Delta tables, which doesn\u2019t match Hive\u2019s SerDe (Serializer/Deserializer). This warning comes up because Hive can\u2019t read Delta tables directly unless special Delta support is added or managed via Spark. Solution: Since Delta tables are built for Spark, the simplest way is to keep using Spark to query your Delta tables. Don\u2019t register them in Hive\u2019s metastore. Instead, manage them directly with Spark to avoid Hive trying to read the Delta format. Note: Hive doesn\u2019t natively support Delta, but you can use Delta Lake open-source libraries to connect Hive and Delta. However, this setup is complicated and not very reliable.","title":"Errors Encountered"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive/#jars-download","text":"When you use the following configuration: .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.1\") It automatically downloads the necessary JARs from these locations: Delta Core 2.12 JAR Delta Storage JAR ANTLR Runtime JAR","title":"JARs Download"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive/#seeing-the-table-using-beeline","text":"From any server, not hiveserver2, connect to the beeling in hiveserver2 using this command: beeline -u jdbc:hive2://hiveserver2:10000/default Issue a command DESCRIBE FORMATTED default.movies_delta; YOu will see an output like this:","title":"Seeing the table using beeline"},{"location":"Spark-DataBricks/1.0_Spark/1.1_NarrowVsWideTransformation/","text":"Background In Spark, transformations are classified into two types: narrow and wide . Narrow Transformations Definition : Each input partition contributes to only one output partition. Example Operations : map() , filter() , select() Characteristics : No data shuffling across the network. Fast and efficient. Data can be processed in parallel without dependencies on other partitions. Remember: When we say \"each input partition contributes to only one output partition,\" we mean that in a narrow transformation, the data from an input partition is processed and stored in exactly one corresponding output partition without the need to shuffle or move data between different partitions. Example Consider a simple filter() operation that filters out rows based on some condition: # Assume we have an RDD with 4 partitions rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8], 4) # Apply a filter transformation filtered_rdd = rdd.filter(lambda x: x % 2 == 0) Input Partitions : [ [1, 2], [3, 4], [5, 6], [7, 8] ] Filter Condition : Keep only even numbers Output Partitions : Partition 1 (Input: [1, 2]) \u2192 Partition 1 (Output: [2]) Partition 2 (Input: [3, 4]) \u2192 Partition 2 (Output: [4]) Partition 3 (Input: [5, 6]) \u2192 Partition 3 (Output: [6]) Partition 4 (Input: [7, 8]) \u2192 Partition 4 (Output: [8]) Each input partition (e.g., [1, 2]) is processed and produces output that remains within the same partition (e.g., [2]). There is no data movement between partitions. Wide Transformations Definition : Each input partition contributes to multiple output partitions. Example Operations : reduceByKey() , groupBy() , join() Characteristics : Data is shuffled across the network. Slower compared to narrow transformations due to data movement. Requires synchronization between partitions. Example Now, consider a reduceByKey() operation which involves shuffling: # Assume we have an RDD with key-value pairs rdd = sc.parallelize([(1, 2), (3, 4), (3, 6), (1, 4)], 2) # Apply a reduceByKey transformation reduced_rdd = rdd.reduceByKey(lambda x, y: x + y) Input Partitions : [ [(1, 2), (3, 4)], [(3, 6), (1, 4)] ] Reduce by Key : Combine values with the same key Output Partitions : Partition 1 (Input: [(1, 2), (1, 4)]) \u2192 Partition 1 (Output: [(1, 6)]) Partition 2 (Input: [(3, 4), (3, 6)]) \u2192 Partition 2 (Output: [(3, 10)]) In this case, keys with the same value (e.g., key = 1 or key = 3) need to be brought together into the same partition to perform the reduction. This requires shuffling data across the network, making it a wide transformation. Key Differences Data Movement: Narrow transformations do not shuffle data, while wide transformations do. Performance: Narrow transformations are faster and more efficient because there's no data movement. Parallelism: Narrow transformations are easier to process in parallel, while wide transformations need to handle data dependencies and synchronization.","title":"Narrow_Vs_Wide_Transformation"},{"location":"Spark-DataBricks/1.0_Spark/1.1_NarrowVsWideTransformation/#background","text":"In Spark, transformations are classified into two types: narrow and wide .","title":"Background"},{"location":"Spark-DataBricks/1.0_Spark/1.1_NarrowVsWideTransformation/#narrow-transformations","text":"Definition : Each input partition contributes to only one output partition. Example Operations : map() , filter() , select() Characteristics : No data shuffling across the network. Fast and efficient. Data can be processed in parallel without dependencies on other partitions. Remember: When we say \"each input partition contributes to only one output partition,\" we mean that in a narrow transformation, the data from an input partition is processed and stored in exactly one corresponding output partition without the need to shuffle or move data between different partitions.","title":"Narrow Transformations"},{"location":"Spark-DataBricks/1.0_Spark/1.1_NarrowVsWideTransformation/#example","text":"Consider a simple filter() operation that filters out rows based on some condition: # Assume we have an RDD with 4 partitions rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8], 4) # Apply a filter transformation filtered_rdd = rdd.filter(lambda x: x % 2 == 0) Input Partitions : [ [1, 2], [3, 4], [5, 6], [7, 8] ] Filter Condition : Keep only even numbers Output Partitions : Partition 1 (Input: [1, 2]) \u2192 Partition 1 (Output: [2]) Partition 2 (Input: [3, 4]) \u2192 Partition 2 (Output: [4]) Partition 3 (Input: [5, 6]) \u2192 Partition 3 (Output: [6]) Partition 4 (Input: [7, 8]) \u2192 Partition 4 (Output: [8]) Each input partition (e.g., [1, 2]) is processed and produces output that remains within the same partition (e.g., [2]). There is no data movement between partitions.","title":"Example"},{"location":"Spark-DataBricks/1.0_Spark/1.1_NarrowVsWideTransformation/#wide-transformations","text":"Definition : Each input partition contributes to multiple output partitions. Example Operations : reduceByKey() , groupBy() , join() Characteristics : Data is shuffled across the network. Slower compared to narrow transformations due to data movement. Requires synchronization between partitions.","title":"Wide Transformations"},{"location":"Spark-DataBricks/1.0_Spark/1.1_NarrowVsWideTransformation/#example_1","text":"Now, consider a reduceByKey() operation which involves shuffling: # Assume we have an RDD with key-value pairs rdd = sc.parallelize([(1, 2), (3, 4), (3, 6), (1, 4)], 2) # Apply a reduceByKey transformation reduced_rdd = rdd.reduceByKey(lambda x, y: x + y) Input Partitions : [ [(1, 2), (3, 4)], [(3, 6), (1, 4)] ] Reduce by Key : Combine values with the same key Output Partitions : Partition 1 (Input: [(1, 2), (1, 4)]) \u2192 Partition 1 (Output: [(1, 6)]) Partition 2 (Input: [(3, 4), (3, 6)]) \u2192 Partition 2 (Output: [(3, 10)]) In this case, keys with the same value (e.g., key = 1 or key = 3) need to be brought together into the same partition to perform the reduction. This requires shuffling data across the network, making it a wide transformation.","title":"Example"},{"location":"Spark-DataBricks/1.0_Spark/1.1_NarrowVsWideTransformation/#key-differences","text":"Data Movement: Narrow transformations do not shuffle data, while wide transformations do. Performance: Narrow transformations are faster and more efficient because there's no data movement. Parallelism: Narrow transformations are easier to process in parallel, while wide transformations need to handle data dependencies and synchronization.","title":"Key Differences"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture/","text":"Spark Architecture Spark Components Spark Core: The main part of Spark with a core engine. Spark SQL: A SQL engine, but different from traditional databases. Here, data is processed mainly using DataFrames. Spark Streaming: This part allows Spark to process real-time data. Spark MLlib: A collection of machine learning libraries. GraphX: Used for graphs in reports, such as data collected from networks like Facebook. RDDs: Spark Core has RDDs (Resilient Distributed Datasets), which are the building blocks of Spark. Cluster & Nodes Nodes are individual machines (physical or virtual). Cluster is a group of nodes. Driver & Worker Driver Machine where Main() method runs. It contains the SparkContext object. Converts the application into stages using a DAG (Directed Acyclic Graph). Schedules tasks on worker nodes and collects the results. Should be close to worker nodes for better performance. Workers(now Executors) Workers are simply machines(Virtual/Real). These workers run JVM processes, called Executors. Multiple JVM Process(Executor) can be configured in a worker. Configuration: In your spark configuration you can set: --num-executors : Specifies the total number of executors to be launched for the application. --executor-cores : Specifies the number of cores (slots) to be used by each executor. --executor-memory : Specifies the amount of memory to be allocated to each executor. Example: With a worker machine having 16 CPU cores and 64 GB of memory, you can configure Spark to run either 4 executors (4 cores, 16 GB each) or 2 executors (8 cores, 32 GB each). Note: Executor is a JVM process running on a worker node that executes tasks. The Spark cluster manager (e.g., YARN, Mesos, or the standalone cluster manager) is responsible for allocating resources to executors. Slots = Spark cores = Synapse vCore \u2248 Threads Cores in Spark = vCores in Synapse = Slots in Databricks = Total threads = Total parallel tasks Note: Don't confuse cores with Intel/AMD CPU Ads. Cores in Spark means threads. Note: Spark supports one task for each virtual CPU (vCPU) core by default. For example, if an executor has four CPU cores, it can run four concurrent tasks. Note: Multiple threads can run on each core, but Spark typically uses one thread per core for each task to simplify execution and avoid the complexities of managing multiple threads on a single core. This Databricks spark cluster can run 32 tasks parallely: In Docker Compose, SPARK_WORKER_CORES sets worker threads (cores/slots). A cluster with 3 workers, each set to 2 cores, has 6 total threads. Executors, cores & memory for a 10 GB data Say, you have 10 GB of data to be processed. How can you calcultate the executors, cores and memory for such a secenairo? Step Description Calculation/Value Calculate number of partitions Default partition size: 128 MB 10 GB / 128 MB = 80 partitions Determine CPU cores needed One core per partition for maximum parallelism 80 cores Max cores per executor Cores per executor in YARN 5 cores per executor Calculate number of executors Total cores / Cores per executor 80 / 5 = 16 executors Partition size Default partition size: 128 MB 128 MB Memory per core Minimum memory per core (4x partition size) 128 MB * 4 = 512 MB Executor cores Cores per executor 5 cores Executor memory Memory per core * Number of cores per executor 512 MB * 5 = 2560 MB Each Executor Requires Cores 5 CPU cores Memory 2560 MB Application, Jobs, Stages, Tasks Applications -> jobs -> stages -> tasks . Term Definition Example Application An application in Spark is a complete program that runs on the Spark cluster. This program includes the user's code that uses Spark\u2019s API to perform data processing. A Spark application can be a Python script that processes data from a CSV file, performs transformations, and writes the results to a database. Job A job is triggered by an action (e.g., count() , collect() , saveAsTextFile() ) in a Spark application. Each action in the code triggers a new job. If your application has two actions, like counting the number of rows and saving the result to a file, it will trigger two jobs. Stages A job is divided into stages, where each stage is a set of tasks that can be executed in parallel. Stages are separated by shuffle operations. If a job involves filtering and then aggregating data, the filtering might be one stage, and the aggregation another, especially if a shuffle operation (like a group by) is required between them. Tasks A stage is further divided into tasks, where each task is a unit of work that operates on a partition of the data. Tasks are the smallest unit of execution in Spark. If a stage needs to process 100 partitions of data, it will have 100 tasks, with each task processing one partition. Let's put it all together Let's see an example to understand these concepts: Application: A Spark application that reads data from a CSV file, filters out certain rows, and then calculates the average of a column. Code snippet: python df1 = spark.read.csv(\"data.csv\") //Job1 df2 = df.filter(df[\"value\"] > 10) //Job2 average = df2.agg({\"value\": \"avg\"}).collect() Jobs: Job 1: Triggered by the action spark.read.csv() . This job reads the data from the CSV file. Job 2: Triggered by the action filtered_df.agg().collect() . This job includes filtering the data and then calculating the average. Stages in Job 2: Stage 1: Filtering the data. All tasks in this stage can run in parallel because filtering is a transformation that operates on individual partitions. Stage 2: Aggregating the data. This stage requires a shuffle because the aggregation (calculating the average) involves data from all partitions. Tasks: For each stage, Spark creates tasks based on the number of partitions. If there are 10 partitions, Stage 1 (filtering) will have 10 tasks, and Stage 2 (aggregation) will also have 10 tasks, each processing one partition of data. Transformation & Actions In PySpark, operations on data can be classified into two types: transformations and actions . Transformations Transformations are operations on RDDs that return a new RDD, meaning they create a new dataset from an existing one. Transformations are lazy , meaning they are computed only when an action is called. (e.g., map , filter ): Create a new RDD from an existing one. They are lazy and not executed until an action is called. Actions Actions trigger the execution of the transformations to return a result to the driver program or write it to storage. When an action is called, Spark's execution engine computes the result of the transformations. (e.g., collect , count ): Trigger the execution of the transformations and return a result. Example # Create an RDD from a list data = [1, 2, 3, 4, 5] rdd = spark.sparkContext.parallelize(data) # Transformation 1: Multiply each number by 2 rdd_transformed = rdd.map(lambda x: x * 2) # Transformation 2: Filter out even numbers rdd_filtered = rdd_transformed.filter(lambda x: x % 2 == 0) # Action: Collect the results result = rdd_filtered.collect() Common transformations and actions in PySpark Transformation Example API Description map rdd.map(lambda x: x * 2) Applies a function to each element in the RDD. filter rdd.filter(lambda x: x % 2 == 0) Returns a new RDD containing only elements that satisfy a predicate. flatMap rdd.flatMap(lambda x: (x, x * 2)) Similar to map , but each input item can be mapped to 0 or more output items (returns a flattened structure). mapPartitions rdd.mapPartitions(lambda iter: [sum(iter)]) Applies a function to each partition of the RDD. distinct rdd.distinct() Returns a new RDD containing the distinct elements. union rdd1.union(rdd2) Returns a new RDD containing the union of elements. intersection rdd1.intersection(rdd2) Returns a new RDD containing the intersection of elements. groupByKey rdd.groupByKey() Groups the values for each key in the RDD. reduceByKey rdd.reduceByKey(lambda a, b: a + b) Merges the values for each key using an associative function. sortBy rdd.sortBy(lambda x: x) Returns a new RDD sorted by the specified function. Action Example API Description collect rdd.collect() Returns all the elements of the RDD as a list. count rdd.count() Returns the number of elements in the RDD. first rdd.first() Returns the first element of the RDD. take rdd.take(5) Returns the first n elements of the RDD. reduce rdd.reduce(lambda a, b: a + b) Aggregates the elements of the RDD using a function. saveAsTextFile rdd.saveAsTextFile(\"path\") Saves the RDD to a text file. countByKey rdd.countByKey() Returns the count of each key in the RDD. foreach rdd.foreach(lambda x: print(x)) Applies a function to each element of the RDD. What is a Shuffle? A shuffle in Spark is the process of redistributing data across different nodes in the cluster. It involves copying data between Executors(JVM Proceses). It typically happens when a transformation requires data exchange between partitions, involving disk I/O, data serialization, and network I/O. Shuffle is one of the most substantial factors in degraded performance of your Spark application. While storing the intermediate data, it can exhaust space on the executor's local disk, which causes the Spark job to fail. When Does a Shuffle Occur? Operation Example Description groupByKey rdd.groupByKey() Groups elements by key, requiring all data for a key to be in the same partition. reduceByKey rdd.reduceByKey(lambda a, b: a + b) Combines values for each key using a function, requiring data colocation. sortByKey rdd.sortByKey() Sorts data, requiring all data for a key to be in the same partition. join rdd1.join(rdd2) Joins two RDDs or DataFrames, requiring data with the same key to be colocated. distinct rdd.distinct() Removes duplicates, requiring comparison across partitions. How to Optimize Shuffle in Spark Optimization Description Example/Note Avoid join() unless essential The join() operation is a costly shuffle operation and can be a performance bottleneck. Use join only if necessary for your business requirements. Avoid collect() in production The collect() action returns all results to the Spark driver, which can cause OOM errors. Default setting: spark.driver.maxResultSize = 1GB . Cache/Persist DataFrames Use df.cache() or df.persist() to cache repetitive DataFrames to avoid additional shuffle or computation. python df_high_rate = df.filter(col(\"star_rating\") >= 4.0) df_high_rate.persist() Unpersist when done Use unpersist to discard cached data when it is no longer needed. python df_high_rate.unpersist() Overcome data skew Data skew causes uneven distribution of data across partitions, leading to performance bottlenecks. Ensure data is uniformly distributed across partitions. Use bucketing Bucketing pre-shuffles and pre-sorts input on join keys, writing sorted data to intermediary tables to reduce shuffle and sort costs. Reduces load on executors during sort-merge joins. Shuffle and broadcast hash joins Use broadcast hash join for small-to-large table joins to avoid shuffling. Applicable only when the small table can fit in the memory of a single Spark executor. Optimize join Use high-level Spark APIs (SparkSQL, DataFrame, Datasets) for joins instead of RDD API or DynamicFrame join. Convert DynamicFrame to DataFrame if needed. python df_joined = df1.join(df2, [\"key\"])","title":"Spark Architecture"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture/#spark-architecture","text":"","title":"Spark Architecture"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture/#spark-components","text":"Spark Core: The main part of Spark with a core engine. Spark SQL: A SQL engine, but different from traditional databases. Here, data is processed mainly using DataFrames. Spark Streaming: This part allows Spark to process real-time data. Spark MLlib: A collection of machine learning libraries. GraphX: Used for graphs in reports, such as data collected from networks like Facebook. RDDs: Spark Core has RDDs (Resilient Distributed Datasets), which are the building blocks of Spark.","title":"Spark Components"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture/#cluster-nodes","text":"Nodes are individual machines (physical or virtual). Cluster is a group of nodes.","title":"Cluster &amp; Nodes"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture/#driver-worker","text":"","title":"Driver &amp; Worker"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture/#driver","text":"Machine where Main() method runs. It contains the SparkContext object. Converts the application into stages using a DAG (Directed Acyclic Graph). Schedules tasks on worker nodes and collects the results. Should be close to worker nodes for better performance.","title":"Driver"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture/#workersnow-executors","text":"Workers are simply machines(Virtual/Real). These workers run JVM processes, called Executors. Multiple JVM Process(Executor) can be configured in a worker. Configuration: In your spark configuration you can set: --num-executors : Specifies the total number of executors to be launched for the application. --executor-cores : Specifies the number of cores (slots) to be used by each executor. --executor-memory : Specifies the amount of memory to be allocated to each executor. Example: With a worker machine having 16 CPU cores and 64 GB of memory, you can configure Spark to run either 4 executors (4 cores, 16 GB each) or 2 executors (8 cores, 32 GB each). Note: Executor is a JVM process running on a worker node that executes tasks. The Spark cluster manager (e.g., YARN, Mesos, or the standalone cluster manager) is responsible for allocating resources to executors.","title":"Workers(now Executors)"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture/#slots-spark-cores-synapse-vcore-threads","text":"Cores in Spark = vCores in Synapse = Slots in Databricks = Total threads = Total parallel tasks Note: Don't confuse cores with Intel/AMD CPU Ads. Cores in Spark means threads. Note: Spark supports one task for each virtual CPU (vCPU) core by default. For example, if an executor has four CPU cores, it can run four concurrent tasks. Note: Multiple threads can run on each core, but Spark typically uses one thread per core for each task to simplify execution and avoid the complexities of managing multiple threads on a single core. This Databricks spark cluster can run 32 tasks parallely: In Docker Compose, SPARK_WORKER_CORES sets worker threads (cores/slots). A cluster with 3 workers, each set to 2 cores, has 6 total threads.","title":"Slots = Spark cores = Synapse vCore \u2248 Threads"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture/#executors-cores-memory-for-a-10-gb-data","text":"Say, you have 10 GB of data to be processed. How can you calcultate the executors, cores and memory for such a secenairo? Step Description Calculation/Value Calculate number of partitions Default partition size: 128 MB 10 GB / 128 MB = 80 partitions Determine CPU cores needed One core per partition for maximum parallelism 80 cores Max cores per executor Cores per executor in YARN 5 cores per executor Calculate number of executors Total cores / Cores per executor 80 / 5 = 16 executors Partition size Default partition size: 128 MB 128 MB Memory per core Minimum memory per core (4x partition size) 128 MB * 4 = 512 MB Executor cores Cores per executor 5 cores Executor memory Memory per core * Number of cores per executor 512 MB * 5 = 2560 MB Each Executor Requires Cores 5 CPU cores Memory 2560 MB","title":"Executors, cores &amp; memory for a 10 GB data"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture/#application-jobs-stages-tasks","text":"Applications -> jobs -> stages -> tasks . Term Definition Example Application An application in Spark is a complete program that runs on the Spark cluster. This program includes the user's code that uses Spark\u2019s API to perform data processing. A Spark application can be a Python script that processes data from a CSV file, performs transformations, and writes the results to a database. Job A job is triggered by an action (e.g., count() , collect() , saveAsTextFile() ) in a Spark application. Each action in the code triggers a new job. If your application has two actions, like counting the number of rows and saving the result to a file, it will trigger two jobs. Stages A job is divided into stages, where each stage is a set of tasks that can be executed in parallel. Stages are separated by shuffle operations. If a job involves filtering and then aggregating data, the filtering might be one stage, and the aggregation another, especially if a shuffle operation (like a group by) is required between them. Tasks A stage is further divided into tasks, where each task is a unit of work that operates on a partition of the data. Tasks are the smallest unit of execution in Spark. If a stage needs to process 100 partitions of data, it will have 100 tasks, with each task processing one partition.","title":"Application, Jobs, Stages, Tasks"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture/#lets-put-it-all-together","text":"Let's see an example to understand these concepts: Application: A Spark application that reads data from a CSV file, filters out certain rows, and then calculates the average of a column. Code snippet: python df1 = spark.read.csv(\"data.csv\") //Job1 df2 = df.filter(df[\"value\"] > 10) //Job2 average = df2.agg({\"value\": \"avg\"}).collect() Jobs: Job 1: Triggered by the action spark.read.csv() . This job reads the data from the CSV file. Job 2: Triggered by the action filtered_df.agg().collect() . This job includes filtering the data and then calculating the average. Stages in Job 2: Stage 1: Filtering the data. All tasks in this stage can run in parallel because filtering is a transformation that operates on individual partitions. Stage 2: Aggregating the data. This stage requires a shuffle because the aggregation (calculating the average) involves data from all partitions. Tasks: For each stage, Spark creates tasks based on the number of partitions. If there are 10 partitions, Stage 1 (filtering) will have 10 tasks, and Stage 2 (aggregation) will also have 10 tasks, each processing one partition of data.","title":"Let's put it all together"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture/#transformation-actions","text":"In PySpark, operations on data can be classified into two types: transformations and actions .","title":"Transformation &amp; Actions"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture/#transformations","text":"Transformations are operations on RDDs that return a new RDD, meaning they create a new dataset from an existing one. Transformations are lazy , meaning they are computed only when an action is called. (e.g., map , filter ): Create a new RDD from an existing one. They are lazy and not executed until an action is called.","title":"Transformations"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture/#actions","text":"Actions trigger the execution of the transformations to return a result to the driver program or write it to storage. When an action is called, Spark's execution engine computes the result of the transformations. (e.g., collect , count ): Trigger the execution of the transformations and return a result.","title":"Actions"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture/#example","text":"# Create an RDD from a list data = [1, 2, 3, 4, 5] rdd = spark.sparkContext.parallelize(data) # Transformation 1: Multiply each number by 2 rdd_transformed = rdd.map(lambda x: x * 2) # Transformation 2: Filter out even numbers rdd_filtered = rdd_transformed.filter(lambda x: x % 2 == 0) # Action: Collect the results result = rdd_filtered.collect()","title":"Example"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture/#common-transformations-and-actions-in-pyspark","text":"Transformation Example API Description map rdd.map(lambda x: x * 2) Applies a function to each element in the RDD. filter rdd.filter(lambda x: x % 2 == 0) Returns a new RDD containing only elements that satisfy a predicate. flatMap rdd.flatMap(lambda x: (x, x * 2)) Similar to map , but each input item can be mapped to 0 or more output items (returns a flattened structure). mapPartitions rdd.mapPartitions(lambda iter: [sum(iter)]) Applies a function to each partition of the RDD. distinct rdd.distinct() Returns a new RDD containing the distinct elements. union rdd1.union(rdd2) Returns a new RDD containing the union of elements. intersection rdd1.intersection(rdd2) Returns a new RDD containing the intersection of elements. groupByKey rdd.groupByKey() Groups the values for each key in the RDD. reduceByKey rdd.reduceByKey(lambda a, b: a + b) Merges the values for each key using an associative function. sortBy rdd.sortBy(lambda x: x) Returns a new RDD sorted by the specified function. Action Example API Description collect rdd.collect() Returns all the elements of the RDD as a list. count rdd.count() Returns the number of elements in the RDD. first rdd.first() Returns the first element of the RDD. take rdd.take(5) Returns the first n elements of the RDD. reduce rdd.reduce(lambda a, b: a + b) Aggregates the elements of the RDD using a function. saveAsTextFile rdd.saveAsTextFile(\"path\") Saves the RDD to a text file. countByKey rdd.countByKey() Returns the count of each key in the RDD. foreach rdd.foreach(lambda x: print(x)) Applies a function to each element of the RDD.","title":"Common transformations and actions in PySpark"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture/#what-is-a-shuffle","text":"A shuffle in Spark is the process of redistributing data across different nodes in the cluster. It involves copying data between Executors(JVM Proceses). It typically happens when a transformation requires data exchange between partitions, involving disk I/O, data serialization, and network I/O. Shuffle is one of the most substantial factors in degraded performance of your Spark application. While storing the intermediate data, it can exhaust space on the executor's local disk, which causes the Spark job to fail.","title":"What is a Shuffle?"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture/#when-does-a-shuffle-occur","text":"Operation Example Description groupByKey rdd.groupByKey() Groups elements by key, requiring all data for a key to be in the same partition. reduceByKey rdd.reduceByKey(lambda a, b: a + b) Combines values for each key using a function, requiring data colocation. sortByKey rdd.sortByKey() Sorts data, requiring all data for a key to be in the same partition. join rdd1.join(rdd2) Joins two RDDs or DataFrames, requiring data with the same key to be colocated. distinct rdd.distinct() Removes duplicates, requiring comparison across partitions.","title":"When Does a Shuffle Occur?"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture/#how-to-optimize-shuffle-in-spark","text":"Optimization Description Example/Note Avoid join() unless essential The join() operation is a costly shuffle operation and can be a performance bottleneck. Use join only if necessary for your business requirements. Avoid collect() in production The collect() action returns all results to the Spark driver, which can cause OOM errors. Default setting: spark.driver.maxResultSize = 1GB . Cache/Persist DataFrames Use df.cache() or df.persist() to cache repetitive DataFrames to avoid additional shuffle or computation. python df_high_rate = df.filter(col(\"star_rating\") >= 4.0) df_high_rate.persist() Unpersist when done Use unpersist to discard cached data when it is no longer needed. python df_high_rate.unpersist() Overcome data skew Data skew causes uneven distribution of data across partitions, leading to performance bottlenecks. Ensure data is uniformly distributed across partitions. Use bucketing Bucketing pre-shuffles and pre-sorts input on join keys, writing sorted data to intermediary tables to reduce shuffle and sort costs. Reduces load on executors during sort-merge joins. Shuffle and broadcast hash joins Use broadcast hash join for small-to-large table joins to avoid shuffling. Applicable only when the small table can fit in the memory of a single Spark executor. Optimize join Use high-level Spark APIs (SparkSQL, DataFrame, Datasets) for joins instead of RDD API or DynamicFrame join. Convert DynamicFrame to DataFrame if needed. python df_joined = df1.join(df2, [\"key\"])","title":"How to Optimize Shuffle in Spark"},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache/","text":"Understanding persist() in Spark Syntax of persist() Storage Levels Different Values for StorageLevel Example Removing Saved Data When to use persist() Conclusion Understanding cache() in Spark Understanding persist() in Spark In Spark, the persist() method is used to save a dataset (RDD or DataFrame) in memory or on disk, so you can use it multiple times without recalculating it every time. If you need to use the same data repeatedly, persist() can help speed up your work and add some fault tolerance. Syntax of persist() Usage Syntax (DataFrame) Syntax (RDD) No Argument dfPersist = df.persist() rdd.persist() With Argument dfPersist = df.persist(StorageLevel.XXXXXXX) rdd.persist(StorageLevel.XXXXXXX) Storage Levels Note : The default storage level is different for DataFrames and RDDs: - DataFrames : Default is MEMORY_AND_DISK . ( DataFrame API Documentation ) - RDDs : Default is MEMORY_ONLY . ( RDD Persistence Documentation ) Different Values for StorageLevel StorageLevel values are available in the pyspark.StorageLevel class. Here is the complete list: DISK_ONLY : Store data on disk only. DISK_ONLY_2 : Store data on disk with replication to two nodes. DISK_ONLY_3 : Store data on disk with replication to three nodes. MEMORY_AND_DISK : Store data in memory and spill to disk if necessary. MEMORY_AND_DISK_2 : Store data in memory and spill to disk if necessary, with replication to two nodes. MEMORY_AND_DISK_SER : Store data in JVM memory as serialized objects and spill to disk if necessary. MEMORY_AND_DISK_DESER : Store data in JVM memory as deserialized objects and spill to disk if necessary. MEMORY_ONLY : Store data as deserialized objects in JVM memory only. MEMORY_ONLY_2 : Store data in memory only, with replication to two nodes. NONE : No storage level. (Note: This can't be used as an argument.) OFF_HEAP : Store data in off-heap memory (experimental). Note : The official Spark documentation states that the default storage level for RDD.persist() is MEMORY_ONLY . For df.persist() , it is MEMORY_AND_DISK , and starting from version 3.4.0, it is MEMORY_AND_DISK_DESER . ( Spark Persistence Documentation ) Example Imagine you have a list of numbers, and you want to do some calculations on it multiple times. Here\u2019s how you can use persist() : from pyspark import SparkContext # Initialize Spark Context sc = SparkContext(\"local\", \"PersistExample\") # Create an RDD numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] rdd = sc.parallelize(numbers) # Persist the RDD in memory rdd.persist() # Do some actions print(rdd.count()) # Count the numbers print(rdd.collect()) # Collect all the numbers # This will show the storage level being used print(dfPersistMemoryOnly.storageLevel) In this example, persist() saves the rdd in memory, so if you do more actions like count() or collect() , it doesn\u2019t have to recalculate the data each time. Removing Saved Data When you don\u2019t need the saved data anymore, you can remove it from memory or disk using unpersist() : rdd.unpersist() When to use persist() So, we have learned about persist() . Does that mean you should use persist() every time you have a DataFrame (df)? No. Here are the situations where it is recommended to use it. Running Multiple Machine Learning Models : When you run multiple machine learning models on the same dataset, using persist() can save time. Interactive Data Analysis : If you are using a notebook for interactive data analysis, where you need to understand the data by running multiple queries, persist() will make the results come faster. ETL Process : In your ETL (Extract, Transform, Load) process, if you have a cleaned DataFrame and you are using the same DataFrame in many steps, using persist() can help. Graph Processing : When processing graphs using libraries like GraphX, persist() can improve performance. Remember, persist() comes later in the tools for fixing speed. For significant improvement, focus on better partitioning of data (like fixing data skewness) and using broadcast variables for joins first. Don't expect persist() to magically improve speed. Also, if your DataFrame (df) is very large and you persist it, it will consume a lot of memory on the worker nodes. This means they won't have enough memory left for their other tasks. To avoid this, first assess the memory of the workers and the size of the df you are persisting. For smaller DataFrames, use MEMORY_ONLY , and for larger ones, use MEMORY_AND_DISK (this will spill some data to disk if memory is low). Remember, MEMORY_AND_DISK also has a performance impact because it increases I/O operations. Additionally, large DataFrames can increase the frequency of garbage collection in the JVM, which affects overall performance. Conclusion Using persist() in Spark is like telling Spark to remember your data so it doesn\u2019t have to start from scratch every time you need it. This can make your work much faster, especially when working with large datasets, and also adds fault tolerance. Understanding cache() in Spark The cache() function is a shorthand method for persist() with the default storage level, which is MEMORY_ONLY . This means, cache() is same as persist(StorageLevel.MEMORY_ONLY)","title":"persist and cache"},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache/#understanding-persist-in-spark","text":"In Spark, the persist() method is used to save a dataset (RDD or DataFrame) in memory or on disk, so you can use it multiple times without recalculating it every time. If you need to use the same data repeatedly, persist() can help speed up your work and add some fault tolerance.","title":"Understanding persist() in Spark"},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache/#syntax-of-persist","text":"Usage Syntax (DataFrame) Syntax (RDD) No Argument dfPersist = df.persist() rdd.persist() With Argument dfPersist = df.persist(StorageLevel.XXXXXXX) rdd.persist(StorageLevel.XXXXXXX)","title":"Syntax of persist()"},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache/#storage-levels","text":"Note : The default storage level is different for DataFrames and RDDs: - DataFrames : Default is MEMORY_AND_DISK . ( DataFrame API Documentation ) - RDDs : Default is MEMORY_ONLY . ( RDD Persistence Documentation )","title":"Storage Levels"},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache/#different-values-for-storagelevel","text":"StorageLevel values are available in the pyspark.StorageLevel class. Here is the complete list: DISK_ONLY : Store data on disk only. DISK_ONLY_2 : Store data on disk with replication to two nodes. DISK_ONLY_3 : Store data on disk with replication to three nodes. MEMORY_AND_DISK : Store data in memory and spill to disk if necessary. MEMORY_AND_DISK_2 : Store data in memory and spill to disk if necessary, with replication to two nodes. MEMORY_AND_DISK_SER : Store data in JVM memory as serialized objects and spill to disk if necessary. MEMORY_AND_DISK_DESER : Store data in JVM memory as deserialized objects and spill to disk if necessary. MEMORY_ONLY : Store data as deserialized objects in JVM memory only. MEMORY_ONLY_2 : Store data in memory only, with replication to two nodes. NONE : No storage level. (Note: This can't be used as an argument.) OFF_HEAP : Store data in off-heap memory (experimental). Note : The official Spark documentation states that the default storage level for RDD.persist() is MEMORY_ONLY . For df.persist() , it is MEMORY_AND_DISK , and starting from version 3.4.0, it is MEMORY_AND_DISK_DESER . ( Spark Persistence Documentation )","title":"Different Values for StorageLevel"},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache/#example","text":"Imagine you have a list of numbers, and you want to do some calculations on it multiple times. Here\u2019s how you can use persist() : from pyspark import SparkContext # Initialize Spark Context sc = SparkContext(\"local\", \"PersistExample\") # Create an RDD numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] rdd = sc.parallelize(numbers) # Persist the RDD in memory rdd.persist() # Do some actions print(rdd.count()) # Count the numbers print(rdd.collect()) # Collect all the numbers # This will show the storage level being used print(dfPersistMemoryOnly.storageLevel) In this example, persist() saves the rdd in memory, so if you do more actions like count() or collect() , it doesn\u2019t have to recalculate the data each time.","title":"Example"},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache/#removing-saved-data","text":"When you don\u2019t need the saved data anymore, you can remove it from memory or disk using unpersist() : rdd.unpersist()","title":"Removing Saved Data"},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache/#when-to-use-persist","text":"So, we have learned about persist() . Does that mean you should use persist() every time you have a DataFrame (df)? No. Here are the situations where it is recommended to use it. Running Multiple Machine Learning Models : When you run multiple machine learning models on the same dataset, using persist() can save time. Interactive Data Analysis : If you are using a notebook for interactive data analysis, where you need to understand the data by running multiple queries, persist() will make the results come faster. ETL Process : In your ETL (Extract, Transform, Load) process, if you have a cleaned DataFrame and you are using the same DataFrame in many steps, using persist() can help. Graph Processing : When processing graphs using libraries like GraphX, persist() can improve performance. Remember, persist() comes later in the tools for fixing speed. For significant improvement, focus on better partitioning of data (like fixing data skewness) and using broadcast variables for joins first. Don't expect persist() to magically improve speed. Also, if your DataFrame (df) is very large and you persist it, it will consume a lot of memory on the worker nodes. This means they won't have enough memory left for their other tasks. To avoid this, first assess the memory of the workers and the size of the df you are persisting. For smaller DataFrames, use MEMORY_ONLY , and for larger ones, use MEMORY_AND_DISK (this will spill some data to disk if memory is low). Remember, MEMORY_AND_DISK also has a performance impact because it increases I/O operations. Additionally, large DataFrames can increase the frequency of garbage collection in the JVM, which affects overall performance.","title":"When to use persist()"},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache/#conclusion","text":"Using persist() in Spark is like telling Spark to remember your data so it doesn\u2019t have to start from scratch every time you need it. This can make your work much faster, especially when working with large datasets, and also adds fault tolerance.","title":"Conclusion"},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache/#understanding-cache-in-spark","text":"The cache() function is a shorthand method for persist() with the default storage level, which is MEMORY_ONLY . This means, cache() is same as persist(StorageLevel.MEMORY_ONLY)","title":"Understanding cache() in Spark"},{"location":"Spark-DataBricks/1.0_Spark/1.4_broadcastvariables/","text":"Understanding Broadcast Variables in Spark When to Use Broadcast Variables Example Scenario Without Broadcast Variables Scenario With Broadcast Variables How Broadcast Variables Improve Performance Knowledge Check Summary Understanding Broadcast Variables in Spark Imagine a scenario from the movie \"The Matrix\" where Morpheus shares a training program with Neo and the other rebels. Instead of loading the training program into the simulation multiple times for each person, Morpheus sends it once to each person's mind, and they use it as needed. This is similar to broadcast variables in Spark. These variables are used mainly for performance tuning . Using broadcast variables, you can send small read-only tables to all worker nodes in a cluster. This can reduce shuffling and make operations faster. When to Use Broadcast Variables Broadcast variables should be used when you need to send a small table to all nodes. This is ideal for joins where one table is small (like a dimension table). It is not suitable for large tables, as it would cause an out-of-memory exception. If the size of the broadcasted table is larger than the memory, it will surely cause OOM. Example Here I will give you two scenario and show you how broadcast variables may imporve joins in a Spark cluster. Scenario Without Broadcast Variables Suppose you have a small lookup table (e.g., lookup_dict ) and a large dataset ( rdd ). You want to join the large dataset with the lookup table. # Small lookup table lookup_dict = {\"A\": 1, \"B\": 2, \"C\": 3} # Large RDD rdd = sc.parallelize([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]) # Join operation without broadcast lookup_rdd = sc.parallelize(list(lookup_dict.items())) joined_rdd = rdd.map(lambda x: (x,)).join(lookup_rdd) In this scenario, Spark needs to shuffle data to perform the join operation, which is expensive and time-consuming. Scenario With Broadcast Variables Using a broadcast variable, you can send the lookup table to all nodes just once, avoiding the shuffle operation. # Small lookup table lookup_dict = {\"A\": 1, \"B\": 2, \"C\": 3} # Broadcast the lookup table broadcast_var = sc.broadcast(lookup_dict) # Large RDD rdd = sc.parallelize([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]) # Use the broadcast variable to look up values result_rdd = rdd.map(lambda x: (x, broadcast_var.value.get(x, 0))) # Collect and print the results print(result_rdd.collect()) How Broadcast Variables Improve Performance When you join or group DataFrames, Spark usually shuffles data across all worker nodes, which is slow and costly. Avoiding Shuffle : Without Broadcast : Spark shuffles the lookup table data across the network to join with the large dataset. With Broadcast : The lookup table is sent once to all nodes, allowing local lookups without moving data around. Efficiency : Without Broadcast : The join operation involves heavy data movement and sorting, which is slow. With Broadcast : The lookup is done locally on each node using the cached copy, making the operation much faster and reducing network traffic. Knowledge Check A. A broadcast variable is a Spark object that needs to be partitioned onto multiple worker nodes because it's too large to fit on a single worker node. - This statement is incorrect. Broadcast variables are designed to be efficiently distributed to each worker node. They are not partitioned but rather fully replicated on each worker node to avoid being sent multiple times. B. A broadcast variable can only be created by an explicit call to the broadcast() operation. - This statement is correct but not complete. While it's true that broadcast variables are created by calling the broadcast() method, this option doesn't describe the defining characteristic of broadcast variables, which is how they are used in the Spark architecture. C. A broadcast variable is entirely cached on the driver node so it doesn't need to be present on any worker nodes. - This statement is incorrect. Broadcast variables are distributed to all worker nodes, not just cached on the driver node. Their purpose is to be available on each worker node to prevent the need to send them multiple times during execution. D. A broadcast variable is entirely cached on each worker node so it doesn't need to be shipped or shuffled between nodes with each stage. - This statement is correct. The primary characteristic of a broadcast variable is that it is distributed and cached on each worker node. This reduces communication overhead by avoiding repeated transmission of the variable during different stages of the job. E. A broadcast variable is saved to the disk of each worker node to be easily read into memory when needed. - This statement is incorrect. Broadcast variables are stored in memory on each worker node to ensure quick access without the latency involved in reading from disk. The purpose of broadcast variables is to provide fast, in-memory access to frequently used data. Summary Broadcast variables are useful for distributing small datasets (like lookup tables) to all nodes in a Spark cluster. By broadcasting the lookup table, you avoid the need to shuffle data during join operations, which can significantly improve performance by reducing network overhead and computation time. This makes operations faster and more efficient.","title":"Broadcast Variables"},{"location":"Spark-DataBricks/1.0_Spark/1.4_broadcastvariables/#understanding-broadcast-variables-in-spark","text":"Imagine a scenario from the movie \"The Matrix\" where Morpheus shares a training program with Neo and the other rebels. Instead of loading the training program into the simulation multiple times for each person, Morpheus sends it once to each person's mind, and they use it as needed. This is similar to broadcast variables in Spark. These variables are used mainly for performance tuning . Using broadcast variables, you can send small read-only tables to all worker nodes in a cluster. This can reduce shuffling and make operations faster.","title":"Understanding Broadcast Variables in Spark"},{"location":"Spark-DataBricks/1.0_Spark/1.4_broadcastvariables/#when-to-use-broadcast-variables","text":"Broadcast variables should be used when you need to send a small table to all nodes. This is ideal for joins where one table is small (like a dimension table). It is not suitable for large tables, as it would cause an out-of-memory exception. If the size of the broadcasted table is larger than the memory, it will surely cause OOM.","title":"When to Use Broadcast Variables"},{"location":"Spark-DataBricks/1.0_Spark/1.4_broadcastvariables/#example","text":"Here I will give you two scenario and show you how broadcast variables may imporve joins in a Spark cluster.","title":"Example"},{"location":"Spark-DataBricks/1.0_Spark/1.4_broadcastvariables/#scenario-without-broadcast-variables","text":"Suppose you have a small lookup table (e.g., lookup_dict ) and a large dataset ( rdd ). You want to join the large dataset with the lookup table. # Small lookup table lookup_dict = {\"A\": 1, \"B\": 2, \"C\": 3} # Large RDD rdd = sc.parallelize([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]) # Join operation without broadcast lookup_rdd = sc.parallelize(list(lookup_dict.items())) joined_rdd = rdd.map(lambda x: (x,)).join(lookup_rdd) In this scenario, Spark needs to shuffle data to perform the join operation, which is expensive and time-consuming.","title":"Scenario Without Broadcast Variables"},{"location":"Spark-DataBricks/1.0_Spark/1.4_broadcastvariables/#scenario-with-broadcast-variables","text":"Using a broadcast variable, you can send the lookup table to all nodes just once, avoiding the shuffle operation. # Small lookup table lookup_dict = {\"A\": 1, \"B\": 2, \"C\": 3} # Broadcast the lookup table broadcast_var = sc.broadcast(lookup_dict) # Large RDD rdd = sc.parallelize([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]) # Use the broadcast variable to look up values result_rdd = rdd.map(lambda x: (x, broadcast_var.value.get(x, 0))) # Collect and print the results print(result_rdd.collect())","title":"Scenario With Broadcast Variables"},{"location":"Spark-DataBricks/1.0_Spark/1.4_broadcastvariables/#how-broadcast-variables-improve-performance","text":"When you join or group DataFrames, Spark usually shuffles data across all worker nodes, which is slow and costly. Avoiding Shuffle : Without Broadcast : Spark shuffles the lookup table data across the network to join with the large dataset. With Broadcast : The lookup table is sent once to all nodes, allowing local lookups without moving data around. Efficiency : Without Broadcast : The join operation involves heavy data movement and sorting, which is slow. With Broadcast : The lookup is done locally on each node using the cached copy, making the operation much faster and reducing network traffic.","title":"How Broadcast Variables Improve Performance"},{"location":"Spark-DataBricks/1.0_Spark/1.4_broadcastvariables/#knowledge-check","text":"A. A broadcast variable is a Spark object that needs to be partitioned onto multiple worker nodes because it's too large to fit on a single worker node. - This statement is incorrect. Broadcast variables are designed to be efficiently distributed to each worker node. They are not partitioned but rather fully replicated on each worker node to avoid being sent multiple times. B. A broadcast variable can only be created by an explicit call to the broadcast() operation. - This statement is correct but not complete. While it's true that broadcast variables are created by calling the broadcast() method, this option doesn't describe the defining characteristic of broadcast variables, which is how they are used in the Spark architecture. C. A broadcast variable is entirely cached on the driver node so it doesn't need to be present on any worker nodes. - This statement is incorrect. Broadcast variables are distributed to all worker nodes, not just cached on the driver node. Their purpose is to be available on each worker node to prevent the need to send them multiple times during execution. D. A broadcast variable is entirely cached on each worker node so it doesn't need to be shipped or shuffled between nodes with each stage. - This statement is correct. The primary characteristic of a broadcast variable is that it is distributed and cached on each worker node. This reduces communication overhead by avoiding repeated transmission of the variable during different stages of the job. E. A broadcast variable is saved to the disk of each worker node to be easily read into memory when needed. - This statement is incorrect. Broadcast variables are stored in memory on each worker node to ensure quick access without the latency involved in reading from disk. The purpose of broadcast variables is to provide fast, in-memory access to frequently used data.","title":"Knowledge Check"},{"location":"Spark-DataBricks/1.0_Spark/1.4_broadcastvariables/#summary","text":"Broadcast variables are useful for distributing small datasets (like lookup tables) to all nodes in a Spark cluster. By broadcasting the lookup table, you avoid the need to shuffle data during join operations, which can significantly improve performance by reducing network overhead and computation time. This makes operations faster and more efficient.","title":"Summary"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling/","text":"How to handle data skew in Spark What causes data skew? Uneven Key Distribution in Join Operations Presence of Null Values or a Dominant Key Poorly Chosen Partitioning Key Faulty GroupBy Handling Data Skew Salting (for join operations) Custom Partitioning Broadcast Join Separate out the skewed keys Using AQE (Adaptive Query Execution): Avoid GroupBy for Large Datasets Check your knowledge How to handle data skew in Spark In spark, we partition(split) data to process it parallely. But, if the splitting 'unfair', like some parition has humougus amount of data while others have too little. This is called data skew. This is undesirable. If we process such skewed data, some nodes will be OOM while some others will sit idle(as they alreaady processed the little data they were assigned). What causes data skew? Uneven Key Distribution in Join Operations Suppose you have two datasets (key-value pairs) you want to join: Dataset A : [(China, \"A\"), (Fiji, \"B\"), (Bhutan, \"C\")] Dataset B : [(China, \"X\"), (China, \"Y\"), (China, \"Z\"), (Fiji, \"P\"), (Bhutan, \"Q\")] When joining on the first column (key), the resulting dataset would look like: Joined Data : [(China, \"A\", \"X\"), (China, \"A\", \"Y\"), (China, \"A\", \"Z\"), (Fiji, \"B\", \"P\"), (Bhutan, \"C\", \"Q\")] Now, if you split (partition) the joined data based on the key (China/Fiji/Bhutan), there will be three partitions: Partition 1: [(China, \"A\", \"X\"), (China, \"A\", \"Y\"), (China, \"A\", \"Z\")] Partition 2: [(Fiji, \"B\", \"P\")] Partition 3: [(Bhutan, \"C\", \"Q\")] This means the first partition is unfair. The node with Partition 1 will be overloaded, while the other nodes will be sitting idle after some time. Presence of Null Values or a Dominant Key Suppose you have a dataset with some entries having a null key or the same key: Dataset: [(null, \"A\"), (null, \"B\"), (1, \"C\"), (1, \"D\"), (2, \"E\")] When partitioning based on the key: Partition 1: [(null, \"A\"), (null, \"B\")] Partition 2: [(1, \"C\"), (1, \"D\")] Partition 3: [(2, \"E\")] The partitions for null and 1 have more entries, causing an imbalance. Poorly Chosen Partitioning Key Suppose you have a dataset and you partition it using a key that does not distribute data well: Dataset: [(10, \"A\"), (20, \"B\"), (30, \"C\"), (10, \"D\"), (20, \"E\")] If you partition by the key (first column) and the data is unevenly distributed: Partition 1: [(10, \"A\"), (10, \"D\")] Partition 2: [(20, \"B\"), (20, \"E\")] Partition 3: [(30, \"C\")] Here, Partition 1 and 2 have more entries compared to Partition 3, leading to skew. Faulty GroupBy Just like join operations, GroupBy operations can also cause data imbalance when some keys have a lot more values than others. Example: If you group sales data by product ID and one product has many more sales records than others, the partition for that product will be much bigger and could slow down processing. Handling Data Skew Salting (for join operations) Add a random number to keys to distribute them more evenly. Example: from pyspark.sql.functions import rand, col # Assume df1 has skewed keys salted_df1 = df1.withColumn(\"salt\", (rand()*5).cast(\"int\")) .withColumn(\"new_key\", col(\"key\") * 10 + col(\"salt\")) salted_df2 = df2.withColumn(\"salt\", explode(array([lit(i) for i in range(5)]))) .withColumn(\"new_key\", col(\"key\") * 10 + col(\"salt\")) result = salted_df1.join(salted_df2, \"new_key\").drop(\"salt\", \"new_key\") Custom Partitioning Repartition data based on a more evenly distributed column. Example: from pyspark.sql.functions import col # Assume 'id' is more evenly distributed than 'skewed_column' evenly_distributed_df = df.repartition(col(\"id\")) Broadcast Join For scenarios where one DataFrame is small enough to fit in memory. Example: from pyspark.sql.functions import broadcast result = large_df.join(broadcast(small_df), \"key\") Separate out the skewed keys Handle the most skewed keys separately. Example: # Identify skewed keys key_counts = df.groupBy(\"key\").count().orderBy(col(\"count\").desc()) skewed_keys = key_counts.limit(5).select(\"key\").collect() # Separate skewed and non-skewed data skewed_data = df.filter(col(\"key\").isin(skewed_keys)) non_skewed_data = df.filter(~col(\"key\").isin(skewed_keys)) # Process separately and union results skewed_result = process_skewed_data(skewed_data) non_skewed_result = process_non_skewed_data(non_skewed_data) final_result = skewed_result.union(non_skewed_result) Using AQE (Adaptive Query Execution): Enable AQE in Spark 3.0+ to dynamically optimize query plans. Example: spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\") Avoid GroupBy for Large Datasets When one key has a much larger presence in the dataset, try to avoid using GroupBy with that key or avoid GroupBy altogether. Instead, use alternatives like reduceByKey, which first combines data locally on each partition before grouping, making it more efficient. Example with DataFrames: Instead of this: # GroupBy operation grouped_data = df.groupBy(\"key\").agg(sum(\"value\")) Use this: from pyspark.sql import functions as F # ReduceByKey equivalent reduced_data = df.groupBy(\"key\").agg(F.sum(\"value\")) In this example with DataFrames, the data is aggregated locally on each partition before the final grouping, helping to avoid data imbalance and improve efficiency. Check your knowledge Question: Which of the following operations is most likely to induce a skew in the size of your data's partitions? A. DataFrame.collect() B. DataFrame.cache() C. DataFrame.repartition(n) D. DataFrame.coalesce(n) E. DataFrame.persist() Answer: D. DataFrame.coalesce(n) Explanation: DataFrame.collect() : This operation brings all the data to the driver node, which doesn\u2019t affect the partition sizes. DataFrame.cache() : This operation stores the DataFrame in memory but doesn\u2019t change the partition sizes. DataFrame.repartition(n) : This operation reshuffles the data into a specified number of partitions with a relatively even distribution. DataFrame.coalesce(n) : This operation reduces the number of partitions by merging them without a shuffle, which can lead to uneven partition sizes if the original data was not evenly distributed. DataFrame.persist() : Similar to cache() , this stores the DataFrame in memory/disk but doesn\u2019t change partition sizes. Therefore, coalesce(n) is the operation that is most likely to cause skew in the size of your data's partitions.","title":"Data Skew in Spark"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling/#how-to-handle-data-skew-in-spark","text":"In spark, we partition(split) data to process it parallely. But, if the splitting 'unfair', like some parition has humougus amount of data while others have too little. This is called data skew. This is undesirable. If we process such skewed data, some nodes will be OOM while some others will sit idle(as they alreaady processed the little data they were assigned).","title":"How to handle data skew in Spark"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling/#what-causes-data-skew","text":"","title":"What causes data skew?"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling/#uneven-key-distribution-in-join-operations","text":"Suppose you have two datasets (key-value pairs) you want to join: Dataset A : [(China, \"A\"), (Fiji, \"B\"), (Bhutan, \"C\")] Dataset B : [(China, \"X\"), (China, \"Y\"), (China, \"Z\"), (Fiji, \"P\"), (Bhutan, \"Q\")] When joining on the first column (key), the resulting dataset would look like: Joined Data : [(China, \"A\", \"X\"), (China, \"A\", \"Y\"), (China, \"A\", \"Z\"), (Fiji, \"B\", \"P\"), (Bhutan, \"C\", \"Q\")] Now, if you split (partition) the joined data based on the key (China/Fiji/Bhutan), there will be three partitions: Partition 1: [(China, \"A\", \"X\"), (China, \"A\", \"Y\"), (China, \"A\", \"Z\")] Partition 2: [(Fiji, \"B\", \"P\")] Partition 3: [(Bhutan, \"C\", \"Q\")] This means the first partition is unfair. The node with Partition 1 will be overloaded, while the other nodes will be sitting idle after some time.","title":"Uneven Key Distribution in Join Operations"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling/#presence-of-null-values-or-a-dominant-key","text":"Suppose you have a dataset with some entries having a null key or the same key: Dataset: [(null, \"A\"), (null, \"B\"), (1, \"C\"), (1, \"D\"), (2, \"E\")] When partitioning based on the key: Partition 1: [(null, \"A\"), (null, \"B\")] Partition 2: [(1, \"C\"), (1, \"D\")] Partition 3: [(2, \"E\")] The partitions for null and 1 have more entries, causing an imbalance.","title":"Presence of Null Values or a Dominant Key"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling/#poorly-chosen-partitioning-key","text":"Suppose you have a dataset and you partition it using a key that does not distribute data well: Dataset: [(10, \"A\"), (20, \"B\"), (30, \"C\"), (10, \"D\"), (20, \"E\")] If you partition by the key (first column) and the data is unevenly distributed: Partition 1: [(10, \"A\"), (10, \"D\")] Partition 2: [(20, \"B\"), (20, \"E\")] Partition 3: [(30, \"C\")] Here, Partition 1 and 2 have more entries compared to Partition 3, leading to skew.","title":"Poorly Chosen Partitioning Key"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling/#faulty-groupby","text":"Just like join operations, GroupBy operations can also cause data imbalance when some keys have a lot more values than others. Example: If you group sales data by product ID and one product has many more sales records than others, the partition for that product will be much bigger and could slow down processing.","title":"Faulty GroupBy"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling/#handling-data-skew","text":"","title":"Handling Data Skew"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling/#salting-for-join-operations","text":"Add a random number to keys to distribute them more evenly. Example: from pyspark.sql.functions import rand, col # Assume df1 has skewed keys salted_df1 = df1.withColumn(\"salt\", (rand()*5).cast(\"int\")) .withColumn(\"new_key\", col(\"key\") * 10 + col(\"salt\")) salted_df2 = df2.withColumn(\"salt\", explode(array([lit(i) for i in range(5)]))) .withColumn(\"new_key\", col(\"key\") * 10 + col(\"salt\")) result = salted_df1.join(salted_df2, \"new_key\").drop(\"salt\", \"new_key\")","title":"Salting (for join operations)"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling/#custom-partitioning","text":"Repartition data based on a more evenly distributed column. Example: from pyspark.sql.functions import col # Assume 'id' is more evenly distributed than 'skewed_column' evenly_distributed_df = df.repartition(col(\"id\"))","title":"Custom Partitioning"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling/#broadcast-join","text":"For scenarios where one DataFrame is small enough to fit in memory. Example: from pyspark.sql.functions import broadcast result = large_df.join(broadcast(small_df), \"key\")","title":"Broadcast Join"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling/#separate-out-the-skewed-keys","text":"Handle the most skewed keys separately. Example: # Identify skewed keys key_counts = df.groupBy(\"key\").count().orderBy(col(\"count\").desc()) skewed_keys = key_counts.limit(5).select(\"key\").collect() # Separate skewed and non-skewed data skewed_data = df.filter(col(\"key\").isin(skewed_keys)) non_skewed_data = df.filter(~col(\"key\").isin(skewed_keys)) # Process separately and union results skewed_result = process_skewed_data(skewed_data) non_skewed_result = process_non_skewed_data(non_skewed_data) final_result = skewed_result.union(non_skewed_result)","title":"Separate out the skewed keys"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling/#using-aqe-adaptive-query-execution","text":"Enable AQE in Spark 3.0+ to dynamically optimize query plans. Example: spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")","title":"Using AQE (Adaptive Query Execution):"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling/#avoid-groupby-for-large-datasets","text":"When one key has a much larger presence in the dataset, try to avoid using GroupBy with that key or avoid GroupBy altogether. Instead, use alternatives like reduceByKey, which first combines data locally on each partition before grouping, making it more efficient. Example with DataFrames: Instead of this: # GroupBy operation grouped_data = df.groupBy(\"key\").agg(sum(\"value\")) Use this: from pyspark.sql import functions as F # ReduceByKey equivalent reduced_data = df.groupBy(\"key\").agg(F.sum(\"value\")) In this example with DataFrames, the data is aggregated locally on each partition before the final grouping, helping to avoid data imbalance and improve efficiency.","title":"Avoid GroupBy for Large Datasets"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling/#check-your-knowledge","text":"Question: Which of the following operations is most likely to induce a skew in the size of your data's partitions? A. DataFrame.collect() B. DataFrame.cache() C. DataFrame.repartition(n) D. DataFrame.coalesce(n) E. DataFrame.persist() Answer: D. DataFrame.coalesce(n) Explanation: DataFrame.collect() : This operation brings all the data to the driver node, which doesn\u2019t affect the partition sizes. DataFrame.cache() : This operation stores the DataFrame in memory but doesn\u2019t change the partition sizes. DataFrame.repartition(n) : This operation reshuffles the data into a specified number of partitions with a relatively even distribution. DataFrame.coalesce(n) : This operation reduces the number of partitions by merging them without a shuffle, which can lead to uneven partition sizes if the original data was not evenly distributed. DataFrame.persist() : Similar to cache() , this stores the DataFrame in memory/disk but doesn\u2019t change partition sizes. Therefore, coalesce(n) is the operation that is most likely to cause skew in the size of your data's partitions.","title":"Check your knowledge"},{"location":"Spark-DataBricks/1.0_Spark/1.6_dropna_fillna_df_missing_val_handling/","text":"Table of contents {: .text-delta } 1. TOC {:toc} dropna & fillna - Handling missing values in dfs In PySpark dataframes, missing values are represented as NULL or None . Here, I will show you how to handle these missing values using various functions in PySpark. Dropping Rows with Null Values Drop Rows with Any Null Values : python df.dropna() # or df.na.drop() This will drop rows that have even one null value. Drop Rows Where All Values Are Null : python df.dropna(how='all') # or df.na.drop(\"all\") This will drop rows where all values are null. Drop Rows with Null Values in Specific Columns: Drop rows if country OR region have null values python df = df.dropna(subset=[\"country\", \"region\"]) # Alternative: df.na.drop(subset=[\"country\", \"region\"]) Filling Missing Values Fill Null Values in Specific Columns : python df.fillna({\"price\": 0, \"country\": \"unknown\"}) If the price column has null values, replace them with 0 . If the country column has null values, replace them with \"unknown\" . Using a Dictionary python replacements = { \"age\": 0, \"country\": \"Unknown\", \"region\": \"Unknown\", \"income\": 0, # Adding more columns as needed \"population\": 0 } df = df.fillna(replacements) # Alternative: df.na.fill(replacements) Replacing Specific Values Using replace : python df.replace({None: \"godknows\"}, subset=[\"country\"]) This will replace None (null) values in the country column with \"godknows\" . Using withColumn, when & otherwise : ```python from pyspark.sql.functions import when df = df.withColumn(\"country\", when(df[\"country\"].isNull(), \"godknows\").otherwise(df[\"country\"])) `` This will replace null values in the country column with \"godknows\"`. Using Filter python df.filter(df[\"age\"] > 30) # Alternative: df.where(df[\"age\"] > 30) Imputation Fill Null Values with Mean of the Column : ```python from pyspark.sql.functions import mean mean_price = df.select(mean(\"price\")).collect()[0][0] df = df.na.fill({\"price\": mean_price}) `` This will replace null values in the price` column with the mean value of that column.","title":"dropna and fillna"},{"location":"Spark-DataBricks/1.0_Spark/1.6_dropna_fillna_df_missing_val_handling/#dropna-fillna-handling-missing-values-in-dfs","text":"In PySpark dataframes, missing values are represented as NULL or None . Here, I will show you how to handle these missing values using various functions in PySpark.","title":"dropna &amp; fillna - Handling missing values in dfs"},{"location":"Spark-DataBricks/1.0_Spark/1.6_dropna_fillna_df_missing_val_handling/#dropping-rows-with-null-values","text":"Drop Rows with Any Null Values : python df.dropna() # or df.na.drop() This will drop rows that have even one null value. Drop Rows Where All Values Are Null : python df.dropna(how='all') # or df.na.drop(\"all\") This will drop rows where all values are null. Drop Rows with Null Values in Specific Columns: Drop rows if country OR region have null values python df = df.dropna(subset=[\"country\", \"region\"]) # Alternative: df.na.drop(subset=[\"country\", \"region\"])","title":"Dropping Rows with Null Values"},{"location":"Spark-DataBricks/1.0_Spark/1.6_dropna_fillna_df_missing_val_handling/#filling-missing-values","text":"Fill Null Values in Specific Columns : python df.fillna({\"price\": 0, \"country\": \"unknown\"}) If the price column has null values, replace them with 0 . If the country column has null values, replace them with \"unknown\" . Using a Dictionary python replacements = { \"age\": 0, \"country\": \"Unknown\", \"region\": \"Unknown\", \"income\": 0, # Adding more columns as needed \"population\": 0 } df = df.fillna(replacements) # Alternative: df.na.fill(replacements)","title":"Filling Missing Values"},{"location":"Spark-DataBricks/1.0_Spark/1.6_dropna_fillna_df_missing_val_handling/#replacing-specific-values","text":"Using replace : python df.replace({None: \"godknows\"}, subset=[\"country\"]) This will replace None (null) values in the country column with \"godknows\" . Using withColumn, when & otherwise : ```python from pyspark.sql.functions import when df = df.withColumn(\"country\", when(df[\"country\"].isNull(), \"godknows\").otherwise(df[\"country\"])) `` This will replace null values in the country column with \"godknows\"`. Using Filter python df.filter(df[\"age\"] > 30) # Alternative: df.where(df[\"age\"] > 30)","title":"Replacing Specific Values"},{"location":"Spark-DataBricks/1.0_Spark/1.6_dropna_fillna_df_missing_val_handling/#imputation","text":"Fill Null Values with Mean of the Column : ```python from pyspark.sql.functions import mean mean_price = df.select(mean(\"price\")).collect()[0][0] df = df.na.fill({\"price\": mean_price}) `` This will replace null values in the price` column with the mean value of that column.","title":"Imputation"},{"location":"Spark-DataBricks/1.0_Spark/1.7_distinct_dropDuplicate_windowsFunc/","text":"Removing Duplicate Rows in PySpark DataFrames What is a Duplicate Row? Common Methods to Remove Duplicates Advanced Duplicate Removal Removing Duplicate Rows in PySpark DataFrames Duplicate rows in dataframes are very common. Here, I will show you some common methods to remove duplicate rows from dataframes. What is a Duplicate Row? A duplicate row is a row that is an exact copy of another row. Let's look at an example: data = [ (\"Donald\", \"Trump\", 70), (\"Jo\", \"Biden\", 99), (\"Barrack\", \"Obama\", 60), (\"Donald\", \"Trump\", 70), # A real duplicate row. A mirror copy of another row (\"Donald\", \"Duck\", 70) # Not a duplicate ] Common Methods to Remove Duplicates Using distinct() : Using distinct() : The first method that comes to mind. It removes duplicate rows that are 100% the same, meaning all columns are checked. python df = df.distinct() Using dropDuplicates() : Using dropDuplicates() : This method is exactly like distinct() . python df = df.dropDuplicates() # Alternative: df = df.drop_duplicates() Removing Duplicates when only few columns are duplicates : Using subset=[\"col1\", \"col2\"] : If it's not 100% the same, but some columns are the same, you can still remove duplicates based on those columns using df.dropDuplicates(subset=[\"col1\", \"col2\"]) . python df = df.dropDuplicates(subset=[\"name\", \"age\"]) # Alternative: df = df.drop_duplicates(subset=[\"name\", \"age\"]) Advanced Duplicate Removal Normally, using one function, df.dropDuplicates(subset=[\"name\", \"age\"]) , is enough to handle all kinds of duplicate removal. But, in some cases, the requirement may be more customized. In such cases, we have to use window functions. The code below shows how to handle special cases of removing duplicates using window functions in PySpark. Each example shows how window functions can be used for specific needs that dropDuplicates() cannot handle. from pyspark.sql import SparkSession from pyspark.sql.window import Window from pyspark.sql.functions import row_number, sum, desc # Initialize Spark session spark = SparkSession.builder.appName(\"AdvancedDuplicateRemoval\").getOrCreate() # Sample DataFrames for each scenario data1 = [ (1, \"Alice\", \"2021-01-01\"), (1, \"Alice\", \"2021-01-02\"), (2, \"Bob\", \"2021-01-01\"), (2, \"Bob\", \"2021-01-03\"), (3, \"Charlie\", \"2021-01-01\") ] columns1 = [\"user_id\", \"name\", \"timestamp\"] df1 = spark.createDataFrame(data1, columns1) data2 = [ (1, \"GroupA\", 5), (2, \"GroupA\", 10), (3, \"GroupB\", 15), (4, \"GroupB\", 8), (5, \"GroupC\", 12) ] columns2 = [\"id\", \"group_id\", \"priority\"] df2 = spark.createDataFrame(data2, columns2) data3 = [ (1, \"TXN1\", 100), (2, \"TXN1\", 200), (3, \"TXN2\", 300), (4, \"TXN2\", 400), (5, \"TXN3\", 500) ] columns3 = [\"id\", \"transaction_id\", \"sales_amount\"] df3 = spark.createDataFrame(data3, columns3) # Scenario 1: Keep the latest duplicate based on a timestamp column window_spec1 = Window.partitionBy(\"user_id\").orderBy(desc(\"timestamp\")) df1_with_row_num = df1.withColumn(\"row_num\", row_number().over(window_spec1)) unique_df1 = df1_with_row_num.filter(df1_with_row_num.row_num == 1).drop(\"row_num\") print(\"Scenario 1: Keep the most recent entry\") unique_df1.show() # Scenario 2: Keep the duplicate with the highest priority value within each group window_spec2 = Window.partitionBy(\"group_id\").orderBy(desc(\"priority\")) df2_with_row_num = df2.withColumn(\"row_num\", row_number().over(window_spec2)) unique_df2 = df2_with_row_num.filter(df2_with_row_num.row_num == 1).drop(\"row_num\") print(\"Scenario 2: Keep the highest priority entry within each group\") unique_df2.show() # Scenario 3: Aggregate data from duplicate rows before removing them window_spec3 = Window.partitionBy(\"transaction_id\") df3_with_aggregates = df3.withColumn(\"total_sales\", sum(\"sales_amount\").over(window_spec3)) unique_df3 = df3_with_aggregates.dropDuplicates([\"transaction_id\"]) print(\"Scenario 3: Aggregate sales amounts and keep one entry per transaction ID\") unique_df3.show()","title":"Removing Duplicates - PySpark"},{"location":"Spark-DataBricks/1.0_Spark/1.7_distinct_dropDuplicate_windowsFunc/#removing-duplicate-rows-in-pyspark-dataframes","text":"Duplicate rows in dataframes are very common. Here, I will show you some common methods to remove duplicate rows from dataframes.","title":"Removing Duplicate Rows in PySpark DataFrames"},{"location":"Spark-DataBricks/1.0_Spark/1.7_distinct_dropDuplicate_windowsFunc/#what-is-a-duplicate-row","text":"A duplicate row is a row that is an exact copy of another row. Let's look at an example: data = [ (\"Donald\", \"Trump\", 70), (\"Jo\", \"Biden\", 99), (\"Barrack\", \"Obama\", 60), (\"Donald\", \"Trump\", 70), # A real duplicate row. A mirror copy of another row (\"Donald\", \"Duck\", 70) # Not a duplicate ]","title":"What is a Duplicate Row?"},{"location":"Spark-DataBricks/1.0_Spark/1.7_distinct_dropDuplicate_windowsFunc/#common-methods-to-remove-duplicates","text":"Using distinct() : Using distinct() : The first method that comes to mind. It removes duplicate rows that are 100% the same, meaning all columns are checked. python df = df.distinct() Using dropDuplicates() : Using dropDuplicates() : This method is exactly like distinct() . python df = df.dropDuplicates() # Alternative: df = df.drop_duplicates() Removing Duplicates when only few columns are duplicates : Using subset=[\"col1\", \"col2\"] : If it's not 100% the same, but some columns are the same, you can still remove duplicates based on those columns using df.dropDuplicates(subset=[\"col1\", \"col2\"]) . python df = df.dropDuplicates(subset=[\"name\", \"age\"]) # Alternative: df = df.drop_duplicates(subset=[\"name\", \"age\"])","title":"Common Methods to Remove Duplicates"},{"location":"Spark-DataBricks/1.0_Spark/1.7_distinct_dropDuplicate_windowsFunc/#advanced-duplicate-removal","text":"Normally, using one function, df.dropDuplicates(subset=[\"name\", \"age\"]) , is enough to handle all kinds of duplicate removal. But, in some cases, the requirement may be more customized. In such cases, we have to use window functions. The code below shows how to handle special cases of removing duplicates using window functions in PySpark. Each example shows how window functions can be used for specific needs that dropDuplicates() cannot handle. from pyspark.sql import SparkSession from pyspark.sql.window import Window from pyspark.sql.functions import row_number, sum, desc # Initialize Spark session spark = SparkSession.builder.appName(\"AdvancedDuplicateRemoval\").getOrCreate() # Sample DataFrames for each scenario data1 = [ (1, \"Alice\", \"2021-01-01\"), (1, \"Alice\", \"2021-01-02\"), (2, \"Bob\", \"2021-01-01\"), (2, \"Bob\", \"2021-01-03\"), (3, \"Charlie\", \"2021-01-01\") ] columns1 = [\"user_id\", \"name\", \"timestamp\"] df1 = spark.createDataFrame(data1, columns1) data2 = [ (1, \"GroupA\", 5), (2, \"GroupA\", 10), (3, \"GroupB\", 15), (4, \"GroupB\", 8), (5, \"GroupC\", 12) ] columns2 = [\"id\", \"group_id\", \"priority\"] df2 = spark.createDataFrame(data2, columns2) data3 = [ (1, \"TXN1\", 100), (2, \"TXN1\", 200), (3, \"TXN2\", 300), (4, \"TXN2\", 400), (5, \"TXN3\", 500) ] columns3 = [\"id\", \"transaction_id\", \"sales_amount\"] df3 = spark.createDataFrame(data3, columns3) # Scenario 1: Keep the latest duplicate based on a timestamp column window_spec1 = Window.partitionBy(\"user_id\").orderBy(desc(\"timestamp\")) df1_with_row_num = df1.withColumn(\"row_num\", row_number().over(window_spec1)) unique_df1 = df1_with_row_num.filter(df1_with_row_num.row_num == 1).drop(\"row_num\") print(\"Scenario 1: Keep the most recent entry\") unique_df1.show() # Scenario 2: Keep the duplicate with the highest priority value within each group window_spec2 = Window.partitionBy(\"group_id\").orderBy(desc(\"priority\")) df2_with_row_num = df2.withColumn(\"row_num\", row_number().over(window_spec2)) unique_df2 = df2_with_row_num.filter(df2_with_row_num.row_num == 1).drop(\"row_num\") print(\"Scenario 2: Keep the highest priority entry within each group\") unique_df2.show() # Scenario 3: Aggregate data from duplicate rows before removing them window_spec3 = Window.partitionBy(\"transaction_id\") df3_with_aggregates = df3.withColumn(\"total_sales\", sum(\"sales_amount\").over(window_spec3)) unique_df3 = df3_with_aggregates.dropDuplicates([\"transaction_id\"]) print(\"Scenario 3: Aggregate sales amounts and keep one entry per transaction ID\") unique_df3.show()","title":"Advanced Duplicate Removal"},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping/","text":"Partitioning and Bucketing in Spark Partitioning Example Bucketing How partion and bucket info is stored? Adding New Rows Combining Partitioning and Bucketing Example Lets summarize Partitioning and Bucketing in Spark Both are techniques to make things run faster. Both partitioning and bucketing split rows in a table into separate files . But, there are differences. In this article I will try to highlight some main points about these two techniques. Partitioning Partitioning in Spark is simple: it splits data by rows, not by columns. For example, all rows where the country is USA become one partition. Columns are not split, so never confuse this with column-based splitting. Partitioning divides data into separate folders based on the values of one or more columns. Each partition folder contains all rows that share the same value for the partitioning column(s). This helps improve read performance for queries that filter on the partition column. Example Consider the following sample data: # Sample data +---+----+-------+ | id|name|country| +---+----+-------+ | 1 | A | USA | | 2 | B | CAN | | 3 | C | USA | | 4 | D | CAN | +---+----+-------+ Partitioning this data by country : # Partitioning by country partDF = df.repartition(\"country\") Resulting in partitions: Partition 1 (for USA) : python +---+----+-------+ | id|name|country| +---+----+-------+ | 1 | A | USA | | 3 | C | USA | +---+----+-------+ Partition 2 (for Canada) : python +---+----+-------+ | id|name|country| +---+----+-------+ | 2 | B | CAN | | 4 | D | CAN | +---+----+-------+ When you save this partitioned data to disk: df.write.partitionBy(\"country\").parquet(\"/tables_root_folder\") This will create a folder structure like this: /table_root_folder/ \u251c\u2500\u2500 country=USA/ \u2502 \u2514\u2500\u2500 part-00000.parquet \u2514\u2500\u2500 country=CAN/ \u2514\u2500\u2500 part-00000.parquet Bucketing Bucketing is another way to organize data. It divides data into a fixed number of files(called buckets) based on the hash value of a specified column. Each bucket is a separate physical file within the table's folder. Rows with the same value for the bucket column are stored in the same bucket file. This makes data operations like joins and aggregations faster. For example, if you bucket a dataset by customer_id into 4 buckets, the table's folder will contain 4 bucket files. # Bucketing by customer_id df.write.bucketBy(4, \"customer_id\").saveAsTable(\"tables_root_folder\") The folder structure will look like this: /tables_root_folder/ \u251c\u2500\u2500 part-00000 \u251c\u2500\u2500 part-00001 \u251c\u2500\u2500 part-00002 \u2514\u2500\u2500 part-00003 How partion and bucket info is stored? When using Hive Metastore, both partitioning and bucketing metadata are stored in the Metastore. This includes information about which columns are used for partitioning and bucketing, and the number of buckets. Partitioning Metadata: The Metastore keeps track of the partition columns and the structure of the directories. Bucketing Metadata: The Metastore stores information about the bucketing column(s) and the number of buckets. This metadata allows Spark to efficiently read and write data by knowing the organization of the table. Adding New Rows When you add new rows to a partitioned table, Spark places them in the appropriate partition folder without reorganizing the entire dataset. # Adding new rows to a partitioned table new_data.write.mode(\"append\").partitionBy(\"country\").parquet(\"/path/to/partitioned_data\") For a bucketed table, new rows are appended to the existing bucket files based on the hash of the bucketing column. This keeps the bucketing scheme efficient without needing to rebucket the entire dataset. # Adding new rows to a bucketed table new_data.write.bucketBy(4, \"customer_id\").mode(\"append\").saveAsTable(\"bucketed_table\") Combining Partitioning and Bucketing You can also combine partitioning and bucketing to leverage the benefits of both techniques. This is useful when you need to optimize data access for multiple dimensions. Example Consider partitioning by country and bucketing by customer_id . # Partitioning by country and bucketing by customer_id df.write.partitionBy(\"country\").bucketBy(4, \"customer_id\").saveAsTable(\"part_buck_table\") This creates a folder structure where each partition is further divided into buckets: /part_buck_table/ \u251c\u2500\u2500 country=USA/ \u2502 \u251c\u2500\u2500 part-00000 \u2502 \u251c\u2500\u2500 part-00001 \u2502 \u251c\u2500\u2500 part-00002 \u2502 \u2514\u2500\u2500 part-00003 \u2514\u2500\u2500 country=CAN/ \u251c\u2500\u2500 part-00000 \u251c\u2500\u2500 part-00001 \u251c\u2500\u2500 part-00002 \u2514\u2500\u2500 part-00003 Lets summarize Feature Partitioning Bucketing Method Splits data by rows based on column values Splits data into a fixed number of buckets based on hash Storage Separate directories for each partition value Separate files (buckets) within the table folder Example df.write.partitionBy(\"country\").parquet(\"/table_root_folder\") df.write.bucketBy(4, \"customer_id\").saveAsTable(\"tableName\") folder Structure /table_root_folder/country=USA/ /table_root_folder/country=CAN/ /table_root_folder/part-00000 /table_root_folder/part-00001 Efficiency Optimizes read performance for specific queries Optimizes join and aggregation performance Metadata Storage Stored in Hive Metastore for partition columns Stored in Hive Metastore for bucketing columns and number of buckets Adding New Rows New rows go to the appropriate partition folder New rows are appended to the appropriate bucket files Combining Can be combined with bucketing for multi-dimensional optimization Can be combined with partitioning for multi-dimensional optimization","title":"Partition And Bucket"},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping/#partitioning-and-bucketing-in-spark","text":"Both are techniques to make things run faster. Both partitioning and bucketing split rows in a table into separate files . But, there are differences. In this article I will try to highlight some main points about these two techniques.","title":"Partitioning and Bucketing in Spark"},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping/#partitioning","text":"Partitioning in Spark is simple: it splits data by rows, not by columns. For example, all rows where the country is USA become one partition. Columns are not split, so never confuse this with column-based splitting. Partitioning divides data into separate folders based on the values of one or more columns. Each partition folder contains all rows that share the same value for the partitioning column(s). This helps improve read performance for queries that filter on the partition column.","title":"Partitioning"},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping/#example","text":"Consider the following sample data: # Sample data +---+----+-------+ | id|name|country| +---+----+-------+ | 1 | A | USA | | 2 | B | CAN | | 3 | C | USA | | 4 | D | CAN | +---+----+-------+ Partitioning this data by country : # Partitioning by country partDF = df.repartition(\"country\") Resulting in partitions: Partition 1 (for USA) : python +---+----+-------+ | id|name|country| +---+----+-------+ | 1 | A | USA | | 3 | C | USA | +---+----+-------+ Partition 2 (for Canada) : python +---+----+-------+ | id|name|country| +---+----+-------+ | 2 | B | CAN | | 4 | D | CAN | +---+----+-------+ When you save this partitioned data to disk: df.write.partitionBy(\"country\").parquet(\"/tables_root_folder\") This will create a folder structure like this: /table_root_folder/ \u251c\u2500\u2500 country=USA/ \u2502 \u2514\u2500\u2500 part-00000.parquet \u2514\u2500\u2500 country=CAN/ \u2514\u2500\u2500 part-00000.parquet","title":"Example"},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping/#bucketing","text":"Bucketing is another way to organize data. It divides data into a fixed number of files(called buckets) based on the hash value of a specified column. Each bucket is a separate physical file within the table's folder. Rows with the same value for the bucket column are stored in the same bucket file. This makes data operations like joins and aggregations faster. For example, if you bucket a dataset by customer_id into 4 buckets, the table's folder will contain 4 bucket files. # Bucketing by customer_id df.write.bucketBy(4, \"customer_id\").saveAsTable(\"tables_root_folder\") The folder structure will look like this: /tables_root_folder/ \u251c\u2500\u2500 part-00000 \u251c\u2500\u2500 part-00001 \u251c\u2500\u2500 part-00002 \u2514\u2500\u2500 part-00003","title":"Bucketing"},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping/#how-partion-and-bucket-info-is-stored","text":"When using Hive Metastore, both partitioning and bucketing metadata are stored in the Metastore. This includes information about which columns are used for partitioning and bucketing, and the number of buckets. Partitioning Metadata: The Metastore keeps track of the partition columns and the structure of the directories. Bucketing Metadata: The Metastore stores information about the bucketing column(s) and the number of buckets. This metadata allows Spark to efficiently read and write data by knowing the organization of the table.","title":"How partion and bucket info is stored?"},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping/#adding-new-rows","text":"When you add new rows to a partitioned table, Spark places them in the appropriate partition folder without reorganizing the entire dataset. # Adding new rows to a partitioned table new_data.write.mode(\"append\").partitionBy(\"country\").parquet(\"/path/to/partitioned_data\") For a bucketed table, new rows are appended to the existing bucket files based on the hash of the bucketing column. This keeps the bucketing scheme efficient without needing to rebucket the entire dataset. # Adding new rows to a bucketed table new_data.write.bucketBy(4, \"customer_id\").mode(\"append\").saveAsTable(\"bucketed_table\")","title":"Adding New Rows"},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping/#combining-partitioning-and-bucketing","text":"You can also combine partitioning and bucketing to leverage the benefits of both techniques. This is useful when you need to optimize data access for multiple dimensions.","title":"Combining Partitioning and Bucketing"},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping/#example_1","text":"Consider partitioning by country and bucketing by customer_id . # Partitioning by country and bucketing by customer_id df.write.partitionBy(\"country\").bucketBy(4, \"customer_id\").saveAsTable(\"part_buck_table\") This creates a folder structure where each partition is further divided into buckets: /part_buck_table/ \u251c\u2500\u2500 country=USA/ \u2502 \u251c\u2500\u2500 part-00000 \u2502 \u251c\u2500\u2500 part-00001 \u2502 \u251c\u2500\u2500 part-00002 \u2502 \u2514\u2500\u2500 part-00003 \u2514\u2500\u2500 country=CAN/ \u251c\u2500\u2500 part-00000 \u251c\u2500\u2500 part-00001 \u251c\u2500\u2500 part-00002 \u2514\u2500\u2500 part-00003","title":"Example"},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping/#lets-summarize","text":"Feature Partitioning Bucketing Method Splits data by rows based on column values Splits data into a fixed number of buckets based on hash Storage Separate directories for each partition value Separate files (buckets) within the table folder Example df.write.partitionBy(\"country\").parquet(\"/table_root_folder\") df.write.bucketBy(4, \"customer_id\").saveAsTable(\"tableName\") folder Structure /table_root_folder/country=USA/ /table_root_folder/country=CAN/ /table_root_folder/part-00000 /table_root_folder/part-00001 Efficiency Optimizes read performance for specific queries Optimizes join and aggregation performance Metadata Storage Stored in Hive Metastore for partition columns Stored in Hive Metastore for bucketing columns and number of buckets Adding New Rows New rows go to the appropriate partition folder New rows are appended to the appropriate bucket files Combining Can be combined with bucketing for multi-dimensional optimization Can be combined with partitioning for multi-dimensional optimization","title":"Lets summarize"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset/","text":"Understanding RDDs, DataFrames, and Datasets in Apache Spark Apache Spark provides three main ways to handle data: RDDs, DataFrames, and Datasets. Let's see what they are and why three? RDD (Resilient Distributed Dataset) Imagine you have a Python list. Now, if we make this list capable of parallel processing, that\u2019s an RDD. It\u2019s just a collection of JVM objects that Spark can process in parallel. Most of the features that Spark boasts about, such as in-memory processing, lazy evaluation, partitioning for parallel processing, and fault tolerance, are all due to RDDs. They are the foundation of Spark's powerful capabilities. Features of RDDs Functional operations like map , filter , and reduce . But, no SQL-capability. This is like C programming. Godo for hard-code programmers who need find-grained control. However, RDDs do not have type safety . This means you might run into runtime errors if your data types are not consistent. Look at this example: ```python from pyspark import SparkContext Initialize SparkContext sc = SparkContext(\"local\", \"RDD Example\") Create an RDD with mixed data types data = [1, 2, \"three\", 4] rdd = sc.parallelize(data) This will cause a runtime error because 'three' cannot be squared rdd.foreach(lambda x: print(x ** 2)) ``` Here, trying to square the string \"three\" will cause a runtime error. This is a main problem of RDDs \u2013 they are not type safe. Also RDDs are read-only (Immutable). You can't change them: ```python Create an RDD data = [1, 2, 3, 4, 5] rdd = sc.parallelize(data) rdd[0] = 10 # This will cause an error See, how dataframe doesn't have this issue Create a DataFrame data = [(1,), (2,), (3,)] df = spark.createDataFrame(data, [\"value\"]) Modify the value 1 to 10 df = df.withColumn(\"value\", when(col(\"value\") == 1, 10).otherwise(col(\"value\"))) ``` What happens when we create an RDD Say we have a standard Python list object, containing elements [1, 2, 3, 4, 5] . We call sc.parallelize(data) to create an RDD. What happens? data = [1, 2, 3, 4, 5] # Standard Python List Object rdd = sc.parallelize(data) # RDD Object Partitioning: The list [1, 2, 3, 4, 5] is split into partitions, such as [1, 2] , [3, 4] , and [5] . RDD Metadata: An RDD object is created . This RDD object contains info(metadata) about partioning and how they are distributed to which node. The partitions are sent to different worker nodes in the Spark cluster for processing. Functions in RDDs Transformations: map(), filter(), reduceByKey(), groupByKey(), union(), intersection() Actions: count(), collect(), reduce(), take(n), foreach(), first() DataFrame Now, let\u2019s think about SQL programmers. They are used to working with tables. This is where DataFrames come in. A DataFrame is like a table with rows and columns. Advantages of DataFrames: - Schema Support: Dataframes are just like tables. They have schema, they have columns. Columns are of type, like int, string etc. - SQL Query engine: You can run a SQL query using a DataFrame. When the query is run, it uses the Spark SQL engine. This engine does the query optimization and execution optimization for you. This is not possible with RDDs. When working with big data, this is a big deal. What if your query takes an eternity to complete? The Spark SQL engine ensures that your queries run efficiently and quickly. - Easy to use: Using DataFrames is also very easy, especially if you are familiar with SQL, because you can run SQL queries directly on them. Plus, DataFrames can integrate easily with various data sources like Hive, Avro, Parquet, and JSON. This makes DataFrames not only powerful but also very flexible and user-friendly. However, DataFrames are not type safe , similar to RDDs. This means type errors can still occur at runtime. Dataset Datasets combine the best features of RDDs and DataFrames. They offer both type safety and optimized execution. Why Datasets? - Type Safety: Datasets provide compile-time type checking, reducing runtime errors. - Optimized Execution: They use the same optimized execution engine as DataFrames, which means efficient processing and query optimization. - Functional and Relational Operations: Datasets support both functional programming (like RDDs) and SQL operations (like DataFrames). - Ease of Use: They combine the ease of use of DataFrames with the type safety of RDDs. Dataset is only for Java and Scale. No PySpark Evolution Summary RDDs (2011): Just a collection of JVM objects. Good for parallel processing and lets you use many functional APIs like map, filter, etc. However, they lacked type safety and could cause runtime errors. DataFrames (2013): Introduced SQL-like tables and the Spark SQL engine. Now, we have tables to work on with schemas. They are also faster than RDDs due to the optimized engine but still lack type safety. Datasets (2015): Combine type safety, optimized execution, and ease of use, offering the best of both RDDs and DataFrames. Conclusion RDD, Dfs, Ds conversion RDDs: toDF() and rdd() for conversion. DataFrames: rdd() for RDDs, as[] for Datasets. Datasets: toDF() for DataFrames, rdd() for RDDs. What to use? If you use Pyspark, use Dataframe. If you use Scale use Dataset. Period. RDD? If you are a coding ninja working for a product company and spend your time living inside a compuer ram. Use RDD.","title":"RDD-Dataframe-Dataset"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset/#understanding-rdds-dataframes-and-datasets-in-apache-spark","text":"Apache Spark provides three main ways to handle data: RDDs, DataFrames, and Datasets. Let's see what they are and why three?","title":"Understanding RDDs, DataFrames, and Datasets in Apache Spark"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset/#rdd-resilient-distributed-dataset","text":"Imagine you have a Python list. Now, if we make this list capable of parallel processing, that\u2019s an RDD. It\u2019s just a collection of JVM objects that Spark can process in parallel. Most of the features that Spark boasts about, such as in-memory processing, lazy evaluation, partitioning for parallel processing, and fault tolerance, are all due to RDDs. They are the foundation of Spark's powerful capabilities.","title":"RDD (Resilient Distributed Dataset)"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset/#features-of-rdds","text":"Functional operations like map , filter , and reduce . But, no SQL-capability. This is like C programming. Godo for hard-code programmers who need find-grained control. However, RDDs do not have type safety . This means you might run into runtime errors if your data types are not consistent. Look at this example: ```python from pyspark import SparkContext","title":"Features of RDDs"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset/#initialize-sparkcontext","text":"sc = SparkContext(\"local\", \"RDD Example\")","title":"Initialize SparkContext"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset/#create-an-rdd-with-mixed-data-types","text":"data = [1, 2, \"three\", 4] rdd = sc.parallelize(data)","title":"Create an RDD with mixed data types"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset/#this-will-cause-a-runtime-error-because-three-cannot-be-squared","text":"rdd.foreach(lambda x: print(x ** 2)) ``` Here, trying to square the string \"three\" will cause a runtime error. This is a main problem of RDDs \u2013 they are not type safe. Also RDDs are read-only (Immutable). You can't change them: ```python","title":"This will cause a runtime error because 'three' cannot be squared"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset/#create-an-rdd","text":"data = [1, 2, 3, 4, 5] rdd = sc.parallelize(data) rdd[0] = 10 # This will cause an error","title":"Create an RDD"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset/#see-how-dataframe-doesnt-have-this-issue","text":"","title":"See, how dataframe doesn't have this issue"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset/#create-a-dataframe","text":"data = [(1,), (2,), (3,)] df = spark.createDataFrame(data, [\"value\"])","title":"Create a DataFrame"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset/#modify-the-value-1-to-10","text":"df = df.withColumn(\"value\", when(col(\"value\") == 1, 10).otherwise(col(\"value\"))) ```","title":"Modify the value 1 to 10"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset/#what-happens-when-we-create-an-rdd","text":"Say we have a standard Python list object, containing elements [1, 2, 3, 4, 5] . We call sc.parallelize(data) to create an RDD. What happens? data = [1, 2, 3, 4, 5] # Standard Python List Object rdd = sc.parallelize(data) # RDD Object Partitioning: The list [1, 2, 3, 4, 5] is split into partitions, such as [1, 2] , [3, 4] , and [5] . RDD Metadata: An RDD object is created . This RDD object contains info(metadata) about partioning and how they are distributed to which node. The partitions are sent to different worker nodes in the Spark cluster for processing.","title":"What happens when we create an RDD"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset/#functions-in-rdds","text":"Transformations: map(), filter(), reduceByKey(), groupByKey(), union(), intersection() Actions: count(), collect(), reduce(), take(n), foreach(), first()","title":"Functions in RDDs"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset/#dataframe","text":"Now, let\u2019s think about SQL programmers. They are used to working with tables. This is where DataFrames come in. A DataFrame is like a table with rows and columns. Advantages of DataFrames: - Schema Support: Dataframes are just like tables. They have schema, they have columns. Columns are of type, like int, string etc. - SQL Query engine: You can run a SQL query using a DataFrame. When the query is run, it uses the Spark SQL engine. This engine does the query optimization and execution optimization for you. This is not possible with RDDs. When working with big data, this is a big deal. What if your query takes an eternity to complete? The Spark SQL engine ensures that your queries run efficiently and quickly. - Easy to use: Using DataFrames is also very easy, especially if you are familiar with SQL, because you can run SQL queries directly on them. Plus, DataFrames can integrate easily with various data sources like Hive, Avro, Parquet, and JSON. This makes DataFrames not only powerful but also very flexible and user-friendly. However, DataFrames are not type safe , similar to RDDs. This means type errors can still occur at runtime.","title":"DataFrame"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset/#dataset","text":"Datasets combine the best features of RDDs and DataFrames. They offer both type safety and optimized execution. Why Datasets? - Type Safety: Datasets provide compile-time type checking, reducing runtime errors. - Optimized Execution: They use the same optimized execution engine as DataFrames, which means efficient processing and query optimization. - Functional and Relational Operations: Datasets support both functional programming (like RDDs) and SQL operations (like DataFrames). - Ease of Use: They combine the ease of use of DataFrames with the type safety of RDDs. Dataset is only for Java and Scale. No PySpark","title":"Dataset"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset/#evolution-summary","text":"RDDs (2011): Just a collection of JVM objects. Good for parallel processing and lets you use many functional APIs like map, filter, etc. However, they lacked type safety and could cause runtime errors. DataFrames (2013): Introduced SQL-like tables and the Spark SQL engine. Now, we have tables to work on with schemas. They are also faster than RDDs due to the optimized engine but still lack type safety. Datasets (2015): Combine type safety, optimized execution, and ease of use, offering the best of both RDDs and DataFrames.","title":"Evolution Summary"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset/#conclusion","text":"","title":"Conclusion"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset/#rdd-dfs-ds-conversion","text":"RDDs: toDF() and rdd() for conversion. DataFrames: rdd() for RDDs, as[] for Datasets. Datasets: toDF() for DataFrames, rdd() for RDDs.","title":"RDD, Dfs, Ds conversion"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset/#what-to-use","text":"If you use Pyspark, use Dataframe. If you use Scale use Dataset. Period. RDD? If you are a coding ninja working for a product company and spend your time living inside a compuer ram. Use RDD.","title":"What to use?"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/","text":"Table of Contents Getting Started with Data Engineering: Key Installations of Java, Spark, \\& Python PySpark Install Java [Oracle JDK] Install Full Apache SPARK Install Python [python.org] Set Env Variables Entries Explanation Link python.exe with PYSPARK_PYTHON %JAVA_HOME%\\bin to PATH Install Pyspark Background Install Pyspark System-Wide Check the Installation See Actual Working Appendix PYSPARK_PYTHON Overview Pyspark Vs Full Spark Overview Getting Started with Data Engineering: Key Installations of Java, Spark, & Python PySpark In this guide, I will show how to set up a complete data engineering setup including Java, Full Hadoop, Python, and PySpark. Additionally, I'll describe the significance of setting different environment variables, their roles, and the key differences between Pyspark and a complete Spark setup. Install Java [Oracle JDK] I've opted for the traditional Java, bringing with it the familiar folder system. Feel free to explore variants like OpenJDK. Download the JDK 11 (or later) installer from Oracle JDK Downloads page Install JDK in the default directory (typically C:\\Program Files\\Java\\jdk-11 ). To verify the installation, enter java -version in your command prompt. You should see output similar to this: Install Full Apache SPARK Download spark-3.5.0-bin-hadoop3.tgz from spark.apache.org Create a folder C:\\Spark . Place the unzipped contents of spark-3.5.0-bin-hadoop3.tgz inside it. Your C:\\Spark folder should now contain lib, bin etc. Establish this folder structure: C:\\hadoop\\bin . Download winutils.exe from github/cdarlint and place it inside C:\\hadoop\\bin Install Python [python.org] Download python-3.12.0-amd64.exe (or similar) from the Python Downloads page Execute the downloaded installer and opt for Customize Installation . Ensure you select Add python.exe to PATH . Proceed with all optional features and click Next. In Advanced Options, select \"Install Python 3.12 for all users\". A successful setup should show up in a Setup Success message. Verify the installation by typing python --version in your command prompt. The Python version number indicates a successful installation. Set Env Variables Entries Navigate to Run \u27a4 SYSDM.CPL \u27a4 Advanced \u27a4 Environment Variables, and create or set these environment variables at the system (recommended) or user levels: Variable Value JAVA_HOME C:\\Program Files\\Java\\jdk-11 SPARK_HOME C:\\Spark HADOOP_HOME C:\\hadoop PYSPARK_PYTHON C:\\Python39\\python.exe Path %JAVA_HOME%\\bin %SPARK_HOME%\\bin %HADOOP_HOME%\\bin For a PowerShell command to set these variables with Admin privileges, remember to change 'Machine' (for system-wide level) to 'User' (for user level) as required. Explanation Link python.exe with PYSPARK_PYTHON We set the PYSPARK_PYTHON environment variable to C:\\Python39\\python.exe to specify which Python executable Spark should use. This is vital, particularly if you have multiple Python installations. %JAVA_HOME%\\bin to PATH While C:\\Program Files\\Common Files\\Oracle\\Java\\javapath might already be in your system's Path environment variable, it's generally advisable to add %JAVA_HOME%\\bin to your Path . This ensures your system uses the JDK's executables, rather than those from another Java installation. Install Pyspark Background If your code involves creating a Spark session and dataframes, you'll need PySpark Libraries. Install it using pip install pyspark . This does two things: - Installs the libraries - Installs a 'miniature, standalone' Spark environment for testing However, in our case, we don't need the 'miniature Spark' that comes with PySpark libraries. We'll manage potential Spark conflicts with the SPARK_HOME variable set to our full Spark environment. Install Pyspark System-Wide Open a command prompt with Admin privilege. Use pip (included with our Python) and execute pip install pyspark for a system-wide installation. Check the Installation After installation, confirm if PySpark is in the global site-packages : pip show pyspark The Location: field in the output reveals the installation location. See Actual Working Test your PySpark installation by starting a Spark session in a Python environment: from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"TestApp\") \\ .getOrCreate() print(spark.version) spark.stop() If Spark starts without errors, your PySpark setup with Python is successful. Appendix PYSPARK_PYTHON Overview Selects Python Interpreter : Designates which Python version Spark executors should use for UDFs and transformations. Key in setups with multiple Python versions. Uniformity in Clusters : Guarantees that all cluster nodes use the same Python environment, maintaining consistency in PySpark. Pyspark Vs Full Spark Overview I have put this in another section here. Read More.. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Install-PySpark-Windows"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/#table-of-contents","text":"Getting Started with Data Engineering: Key Installations of Java, Spark, \\& Python PySpark Install Java [Oracle JDK] Install Full Apache SPARK Install Python [python.org] Set Env Variables Entries Explanation Link python.exe with PYSPARK_PYTHON %JAVA_HOME%\\bin to PATH Install Pyspark Background Install Pyspark System-Wide Check the Installation See Actual Working Appendix PYSPARK_PYTHON Overview Pyspark Vs Full Spark Overview","title":"Table of Contents"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/#getting-started-with-data-engineering-key-installations-of-java-spark-python-pyspark","text":"In this guide, I will show how to set up a complete data engineering setup including Java, Full Hadoop, Python, and PySpark. Additionally, I'll describe the significance of setting different environment variables, their roles, and the key differences between Pyspark and a complete Spark setup.","title":"Getting Started with Data Engineering: Key Installations of Java, Spark, &amp; Python PySpark"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/#install-java-oracle-jdk","text":"I've opted for the traditional Java, bringing with it the familiar folder system. Feel free to explore variants like OpenJDK. Download the JDK 11 (or later) installer from Oracle JDK Downloads page Install JDK in the default directory (typically C:\\Program Files\\Java\\jdk-11 ). To verify the installation, enter java -version in your command prompt. You should see output similar to this:","title":"Install Java [Oracle JDK]"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/#install-full-apache-spark","text":"Download spark-3.5.0-bin-hadoop3.tgz from spark.apache.org Create a folder C:\\Spark . Place the unzipped contents of spark-3.5.0-bin-hadoop3.tgz inside it. Your C:\\Spark folder should now contain lib, bin etc. Establish this folder structure: C:\\hadoop\\bin . Download winutils.exe from github/cdarlint and place it inside C:\\hadoop\\bin","title":"Install Full Apache SPARK"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/#install-python-pythonorg","text":"Download python-3.12.0-amd64.exe (or similar) from the Python Downloads page Execute the downloaded installer and opt for Customize Installation . Ensure you select Add python.exe to PATH . Proceed with all optional features and click Next. In Advanced Options, select \"Install Python 3.12 for all users\". A successful setup should show up in a Setup Success message. Verify the installation by typing python --version in your command prompt. The Python version number indicates a successful installation.","title":"Install Python [python.org]"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/#set-env-variables","text":"","title":"Set Env Variables"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/#entries","text":"Navigate to Run \u27a4 SYSDM.CPL \u27a4 Advanced \u27a4 Environment Variables, and create or set these environment variables at the system (recommended) or user levels: Variable Value JAVA_HOME C:\\Program Files\\Java\\jdk-11 SPARK_HOME C:\\Spark HADOOP_HOME C:\\hadoop PYSPARK_PYTHON C:\\Python39\\python.exe Path %JAVA_HOME%\\bin %SPARK_HOME%\\bin %HADOOP_HOME%\\bin For a PowerShell command to set these variables with Admin privileges, remember to change 'Machine' (for system-wide level) to 'User' (for user level) as required.","title":"Entries"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/#explanation","text":"","title":"Explanation"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/#link-pythonexe-with-pyspark_python","text":"We set the PYSPARK_PYTHON environment variable to C:\\Python39\\python.exe to specify which Python executable Spark should use. This is vital, particularly if you have multiple Python installations.","title":"Link python.exe with PYSPARK_PYTHON"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/#java_homebin-to-path","text":"While C:\\Program Files\\Common Files\\Oracle\\Java\\javapath might already be in your system's Path environment variable, it's generally advisable to add %JAVA_HOME%\\bin to your Path . This ensures your system uses the JDK's executables, rather than those from another Java installation.","title":"%JAVA_HOME%\\bin to PATH"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/#install-pyspark","text":"","title":"Install Pyspark"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/#background","text":"If your code involves creating a Spark session and dataframes, you'll need PySpark Libraries. Install it using pip install pyspark . This does two things: - Installs the libraries - Installs a 'miniature, standalone' Spark environment for testing However, in our case, we don't need the 'miniature Spark' that comes with PySpark libraries. We'll manage potential Spark conflicts with the SPARK_HOME variable set to our full Spark environment.","title":"Background"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/#install-pyspark-system-wide","text":"Open a command prompt with Admin privilege. Use pip (included with our Python) and execute pip install pyspark for a system-wide installation.","title":"Install Pyspark System-Wide"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/#check-the-installation","text":"After installation, confirm if PySpark is in the global site-packages : pip show pyspark The Location: field in the output reveals the installation location.","title":"Check the Installation"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/#see-actual-working","text":"Test your PySpark installation by starting a Spark session in a Python environment: from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"TestApp\") \\ .getOrCreate() print(spark.version) spark.stop() If Spark starts without errors, your PySpark setup with Python is successful.","title":"See Actual Working"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/#appendix","text":"","title":"Appendix"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/#pyspark_python-overview","text":"Selects Python Interpreter : Designates which Python version Spark executors should use for UDFs and transformations. Key in setups with multiple Python versions. Uniformity in Clusters : Guarantees that all cluster nodes use the same Python environment, maintaining consistency in PySpark.","title":"PYSPARK_PYTHON Overview"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/#pyspark-vs-full-spark-overview","text":"I have put this in another section here. Read More.. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Pyspark Vs Full Spark Overview"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/","text":"Table of contents Project Sparkzure Part1 - Connecting Local Spark to Azure Data Lake Overview Kickstart: Integrating Spark with Azure Data Lake Create the containerized setup Register an App for OAuth Authentication Register a new App using App Registration Copy Ids and secret From The App Give the App Permission to the Container Access ADLS Data From Spark Using OAuth Authentication and Service Principal Open VS Code and Connect To the Container Run the code Access data in ADLS container using Storage Account's Access Key Method Common Errors AuthorizationPermissionMismatch During OAuth Authenticaiton Appendix Why Does Spark Rely on Hadoop Libraries to Access Azure Data Lake Storage (ADLS)? Understanding Essential JARs for Azure Data Lake Operations with Spark Project Sparkzure Part2 - Sorting Files in ADLS Container Using Standalone Spark Overview of the Article My Environment The scenario Kickstart Environment Setup Download the jars by running this command from terminal: Copy the jars to the SPARK_HOME/Jars location Run the Spark Code Conclusion Appendix Programmatic options for Creating Containers, Sorting Files etc: Understanding Spark Configuration: spark.jars.packages vs spark.jars The Role of .config('spark.jars.packages', '...') How it Works Example Usage The Utility of .config('spark.jars', '...') How it Functions Example Implementation Conclusion Project Sparkzure Part1 - Connecting Local Spark to Azure Data Lake Overview Azure Databricks to Azure Data Lake is easy and straightforward. All the requied jars pre-installed in Databricks. All you need to do is to create a session and connect. However, connecting a local Spark instance to Azure Data Lake can be complicated, especially when managing JAR dependencies. In this project, I will show you how to connect your local Spark application to ADLS and run a Spark query using Visual Studio Code. The local Spark application will be hosted in a container, but it can also be hosted locally locally ;-) Kickstart: Integrating Spark with Azure Data Lake Create the containerized setup Our environment is set up inside a Docker container running Ubuntu on a Windows OS host. Within this container, Python 3 and Spark are installed. But the steps can be used in local environments as well. Check the python version in the container and find out site-packages directory Often, systems have both Python 2.x and Python 3.x installed. Use the following commands to determine which versions are available: bash python --version python3 --version Determine where PySpark is installed using pip . Your enviornment may have multiple python installation especially if its linux or in a docker. You need to find the right site-packages directory so that the packages are copied to right location. To find out run this command in docker terminal or normal command prompt: bash pip3 show pyspark | grep Location Alternatively, you can get the location by running the command: bash python3 -c \"from distutils.sysconfig import get_python_lib; print(get_python_lib())\" Install wget wget is a tool for downloading files from the internet. If you don\u2019t have it in your environment you can get it using the given command: bash apt-get update && apt-get install -y wget Download Hadoop ADLS JARs I've downloaded and placed the jars here . Download and copy it to a desired location. Alternatively, run the command below to download jars to your home directory bash cd ~ wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.3.3/hadoop-azure-3.3.3.jar wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure-datalake/3.3.3/hadoop-azure-datalake-3.3.3.jar wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.3.3/hadoop-common-3.3.3.jar wget https://repo1.maven.org/maven2/com/microsoft/azure/azure-storage/8.6.6/azure-storage-8.6.6.jar wget https://repo1.maven.org/maven2/com/azure/azure-security-keyvault-secrets/4.3.0/azure-security-keyvault-secrets-4.3.0.jar wget https://repo1.maven.org/maven2/com/azure/azure-identity/1.3.0/azure-identity-1.3.0.jar - After downloading, place the jars in any desired folder. These jars will be referenced during spark session creation. - Alternatively you can use download the jars on-the-fly using maven coordinates using .config('spark.jars.packages', '...') Register an App for OAuth Authentication If you want to access a file(say CSV) in Azure through OAuth authentication, you need to create an App registration and grant this app permission to the CSV. This registered App's identity is used by Spark to authenticate. The same principle applies in Databricks , where an app is already created, named AzureDatabricks . Follow the steps below to register the app and give it permission to the file. Register a new App using App Registration In the Azure Portal to search for App registrations ', select it, and opt for '+ New registration'. Give a name, say adlssparkapp , choose 'Accounts in this organizational directory only', keep the Redirect URI empty, and click Register . Copy Ids and secret From The App After registration, jot down the Application ID and Directory ID . Go to Manage > Certificates & secrets , select + New client secret , label it SparkAppSecret , set an expiration, and click 'Add'. Post-creation, make note of the one-time viewable secret value essential for the Spark-Azure handshake. Give the App Permission to the Container Open the container, navigate to Access Control (IAM) > Role assignments , click Add > Add role assignment , select Storage Blob Contributor , Search for the app adlssparkapp , and click OK . Access ADLS Data From Spark Using OAuth Authentication and Service Principal With the app now registered and the key, ID, and secret in hand, we can proceed to execute the main code. Follow the steps outlined below to continue: Open VS Code and Connect To the Container Open VS Code : Launch Visual Studio Code and click the remote container icon at the bottom left. Attach to Container : From the top menu, choose \"Attach to running container\". Select Container : Pick the displayed running container. This action launches a new VS Code instance connected to that container. Create Notebook : In this instance, create a .ipynb (Jupyter notebook) to execute the subsequent section's code. Connect to the python version where we copied the hadoop jars There could be multiple python versions in a linux enviornment. From VS Code choose the python version whcih has our jars Run the code Run the code below in the jupyter notebok: from pyspark.sql import SparkSession # Initialize a Spark session with necessary configurations for connecting to ADLS #Offline version spark = SparkSession.builder \\ .appName(\"ADLS Access\") \\ .config(\"spark.jars\", \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar\") \\ .getOrCreate() # Or using maven coordinates # Online version spark = SparkSession.builder \\ .appName(\"ADLS Access\") \\ .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.3,\" \"org.apache.hadoop:hadoop-azure-datalake:3.3.3,\" \"org.apache.hadoop:hadoop-common:3.3.3\") \\ .getOrCreate() # Define credentials and storage account details for ADLS access storage_account = \"<The_Storage_Act_Name_Containing_Container>\" app_client_id = \"<The_Client_ID_From_Registered_App>\" app_directory_tenant_id = \"<The_Client_ID_From_Registered_App>\" app_client_secret = \"<The_Secret_Value_From_Registered_App>\" # Configure Spark to use OAuth authentication for ADLS access spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\") spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", app_client_id) spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", app_client_secret) spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{app_directory_tenant_id}/oauth2/token\") # Define the path to the dataset in ADLS and read the CSV file using Spark path = \"abfss://<containerName>@<storaegaccountname>.dfs.core.windows.net/<CSV_File_Name.csv>\" spark.read.format(\"csv\").load(path).show() Access data in ADLS container using Storage Account's Access Key Method Another methods to access ADLS is using the Access key method. Here we get the access key from the storage account then use it to access the files inside it. To use this method, follow these steps: Get the Access Keys from the Storage Account In your storage account, under the Security + networking section in the left sidebar, find and select Access keys . You\u2019ll be presented with two keys: key1 and key2 . Both keys can be used to authenticate, so choose one and copy it. This will be used in the subsequent steps. Execute the code After getting the access key use this code. Replace your access key in the access key location: # Import the required module for creating a Spark session. from pyspark.sql import SparkSession # Initialize the Spark session. The builder pattern is utilized to configure the session. # We set the application name to \"ADLS Access\" for identification in Spark UI. # Necessary JAR files are specified for Spark to connect and interact with Azure Data Lake Storage (ADLS). spark = SparkSession.builder \\ .appName(\"ADLS Access\") \\ .config(\"spark.jars\", \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar\") \\ .getOrCreate() # Specify the Azure storage account name and the associated access key for authentication purposes. storage_account_name = \"<The_Storage_Account_Name>\" storage_account_key = \"<key1_or_key2>\" # Configure Spark to utilize AzureBlobFileSystem. This is essential for Azure Blob storage connectivity. spark.conf.set(f\"fs.azure\", \"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem\") # Authenticate the Spark session by providing the access key for the specified Azure storage account. spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", storage_account_key) # Read the desired CSV file located in ADLS into a DataFrame (df) using Spark. df = spark.read.csv(f\"abfss://<container_name>@{storage_account_name}.dfs.core.windows.net/<filename.csv>\") Common Errors AuthorizationPermissionMismatch During OAuth Authenticaiton While executing the code you may encounter errors like: AuthorizationPermissionMismatch, \"This request is not authorized to perform this operation using this permission.\" or java.nio.file.AccessDeniedException: Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://strgacweatherapp.dfs.core.windows.net/weather-timer/2023-10-19-09.json?upn=false&action=getStatus&timeout=90 at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.checkException(AzureBlobFileSystem.java:1384) OAuth uses a registered apps identity to connect. This app should have permission to the folder where the file resides. Appendix Why Does Spark Rely on Hadoop Libraries to Access Azure Data Lake Storage (ADLS)? Long story short : In a standalone Spark setup, we use specific Hadoop JARs solely for connecting to ADLS. It's important to note that these are just JARs and don't represent the full Hadoop ecosystem. Apache Spark is used for distributed data processing. But for data storage it relies on other systems like ADLS, S3 etc. But why, when connecting Spark to ADLS, do we bring Hadoop into the picture? Let\u2019s find out. Spark's Core Functionality: Spark is designed to process data, not to understand the intricacies of every storage system. It can pull data from various sources, but it doesn't always have native integrations for each one. Hadoop's Role: Hadoop, primarily known for its distributed file system (HDFS), also offers connectors to diverse storage systems . Over time, it has become the standard bridge between storage solutions and big data tools. ADLS and Hadoop Integration: When Microsoft developed ADLS, they provided a connector to the Hadoop FileSystem API. This approach made sense. Why reinvent the wheel when big data tools already communicate efficiently with HDFS via Hadoop's API? Conclusion HSpark uses Hadoop libraries to access ADLS due to the standardized and robust nature of the Hadoop FileSystem API. Microsoft integrated ADLS with this Hadoop API to ensure that ADLS would be compatible with a broad range of big data tools, such as Spark and Hive. This decision was to use the extensive community support of the Hadoop ecosystem and also allowed Microsoft to reuse what was already working In essence, the Hadoop API serves as a bridge between Spark and ADLS. Understanding Essential JARs for Azure Data Lake Operations with Spark hadoop-azure-3.3.3.jar : Description : This library provides support for Azure Blob Storage integration with Hadoop. It contains the WASB (Windows Azure Storage Blob) file system connector. Use-Cases : Reading/writing data from/to Azure Blob Storage (often ADLS Gen1) using Hadoop's FileSystem API. hadoop-azure-datalake-3.3.3.jar : Description : This is the Data Lake connector for Hadoop, providing support for ADLS Gen1. Use-Cases : If you're working with ADLS Gen1, this JAR lets Spark access the data lake using the Hadoop FileSystem API. hadoop-common-3.3.3.jar : Description : The core library for Hadoop, it contains common utilities and the Hadoop FileSystem API. Use-Cases : Fundamental for almost all Hadoop-related operations. It's the foundational library upon which other Hadoop components rely. azure-storage-8.6.6.jar : Description : Azure's storage SDK, facilitating interaction with Azure Storage services like Blob, Queue, and Table. Use-Cases : Interacting with Azure Blob Storage (and by extension, ADLS Gen2 which is built on Blob). It's essential for Spark to communicate and access Azure storage services. azure-security-keyvault-secrets-4.3.0.jar : Description : Provides capabilities to interact with Azure Key Vault's secrets. It facilitates fetching, setting, or managing secrets. Use-Cases : Whenever you need to securely access or manage secrets (like storage account keys or database connection strings) stored in Azure Key Vault from your Spark application. azure-identity-1.3.0.jar : Description : Azure SDK's identity library, providing various credentials classes for Azure Active Directory (AAD) token authentication. Use-Cases : Authenticating against Azure services using AAD-based credentials, especially when trying to securely access resources like Key Vault or ADLS Gen2. Project Sparkzure Part2 - Sorting Files in ADLS Container Using Standalone Spark Overview of the Article In Part 1, we dived into accessing ADLS files with Pyspark and Hadoop Jars. Now, let's switch gears a bit. In this article, we'll explore how to sort\u2014by creating containers and moving/renaming files\u2014the content in an Azure Data Lake Container using just a Standalone Spark application. While there's always the route of Azure Data Factory, Databricks, or Azure Logic Apps, I want to spotlight this approach. Why? Because it's not only a viable alternative, but it also comes with the perk of being nearly cost-free compared to the other Azure services I mentioned. My Environment Deployment Platform: Docker Operating System: Ubuntu Python Version: 3.8.10 Development IDE: Visual Studio Code connected to the container Spark Setup: Standalone Spark installed as part of Pyspark( pip install pyspark ) Spark Home /usr/local/lib/python3.8/dist-packages/pyspark/ Jars Location /usr/local/lib/python3.8/dist-packages/pyspark/jars/ The scenario We have a container name \"weather-timer\" that contains JSON files formatted as YYYY-10-22-12.json . These files hold weather information retrieved from a web API. The files need to be sorted in the format year=yyyy/month=mm/day=dd/hour=hh.json . This is a real-world requirement, as a structure like this can make partition pruning more efficient during query time if you're using a system like Apache Hive or Delta Lake. Kickstart Environment Setup For ADLS connectivity in standalone PySpark, we need to download these 3 super-important Jars: - hadoop-azure-<version>.jar : Supports Azure Blob Storage and Spark integration. - hadoop-azure-datalake-<version>.jar : For for ADLS access, including authentication features. - hadoop-common-<version>.jar : Contains utilities for the other JARs. Download the jars by running this command from terminal: Run teh following command in terminal. Note: The version of jars might change over time. ```bash cd ~ wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.3.3/hadoop-azure-3.3.3.jar wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure-datalake/3.3.3/hadoop-azure-datalake-3.3.3.jar wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.3.3/hadoop-common-3.3.3.jar ##### Copy the jars to the SPARK_HOME/Jars location ```bash cd ~ # assuming you downloaded the JARs in the home directory cp *.jar /usr/local/lib/python3.8/dist-packages/pyspark/jars/ Run the Spark Code This is the code for performing the sorting. It checks if there are containers. If not, it makes them. Before you run the code, make sure you replace placeholders like <YOUR_STORAGE_ACT_NAME> , <YOUR_REG_APP_CLIENT_ID> , <YOUR_REG_APP_TENANT_ID> , <YOUR_REG_APP_CLIENT_SECRET> , <YOUR_CONTAINER_NAME> , and paths to JAR files with the actual values.. Also, update any other settings to match your system. # Importing the necessary module for SparkSession from the PySpark library. from pyspark.sql import SparkSession #Note: The location of jars is where we copied them after downloadign with wget. Just 3 jars. spark = SparkSession.builder \\ .appName(\"ADLS Access\") \\ .config(\"spark.jars\", \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,\"\\ \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,\"\\ \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar\") \\ .getOrCreate() # Configuring PySpark for Azure Data Lake Storage (ADLS) Authentication using OAuth and Service Principal # Credentials and configurations storage_account_name = \"<YOUR_STORAGE_ACT_NAME>\" regapp_client_id = \"<YOUR_REG_APP_CLIENT_ID>\" # Application (client) ID of the registered app regapp_directory_id = \"<YOUR_REG_APP_TENANT_ID>\" # Directory (tenant) ID of the registered app regapp_client_secret = \"<YOUR_REG_APP_CLIENT_SECRET>\" # Set the authentication type to OAuth for the specified storage account--------------- spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\") # Define the token provider type for OAuth. The 'ClientCredsTokenProvider' is specified for the client credentials flow. spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") # Provide the client ID (application ID) of the registered application in Azure Active Directory (AD). spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", regapp_client_id) # Set the client secret of the registered application. This acts as a password for the application to verify its identity. spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", regapp_client_secret) # Specify the OAuth 2.0 token endpoint, allowing the application to retrieve tokens for authentication. spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{regapp_directory_id}/oauth2/token\") #---------------------------------------------------------------------------------------- #---------Code to perform the sorting---------------------------------------------------- # Define the ADLS Gen2 base path base_path = f\"abfss://<YOUR_CONTAINER_NAME>@<YOUR_STORAGE_ACT_NAME>.dfs.core.windows.net/\" conf = spark._jsc.hadoopConfiguration() conf.set(\"fs.abfss.impl\", \"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem\") uri = spark._jvm.java.net.URI path_obj = spark._jvm.org.apache.hadoop.fs.Path(base_path) file_system = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri(base_path), conf) old_files = [status.getPath().toString() for status in file_system.globStatus(spark._jvm.org.apache.hadoop.fs.Path(base_path + \"*-*.json\"))] # Diagnostic: Check the number of files fetched print(f\"Number of files to be processed: {len(old_files)}\") # Test with a subset (for diagnostic purposes) subset_of_files = old_files[:5] for old_file_path in old_files: # Extract year, month, day, and hour from the old file path filename = old_file_path.split('/')[-1] year, month, day, hour = filename.split('-')[:4] # Construct the new directory structure based on the desired format new_directory = base_path + f\"year={year}/month={month}/day={day}/\" # Check if the directory exists; if not, create it if not file_system.exists(spark._jvm.org.apache.hadoop.fs.Path(new_directory)): file_system.mkdirs(spark._jvm.org.apache.hadoop.fs.Path(new_directory)) # Construct the new file path new_file_path = new_directory + f\"hour={hour}\" # Diagnostic: Printing the move action print(f\"Moving {old_file_path} to {new_file_path}\") # Rename (move) the file to the new path and check if it's successful success = file_system.rename(spark._jvm.org.apache.hadoop.fs.Path(old_file_path), spark._jvm.org.apache.hadoop.fs.Path(new_file_path)) # Diagnostic: Check if the move was successful print(f\"Move success: {success}\") print(\"Files rearranged successfully for the subset!\") #---------Code to perform the sorting---------------------------------------------------- Conclusion There are many ways to organize files in a container, like using ADF, Databricks, or Logic Apps. But this way is good too because it's free, unlike some pretty-expensive options like Databricks. I shared this article to let us know there's another option out there. It shows how we perform such operation on Azure DataLake from an outside standalone application. Appendix Programmatic options for Creating Containers, Sorting Files etc: If you want to compare what other programmatic options we have to peroform such operation, here is the comparison: Reading/Writing Large Datasets : Best Tool : PySpark. Reason : Spark is designed for distributed data processing. Reading and processing large datasets from ADLS Gen2 into Spark dataframes will be efficient. Listing Files in a Container/Directory : Best Tool : PySpark or Hadoop FileSystem API. Reason : PySpark provides simple methods to list files, but if you're already interfacing with Hadoop's FileSystem API for other tasks, it's also a good choice. Renaming or Moving Files : Best Tool : Hadoop FileSystem API. Reason : While this can be done with the Azure SDK, the Hadoop FileSystem API provides a more direct interface when working alongside Spark. Creating Containers or Directories : Best Tool : Azure SDK ( azure-storage-file-datalake ). Reason : Creating containers or directories is a simple storage management task. The Azure SDK provides direct methods to do this without unnecessary overhead. Setting Permissions or Managing Access : Best Tool : Azure SDK. Reason : Managing permissions or access control is more straightforward with the Azure SDK, which provides methods tailored for these tasks. Understanding Spark Configuration: spark.jars.packages vs spark.jars Apache Spark offers robust options for integrating external libraries, crucial for expanding its native capabilities. Two such configurations often used are spark.jars.packages and spark.jars . Understanding the distinct roles and applications of these configurations can significantly enhance how you manage dependencies in your Spark applications. The Role of .config('spark.jars.packages', '...') This configuration is quintessential when it comes to managing library dependencies via Maven coordinates. It's designed to streamline the process of including external libraries in your Spark application. How it Works Maven Coordinates : You specify the library using its Maven coordinates in the format 'groupId:artifactId:version' . Automatic Download : Spark automates the download process, fetching the specified library from Maven Central or another configured Maven repository. Ease of Use : This method is particularly user-friendly, ensuring you're incorporating the correct library version without manually downloading the JAR files. Example Usage .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') In this instance, Spark is instructed to download and include the Kafka connector compatible with Spark version 2.12 and version 3.3.0 of the library. The Utility of .config('spark.jars', '...') Contrasting spark.jars.packages , the spark.jars configuration is utilized when directly referencing locally stored JAR files. How it Functions Local File Path : You provide the absolute path to the JAR file already present on your system. No Automatic Download : Spark bypasses any downloading process, relying on the specified JAR file's presence in the given location. Custom or Offline Use : This approach is ideal for using custom library versions or in environments with restricted internet access. Example Implementation .config('spark.jars', '/opt/shared-data/spark-sql-kafka-0-10_2.13-3.4.0.jar') Here, Spark is directed to incorporate a Kafka connector JAR file located at /opt/shared-data/spark-sql-kafka-0-10_2.13-3.4.0.jar . Conclusion In summary, spark.jars.packages is a hassle-free solution for incorporating libraries using Maven coordinates, automating the downloading and version management. In contrast, spark.jars is suited for scenarios where you have a local JAR file, offering more control over the specific version and source of the library being used. The choice between these configurations hinges on your project's requirements and operational environment, providing flexibility in managing your Spark application's dependencies. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Spark-To-ADLS-Connection"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#table-of-contents","text":"Project Sparkzure Part1 - Connecting Local Spark to Azure Data Lake Overview Kickstart: Integrating Spark with Azure Data Lake Create the containerized setup Register an App for OAuth Authentication Register a new App using App Registration Copy Ids and secret From The App Give the App Permission to the Container Access ADLS Data From Spark Using OAuth Authentication and Service Principal Open VS Code and Connect To the Container Run the code Access data in ADLS container using Storage Account's Access Key Method Common Errors AuthorizationPermissionMismatch During OAuth Authenticaiton Appendix Why Does Spark Rely on Hadoop Libraries to Access Azure Data Lake Storage (ADLS)? Understanding Essential JARs for Azure Data Lake Operations with Spark Project Sparkzure Part2 - Sorting Files in ADLS Container Using Standalone Spark Overview of the Article My Environment The scenario Kickstart Environment Setup Download the jars by running this command from terminal: Copy the jars to the SPARK_HOME/Jars location Run the Spark Code Conclusion Appendix Programmatic options for Creating Containers, Sorting Files etc: Understanding Spark Configuration: spark.jars.packages vs spark.jars The Role of .config('spark.jars.packages', '...') How it Works Example Usage The Utility of .config('spark.jars', '...') How it Functions Example Implementation Conclusion","title":"Table of contents"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#project-sparkzure-part1-connecting-local-spark-to-azure-data-lake","text":"","title":"Project Sparkzure Part1 - Connecting Local Spark to Azure Data Lake"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#overview","text":"Azure Databricks to Azure Data Lake is easy and straightforward. All the requied jars pre-installed in Databricks. All you need to do is to create a session and connect. However, connecting a local Spark instance to Azure Data Lake can be complicated, especially when managing JAR dependencies. In this project, I will show you how to connect your local Spark application to ADLS and run a Spark query using Visual Studio Code. The local Spark application will be hosted in a container, but it can also be hosted locally locally ;-)","title":"Overview"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#kickstart-integrating-spark-with-azure-data-lake","text":"","title":"Kickstart: Integrating Spark with Azure Data Lake"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#create-the-containerized-setup","text":"Our environment is set up inside a Docker container running Ubuntu on a Windows OS host. Within this container, Python 3 and Spark are installed. But the steps can be used in local environments as well. Check the python version in the container and find out site-packages directory Often, systems have both Python 2.x and Python 3.x installed. Use the following commands to determine which versions are available: bash python --version python3 --version Determine where PySpark is installed using pip . Your enviornment may have multiple python installation especially if its linux or in a docker. You need to find the right site-packages directory so that the packages are copied to right location. To find out run this command in docker terminal or normal command prompt: bash pip3 show pyspark | grep Location Alternatively, you can get the location by running the command: bash python3 -c \"from distutils.sysconfig import get_python_lib; print(get_python_lib())\" Install wget wget is a tool for downloading files from the internet. If you don\u2019t have it in your environment you can get it using the given command: bash apt-get update && apt-get install -y wget Download Hadoop ADLS JARs I've downloaded and placed the jars here . Download and copy it to a desired location. Alternatively, run the command below to download jars to your home directory bash cd ~ wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.3.3/hadoop-azure-3.3.3.jar wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure-datalake/3.3.3/hadoop-azure-datalake-3.3.3.jar wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.3.3/hadoop-common-3.3.3.jar wget https://repo1.maven.org/maven2/com/microsoft/azure/azure-storage/8.6.6/azure-storage-8.6.6.jar wget https://repo1.maven.org/maven2/com/azure/azure-security-keyvault-secrets/4.3.0/azure-security-keyvault-secrets-4.3.0.jar wget https://repo1.maven.org/maven2/com/azure/azure-identity/1.3.0/azure-identity-1.3.0.jar - After downloading, place the jars in any desired folder. These jars will be referenced during spark session creation. - Alternatively you can use download the jars on-the-fly using maven coordinates using .config('spark.jars.packages', '...')","title":"Create the containerized setup"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#register-an-app-for-oauth-authentication","text":"If you want to access a file(say CSV) in Azure through OAuth authentication, you need to create an App registration and grant this app permission to the CSV. This registered App's identity is used by Spark to authenticate. The same principle applies in Databricks , where an app is already created, named AzureDatabricks . Follow the steps below to register the app and give it permission to the file.","title":"Register an App for OAuth Authentication"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#register-a-new-app-using-app-registration","text":"In the Azure Portal to search for App registrations ', select it, and opt for '+ New registration'. Give a name, say adlssparkapp , choose 'Accounts in this organizational directory only', keep the Redirect URI empty, and click Register .","title":"Register a new App using App Registration"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#copy-ids-and-secret-from-the-app","text":"After registration, jot down the Application ID and Directory ID . Go to Manage > Certificates & secrets , select + New client secret , label it SparkAppSecret , set an expiration, and click 'Add'. Post-creation, make note of the one-time viewable secret value essential for the Spark-Azure handshake.","title":"Copy Ids and secret From The App"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#give-the-app-permission-to-the-container","text":"Open the container, navigate to Access Control (IAM) > Role assignments , click Add > Add role assignment , select Storage Blob Contributor , Search for the app adlssparkapp , and click OK .","title":"Give the App Permission to the Container"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#access-adls-data-from-spark-using-oauth-authentication-and-service-principal","text":"With the app now registered and the key, ID, and secret in hand, we can proceed to execute the main code. Follow the steps outlined below to continue:","title":"Access ADLS Data From Spark Using OAuth Authentication and Service Principal"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#open-vs-code-and-connect-to-the-container","text":"Open VS Code : Launch Visual Studio Code and click the remote container icon at the bottom left. Attach to Container : From the top menu, choose \"Attach to running container\". Select Container : Pick the displayed running container. This action launches a new VS Code instance connected to that container. Create Notebook : In this instance, create a .ipynb (Jupyter notebook) to execute the subsequent section's code. Connect to the python version where we copied the hadoop jars There could be multiple python versions in a linux enviornment. From VS Code choose the python version whcih has our jars","title":"Open VS Code and Connect To the Container"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#run-the-code","text":"Run the code below in the jupyter notebok: from pyspark.sql import SparkSession # Initialize a Spark session with necessary configurations for connecting to ADLS #Offline version spark = SparkSession.builder \\ .appName(\"ADLS Access\") \\ .config(\"spark.jars\", \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar\") \\ .getOrCreate() # Or using maven coordinates # Online version spark = SparkSession.builder \\ .appName(\"ADLS Access\") \\ .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.3,\" \"org.apache.hadoop:hadoop-azure-datalake:3.3.3,\" \"org.apache.hadoop:hadoop-common:3.3.3\") \\ .getOrCreate() # Define credentials and storage account details for ADLS access storage_account = \"<The_Storage_Act_Name_Containing_Container>\" app_client_id = \"<The_Client_ID_From_Registered_App>\" app_directory_tenant_id = \"<The_Client_ID_From_Registered_App>\" app_client_secret = \"<The_Secret_Value_From_Registered_App>\" # Configure Spark to use OAuth authentication for ADLS access spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\") spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", app_client_id) spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", app_client_secret) spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{app_directory_tenant_id}/oauth2/token\") # Define the path to the dataset in ADLS and read the CSV file using Spark path = \"abfss://<containerName>@<storaegaccountname>.dfs.core.windows.net/<CSV_File_Name.csv>\" spark.read.format(\"csv\").load(path).show()","title":"Run the code"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#access-data-in-adls-container-using-storage-accounts-access-key-method","text":"Another methods to access ADLS is using the Access key method. Here we get the access key from the storage account then use it to access the files inside it. To use this method, follow these steps: Get the Access Keys from the Storage Account In your storage account, under the Security + networking section in the left sidebar, find and select Access keys . You\u2019ll be presented with two keys: key1 and key2 . Both keys can be used to authenticate, so choose one and copy it. This will be used in the subsequent steps. Execute the code After getting the access key use this code. Replace your access key in the access key location: # Import the required module for creating a Spark session. from pyspark.sql import SparkSession # Initialize the Spark session. The builder pattern is utilized to configure the session. # We set the application name to \"ADLS Access\" for identification in Spark UI. # Necessary JAR files are specified for Spark to connect and interact with Azure Data Lake Storage (ADLS). spark = SparkSession.builder \\ .appName(\"ADLS Access\") \\ .config(\"spark.jars\", \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar\") \\ .getOrCreate() # Specify the Azure storage account name and the associated access key for authentication purposes. storage_account_name = \"<The_Storage_Account_Name>\" storage_account_key = \"<key1_or_key2>\" # Configure Spark to utilize AzureBlobFileSystem. This is essential for Azure Blob storage connectivity. spark.conf.set(f\"fs.azure\", \"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem\") # Authenticate the Spark session by providing the access key for the specified Azure storage account. spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", storage_account_key) # Read the desired CSV file located in ADLS into a DataFrame (df) using Spark. df = spark.read.csv(f\"abfss://<container_name>@{storage_account_name}.dfs.core.windows.net/<filename.csv>\")","title":"Access data in ADLS container using Storage Account's Access Key Method"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#common-errors","text":"","title":"Common Errors"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#authorizationpermissionmismatch-during-oauth-authenticaiton","text":"While executing the code you may encounter errors like: AuthorizationPermissionMismatch, \"This request is not authorized to perform this operation using this permission.\" or java.nio.file.AccessDeniedException: Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://strgacweatherapp.dfs.core.windows.net/weather-timer/2023-10-19-09.json?upn=false&action=getStatus&timeout=90 at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.checkException(AzureBlobFileSystem.java:1384) OAuth uses a registered apps identity to connect. This app should have permission to the folder where the file resides.","title":"AuthorizationPermissionMismatch During OAuth Authenticaiton"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#appendix","text":"","title":"Appendix"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#why-does-spark-rely-on-hadoop-libraries-to-access-azure-data-lake-storage-adls","text":"Long story short : In a standalone Spark setup, we use specific Hadoop JARs solely for connecting to ADLS. It's important to note that these are just JARs and don't represent the full Hadoop ecosystem. Apache Spark is used for distributed data processing. But for data storage it relies on other systems like ADLS, S3 etc. But why, when connecting Spark to ADLS, do we bring Hadoop into the picture? Let\u2019s find out. Spark's Core Functionality: Spark is designed to process data, not to understand the intricacies of every storage system. It can pull data from various sources, but it doesn't always have native integrations for each one. Hadoop's Role: Hadoop, primarily known for its distributed file system (HDFS), also offers connectors to diverse storage systems . Over time, it has become the standard bridge between storage solutions and big data tools. ADLS and Hadoop Integration: When Microsoft developed ADLS, they provided a connector to the Hadoop FileSystem API. This approach made sense. Why reinvent the wheel when big data tools already communicate efficiently with HDFS via Hadoop's API? Conclusion HSpark uses Hadoop libraries to access ADLS due to the standardized and robust nature of the Hadoop FileSystem API. Microsoft integrated ADLS with this Hadoop API to ensure that ADLS would be compatible with a broad range of big data tools, such as Spark and Hive. This decision was to use the extensive community support of the Hadoop ecosystem and also allowed Microsoft to reuse what was already working In essence, the Hadoop API serves as a bridge between Spark and ADLS.","title":"Why Does Spark Rely on Hadoop Libraries to Access Azure Data Lake Storage (ADLS)?"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#understanding-essential-jars-for-azure-data-lake-operations-with-spark","text":"hadoop-azure-3.3.3.jar : Description : This library provides support for Azure Blob Storage integration with Hadoop. It contains the WASB (Windows Azure Storage Blob) file system connector. Use-Cases : Reading/writing data from/to Azure Blob Storage (often ADLS Gen1) using Hadoop's FileSystem API. hadoop-azure-datalake-3.3.3.jar : Description : This is the Data Lake connector for Hadoop, providing support for ADLS Gen1. Use-Cases : If you're working with ADLS Gen1, this JAR lets Spark access the data lake using the Hadoop FileSystem API. hadoop-common-3.3.3.jar : Description : The core library for Hadoop, it contains common utilities and the Hadoop FileSystem API. Use-Cases : Fundamental for almost all Hadoop-related operations. It's the foundational library upon which other Hadoop components rely. azure-storage-8.6.6.jar : Description : Azure's storage SDK, facilitating interaction with Azure Storage services like Blob, Queue, and Table. Use-Cases : Interacting with Azure Blob Storage (and by extension, ADLS Gen2 which is built on Blob). It's essential for Spark to communicate and access Azure storage services. azure-security-keyvault-secrets-4.3.0.jar : Description : Provides capabilities to interact with Azure Key Vault's secrets. It facilitates fetching, setting, or managing secrets. Use-Cases : Whenever you need to securely access or manage secrets (like storage account keys or database connection strings) stored in Azure Key Vault from your Spark application. azure-identity-1.3.0.jar : Description : Azure SDK's identity library, providing various credentials classes for Azure Active Directory (AAD) token authentication. Use-Cases : Authenticating against Azure services using AAD-based credentials, especially when trying to securely access resources like Key Vault or ADLS Gen2.","title":"Understanding Essential JARs for Azure Data Lake Operations with Spark"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#project-sparkzure-part2-sorting-files-in-adls-container-using-standalone-spark","text":"","title":"Project Sparkzure Part2 - Sorting Files in ADLS Container Using Standalone Spark"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#overview-of-the-article","text":"In Part 1, we dived into accessing ADLS files with Pyspark and Hadoop Jars. Now, let's switch gears a bit. In this article, we'll explore how to sort\u2014by creating containers and moving/renaming files\u2014the content in an Azure Data Lake Container using just a Standalone Spark application. While there's always the route of Azure Data Factory, Databricks, or Azure Logic Apps, I want to spotlight this approach. Why? Because it's not only a viable alternative, but it also comes with the perk of being nearly cost-free compared to the other Azure services I mentioned.","title":"Overview of the Article"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#my-environment","text":"Deployment Platform: Docker Operating System: Ubuntu Python Version: 3.8.10 Development IDE: Visual Studio Code connected to the container Spark Setup: Standalone Spark installed as part of Pyspark( pip install pyspark ) Spark Home /usr/local/lib/python3.8/dist-packages/pyspark/ Jars Location /usr/local/lib/python3.8/dist-packages/pyspark/jars/","title":"My Environment"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#the-scenario","text":"We have a container name \"weather-timer\" that contains JSON files formatted as YYYY-10-22-12.json . These files hold weather information retrieved from a web API. The files need to be sorted in the format year=yyyy/month=mm/day=dd/hour=hh.json . This is a real-world requirement, as a structure like this can make partition pruning more efficient during query time if you're using a system like Apache Hive or Delta Lake.","title":"The scenario"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#kickstart","text":"","title":"Kickstart"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#environment-setup","text":"For ADLS connectivity in standalone PySpark, we need to download these 3 super-important Jars: - hadoop-azure-<version>.jar : Supports Azure Blob Storage and Spark integration. - hadoop-azure-datalake-<version>.jar : For for ADLS access, including authentication features. - hadoop-common-<version>.jar : Contains utilities for the other JARs.","title":"Environment Setup"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#download-the-jars-by-running-this-command-from-terminal","text":"Run teh following command in terminal. Note: The version of jars might change over time. ```bash cd ~ wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.3.3/hadoop-azure-3.3.3.jar wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure-datalake/3.3.3/hadoop-azure-datalake-3.3.3.jar wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.3.3/hadoop-common-3.3.3.jar ##### Copy the jars to the SPARK_HOME/Jars location ```bash cd ~ # assuming you downloaded the JARs in the home directory cp *.jar /usr/local/lib/python3.8/dist-packages/pyspark/jars/","title":"Download the jars by running this command from terminal:"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#run-the-spark-code","text":"This is the code for performing the sorting. It checks if there are containers. If not, it makes them. Before you run the code, make sure you replace placeholders like <YOUR_STORAGE_ACT_NAME> , <YOUR_REG_APP_CLIENT_ID> , <YOUR_REG_APP_TENANT_ID> , <YOUR_REG_APP_CLIENT_SECRET> , <YOUR_CONTAINER_NAME> , and paths to JAR files with the actual values.. Also, update any other settings to match your system. # Importing the necessary module for SparkSession from the PySpark library. from pyspark.sql import SparkSession #Note: The location of jars is where we copied them after downloadign with wget. Just 3 jars. spark = SparkSession.builder \\ .appName(\"ADLS Access\") \\ .config(\"spark.jars\", \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,\"\\ \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,\"\\ \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar\") \\ .getOrCreate() # Configuring PySpark for Azure Data Lake Storage (ADLS) Authentication using OAuth and Service Principal # Credentials and configurations storage_account_name = \"<YOUR_STORAGE_ACT_NAME>\" regapp_client_id = \"<YOUR_REG_APP_CLIENT_ID>\" # Application (client) ID of the registered app regapp_directory_id = \"<YOUR_REG_APP_TENANT_ID>\" # Directory (tenant) ID of the registered app regapp_client_secret = \"<YOUR_REG_APP_CLIENT_SECRET>\" # Set the authentication type to OAuth for the specified storage account--------------- spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\") # Define the token provider type for OAuth. The 'ClientCredsTokenProvider' is specified for the client credentials flow. spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") # Provide the client ID (application ID) of the registered application in Azure Active Directory (AD). spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", regapp_client_id) # Set the client secret of the registered application. This acts as a password for the application to verify its identity. spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", regapp_client_secret) # Specify the OAuth 2.0 token endpoint, allowing the application to retrieve tokens for authentication. spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{regapp_directory_id}/oauth2/token\") #---------------------------------------------------------------------------------------- #---------Code to perform the sorting---------------------------------------------------- # Define the ADLS Gen2 base path base_path = f\"abfss://<YOUR_CONTAINER_NAME>@<YOUR_STORAGE_ACT_NAME>.dfs.core.windows.net/\" conf = spark._jsc.hadoopConfiguration() conf.set(\"fs.abfss.impl\", \"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem\") uri = spark._jvm.java.net.URI path_obj = spark._jvm.org.apache.hadoop.fs.Path(base_path) file_system = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri(base_path), conf) old_files = [status.getPath().toString() for status in file_system.globStatus(spark._jvm.org.apache.hadoop.fs.Path(base_path + \"*-*.json\"))] # Diagnostic: Check the number of files fetched print(f\"Number of files to be processed: {len(old_files)}\") # Test with a subset (for diagnostic purposes) subset_of_files = old_files[:5] for old_file_path in old_files: # Extract year, month, day, and hour from the old file path filename = old_file_path.split('/')[-1] year, month, day, hour = filename.split('-')[:4] # Construct the new directory structure based on the desired format new_directory = base_path + f\"year={year}/month={month}/day={day}/\" # Check if the directory exists; if not, create it if not file_system.exists(spark._jvm.org.apache.hadoop.fs.Path(new_directory)): file_system.mkdirs(spark._jvm.org.apache.hadoop.fs.Path(new_directory)) # Construct the new file path new_file_path = new_directory + f\"hour={hour}\" # Diagnostic: Printing the move action print(f\"Moving {old_file_path} to {new_file_path}\") # Rename (move) the file to the new path and check if it's successful success = file_system.rename(spark._jvm.org.apache.hadoop.fs.Path(old_file_path), spark._jvm.org.apache.hadoop.fs.Path(new_file_path)) # Diagnostic: Check if the move was successful print(f\"Move success: {success}\") print(\"Files rearranged successfully for the subset!\") #---------Code to perform the sorting----------------------------------------------------","title":"Run the Spark Code"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#conclusion","text":"There are many ways to organize files in a container, like using ADF, Databricks, or Logic Apps. But this way is good too because it's free, unlike some pretty-expensive options like Databricks. I shared this article to let us know there's another option out there. It shows how we perform such operation on Azure DataLake from an outside standalone application.","title":"Conclusion"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#appendix_1","text":"","title":"Appendix"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#programmatic-options-for-creating-containers-sorting-files-etc","text":"If you want to compare what other programmatic options we have to peroform such operation, here is the comparison: Reading/Writing Large Datasets : Best Tool : PySpark. Reason : Spark is designed for distributed data processing. Reading and processing large datasets from ADLS Gen2 into Spark dataframes will be efficient. Listing Files in a Container/Directory : Best Tool : PySpark or Hadoop FileSystem API. Reason : PySpark provides simple methods to list files, but if you're already interfacing with Hadoop's FileSystem API for other tasks, it's also a good choice. Renaming or Moving Files : Best Tool : Hadoop FileSystem API. Reason : While this can be done with the Azure SDK, the Hadoop FileSystem API provides a more direct interface when working alongside Spark. Creating Containers or Directories : Best Tool : Azure SDK ( azure-storage-file-datalake ). Reason : Creating containers or directories is a simple storage management task. The Azure SDK provides direct methods to do this without unnecessary overhead. Setting Permissions or Managing Access : Best Tool : Azure SDK. Reason : Managing permissions or access control is more straightforward with the Azure SDK, which provides methods tailored for these tasks.","title":"Programmatic options for Creating Containers, Sorting Files etc:"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#understanding-spark-configuration-sparkjarspackages-vs-sparkjars","text":"Apache Spark offers robust options for integrating external libraries, crucial for expanding its native capabilities. Two such configurations often used are spark.jars.packages and spark.jars . Understanding the distinct roles and applications of these configurations can significantly enhance how you manage dependencies in your Spark applications.","title":"Understanding Spark Configuration: spark.jars.packages vs spark.jars"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#the-role-of-configsparkjarspackages","text":"This configuration is quintessential when it comes to managing library dependencies via Maven coordinates. It's designed to streamline the process of including external libraries in your Spark application.","title":"The Role of .config('spark.jars.packages', '...')"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#how-it-works","text":"Maven Coordinates : You specify the library using its Maven coordinates in the format 'groupId:artifactId:version' . Automatic Download : Spark automates the download process, fetching the specified library from Maven Central or another configured Maven repository. Ease of Use : This method is particularly user-friendly, ensuring you're incorporating the correct library version without manually downloading the JAR files.","title":"How it Works"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#example-usage","text":".config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') In this instance, Spark is instructed to download and include the Kafka connector compatible with Spark version 2.12 and version 3.3.0 of the library.","title":"Example Usage"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#the-utility-of-configsparkjars","text":"Contrasting spark.jars.packages , the spark.jars configuration is utilized when directly referencing locally stored JAR files.","title":"The Utility of .config('spark.jars', '...')"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#how-it-functions","text":"Local File Path : You provide the absolute path to the JAR file already present on your system. No Automatic Download : Spark bypasses any downloading process, relying on the specified JAR file's presence in the given location. Custom or Offline Use : This approach is ideal for using custom library versions or in environments with restricted internet access.","title":"How it Functions"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#example-implementation","text":".config('spark.jars', '/opt/shared-data/spark-sql-kafka-0-10_2.13-3.4.0.jar') Here, Spark is directed to incorporate a Kafka connector JAR file located at /opt/shared-data/spark-sql-kafka-0-10_2.13-3.4.0.jar .","title":"Example Implementation"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/#conclusion_1","text":"In summary, spark.jars.packages is a hassle-free solution for incorporating libraries using Maven coordinates, automating the downloading and version management. In contrast, spark.jars is suited for scenarios where you have a local JAR file, offering more control over the specific version and source of the library being used. The choice between these configurations hinges on your project's requirements and operational environment, providing flexibility in managing your Spark application's dependencies. \u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com","title":"Conclusion"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary/","text":"Table Of Contents Connecting to Azure Storage from Spark: Methods and Code Samples Overview Method 1: ABFS Driver for ADLS Gen2 Sample Code for ABFS Driver: Method 2: Managed Identity for Azure-hosted Spark Sample Code for Managed Identity: Method 3: Azure Blob Storage with Access Key Sample Code for Access Key: Method 4: Shared Access Signature (SAS) Sample Code for SAS: Method 5: Environment Variables/Secrets Sample Code for Using Environment Variables/Secrets: Connecting to Azure Storage from Spark: Methods and Code Samples In this guide, I will walk you through the different methods to connect Apache Spark to Azure Storage services including Azure Blob Storage and Azure Data Lake Storage. You will learn how to set up your Spark session to read and write data from and to Azure Storage using various authentication methods. Overview Connecting Apache Spark to Azure Storage can be achieved through several methods, each suited for different scenarios and security requirements. We will cover the use of ABFS driver for Azure Data Lake Storage Gen2, managed identities in Azure-hosted Spark, shared access signatures, and more. Method 1: ABFS Driver for ADLS Gen2 The ABFS (Azure Blob File System) driver is specially designed for Azure Data Lake Storage Gen2 and supports OAuth2 authentication, providing a secure method to access your data. Sample Code for ABFS Driver : Here is a sample code to connect using OAuth authentication and service principal. The code requires Haddop Azure Storagae Jars which needs to be downloaded spearately. from pyspark.sql import SparkSession # Replace with your Azure Storage account information storage_account_name = \"your_storage_account_name\" client_id = \"your_client_id_of_the_registered_app\" client_secret = \"your_client_secret_of_the_registered_app\" tenant_id = \"your_tenant_id_of_the_registered_app\" spark = SparkSession.builder \\ .appName(\"Any_App_Name\") \\ .config(\"spark.jars\", \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,\"\\ \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,\"\\ \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar\") \\ .config(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\") \\ .config(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") \\ .config(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", client_id) \\ .config(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", client_secret) \\ .config(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\") \\ .getOrCreate() Method 2: Managed Identity for Azure-hosted Spark For Spark clusters hosted on Azure, Managed Identity offers a way to securely access Azure services without storing credentials in your code. Sample Code for Managed Identity : from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"Azure Blob with Managed Identity\") \\ .config(\"fs.azure.account.auth.type.<storage_account_name>.blob.core.windows.net\", \"CustomAccessToken\") \\ .config(\"fs.azure.account.custom.token.provider.class.<storage_account_name>.blob.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ManagedIdentityCredentialProvider\") \\ .getOrCreate() Method 3: Azure Blob Storage with Access Key Using the Azure Blob Storage access key is a straightforward method to establish a connection, but it's less secure than using OAuth2 or Managed Identities. Sample Code for Access Key : from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"Azure Blob Access with Access Key\") \\ .config(\"fs.azure.account.key.<storage_account_name>.blob.core.windows.net\", \"<access_key>\") \\ .getOrCreate() Method 4: Shared Access Signature (SAS) Shared Access Signatures (SAS) provide a secure way to grant limited access to your Azure Storage resources without exposing your account key. Sample Code for SAS : from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"Azure Blob Access with SAS\") \\ .config(\"fs.azure.sas.<container_name>.<storage_account_name>.blob.core.windows.net\", \"<sas_token>\") \\ .getOrCreate() Method 5: Environment Variables/Secrets For an extra layer of security, use environment variables or a secret scope to manage your credentials, keeping them out of your code base. Sample Code for Using Environment Variables/Secrets : import os from pyspark.sql import SparkSession # Assume the environment variables or secrets are already set storage_account_name = os.getenv('STORAGE_ACCOUNT_NAME') sas_token = os.getenv('SAS_TOKEN') spark = SparkSession.builder \\ .appName(\"Azure Blob Access with Environment Variables\") \\ .config(f\"fs.azure.sas.<container_name>.{storage_account_name}.blob.core.windows.net\", sas_token) \\ .getOrCreate()","title":"Spark-To-ADLS-Summary"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary/#table-of-contents","text":"Connecting to Azure Storage from Spark: Methods and Code Samples Overview Method 1: ABFS Driver for ADLS Gen2 Sample Code for ABFS Driver: Method 2: Managed Identity for Azure-hosted Spark Sample Code for Managed Identity: Method 3: Azure Blob Storage with Access Key Sample Code for Access Key: Method 4: Shared Access Signature (SAS) Sample Code for SAS: Method 5: Environment Variables/Secrets Sample Code for Using Environment Variables/Secrets:","title":"Table Of Contents"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary/#connecting-to-azure-storage-from-spark-methods-and-code-samples","text":"In this guide, I will walk you through the different methods to connect Apache Spark to Azure Storage services including Azure Blob Storage and Azure Data Lake Storage. You will learn how to set up your Spark session to read and write data from and to Azure Storage using various authentication methods.","title":"Connecting to Azure Storage from Spark: Methods and Code Samples"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary/#overview","text":"Connecting Apache Spark to Azure Storage can be achieved through several methods, each suited for different scenarios and security requirements. We will cover the use of ABFS driver for Azure Data Lake Storage Gen2, managed identities in Azure-hosted Spark, shared access signatures, and more.","title":"Overview"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary/#method-1-abfs-driver-for-adls-gen2","text":"The ABFS (Azure Blob File System) driver is specially designed for Azure Data Lake Storage Gen2 and supports OAuth2 authentication, providing a secure method to access your data.","title":"Method 1: ABFS Driver for ADLS Gen2"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary/#sample-code-for-abfs-driver","text":"Here is a sample code to connect using OAuth authentication and service principal. The code requires Haddop Azure Storagae Jars which needs to be downloaded spearately. from pyspark.sql import SparkSession # Replace with your Azure Storage account information storage_account_name = \"your_storage_account_name\" client_id = \"your_client_id_of_the_registered_app\" client_secret = \"your_client_secret_of_the_registered_app\" tenant_id = \"your_tenant_id_of_the_registered_app\" spark = SparkSession.builder \\ .appName(\"Any_App_Name\") \\ .config(\"spark.jars\", \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,\"\\ \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,\"\\ \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar\") \\ .config(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\") \\ .config(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") \\ .config(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", client_id) \\ .config(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", client_secret) \\ .config(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\") \\ .getOrCreate()","title":"Sample Code for ABFS Driver:"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary/#method-2-managed-identity-for-azure-hosted-spark","text":"For Spark clusters hosted on Azure, Managed Identity offers a way to securely access Azure services without storing credentials in your code.","title":"Method 2: Managed Identity for Azure-hosted Spark"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary/#sample-code-for-managed-identity","text":"from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"Azure Blob with Managed Identity\") \\ .config(\"fs.azure.account.auth.type.<storage_account_name>.blob.core.windows.net\", \"CustomAccessToken\") \\ .config(\"fs.azure.account.custom.token.provider.class.<storage_account_name>.blob.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ManagedIdentityCredentialProvider\") \\ .getOrCreate()","title":"Sample Code for Managed Identity:"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary/#method-3-azure-blob-storage-with-access-key","text":"Using the Azure Blob Storage access key is a straightforward method to establish a connection, but it's less secure than using OAuth2 or Managed Identities.","title":"Method 3: Azure Blob Storage with Access Key"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary/#sample-code-for-access-key","text":"from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"Azure Blob Access with Access Key\") \\ .config(\"fs.azure.account.key.<storage_account_name>.blob.core.windows.net\", \"<access_key>\") \\ .getOrCreate()","title":"Sample Code for Access Key:"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary/#method-4-shared-access-signature-sas","text":"Shared Access Signatures (SAS) provide a secure way to grant limited access to your Azure Storage resources without exposing your account key.","title":"Method 4: Shared Access Signature (SAS)"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary/#sample-code-for-sas","text":"from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"Azure Blob Access with SAS\") \\ .config(\"fs.azure.sas.<container_name>.<storage_account_name>.blob.core.windows.net\", \"<sas_token>\") \\ .getOrCreate()","title":"Sample Code for SAS:"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary/#method-5-environment-variablessecrets","text":"For an extra layer of security, use environment variables or a secret scope to manage your credentials, keeping them out of your code base.","title":"Method 5: Environment Variables/Secrets"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary/#sample-code-for-using-environment-variablessecrets","text":"import os from pyspark.sql import SparkSession # Assume the environment variables or secrets are already set storage_account_name = os.getenv('STORAGE_ACCOUNT_NAME') sas_token = os.getenv('SAS_TOKEN') spark = SparkSession.builder \\ .appName(\"Azure Blob Access with Environment Variables\") \\ .config(f\"fs.azure.sas.<container_name>.{storage_account_name}.blob.core.windows.net\", sas_token) \\ .getOrCreate()","title":"Sample Code for Using Environment Variables/Secrets:"},{"location":"Spark-DataBricks/3.0_Databricks/3.0_Databricks_Concepts/","text":"Table of contents {: .text-delta } 1. TOC {:toc} What is databricks? Databricks is like repackaged Apache Spark with some extra goodies inside the box. You can\u2019t use Databricks on its own\u2014you always have to start it from within Azure, GCP, or AWS. When you use Databricks, it uses your Azure subscription and resources like Azure VMs, Azure Storage, network and secuirty, to set up a Spark environment in the background. So, everything runs on your Azure account. You don\u2019t get a separate bill from Databricks; you just pay your regular Azure bill, and the money is split between Databricks and Microsoft. Fun fact. All Databricks Spark cluster is created inside Azure. Azure manages it using Azure Kubernetes Service. Now, when I say Databricks is Spark with some extra features, here\u2019s what I mean: Databricks Runtime : This is Spark, but it\u2019s been made faster with some Databricks engine oil . Databricks Workflows : This is a special feature from Databricks that you won\u2019t find in open-source Spark. Databricks Workspace : This is like the main dashboard, and it\u2019s completely from Databricks. Databricks is like a branded bottled water where the content, water, is open-source, but you still pay extra for the assurance and conveneince. Also, Databricks works like Uber\u2014it uses Azure, GCP, etc.\u2019s infrastructure and charges a fancy service fee. If you don\u2019t want Databricks and prefer to stick to open-source, you can have your own cluster (on-prem or on Azure VMs), use Azure HDInsight (a bit of management and extra features from Microsoft, but just Microsoft, no other party), or Google Dataproc (similar to Microsoft). Architecture of Azure Databricks Spark Cluster There are drivers/masters and workers/slaves. So how does the Driver look like? Eh. The your spark notebook is the driver. It contains the main progreamms which make use of workers. Driver Worker Jobs Task Slot Every cluseter has Only One Driver JVM and multiple Executor JVMs hive_metastore folder Databricks Architecture Control Plane : This is where all the management happens. It includes the Databricks web app, Unity Catalog, Cluster Manager, Workflow Manager, Jobs, Notebooks, etc. Think of it as the control center where you manage everything. Data/Compute Plane : This is where the actual work gets done. It has the clusters and the data. The Data Plane is where your data is processed, and where the machines that do the work are located. Serverless Compute Plane : Here, everything is managed inside the Databricks account, even though the actual servers are in Azure, AWS, or GCP. Classic Compute Plane : In this setup, the servers are mainly managed from your cloud account (like Azure, AWS, or GCP). Personas : This is the look and feel of your Databricks web UI. There are different personas like Data Science and Engineering, Analyst, and Practitioner, each giving you a different experience based on what you do. Databricks Clusters Databricks has two types of clusters: All-purpose: Mainly for interactive work. Like, you want to run cells and see what's happening. Many users can use an all-purpose cluster. Job: When things don't need any interaction. To run jobs. And two modes of clusters Standard/Multi-Node: THis is the default mode. 1 Driver + Multiple Worker Nodes. Single node: 1-Drive. No Worker. Driver works as both driver and worker. For light work load. Runtime has three versions Standard: Normal spark and normal stuff. Machine Learning: Has some useful machine learning libraries installed Photon : Has some speed for SQL And Access mode Single User : is always there Shared: Many users can access the same nodes. Seprate environment for each user. One fails, doesn't affecct other. No Isolattioin shared: Multiple users can access. But, enviornment is the same. One process fails, all users affected.","title":"Databricks"},{"location":"Spark-DataBricks/3.0_Databricks/3.0_Databricks_Concepts/#what-is-databricks","text":"Databricks is like repackaged Apache Spark with some extra goodies inside the box. You can\u2019t use Databricks on its own\u2014you always have to start it from within Azure, GCP, or AWS. When you use Databricks, it uses your Azure subscription and resources like Azure VMs, Azure Storage, network and secuirty, to set up a Spark environment in the background. So, everything runs on your Azure account. You don\u2019t get a separate bill from Databricks; you just pay your regular Azure bill, and the money is split between Databricks and Microsoft. Fun fact. All Databricks Spark cluster is created inside Azure. Azure manages it using Azure Kubernetes Service. Now, when I say Databricks is Spark with some extra features, here\u2019s what I mean: Databricks Runtime : This is Spark, but it\u2019s been made faster with some Databricks engine oil . Databricks Workflows : This is a special feature from Databricks that you won\u2019t find in open-source Spark. Databricks Workspace : This is like the main dashboard, and it\u2019s completely from Databricks. Databricks is like a branded bottled water where the content, water, is open-source, but you still pay extra for the assurance and conveneince. Also, Databricks works like Uber\u2014it uses Azure, GCP, etc.\u2019s infrastructure and charges a fancy service fee. If you don\u2019t want Databricks and prefer to stick to open-source, you can have your own cluster (on-prem or on Azure VMs), use Azure HDInsight (a bit of management and extra features from Microsoft, but just Microsoft, no other party), or Google Dataproc (similar to Microsoft).","title":"What is databricks?"},{"location":"Spark-DataBricks/3.0_Databricks/3.0_Databricks_Concepts/#architecture-of-azure-databricks-spark-cluster","text":"There are drivers/masters and workers/slaves. So how does the Driver look like? Eh. The your spark notebook is the driver. It contains the main progreamms which make use of workers. Driver Worker Jobs Task Slot Every cluseter has Only One Driver JVM and multiple Executor JVMs","title":"Architecture of Azure Databricks Spark Cluster"},{"location":"Spark-DataBricks/3.0_Databricks/3.0_Databricks_Concepts/#hive_metastore-folder","text":"","title":"hive_metastore folder"},{"location":"Spark-DataBricks/3.0_Databricks/3.0_Databricks_Concepts/#databricks-architecture","text":"Control Plane : This is where all the management happens. It includes the Databricks web app, Unity Catalog, Cluster Manager, Workflow Manager, Jobs, Notebooks, etc. Think of it as the control center where you manage everything. Data/Compute Plane : This is where the actual work gets done. It has the clusters and the data. The Data Plane is where your data is processed, and where the machines that do the work are located. Serverless Compute Plane : Here, everything is managed inside the Databricks account, even though the actual servers are in Azure, AWS, or GCP. Classic Compute Plane : In this setup, the servers are mainly managed from your cloud account (like Azure, AWS, or GCP). Personas : This is the look and feel of your Databricks web UI. There are different personas like Data Science and Engineering, Analyst, and Practitioner, each giving you a different experience based on what you do.","title":"Databricks Architecture"},{"location":"Spark-DataBricks/3.0_Databricks/3.0_Databricks_Concepts/#databricks-clusters","text":"Databricks has two types of clusters: All-purpose: Mainly for interactive work. Like, you want to run cells and see what's happening. Many users can use an all-purpose cluster. Job: When things don't need any interaction. To run jobs. And two modes of clusters Standard/Multi-Node: THis is the default mode. 1 Driver + Multiple Worker Nodes. Single node: 1-Drive. No Worker. Driver works as both driver and worker. For light work load. Runtime has three versions Standard: Normal spark and normal stuff. Machine Learning: Has some useful machine learning libraries installed Photon : Has some speed for SQL And Access mode Single User : is always there Shared: Many users can access the same nodes. Seprate environment for each user. One fails, doesn't affecct other. No Isolattioin shared: Multiple users can access. But, enviornment is the same. One process fails, all users affected.","title":"Databricks Clusters"},{"location":"Spark-DataBricks/3.0_Databricks/3.1_Catalogs_And_Metastore/","text":"Hive Metastore and the hive_metastore folder Hive is a data warehouse which stores data in HDFS(usually). Hive metastore is the database which gives 'front facing' tables whose data is in HDFS. Any database with JDBC can become a Hive metastore. By default Hive uses in-build Apache derby database. In prod it uses usually MYSQL or Postgress. Catalogs Catalogs and Metastore Catalogs, Metastore related concepts come into picture the moment you start something SQL in Spark. The moment you start to use Spark as a Database rather than just on-the-fly processor of data. What is a metastore? Simply a store for metadata. When you store your Data in Spark itself, you store it as tables inside databases. This metastore is a way to keep that information. Whcih databas stores which table etc. And interstingly this metastore itself ia antoher database to store information about database. Database to store database info. haha! By default, Spark uses 'Hive metatore' as a metastore technology. metastore_db is the Hive's database name which stores the metadata. When you hear Hive catalog, this is the catalog to further classify the data. Like different levels. Folder, subfolder, files. By detault, when you use Spark to store data the data is stored in spark-warehouse folder. B Databricks is actually spark. So initially they used to use Hive metastore to manage the info about tables and db. Hive Metastore in Databricks Before Unity catalog Databricks used to use the default Hive metastore. This is what Spark uses normally. Here the naming schema used to be db_name.table_name. Its two-level. Unity Catalog Metastore in Databricks After Unity catalog. Table names becamse: catalog_name.schema_name.table_name(three tier) Unity Catalog in Azure Databricks helps you manage your data by keeping all access control, tracking, and data discovery in one place. It makes it easier to control who can see and use your data, keeps track of data usage, and helps you find the data you need quickly. Key Features: One-Time Setup for All: Set up your data access rules once and they work everywhere. Easy Security: Use familiar SQL commands to set who can see what. Automatic Tracking: Keeps a log of who accessed the data and shows how the data is used. Find Data Easily: Tag and search for your data easily. System Data Access: Check audit logs and usage details. How It\u2019s Organized: Metastore: The main place where all data info and access rules are kept. One Per Region: Usually set up automatically with a new workspace. Hierarchy: Catalogs: Big containers for your data, like folders. Schemas: Inside catalogs, like subfolders, holding tables and more. Detailed Data: Volumes: Storage for unstructured data. Tables: Data organized in rows and columns. Views: Saved searches or queries on your tables. Functions: Small bits of code that do calculations. Models: AI models stored and managed. This setup makes it simple to manage, secure, and find your data across all your Azure Databricks workspaces. Query Hive Metastore from Unity Metastore Query the Hive metastore in Unity Catalog The Unity Catalog metastore is additive, meaning it can be used with the per-workspace Hive metastore in Azure Databricks. The Hive metastore appears as a top-level catalog called hive_metastore in the three-level namespace. For example, you can refer to a table called sales_raw in the sales schema in the legacy Hive metastore by using the following notation: SQL SQL Copy SELECT * from hive_metastore.sales.sales_raw; Python Python Copy display(spark.table(\"hive_metastore.sales.sales_raw\"))","title":"Hive Metastore and the hive_metastore folder"},{"location":"Spark-DataBricks/3.0_Databricks/3.1_Catalogs_And_Metastore/#hive-metastore-and-the-hive_metastore-folder","text":"Hive is a data warehouse which stores data in HDFS(usually). Hive metastore is the database which gives 'front facing' tables whose data is in HDFS. Any database with JDBC can become a Hive metastore. By default Hive uses in-build Apache derby database. In prod it uses usually MYSQL or Postgress.","title":"Hive Metastore and the hive_metastore folder"},{"location":"Spark-DataBricks/3.0_Databricks/3.1_Catalogs_And_Metastore/#catalogs","text":"","title":"Catalogs"},{"location":"Spark-DataBricks/3.0_Databricks/3.1_Catalogs_And_Metastore/#catalogs-and-metastore","text":"Catalogs, Metastore related concepts come into picture the moment you start something SQL in Spark. The moment you start to use Spark as a Database rather than just on-the-fly processor of data. What is a metastore? Simply a store for metadata. When you store your Data in Spark itself, you store it as tables inside databases. This metastore is a way to keep that information. Whcih databas stores which table etc. And interstingly this metastore itself ia antoher database to store information about database. Database to store database info. haha! By default, Spark uses 'Hive metatore' as a metastore technology. metastore_db is the Hive's database name which stores the metadata. When you hear Hive catalog, this is the catalog to further classify the data. Like different levels. Folder, subfolder, files. By detault, when you use Spark to store data the data is stored in spark-warehouse folder. B Databricks is actually spark. So initially they used to use Hive metastore to manage the info about tables and db.","title":"Catalogs and Metastore"},{"location":"Spark-DataBricks/3.0_Databricks/3.1_Catalogs_And_Metastore/#hive-metastore-in-databricks","text":"Before Unity catalog Databricks used to use the default Hive metastore. This is what Spark uses normally. Here the naming schema used to be db_name.table_name. Its two-level.","title":"Hive Metastore in Databricks"},{"location":"Spark-DataBricks/3.0_Databricks/3.1_Catalogs_And_Metastore/#unity-catalog-metastore-in-databricks","text":"After Unity catalog. Table names becamse: catalog_name.schema_name.table_name(three tier) Unity Catalog in Azure Databricks helps you manage your data by keeping all access control, tracking, and data discovery in one place. It makes it easier to control who can see and use your data, keeps track of data usage, and helps you find the data you need quickly. Key Features: One-Time Setup for All: Set up your data access rules once and they work everywhere. Easy Security: Use familiar SQL commands to set who can see what. Automatic Tracking: Keeps a log of who accessed the data and shows how the data is used. Find Data Easily: Tag and search for your data easily. System Data Access: Check audit logs and usage details. How It\u2019s Organized: Metastore: The main place where all data info and access rules are kept. One Per Region: Usually set up automatically with a new workspace. Hierarchy: Catalogs: Big containers for your data, like folders. Schemas: Inside catalogs, like subfolders, holding tables and more. Detailed Data: Volumes: Storage for unstructured data. Tables: Data organized in rows and columns. Views: Saved searches or queries on your tables. Functions: Small bits of code that do calculations. Models: AI models stored and managed. This setup makes it simple to manage, secure, and find your data across all your Azure Databricks workspaces.","title":"Unity Catalog Metastore in Databricks"},{"location":"Spark-DataBricks/3.0_Databricks/3.1_Catalogs_And_Metastore/#query-hive-metastore-from-unity-metastore","text":"Query the Hive metastore in Unity Catalog The Unity Catalog metastore is additive, meaning it can be used with the per-workspace Hive metastore in Azure Databricks. The Hive metastore appears as a top-level catalog called hive_metastore in the three-level namespace. For example, you can refer to a table called sales_raw in the sales schema in the legacy Hive metastore by using the following notation: SQL SQL Copy SELECT * from hive_metastore.sales.sales_raw; Python Python Copy display(spark.table(\"hive_metastore.sales.sales_raw\"))","title":"Query Hive Metastore from Unity Metastore"},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Authentication methods to connect with ADLS Azure Data Lake Storage (ADLS) offers several methods to authenticate access from Databricks. This guide will cover the three main methods: ADLS Account Keys (Shared Access Key method) Shared Access Signature (SAS) tokens Service Principal with OAuth2 Each method has its use cases and security implications. 1. Accessing ADLS Using Account Keys The Shared Access Key method is the simplest but least secure. Steps to Use ADLS Account Keys: Retrieve the Account Key : Go to the Azure Portal, navigate to your Storage account. Click on Access keys and copy either key1 or key2. Use the Account Key in Databricks : Set up the configuration in your Databricks notebook. # Configuration for accessing ADLS using the Shared Access Key method: storageAccountName = \"your_storage_account_name\" accountKey = \"your_account_key\" # For production, use Databricks secret scope containerName = \"your_container_name\" filename = \"your_file.csv\" spark.conf.set(f\"fs.azure.account.key.{storageAccountName}.dfs.core.windows.net\", accountKey) adls_path = f\"abfss://{containerName}@{storageAccountName}.dfs.core.windows.net/\" spark.read.csv(adls_path + filename).show() Recommended Production Setup: Instead of embedding the account key in your code, use Databricks secrets for better security: # Using Databricks secret management utility accountKey = dbutils.secrets.get(scope=\"your_scope_name\", key=\"your_key_name\") spark.conf.set(f\"fs.azure.account.key.{storageAccountName}.dfs.core.windows.net\", accountKey) adls_path = f\"abfss://{containerName}@{storageAccountName}.dfs.core.windows.net/\" spark.read.csv(adls_path + filename).show() 2. Accessing ADLS Using SAS Tokens SAS tokens provide more granular control over permissions and are generally more secure than account keys. Steps to Use SAS Tokens: Generate a SAS Token : In the Azure Portal, select your Storage account. Navigate to Shared access signature under Security + networking . Configure the permissions and generate the token. Use the SAS Token in Databricks : Set up the configuration in your Databricks notebook. storageAccountName = \"your_storage_account_name\" sasToken = \"your_sas_token\" containerName = \"your_container_name\" filename = \"your_file.csv\" spark.conf.set(f\"fs.azure.sas.{containerName}.{storageAccountName}.dfs.core.windows.net\", sasToken) adls_path = f\"abfss://{containerName}@{storageAccountName}.dfs.core.windows.net/\" spark.read.csv(adls_path + filename).show() 3. Accessing ADLS Using Service Principal with OAuth2 This method is the most secure and suitable for production environments as it uses Azure AD for authentication. Steps to Use Service Principal with OAuth2: Set Up Azure AD : Register an application in Azure AD. Assign roles to the application for accessing ADLS. Note down the Client ID, Client Secret, and Directory (Tenant) ID. Configure Databricks to Use Service Principal : Store your credentials in Databricks secrets. storageAccountName = \"your_storage_account_name\" containerName = \"your_container_name\" filename = \"your_file.csv\" clientID = dbutils.secrets.get(scope=\"your_scope_name\", key=\"your_client_id_key\") clientSecret = dbutils.secrets.get(scope=\"your_scope_name\", key=\"your_client_secret_key\") directoryID = dbutils.secrets.get(scope=\"your_scope_name\", key=\"your_directory_id_key\") spark.conf.set(f\"fs.azure.account.auth.type.{storageAccountName}.dfs.core.windows.net\", \"OAuth\") spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storageAccountName}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storageAccountName}.dfs.core.windows.net\", clientID) spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storageAccountName}.dfs.core.windows.net\", clientSecret) spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storageAccountName}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{directoryID}/oauth2/token\") adls_path = f\"abfss://{containerName}@{storageAccountName}.dfs.core.windows.net/\" spark.read.csv(adls_path + filename).show() Conclusion The Shared Access Key method is the simplest but least secure, while Service Principal with OAuth2 provides the highest level of security suitable for production environments. SAS tokens offer a balance between ease of use and security.","title":"Authentication Method"},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods/#authentication-methods-to-connect-with-adls","text":"Azure Data Lake Storage (ADLS) offers several methods to authenticate access from Databricks. This guide will cover the three main methods: ADLS Account Keys (Shared Access Key method) Shared Access Signature (SAS) tokens Service Principal with OAuth2 Each method has its use cases and security implications.","title":"Authentication methods to connect with ADLS"},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods/#1-accessing-adls-using-account-keys","text":"The Shared Access Key method is the simplest but least secure.","title":"1. Accessing ADLS Using Account Keys"},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods/#steps-to-use-adls-account-keys","text":"Retrieve the Account Key : Go to the Azure Portal, navigate to your Storage account. Click on Access keys and copy either key1 or key2. Use the Account Key in Databricks : Set up the configuration in your Databricks notebook. # Configuration for accessing ADLS using the Shared Access Key method: storageAccountName = \"your_storage_account_name\" accountKey = \"your_account_key\" # For production, use Databricks secret scope containerName = \"your_container_name\" filename = \"your_file.csv\" spark.conf.set(f\"fs.azure.account.key.{storageAccountName}.dfs.core.windows.net\", accountKey) adls_path = f\"abfss://{containerName}@{storageAccountName}.dfs.core.windows.net/\" spark.read.csv(adls_path + filename).show()","title":"Steps to Use ADLS Account Keys:"},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods/#recommended-production-setup","text":"Instead of embedding the account key in your code, use Databricks secrets for better security: # Using Databricks secret management utility accountKey = dbutils.secrets.get(scope=\"your_scope_name\", key=\"your_key_name\") spark.conf.set(f\"fs.azure.account.key.{storageAccountName}.dfs.core.windows.net\", accountKey) adls_path = f\"abfss://{containerName}@{storageAccountName}.dfs.core.windows.net/\" spark.read.csv(adls_path + filename).show()","title":"Recommended Production Setup:"},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods/#2-accessing-adls-using-sas-tokens","text":"SAS tokens provide more granular control over permissions and are generally more secure than account keys.","title":"2. Accessing ADLS Using SAS Tokens"},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods/#steps-to-use-sas-tokens","text":"Generate a SAS Token : In the Azure Portal, select your Storage account. Navigate to Shared access signature under Security + networking . Configure the permissions and generate the token. Use the SAS Token in Databricks : Set up the configuration in your Databricks notebook. storageAccountName = \"your_storage_account_name\" sasToken = \"your_sas_token\" containerName = \"your_container_name\" filename = \"your_file.csv\" spark.conf.set(f\"fs.azure.sas.{containerName}.{storageAccountName}.dfs.core.windows.net\", sasToken) adls_path = f\"abfss://{containerName}@{storageAccountName}.dfs.core.windows.net/\" spark.read.csv(adls_path + filename).show()","title":"Steps to Use SAS Tokens:"},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods/#3-accessing-adls-using-service-principal-with-oauth2","text":"This method is the most secure and suitable for production environments as it uses Azure AD for authentication.","title":"3. Accessing ADLS Using Service Principal with OAuth2"},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods/#steps-to-use-service-principal-with-oauth2","text":"Set Up Azure AD : Register an application in Azure AD. Assign roles to the application for accessing ADLS. Note down the Client ID, Client Secret, and Directory (Tenant) ID. Configure Databricks to Use Service Principal : Store your credentials in Databricks secrets. storageAccountName = \"your_storage_account_name\" containerName = \"your_container_name\" filename = \"your_file.csv\" clientID = dbutils.secrets.get(scope=\"your_scope_name\", key=\"your_client_id_key\") clientSecret = dbutils.secrets.get(scope=\"your_scope_name\", key=\"your_client_secret_key\") directoryID = dbutils.secrets.get(scope=\"your_scope_name\", key=\"your_directory_id_key\") spark.conf.set(f\"fs.azure.account.auth.type.{storageAccountName}.dfs.core.windows.net\", \"OAuth\") spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storageAccountName}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storageAccountName}.dfs.core.windows.net\", clientID) spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storageAccountName}.dfs.core.windows.net\", clientSecret) spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storageAccountName}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{directoryID}/oauth2/token\") adls_path = f\"abfss://{containerName}@{storageAccountName}.dfs.core.windows.net/\" spark.read.csv(adls_path + filename).show()","title":"Steps to Use Service Principal with OAuth2:"},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods/#conclusion","text":"The Shared Access Key method is the simplest but least secure, while Service Principal with OAuth2 provides the highest level of security suitable for production environments. SAS tokens offer a balance between ease of use and security.","title":"Conclusion"},{"location":"Spark-DataBricks/3.0_Databricks/3.3_Mount_ADLS_on_Databricks/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Mount ADLS Gen2 on Databricks Using AAD OAuth & Service Principal Integrate Databricks with Azure Data Lake Storage Gen2 (ADLS Gen2) securely using Azure Active Directory (AAD) OAuth and a service principal. For the busy people Execute this in a databricks notebook: dbutils.fs.mount( source = \"adl://.azuredatalakestore.net/\", mount_point = \"/mnt/\", extra_configs = { \"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\", \"dfs.adls.oauth2.client.id\": dbutils.secrets.get(scope = \"\", key = \"client-id\"), \"dfs.adls.oauth2.credential\": dbutils.secrets.get(scope = \"\", key = \"client-secret\"), \"dfs.adls.oauth2.refresh.url\": \"https://login.microsoftonline.com//oauth2/token\"} ) Detailed steps Azure Setup : Create Service Principal : In the Azure Portal , navigate to Azure Active Directory > App registrations > New registration . Provide a name and register the application. Save the Application (client) ID and create a new client secret under Certificates & secrets . Save the client secret value. Assign Role : In your storage account, assign the Storage Blob Data Contributor role to the service principal. Store Credentials in Key Vault : In Azure Key Vault, add the client ID, client secret, and tenant ID as secrets. Databricks Configuration : In your Databricks notebook, follow these steps to configure and mount ADLS Gen2: Fetch Credentials : python clientID = dbutils.secrets.get(scope=\"azbackedscope\", key=\"regappClientID\") clientSecret = dbutils.secrets.get(scope=\"azbackedscope\", key=\"regappClientSecret\") directoryID = dbutils.secrets.get(scope=\"azbackedscope\", key=\"regappDirectoryID\") Set OAuth Configs : python configs = { \"fs.azure.account.auth.type\": \"OAuth\", \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\", \"fs.azure.account.oauth2.client.id\": clientID, \"fs.azure.account.oauth2.client.secret\": clientSecret, \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{directoryID}/oauth2/token\" } Mount Storage : ```python storageAccountName = \"your_storage_account_name\" containerName = \"your_container_name\" mountPoint = \"/mnt/your_mount_name\" adlsPath = f\"abfss://{containerName}@{storageAccountName}.dfs.core.windows.net/\" dbutils.fs.mount( source=adlsPath, mount_point=mountPoint, extra_configs=configs ) ``` Verify Mount : python display(dbutils.fs.ls(mountPoint)) Conclusion With ADLS Gen2 storage mounted on DBFS, you can read and write data more conveniently. The code becomes simpler as you don't have to authenticate every time, making it feel like accessing local storage.","title":"Mount ADLS on Databricks"},{"location":"Spark-DataBricks/3.0_Databricks/3.3_Mount_ADLS_on_Databricks/#mount-adls-gen2-on-databricks-using-aad-oauth-service-principal","text":"Integrate Databricks with Azure Data Lake Storage Gen2 (ADLS Gen2) securely using Azure Active Directory (AAD) OAuth and a service principal.","title":"Mount ADLS Gen2 on Databricks Using AAD OAuth &amp; Service Principal"},{"location":"Spark-DataBricks/3.0_Databricks/3.3_Mount_ADLS_on_Databricks/#for-the-busy-people","text":"Execute this in a databricks notebook: dbutils.fs.mount( source = \"adl://.azuredatalakestore.net/\", mount_point = \"/mnt/\", extra_configs = { \"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\", \"dfs.adls.oauth2.client.id\": dbutils.secrets.get(scope = \"\", key = \"client-id\"), \"dfs.adls.oauth2.credential\": dbutils.secrets.get(scope = \"\", key = \"client-secret\"), \"dfs.adls.oauth2.refresh.url\": \"https://login.microsoftonline.com//oauth2/token\"} )","title":"For the busy people"},{"location":"Spark-DataBricks/3.0_Databricks/3.3_Mount_ADLS_on_Databricks/#detailed-steps","text":"Azure Setup : Create Service Principal : In the Azure Portal , navigate to Azure Active Directory > App registrations > New registration . Provide a name and register the application. Save the Application (client) ID and create a new client secret under Certificates & secrets . Save the client secret value. Assign Role : In your storage account, assign the Storage Blob Data Contributor role to the service principal. Store Credentials in Key Vault : In Azure Key Vault, add the client ID, client secret, and tenant ID as secrets. Databricks Configuration : In your Databricks notebook, follow these steps to configure and mount ADLS Gen2: Fetch Credentials : python clientID = dbutils.secrets.get(scope=\"azbackedscope\", key=\"regappClientID\") clientSecret = dbutils.secrets.get(scope=\"azbackedscope\", key=\"regappClientSecret\") directoryID = dbutils.secrets.get(scope=\"azbackedscope\", key=\"regappDirectoryID\") Set OAuth Configs : python configs = { \"fs.azure.account.auth.type\": \"OAuth\", \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\", \"fs.azure.account.oauth2.client.id\": clientID, \"fs.azure.account.oauth2.client.secret\": clientSecret, \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{directoryID}/oauth2/token\" } Mount Storage : ```python storageAccountName = \"your_storage_account_name\" containerName = \"your_container_name\" mountPoint = \"/mnt/your_mount_name\" adlsPath = f\"abfss://{containerName}@{storageAccountName}.dfs.core.windows.net/\" dbutils.fs.mount( source=adlsPath, mount_point=mountPoint, extra_configs=configs ) ``` Verify Mount : python display(dbutils.fs.ls(mountPoint))","title":"Detailed steps"},{"location":"Spark-DataBricks/3.0_Databricks/3.3_Mount_ADLS_on_Databricks/#conclusion","text":"With ADLS Gen2 storage mounted on DBFS, you can read and write data more conveniently. The code becomes simpler as you don't have to authenticate every time, making it feel like accessing local storage.","title":"Conclusion"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Working with Secret Scopes in Azure Databricks Introduction Azure Databricks offers two secret scopes: Azure Key Vault-backed and Databricks-backed. This guide walks you through their creation, access in PySpark, and prerequisites such as Databricks CLI installation and understanding key vault-related roles. By the end, you'll possess all the fundamental knowledge required to use key vaults with Azure Databricks. Different Types of Secret Scopes Databricks-backed Secret Scope : This is a proprietary scope managed by Databricks. Secrets stored in this scope are encrypted and can be accessed only by users with the necessary permissions. Azure key-vault backed Secret Scope : If you create a secret scope in Databricks that references an Azure Key Vault, then the scope is Azure Key Vault-backed. The actual secrets are stored in Azure Key Vault, and Databricks retrieves them from there when needed. Create Databricks-backed Secret Scope Pre-requiesites You may need to install databricks CLI if you don't have it installed already. Install Databricks CLI Databricks-backed secret scopes are created using the Databricks CLI and Secrets API. The #secrets/createScope UI in Databricks is reserved for Azure Key Vault-backed secret scopes. Thus, we'll employ the Databricks CLI for this task: Ensure you have Python installed on your system. Use pip, Python\u2019s package manager, to install the CLI: bash pip install databricks-cli Log into Databricks using the CLI Once the CLI is installed, the next step is to log in to your Databricks workspace: Open a terminal or command prompt and enter bash databricks configure --token You will be asked to enter Host. Copy the databricks URI from the browser which will be of format, https://<databricks-instance>#workspace . You'll be prompted for a Token. In the Databricks UI's top-right, go to User Settings > Developer Tab > 'Generate new token'. Set a name, validity days, and click 'Generate'. Copy the token and paste in the CLI. Note: The databricks Host and token is saved inside \"C:\\Users\\user-name\\databrickscfg\" Create secret scope Suppose you want to create a secret scope with the following details: Scope Name : my-scope Backend Type : Databricks-managed (i.e., DATABRICKS ) Initial Manage Principal : users (Means, all users. Must for non-premium acts) With these details, the command will look like: databricks secrets create-scope --scope myscope --scope-backend-type DATABRICKS --initial-manage-principal users Note: In Databricks, when creating a secret scope, the default assigns MANAGE permission only to the creator. For non-Premium accounts, override this by granting MANAGE permission to all users using --initial-manage-principal users during scope creation. Create Azure-Key-Vault-Managed Secret Scope Create a Key Vault : Set up a Key Vault in Azure using the standard way. Get Key Vault Details : Go to the 'Properties' of your Key Vault and make a note of the 'Vault URI' and 'Resource ID'. Open Databricks Secret Scope UI : Go to your Databricks URL. Add #secrets/createScope right after .net . 'S' is caps. Here's how it should look: https://databricks-instance.azuredatabricks.net#secrets/createScope This is a workaround, as there's no direct link in the UI to this page. Set Up the Secret Scope : Give it a name under 'Scope'. Pick a 'Manage Principal'. In the 'DNS Name' field, put the 'Vault URI' you copied earlier from Key Vault properties. For 'Resource ID', use the 'Resource ID' you noted down, something like: /subscriptions/someid/resourceGroups/resgropname/providers/Microsoft.KeyVault/vaults/keyvaultname 5. Create the Secret Scope : Click 'Create'. After you do, you'll get a confirmation. But keep in mind, once created, you can't see this scope in the Databricks UI. To check it, you'll have to use the databricks secrets list-scopes command in the Databricks CLI. Check the Secret Scope : Keep in mind, once created, you can't see this scope in the Databricks UI. To check it, you'll have to use the databricks secrets list-scopes command in the Databricks CLI. Working with secret scopes Adding secrets to scope Add secrets to Databricks scopes using dbrk CLI. For azure-key-vault-backed secrets, use the Azure portal. Ensure admin role on the azure keyvault, even if you created it. More details in the following sections. Add Secret to Databricks-backed scope : To add run the following command in databricks CLI databricks secrets put --scope <scope-name> --key <key-name> --string-value <your-actual-secret-value> If your secret is stored in a file, you can use the following instead: databricks secrets put --scope <scope-name> --key <key-name> --binary-file <path-to-file> Add Secret to Azure-Key-Vault-backed scope : Note: To add secrets in Azure Key Vault, you must use the Azure SetSecret REST API or Azure portal UI. Put operation using databricks CLI will NOT work. Listing Secrets from Scope List Azure-Key-Vault-Backed scopes To list Azure Key Vault-backed scopes, run dbutils.secrets.list(scope='az-kv-backed-secret-scope-name') , in Azure Databricks. Remember: Databricks interacts with Azure Key Vault using the AzureDatabricks application identity, not the logged-in user's identity. Grant required role to AzureDatabricks app in Azure Key Vault: Open your Azure Key Vault. Go to Access control(IAM) , Role assignments . Click Add Icon . Select Add role assignment . In the Members tab , select the desired role, e.g. Key Vaults Secrets .User, click next Now click on Select members and choose AzureDatabricks . Then click Select . Finally click Review + assign Finally you can run the list command in databricks without issues List Databricks-backed Secret scopes To list Databricks-backed scopes, run dbutils.secrets.list(scope='databricks-backed-scope-name') , in Databricks. This is relatively simple operation and doesn't require additinal roles in Azure. Delete a Secret Scope To delete a specific secret scope: databricks secrets delete-scope --scope <scope-name> Replace <scope-name> with the name of the secret scope you wish to delete. Caution : Be certain about deleting a secret scope, as this action cannot be undone and any secrets within the scope will be permanently removed. Connecting ADLS to Databricks using OAuth2 with Service Principal Using Azure-backed Secret Scopes Quick check: Ensure you have an Azure Key Vault set up. The Azure Key Vault should be configured as the backend of the Databricks Azure-backed Secret Scope you've created. To execute dbutils.secrets.get an App callled AzureDatabricks should have role added to the Azure Keyvault Refer to Common Error section to see common errors for such operations Setting Up Secrets in Azure Key Vault: Using the steps below add 3 entries in Azure KeyVault : Navigate to the Azure Key Vault that's linked to the Databricks Secret Scope you created. Under Secrets , click on the '+ Generate/Import' option. Add the following three secret entries: regappDirectoryID : This is the Directory (Tenant) ID found under the 'Overview' section of your registered application in Azure. regappClientID : This is the Application (Client) ID , also found under the 'Overview' section of your registered app. regappClientSecret : Navigate to Certificates and Secrets in your registered app. Create a New Client Secret and use its value here. Code for OAuth Authentication: Code for OAuth Authentication : To access a file in ADLS, use the following code: ```python storage_account = \"saforone\" regappClientID = dbutils.secrets.get(scope=\"azbackedscope\", key=\"regappClientID\") regappDirectoryID = dbutils.secrets.get(scope=\"azbackedscope\", key=\"regappDirectoryID\") regappClientSecret = dbutils.secrets.get(scope=\"azbackedscope\", key=\"regappClientSecret\") Set OAuth as the authentication type for the specified storage account. spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\") Define the token provider type for OAuth (client credentials flow). spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") Provide the Application (Client) ID of the Azure AD registered application. spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", regappClientID) Set the client secret of the registered application. spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", regappClientSecret) Specify the OAuth 2.0 token endpoint . spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{regappDirectoryID}/oauth2/token\") For debugging: retrieve and print the client ID configuration. print(spark.conf.get(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\")) ``` Common Errors And Resolutions Key Vault Access Denied When attempting to add keys to the key vault, one might encounter errors such as: The operation is not allowed by RBAC. Solution : Assign a role like Key Vault Administrator to the logged-in user. This should allow the user to add or remove keys. DeniedWithNoValidRBAC After addressing the first issue, another challenge might arise. When trying to access keys from Databricks using commands like dbutils.secrets.list(scope='foronescope') , errors such as DeniedWithNoValidRBAC and ForbiddenByRbac might appear. Root Cause : Insufficient permission for AzureDatabricks Identity. A hint lies in the error log: Caller: name=AzureDatabricks;appid=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d Solution : Recognize that it's the AzureDatabricks service's identity Azure evaluates, not our group identity. Ensure AzureDatabricks has the right permissions, especially the Admin role. Reference : A related issue can be found in this GitHub discussion . Summary: 1. For AzureDatabricks : Assign the Admin role. 2. Assign the Key Vault Administrator role to the logged-in user ---","title":"Secret Scope"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#working-with-secret-scopes-in-azure-databricks","text":"","title":"Working with Secret Scopes in Azure Databricks"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#introduction","text":"Azure Databricks offers two secret scopes: Azure Key Vault-backed and Databricks-backed. This guide walks you through their creation, access in PySpark, and prerequisites such as Databricks CLI installation and understanding key vault-related roles. By the end, you'll possess all the fundamental knowledge required to use key vaults with Azure Databricks.","title":"Introduction"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#different-types-of-secret-scopes","text":"Databricks-backed Secret Scope : This is a proprietary scope managed by Databricks. Secrets stored in this scope are encrypted and can be accessed only by users with the necessary permissions. Azure key-vault backed Secret Scope : If you create a secret scope in Databricks that references an Azure Key Vault, then the scope is Azure Key Vault-backed. The actual secrets are stored in Azure Key Vault, and Databricks retrieves them from there when needed.","title":"Different Types of Secret Scopes"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#create-databricks-backed-secret-scope","text":"","title":"Create Databricks-backed Secret Scope"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#pre-requiesites","text":"You may need to install databricks CLI if you don't have it installed already.","title":"Pre-requiesites"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#install-databricks-cli","text":"Databricks-backed secret scopes are created using the Databricks CLI and Secrets API. The #secrets/createScope UI in Databricks is reserved for Azure Key Vault-backed secret scopes. Thus, we'll employ the Databricks CLI for this task: Ensure you have Python installed on your system. Use pip, Python\u2019s package manager, to install the CLI: bash pip install databricks-cli","title":"Install Databricks CLI"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#log-into-databricks-using-the-cli","text":"Once the CLI is installed, the next step is to log in to your Databricks workspace: Open a terminal or command prompt and enter bash databricks configure --token You will be asked to enter Host. Copy the databricks URI from the browser which will be of format, https://<databricks-instance>#workspace . You'll be prompted for a Token. In the Databricks UI's top-right, go to User Settings > Developer Tab > 'Generate new token'. Set a name, validity days, and click 'Generate'. Copy the token and paste in the CLI. Note: The databricks Host and token is saved inside \"C:\\Users\\user-name\\databrickscfg\"","title":"Log into Databricks using the CLI"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#create-secret-scope","text":"Suppose you want to create a secret scope with the following details: Scope Name : my-scope Backend Type : Databricks-managed (i.e., DATABRICKS ) Initial Manage Principal : users (Means, all users. Must for non-premium acts) With these details, the command will look like: databricks secrets create-scope --scope myscope --scope-backend-type DATABRICKS --initial-manage-principal users Note: In Databricks, when creating a secret scope, the default assigns MANAGE permission only to the creator. For non-Premium accounts, override this by granting MANAGE permission to all users using --initial-manage-principal users during scope creation.","title":"Create secret scope"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#create-azure-key-vault-managed-secret-scope","text":"Create a Key Vault : Set up a Key Vault in Azure using the standard way. Get Key Vault Details : Go to the 'Properties' of your Key Vault and make a note of the 'Vault URI' and 'Resource ID'. Open Databricks Secret Scope UI : Go to your Databricks URL. Add #secrets/createScope right after .net . 'S' is caps. Here's how it should look: https://databricks-instance.azuredatabricks.net#secrets/createScope This is a workaround, as there's no direct link in the UI to this page. Set Up the Secret Scope : Give it a name under 'Scope'. Pick a 'Manage Principal'. In the 'DNS Name' field, put the 'Vault URI' you copied earlier from Key Vault properties. For 'Resource ID', use the 'Resource ID' you noted down, something like: /subscriptions/someid/resourceGroups/resgropname/providers/Microsoft.KeyVault/vaults/keyvaultname 5. Create the Secret Scope : Click 'Create'. After you do, you'll get a confirmation. But keep in mind, once created, you can't see this scope in the Databricks UI. To check it, you'll have to use the databricks secrets list-scopes command in the Databricks CLI. Check the Secret Scope : Keep in mind, once created, you can't see this scope in the Databricks UI. To check it, you'll have to use the databricks secrets list-scopes command in the Databricks CLI.","title":"Create Azure-Key-Vault-Managed Secret Scope"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#working-with-secret-scopes","text":"","title":"Working with secret scopes"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#adding-secrets-to-scope","text":"Add secrets to Databricks scopes using dbrk CLI. For azure-key-vault-backed secrets, use the Azure portal. Ensure admin role on the azure keyvault, even if you created it. More details in the following sections. Add Secret to Databricks-backed scope : To add run the following command in databricks CLI databricks secrets put --scope <scope-name> --key <key-name> --string-value <your-actual-secret-value> If your secret is stored in a file, you can use the following instead: databricks secrets put --scope <scope-name> --key <key-name> --binary-file <path-to-file> Add Secret to Azure-Key-Vault-backed scope : Note: To add secrets in Azure Key Vault, you must use the Azure SetSecret REST API or Azure portal UI. Put operation using databricks CLI will NOT work.","title":"Adding secrets to scope"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#listing-secrets-from-scope","text":"","title":"Listing Secrets from Scope"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#list-azure-key-vault-backed-scopes","text":"To list Azure Key Vault-backed scopes, run dbutils.secrets.list(scope='az-kv-backed-secret-scope-name') , in Azure Databricks. Remember: Databricks interacts with Azure Key Vault using the AzureDatabricks application identity, not the logged-in user's identity. Grant required role to AzureDatabricks app in Azure Key Vault: Open your Azure Key Vault. Go to Access control(IAM) , Role assignments . Click Add Icon . Select Add role assignment . In the Members tab , select the desired role, e.g. Key Vaults Secrets .User, click next Now click on Select members and choose AzureDatabricks . Then click Select . Finally click Review + assign Finally you can run the list command in databricks without issues","title":"List Azure-Key-Vault-Backed scopes"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#list-databricks-backed-secret-scopes","text":"To list Databricks-backed scopes, run dbutils.secrets.list(scope='databricks-backed-scope-name') , in Databricks. This is relatively simple operation and doesn't require additinal roles in Azure.","title":"List Databricks-backed Secret scopes"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#delete-a-secret-scope","text":"To delete a specific secret scope: databricks secrets delete-scope --scope <scope-name> Replace <scope-name> with the name of the secret scope you wish to delete. Caution : Be certain about deleting a secret scope, as this action cannot be undone and any secrets within the scope will be permanently removed.","title":"Delete a Secret Scope"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#connecting-adls-to-databricks-using-oauth2-with-service-principal-using-azure-backed-secret-scopes","text":"","title":"Connecting ADLS to Databricks using OAuth2 with Service Principal Using Azure-backed Secret Scopes"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#quick-check","text":"Ensure you have an Azure Key Vault set up. The Azure Key Vault should be configured as the backend of the Databricks Azure-backed Secret Scope you've created. To execute dbutils.secrets.get an App callled AzureDatabricks should have role added to the Azure Keyvault Refer to Common Error section to see common errors for such operations","title":"Quick check:"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#setting-up-secrets-in-azure-key-vault","text":"Using the steps below add 3 entries in Azure KeyVault : Navigate to the Azure Key Vault that's linked to the Databricks Secret Scope you created. Under Secrets , click on the '+ Generate/Import' option. Add the following three secret entries: regappDirectoryID : This is the Directory (Tenant) ID found under the 'Overview' section of your registered application in Azure. regappClientID : This is the Application (Client) ID , also found under the 'Overview' section of your registered app. regappClientSecret : Navigate to Certificates and Secrets in your registered app. Create a New Client Secret and use its value here.","title":"Setting Up Secrets in Azure Key Vault:"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#code-for-oauth-authentication","text":"Code for OAuth Authentication : To access a file in ADLS, use the following code: ```python storage_account = \"saforone\" regappClientID = dbutils.secrets.get(scope=\"azbackedscope\", key=\"regappClientID\") regappDirectoryID = dbutils.secrets.get(scope=\"azbackedscope\", key=\"regappDirectoryID\") regappClientSecret = dbutils.secrets.get(scope=\"azbackedscope\", key=\"regappClientSecret\")","title":"Code for OAuth Authentication:"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#set-oauth-as-the-authentication-type-for-the-specified-storage-account","text":"spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")","title":"Set OAuth as the authentication type for the specified storage account."},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#define-the-token-provider-type-for-oauth-client-credentials-flow","text":"spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")","title":"Define the token provider type for OAuth (client credentials flow)."},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#provide-the-application-client-id-of-the-azure-ad-registered-application","text":"spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", regappClientID)","title":"Provide the Application (Client) ID of the Azure AD registered application."},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#set-the-client-secret-of-the-registered-application","text":"spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", regappClientSecret)","title":"Set the client secret of the registered application."},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#specify-the-oauth-20-token-endpoint","text":"spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{regappDirectoryID}/oauth2/token\")","title":"Specify the OAuth 2.0 token endpoint."},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#for-debugging-retrieve-and-print-the-client-id-configuration","text":"print(spark.conf.get(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\")) ```","title":"For debugging: retrieve and print the client ID configuration."},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#common-errors-and-resolutions","text":"","title":"Common Errors And Resolutions"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#key-vault-access-denied","text":"When attempting to add keys to the key vault, one might encounter errors such as: The operation is not allowed by RBAC. Solution : Assign a role like Key Vault Administrator to the logged-in user. This should allow the user to add or remove keys.","title":"Key Vault Access Denied"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#deniedwithnovalidrbac","text":"After addressing the first issue, another challenge might arise. When trying to access keys from Databricks using commands like dbutils.secrets.list(scope='foronescope') , errors such as DeniedWithNoValidRBAC and ForbiddenByRbac might appear. Root Cause : Insufficient permission for AzureDatabricks Identity. A hint lies in the error log: Caller: name=AzureDatabricks;appid=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d Solution : Recognize that it's the AzureDatabricks service's identity Azure evaluates, not our group identity. Ensure AzureDatabricks has the right permissions, especially the Admin role. Reference : A related issue can be found in this GitHub discussion . Summary: 1. For AzureDatabricks : Assign the Admin role. 2. Assign the Key Vault Administrator role to the logged-in user","title":"DeniedWithNoValidRBAC"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope/#-","text":"","title":"---"},{"location":"Spark-DataBricks/3.0_Databricks/3.5_Databricks_SQL/","text":"CREATE TABLE USING Using this SQL technique, you define the table schema, and create the table directly from the CSV file without switching to PySpark for initial data loading and df creation etc. -- Drop the table if it exists DROP TABLE IF EXISTS hollywood; -- Create an external table from the CSV file CREATE TABLE hollywood ( movieName STRING, actor STRING ) USING CSV OPTIONS ( path '/mnt/movies.csv', header 'true', inferSchema 'true' );","title":"CREATE TABLE USING"},{"location":"Spark-DataBricks/3.0_Databricks/3.5_Databricks_SQL/#create-table-using","text":"Using this SQL technique, you define the table schema, and create the table directly from the CSV file without switching to PySpark for initial data loading and df creation etc. -- Drop the table if it exists DROP TABLE IF EXISTS hollywood; -- Create an external table from the CSV file CREATE TABLE hollywood ( movieName STRING, actor STRING ) USING CSV OPTIONS ( path '/mnt/movies.csv', header 'true', inferSchema 'true' );","title":"CREATE TABLE USING"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands/","text":"Useful Databricks Magic Commands Some frequently used magic commands in Databricks Magic Command Description Example %run Runs a Python file or a notebook. %run ./path/to/notebook %sh Executes shell commands on the cluster nodes. %sh ls /dbfs %fs Interacts with the Databricks file system. %fs ls /mnt/data %sql Runs SQL queries. %sql SELECT * FROM table_name %scala Switches the notebook context to Scala. %scala val x = 10 %python Switches the notebook context to Python. %python print(\"Hello, Databricks!\") %md Writes markdown text. %md # This is a Markdown Header %r Switches the notebook context to R. %r summary(data_frame) %lsmagic Lists all the available magic commands. %lsmagic %jobs Lists all the running jobs. %jobs %config Sets configuration options for the notebook. %config notebook.display.max_rows=1000 %reload Reloads the contents of a module. %reload module_name %pip Installs Python packages. %pip install pandas %load Loads the contents of a file into a cell. %load ./path/to/file.py %matplotlib Sets up the matplotlib backend. %matplotlib inline %who Lists all the variables in the current scope. %who %env Sets environment variables. %env MY_VARIABLE=my_value Mounting and Unmounting Storage Command Example Mount ADLS dbutils.fs.mount( source = \"adl:// .azuredatalakestore.net/ \", mount_point = \"/mnt/ \", extra_configs = { \"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\", \"dfs.adls.oauth2.client.id\": dbutils.secrets.get(scope = \" \", key = \"client-id\"), \"dfs.adls.oauth2.credential\": dbutils.secrets.get(scope = \" \", key = \"client-secret\"), \"dfs.adls.oauth2.refresh.url\": \"https://login.microsoftonline.com/ /oauth2/token\"} ) Unmount Storage dbutils.fs.unmount(\"/mnt/ \") List Mount Points display(dbutils.fs.mounts()) File Operations Command Example Read CSV File df = spark.read.csv(\"/mnt/ /data/file.csv\", header=True, inferSchema=True) display(df) Write CSV File df.write.mode(\"overwrite\").csv(\"/mnt/ /output/\") List Files display(dbutils.fs.ls(\"/mnt/ \")) Secret Management Command Example Set a Secret databricks secrets create-scope --scope databricks secrets put --scope --key --string-value Get a Secret secret = dbutils.secrets.get(scope = \" \", key = \" \") Shell Commands Command Example Run Shell Command %sh ls -lh /dbfs/mnt/ / SQL and Context Switching Command Example Run SQL Query %sql SELECT * FROM table_name WHERE column = 'value' Switch to Scala %scala val x = 10 Switch to Python %python print(\"Hello, Databricks!\") Switch to R %r summary(data_frame) Package Management Command Example Install Packages %pip install pandas matplotlib Environment and Module Management Command Example Set Environment Variable %env MY_VARIABLE=my_value Load Python File %load ./scripts/helper.py Reload Module %reload my_module Variable and Configuration Management Command Example List Variables %who Notebook Configuration %config notebook.display.max_rows=1000 Markdown and Plotting Command Example Write Markdown %md # This is a Markdown Header Here is some detailed description. Setup Matplotlib %matplotlib inline import matplotlib.pyplot as plt plt.plot([1, 2, 3], [4, 5, 6]) plt.show() Job Management Command Example List Running Jobs %jobs","title":"Magic Commands"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands/#useful-databricks-magic-commands","text":"","title":"Useful Databricks Magic Commands"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands/#mounting-and-unmounting-storage","text":"Command Example Mount ADLS dbutils.fs.mount( source = \"adl:// .azuredatalakestore.net/ \", mount_point = \"/mnt/ \", extra_configs = { \"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\", \"dfs.adls.oauth2.client.id\": dbutils.secrets.get(scope = \" \", key = \"client-id\"), \"dfs.adls.oauth2.credential\": dbutils.secrets.get(scope = \" \", key = \"client-secret\"), \"dfs.adls.oauth2.refresh.url\": \"https://login.microsoftonline.com/ /oauth2/token\"} ) Unmount Storage dbutils.fs.unmount(\"/mnt/ \") List Mount Points display(dbutils.fs.mounts())","title":"Mounting and Unmounting Storage"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands/#file-operations","text":"Command Example Read CSV File df = spark.read.csv(\"/mnt/ /data/file.csv\", header=True, inferSchema=True) display(df) Write CSV File df.write.mode(\"overwrite\").csv(\"/mnt/ /output/\") List Files display(dbutils.fs.ls(\"/mnt/ \"))","title":"File Operations"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands/#secret-management","text":"Command Example Set a Secret databricks secrets create-scope --scope databricks secrets put --scope --key --string-value Get a Secret secret = dbutils.secrets.get(scope = \" \", key = \" \")","title":"Secret Management"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands/#shell-commands","text":"Command Example Run Shell Command %sh ls -lh /dbfs/mnt/ /","title":"Shell Commands"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands/#sql-and-context-switching","text":"Command Example Run SQL Query %sql SELECT * FROM table_name WHERE column = 'value' Switch to Scala %scala val x = 10 Switch to Python %python print(\"Hello, Databricks!\") Switch to R %r summary(data_frame)","title":"SQL and Context Switching"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands/#package-management","text":"Command Example Install Packages %pip install pandas matplotlib","title":"Package Management"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands/#environment-and-module-management","text":"Command Example Set Environment Variable %env MY_VARIABLE=my_value Load Python File %load ./scripts/helper.py Reload Module %reload my_module","title":"Environment and Module Management"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands/#variable-and-configuration-management","text":"Command Example List Variables %who Notebook Configuration %config notebook.display.max_rows=1000","title":"Variable and Configuration Management"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands/#markdown-and-plotting","text":"Command Example Write Markdown %md # This is a Markdown Header Here is some detailed description. Setup Matplotlib %matplotlib inline import matplotlib.pyplot as plt plt.plot([1, 2, 3], [4, 5, 6]) plt.show()","title":"Markdown and Plotting"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands/#job-management","text":"Command Example List Running Jobs %jobs","title":"Job Management"},{"location":"Spark-DataBricks/3.0_Databricks/3.7_DeltaLake_And_Lakehouse/","text":"Demystifying Lakehouse and Delta Lake Many people get confused by terms like Lakehouse architecture, Data Warehouse, and Data Lake. But the concepts are easy to understand. We just have to oversimplify them a little bit to get the hang ot it: Data Warehouse: A very large SQL database. Data Lake: A cloud-based file system, like Amazon S3 or Google Drive. Data Lakehouse/Lakehouse Architecture: If you store data in a Data Lake in Delta format, it becomes a Data Lakehouse. \"Lakehouse Architecture\" is just a term for this approach. The special _delta_log folder Spark knows a table is a Delta table if the _delta_log folder is present. This folder signals to Spark that the directory is a Delta table. If you try to write data in a different format to this directory, Spark will throw an error because it already recognizes it as a Delta table. Without the _delta_log folder, Spark will not treat the directory as a Delta table; it could be just a regular directory or another type of table. Delta Lake cheatsheet pdf","title":"Delta Lake And Lakehouse"},{"location":"Spark-DataBricks/3.0_Databricks/3.7_DeltaLake_And_Lakehouse/#demystifying-lakehouse-and-delta-lake","text":"Many people get confused by terms like Lakehouse architecture, Data Warehouse, and Data Lake. But the concepts are easy to understand. We just have to oversimplify them a little bit to get the hang ot it: Data Warehouse: A very large SQL database. Data Lake: A cloud-based file system, like Amazon S3 or Google Drive. Data Lakehouse/Lakehouse Architecture: If you store data in a Data Lake in Delta format, it becomes a Data Lakehouse. \"Lakehouse Architecture\" is just a term for this approach.","title":"Demystifying Lakehouse and Delta Lake"},{"location":"Spark-DataBricks/3.0_Databricks/3.7_DeltaLake_And_Lakehouse/#the-special-_delta_log-folder","text":"Spark knows a table is a Delta table if the _delta_log folder is present. This folder signals to Spark that the directory is a Delta table. If you try to write data in a different format to this directory, Spark will throw an error because it already recognizes it as a Delta table. Without the _delta_log folder, Spark will not treat the directory as a Delta table; it could be just a regular directory or another type of table. Delta Lake cheatsheet pdf","title":"The special _delta_log folder"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1/","text":"Real-World Data Management Using Databricks, DBT, and ADF Let\u2019s explore a setup to manage data using Databricks , DBT (Data Build Tool) , Azure Data Lake Storage (ADLS) , and Azure Data Factory (ADF) in a real-world scenario. Scenario: E-Commerce Analytics Platform Imagine we're working for an e-commerce company, and we need to manage and analyze data related to customer interactions, transactions, and website clicks. Here's how we'd handle the entire process using these modern tools. 1. Data Ingestion with Azure Data Factory (ADF) First, the raw data coming from various sources like transaction logs, customer profiles, and clickstreams needs to be stored in a data lake. Azure Data Factory (ADF) is a great tool for this. ADF helps me pull data from multiple sources (APIs, databases, files) into Azure Data Lake Storage (ADLS) , which acts as my data lake. we can set up pipelines in ADF that will regularly fetch new data and store it in ADLS, keeping the data updated. For example, a pipeline can be set up to move daily transaction data into ADLS in formats like CSV or JSON . 2. Storing Data in Azure Data Lake Storage (ADLS) Now, we have raw data being continuously stored in ADLS . This data lake stores both structured (e.g., transaction records) and unstructured data (e.g., user interaction logs). ADLS helps me scale my storage as the amount of data grows, and it is cost-effective for large datasets. In the data lake, files can be stored in directories like: /raw/transactions/ /raw/customer_profiles/ /raw/click_stream/ At this point, the data is still raw and needs to be cleaned and transformed for analysis. 3. Transforming Data Using Databricks and DBT Here comes the power of Databricks and DBT . Databricks : Databricks provides a unified platform for big data processing. Using Apache Spark on Databricks, we can easily process the raw data stored in ADLS. It integrates well with ADLS, so we can read and write data efficiently. DBT : While Databricks is great for processing, DBT helps me automate and manage the transformation logic. DBT allows me to write SQL queries to transform data into the structure we need for analytics, and it keeps everything version-controlled. Let\u2019s walk through an example of how we would clean and transform transaction data. Example Workflow: Reading raw data : we use Databricks to read raw data from ADLS: python raw_transactions_df = spark.read.csv(\"/mnt/adls/raw/transactions/\") Writing a DBT model : In DBT , we create a model that cleans the data. A DBT model is essentially a SQL query that transforms raw data into a usable format: sql -- models/cleaned_transactions.sql SELECT transaction_id, customer_id, CAST(amount AS DOUBLE) AS amount, status FROM raw.transactions WHERE status = 'completed' Running DBT : we run the DBT model, and DBT handles the transformation in Databricks: bash dbt run This creates a new cleaned table, cleaned_transactions , in Delta Lake , which is Databricks\u2019 transactional data storage layer. 4. Storing Transformed Data in Delta Lake Delta Lake adds important features like ACID transactions and version control to the data stored in ADLS. So, after transforming the data with DBT, it gets stored as a Delta Table . In this case, my cleaned transaction data might be saved in Delta Lake at: /mnt/adls/delta/cleaned_transactions/ Delta Lake makes it easy for analysts to query this transformed data and trust its accuracy because of its strong transactional guarantees. 5. Metadata Management with Hive Metastore The transformed data is registered in the Hive Metastore , which is part of Databricks. This metastore acts like a catalog that tracks where the data is stored and what its structure looks like. For example, the cleaned transaction data is now a table called cleaned_transactions , and Hive Metastore tracks this table so anyone using Databricks can easily query it. 6. Data Governance with Unity Catalog As the amount of data grows and more people start using it, we need to make sure that the data is secure and governed properly. Unity Catalog in Databricks helps with this. It ensures that the right people have the right access (for example, analysts can only view data, while engineers can edit it). It also tracks the lineage of data, so we know where the data comes from and how it\u2019s been transformed. Conclusion By combining Databricks , DBT , ADF , and ADLS , we can build a powerful, scalable data management system for my e-commerce platform. Here\u2019s a quick summary of what each tool does: Azure Data Factory (ADF) : Ingests raw data into ADLS. Azure Data Lake Storage (ADLS) : Stores raw and transformed data. Databricks : Processes and transforms large datasets using Spark. DBT : Manages SQL-based transformations in Databricks. Delta Lake : Stores transformed data with transactional guarantees. Hive Metastore : Tracks the metadata for all tables and datasets. Unity Catalog : Ensures governance and security. End to end Project using Databricks and Unity Catalog Concepts Covered: We will use Delta Lake format as this is the format recommended for real-time projects. For access control we will use Unity Catalog We will use Spark structured streaming to see how We will use batch processing mode We will use CI/CD using Azure Devops Drawbacks of ADLS ADLS != Database Relational database is acidic. They store quality data. This quality is can't be guarantted in ADLS Delta lake makes a lakehouse. Importing project(.dbc) file It will import all the notebooks and your import will look like this:","title":"Project-A"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1/#real-world-data-management-using-databricks-dbt-and-adf","text":"Let\u2019s explore a setup to manage data using Databricks , DBT (Data Build Tool) , Azure Data Lake Storage (ADLS) , and Azure Data Factory (ADF) in a real-world scenario.","title":"Real-World Data Management Using Databricks, DBT, and ADF"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1/#scenario-e-commerce-analytics-platform","text":"Imagine we're working for an e-commerce company, and we need to manage and analyze data related to customer interactions, transactions, and website clicks. Here's how we'd handle the entire process using these modern tools.","title":"Scenario: E-Commerce Analytics Platform"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1/#1-data-ingestion-with-azure-data-factory-adf","text":"First, the raw data coming from various sources like transaction logs, customer profiles, and clickstreams needs to be stored in a data lake. Azure Data Factory (ADF) is a great tool for this. ADF helps me pull data from multiple sources (APIs, databases, files) into Azure Data Lake Storage (ADLS) , which acts as my data lake. we can set up pipelines in ADF that will regularly fetch new data and store it in ADLS, keeping the data updated. For example, a pipeline can be set up to move daily transaction data into ADLS in formats like CSV or JSON .","title":"1. Data Ingestion with Azure Data Factory (ADF)"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1/#2-storing-data-in-azure-data-lake-storage-adls","text":"Now, we have raw data being continuously stored in ADLS . This data lake stores both structured (e.g., transaction records) and unstructured data (e.g., user interaction logs). ADLS helps me scale my storage as the amount of data grows, and it is cost-effective for large datasets. In the data lake, files can be stored in directories like: /raw/transactions/ /raw/customer_profiles/ /raw/click_stream/ At this point, the data is still raw and needs to be cleaned and transformed for analysis.","title":"2. Storing Data in Azure Data Lake Storage (ADLS)"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1/#3-transforming-data-using-databricks-and-dbt","text":"Here comes the power of Databricks and DBT . Databricks : Databricks provides a unified platform for big data processing. Using Apache Spark on Databricks, we can easily process the raw data stored in ADLS. It integrates well with ADLS, so we can read and write data efficiently. DBT : While Databricks is great for processing, DBT helps me automate and manage the transformation logic. DBT allows me to write SQL queries to transform data into the structure we need for analytics, and it keeps everything version-controlled. Let\u2019s walk through an example of how we would clean and transform transaction data.","title":"3. Transforming Data Using Databricks and DBT"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1/#example-workflow","text":"Reading raw data : we use Databricks to read raw data from ADLS: python raw_transactions_df = spark.read.csv(\"/mnt/adls/raw/transactions/\") Writing a DBT model : In DBT , we create a model that cleans the data. A DBT model is essentially a SQL query that transforms raw data into a usable format: sql -- models/cleaned_transactions.sql SELECT transaction_id, customer_id, CAST(amount AS DOUBLE) AS amount, status FROM raw.transactions WHERE status = 'completed' Running DBT : we run the DBT model, and DBT handles the transformation in Databricks: bash dbt run This creates a new cleaned table, cleaned_transactions , in Delta Lake , which is Databricks\u2019 transactional data storage layer.","title":"Example Workflow:"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1/#4-storing-transformed-data-in-delta-lake","text":"Delta Lake adds important features like ACID transactions and version control to the data stored in ADLS. So, after transforming the data with DBT, it gets stored as a Delta Table . In this case, my cleaned transaction data might be saved in Delta Lake at: /mnt/adls/delta/cleaned_transactions/ Delta Lake makes it easy for analysts to query this transformed data and trust its accuracy because of its strong transactional guarantees.","title":"4. Storing Transformed Data in Delta Lake"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1/#5-metadata-management-with-hive-metastore","text":"The transformed data is registered in the Hive Metastore , which is part of Databricks. This metastore acts like a catalog that tracks where the data is stored and what its structure looks like. For example, the cleaned transaction data is now a table called cleaned_transactions , and Hive Metastore tracks this table so anyone using Databricks can easily query it.","title":"5. Metadata Management with Hive Metastore"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1/#6-data-governance-with-unity-catalog","text":"As the amount of data grows and more people start using it, we need to make sure that the data is secure and governed properly. Unity Catalog in Databricks helps with this. It ensures that the right people have the right access (for example, analysts can only view data, while engineers can edit it). It also tracks the lineage of data, so we know where the data comes from and how it\u2019s been transformed.","title":"6. Data Governance with Unity Catalog"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1/#conclusion","text":"By combining Databricks , DBT , ADF , and ADLS , we can build a powerful, scalable data management system for my e-commerce platform. Here\u2019s a quick summary of what each tool does: Azure Data Factory (ADF) : Ingests raw data into ADLS. Azure Data Lake Storage (ADLS) : Stores raw and transformed data. Databricks : Processes and transforms large datasets using Spark. DBT : Manages SQL-based transformations in Databricks. Delta Lake : Stores transformed data with transactional guarantees. Hive Metastore : Tracks the metadata for all tables and datasets. Unity Catalog : Ensures governance and security.","title":"Conclusion"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1/#end-to-end-project-using-databricks-and-unity-catalog","text":"","title":"End to end Project using Databricks and Unity Catalog"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1/#concepts-covered","text":"We will use Delta Lake format as this is the format recommended for real-time projects. For access control we will use Unity Catalog We will use Spark structured streaming to see how We will use batch processing mode We will use CI/CD using Azure Devops","title":"Concepts Covered:"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1/#drawbacks-of-adls","text":"ADLS != Database Relational database is acidic. They store quality data. This quality is can't be guarantted in ADLS Delta lake makes a lakehouse.","title":"Drawbacks of ADLS"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1/#importing-projectdbc-file","text":"It will import all the notebooks and your import will look like this:","title":"Importing project(.dbc) file"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts/","text":"Apache Hive Architecture What are Hive Clients? Hive clients are different ways you can connect to and interact with Hive. They help you run queries and manage data in Hive. Here are the main Hive clients: 1. Command Line Interface (CLI): The original Hive CLI allowed users to interact with Hive by typing SQL queries directly into the terminal (CMD or Linux shell). However, the older CLI has been deprecated in favor of Beeline , which connects to HiveServer2. Beeline is now the preferred way to run Hive queries from the command line as it supports multiple users, security (Kerberos authentication), and handles queries in a distributed environment, making it ideal for production. Example : - If you're using Beeline, you would start a session like this: bash beeline -u jdbc:hive2://localhost:10000 -n myuser -p mypassword This command connects to HiveServer2 running on localhost at port 10000 . You can then run queries like: bash SELECT * FROM sales_data; 2. JDBC/ODBC Clients: JDBC and ODBC are protocols used to connect applications with Hive. These clients allow various tools and programs to interact with Hive and run SQL queries on the data. JDBC is most commonly used in Java applications, while ODBC is used by tools like Power BI, Excel, and Tableau. In a Java program, you can connect to Hive using JDBC like this: ```java import java.sql.Connection; import java.sql.DriverManager; import java.sql.ResultSet; import java.sql.Statement; public class HiveJDBCExample { public static void main(String[] args) throws Exception { String url = \"jdbc:hive2://localhost:10000/default\"; Connection conn = DriverManager.getConnection(url, \"myuser\", \"mypassword\"); Statement stmt = conn.createStatement(); ResultSet rs = stmt.executeQuery(\"SELECT * FROM my_table\"); while (rs.next()) { System.out.println(rs.getString(1)); } stmt.close(); conn.close(); } } ``` 3. Apache Hue: Apache Hue is a popular web-based UI for running queries and managing data. It offers a simple interface to write queries, view results, and visualize data without needing the command line. Hue supports multiple Hadoop components like Hive, HBase, and also works with systems like Apache Impala, Presto, SparkSQL, Flink SQL, ElasticSearch, PostgreSQL, Redshift, BigQuery, and more. It\u2019s widely used for its ease of use and visual environment for working with big data. 4. Thrift Clients: Thrift is useful when you want to work with Hive in languages other than Java or use Hive in a broader range of applications. It allows for multi-language support, making Hive more flexible in different ecosystems. Example 1: Python Thrift Client : - Using the PyHive library, you can connect Python programs to Hive: ```python from pyhive import hive # Connect to HiveServer2 conn = hive.Connection(host='localhost', port=10000, username='myuser') # Create a cursor object to run SQL queries cursor = conn.cursor() cursor.execute('SELECT * FROM employees') # Fetch and print results for row in cursor.fetchall(): print(row) ``` What is Hive Metastore? Hive is a Data Warehouse software. The Hive Metastore is like a database where all the table information of the Hive Warehouse is stored. The actual data of the tables might be stored somewhere else(HDFS, ADLS, S3). Hive Metastore Modes Hive Metastore has three installation Mode: 1. Embedded Metastore Mode This is the default mode that comes with Hive. In this mode, the metastore service and the Hive service run in the same JVM (Java Virtual Machine) and use an embedded Apache Derby database, which is stored on the local file system. Limitation : Only one Hive session can run at a time because only one connection can be made to the embedded Derby database. If you try to open another Hive session, you\u2019ll get an error. This mode is good for testing but not for real-world use. If you need more than one session, you can configure Derby as a network server, but it's still mainly used for testing. 2. Local Metastore Mode To solve the problem of only one session, local metastore mode was introduced. In this mode, many Hive sessions can run at the same time, meaning multiple users can use the metastore. This is done by using any JDBC-compliant database, like MySQL, which runs separately from the Hive service. Both Hive and the metastore run in the same JVM, but the database is external. Before starting Hive, you need to add the MySQL JDBC driver to Hive\u2019s lib folder. For MySQL, the javax.jdo.option.ConnectionURL property should be set to jdbc:mysql://host/dbname?createDatabaseIfNotExist=true , and javax.jdo.option.ConnectionDriverName should be set to com.mysql.jdbc.Driver . The JDBC driver JAR file (Connector/J) must be in Hive's classpath by placing it in Hive's lib directory. 3. Remote Metastore Mode In this mode, the metastore runs in its own separate JVM, not in the same JVM as the Hive service. Other processes, like Hive clients or other services, communicate with the metastore using Thrift Network APIs. You can also have multiple metastore servers for higher availability. This setup improves manageability and security because the database can be completely firewalled, and clients don\u2019t need direct access to the database. To use this mode, set the hive.metastore.uris property in Hive's configuration to point to the metastore server\u2019s URI, which looks like thrift://host:port . The port is set by the METASTORE_PORT when starting the metastore server. Databases Supported by Hive Hive supports the following backend databases: - Derby - MySQL - MS SQL Server - Oracle - Postgres Hive clients are categorized into three types: Thrift Clients The Hive server is based on Apache Thrift so that it can serve the request from a thrift client. JDBC client Hive allows for the Java applications to connect to it using the JDBC driver. JDBC driver uses Thrift to communicate with the Hive Server. ODBC client Hive ODBC driver allows applications based on the ODBC protocol to connect to Hive. Similar to the JDBC driver, the ODBC driver uses Thrift to communicate with the Hive Server. Apache Hive Services Some confusion regarding Hive Is Hadoop and MapReduce mandatory for Hive? No, absolutely not. You might be wondering, \"What is Hadoop? I use Databricks and Azure Cloud and have never used any Hadoop components.\" When Hive was first introduced, there weren\u2019t many reliable systems that could store huge amounts of data safely (like with disaster recovery and clustering). At that time, Hadoop was the only system that provided those features, so Hive was closely tied to it. But nowadays, Hive is used with storage systems like S3 or Azure Data Lake Storage (ADLS), so Hive and Hadoop are no longer dependent on each other. As for MapReduce, Spark has fully replaced it because Spark is much better and faster. Hive still supports MapReduce and Tez, but these are optional. You can always use Spark instead. To sum it up, you can have a Hive system without using any Hadoop or MapReduce.","title":"Hive Concepts"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts/#apache-hive-architecture","text":"","title":"Apache Hive Architecture"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts/#what-are-hive-clients","text":"Hive clients are different ways you can connect to and interact with Hive. They help you run queries and manage data in Hive. Here are the main Hive clients:","title":"What are Hive Clients?"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts/#1-command-line-interface-cli","text":"The original Hive CLI allowed users to interact with Hive by typing SQL queries directly into the terminal (CMD or Linux shell). However, the older CLI has been deprecated in favor of Beeline , which connects to HiveServer2. Beeline is now the preferred way to run Hive queries from the command line as it supports multiple users, security (Kerberos authentication), and handles queries in a distributed environment, making it ideal for production. Example : - If you're using Beeline, you would start a session like this: bash beeline -u jdbc:hive2://localhost:10000 -n myuser -p mypassword This command connects to HiveServer2 running on localhost at port 10000 . You can then run queries like: bash SELECT * FROM sales_data;","title":"1. Command Line Interface (CLI):"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts/#2-jdbcodbc-clients","text":"JDBC and ODBC are protocols used to connect applications with Hive. These clients allow various tools and programs to interact with Hive and run SQL queries on the data. JDBC is most commonly used in Java applications, while ODBC is used by tools like Power BI, Excel, and Tableau. In a Java program, you can connect to Hive using JDBC like this: ```java import java.sql.Connection; import java.sql.DriverManager; import java.sql.ResultSet; import java.sql.Statement; public class HiveJDBCExample { public static void main(String[] args) throws Exception { String url = \"jdbc:hive2://localhost:10000/default\"; Connection conn = DriverManager.getConnection(url, \"myuser\", \"mypassword\"); Statement stmt = conn.createStatement(); ResultSet rs = stmt.executeQuery(\"SELECT * FROM my_table\"); while (rs.next()) { System.out.println(rs.getString(1)); } stmt.close(); conn.close(); } } ```","title":"2. JDBC/ODBC Clients:"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts/#3-apache-hue","text":"Apache Hue is a popular web-based UI for running queries and managing data. It offers a simple interface to write queries, view results, and visualize data without needing the command line. Hue supports multiple Hadoop components like Hive, HBase, and also works with systems like Apache Impala, Presto, SparkSQL, Flink SQL, ElasticSearch, PostgreSQL, Redshift, BigQuery, and more. It\u2019s widely used for its ease of use and visual environment for working with big data.","title":"3. Apache Hue:"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts/#4-thrift-clients","text":"Thrift is useful when you want to work with Hive in languages other than Java or use Hive in a broader range of applications. It allows for multi-language support, making Hive more flexible in different ecosystems. Example 1: Python Thrift Client : - Using the PyHive library, you can connect Python programs to Hive: ```python from pyhive import hive # Connect to HiveServer2 conn = hive.Connection(host='localhost', port=10000, username='myuser') # Create a cursor object to run SQL queries cursor = conn.cursor() cursor.execute('SELECT * FROM employees') # Fetch and print results for row in cursor.fetchall(): print(row) ```","title":"4. Thrift Clients:"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts/#what-is-hive-metastore","text":"Hive is a Data Warehouse software. The Hive Metastore is like a database where all the table information of the Hive Warehouse is stored. The actual data of the tables might be stored somewhere else(HDFS, ADLS, S3).","title":"What is Hive Metastore?"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts/#hive-metastore-modes","text":"Hive Metastore has three installation Mode:","title":"Hive Metastore Modes"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts/#1-embedded-metastore-mode","text":"This is the default mode that comes with Hive. In this mode, the metastore service and the Hive service run in the same JVM (Java Virtual Machine) and use an embedded Apache Derby database, which is stored on the local file system. Limitation : Only one Hive session can run at a time because only one connection can be made to the embedded Derby database. If you try to open another Hive session, you\u2019ll get an error. This mode is good for testing but not for real-world use. If you need more than one session, you can configure Derby as a network server, but it's still mainly used for testing.","title":"1. Embedded Metastore Mode"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts/#2-local-metastore-mode","text":"To solve the problem of only one session, local metastore mode was introduced. In this mode, many Hive sessions can run at the same time, meaning multiple users can use the metastore. This is done by using any JDBC-compliant database, like MySQL, which runs separately from the Hive service. Both Hive and the metastore run in the same JVM, but the database is external. Before starting Hive, you need to add the MySQL JDBC driver to Hive\u2019s lib folder. For MySQL, the javax.jdo.option.ConnectionURL property should be set to jdbc:mysql://host/dbname?createDatabaseIfNotExist=true , and javax.jdo.option.ConnectionDriverName should be set to com.mysql.jdbc.Driver . The JDBC driver JAR file (Connector/J) must be in Hive's classpath by placing it in Hive's lib directory.","title":"2. Local Metastore Mode"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts/#3-remote-metastore-mode","text":"In this mode, the metastore runs in its own separate JVM, not in the same JVM as the Hive service. Other processes, like Hive clients or other services, communicate with the metastore using Thrift Network APIs. You can also have multiple metastore servers for higher availability. This setup improves manageability and security because the database can be completely firewalled, and clients don\u2019t need direct access to the database. To use this mode, set the hive.metastore.uris property in Hive's configuration to point to the metastore server\u2019s URI, which looks like thrift://host:port . The port is set by the METASTORE_PORT when starting the metastore server.","title":"3. Remote Metastore Mode"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts/#databases-supported-by-hive","text":"Hive supports the following backend databases: - Derby - MySQL - MS SQL Server - Oracle - Postgres Hive clients are categorized into three types: Thrift Clients The Hive server is based on Apache Thrift so that it can serve the request from a thrift client. JDBC client Hive allows for the Java applications to connect to it using the JDBC driver. JDBC driver uses Thrift to communicate with the Hive Server. ODBC client Hive ODBC driver allows applications based on the ODBC protocol to connect to Hive. Similar to the JDBC driver, the ODBC driver uses Thrift to communicate with the Hive Server.","title":"Databases Supported by Hive"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts/#apache-hive-services","text":"","title":"Apache Hive Services"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts/#some-confusion-regarding-hive","text":"","title":"Some confusion regarding Hive"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts/#is-hadoop-and-mapreduce-mandatory-for-hive","text":"No, absolutely not. You might be wondering, \"What is Hadoop? I use Databricks and Azure Cloud and have never used any Hadoop components.\" When Hive was first introduced, there weren\u2019t many reliable systems that could store huge amounts of data safely (like with disaster recovery and clustering). At that time, Hadoop was the only system that provided those features, so Hive was closely tied to it. But nowadays, Hive is used with storage systems like S3 or Azure Data Lake Storage (ADLS), so Hive and Hadoop are no longer dependent on each other. As for MapReduce, Spark has fully replaced it because Spark is much better and faster. Hive still supports MapReduce and Tez, but these are optional. You can always use Spark instead. To sum it up, you can have a Hive system without using any Hadoop or MapReduce.","title":"Is Hadoop and MapReduce mandatory for Hive?"},{"location":"StreamProcessing/1.0_What_Is_Stream_Processing/","text":"What is Stream Processing? In today's world, real-time data is everywhere\u2014viral posts, online sales, stock market, card transactions etc. Processing such data as it arrives is called stream processing. What are the Main Items in Stream Processing? Source: This is where the data starts its journey. For example, data could come from Twitter hashtags, IoT devices, or transaction logs. Broker(Event-Catch-and-Hold Product): Why is this needed? Because data comes in so fast, there has to be something like a dam to buffer or hold it for a while. This is where tools like Kafka, Azure Event Hubs, Amazon Kinesis come in\u2014they temporarily hold the data to prevent the processing system from getting overwhelmed. Processing Engine: The next step is processing the data, and for this, you use a processing engine. Think of Spark or Kinesis, the superstars of data engineering. These engines take the buffered data and process it in near real-time. Storage: Finally, the processed data needs to be stored. This is a no-brainer\u2014it's just plain old storage. Without stream processing, it would store something else. It's not any special or fancy storage, just where the results are kept. Spark in Stream Processing: When Spark is used for stream processing, it\u2019s not in its usual mode where it processes static data. Instead, it operates in Spark Streaming mode, where it continuously processes incoming data as it arrives. Real-Time? Is It Really Real-Time? No, it's technically impossible to have true real-time processing with zero delay. Even if there were no Kafka or Azure Event Hubs to buffer the data between the source and the processing engine, it still couldn't be 0-second real-time. Even light takes some time to travel! So, real-time in stream processing means something so fast that we humans can't catch the delay. But in today's world, factors like latency, network speed, and processing time mean there's always a tiny bit of delay.","title":"1.0 What Is Stream Processing"},{"location":"StreamProcessing/1.0_What_Is_Stream_Processing/#what-is-stream-processing","text":"In today's world, real-time data is everywhere\u2014viral posts, online sales, stock market, card transactions etc. Processing such data as it arrives is called stream processing.","title":"What is Stream Processing?"},{"location":"StreamProcessing/1.0_What_Is_Stream_Processing/#what-are-the-main-items-in-stream-processing","text":"Source: This is where the data starts its journey. For example, data could come from Twitter hashtags, IoT devices, or transaction logs. Broker(Event-Catch-and-Hold Product): Why is this needed? Because data comes in so fast, there has to be something like a dam to buffer or hold it for a while. This is where tools like Kafka, Azure Event Hubs, Amazon Kinesis come in\u2014they temporarily hold the data to prevent the processing system from getting overwhelmed. Processing Engine: The next step is processing the data, and for this, you use a processing engine. Think of Spark or Kinesis, the superstars of data engineering. These engines take the buffered data and process it in near real-time. Storage: Finally, the processed data needs to be stored. This is a no-brainer\u2014it's just plain old storage. Without stream processing, it would store something else. It's not any special or fancy storage, just where the results are kept.","title":"What are the Main Items in Stream Processing?"},{"location":"StreamProcessing/1.0_What_Is_Stream_Processing/#spark-in-stream-processing","text":"When Spark is used for stream processing, it\u2019s not in its usual mode where it processes static data. Instead, it operates in Spark Streaming mode, where it continuously processes incoming data as it arrives.","title":"Spark in Stream Processing:"},{"location":"StreamProcessing/1.0_What_Is_Stream_Processing/#real-time-is-it-really-real-time","text":"No, it's technically impossible to have true real-time processing with zero delay. Even if there were no Kafka or Azure Event Hubs to buffer the data between the source and the processing engine, it still couldn't be 0-second real-time. Even light takes some time to travel! So, real-time in stream processing means something so fast that we humans can't catch the delay. But in today's world, factors like latency, network speed, and processing time mean there's always a tiny bit of delay.","title":"Real-Time? Is It Really Real-Time?"},{"location":"StreamProcessing/2.0.1_EventHubs_Vs_Kafka/","text":"Azure Event Hubs Vs Kafka: A Quick Comparison What is Azure Event Hubs? Azure Event Hubs is an event ingestion and stream processing service. Sounds complex? Well, it can receive data from millions of events (like posts from a viral hashtag) and process them, then maybe send them to be stored in a database. It\u2019s almost the same as Kafka, the open-source and free product, and also similar to Amazon Kinesis and Google Pub/Sub. I mentioned Event Hubs is similar to Kafka. That means Azure Event Hubs is not Kafka. Kafka is an open-source platform that anyone can set up and run. On the other hand, Azure Event Hubs is a proprietary product by Microsoft. However, Event Hubs offers a Kafka-compatible endpoint, meaning you can run your existing Kafka work without any code changes. Again, Event Hubs is not Kafka. But it supports Kafka natively. \u201cNatively\u201d is the key part. Azure Event Hubs Vs Kafka Who manages the platform? With Kafka, you have full control since you\u2019re running it yourself. With Azure Event Hubs, Microsoft manages the service (backend servers, etc.), so you don\u2019t have to worry about maintenance or scaling. How easy is it? Event Hubs is easier if you want a managed service without the hassle of setup. Kafka requires you to handle everything from setup to scaling. Event Hubs: There are no servers, disks, or networks to manage and monitor and no brokers to consider or configure, ever. You create a namespace, which is an endpoint with a fully qualified domain name, and then you create Event Hubs (topics) within that namespace. Integration: Event Hubs integrates smoothly with other Azure services, making it a good choice if you\u2019re already in the Azure ecosystem. Why Use Azure Event Hubs? Open-source Kafka on-premises is appealing because you don\u2019t pay for licensing. But it\u2019s not a \u201cset it and forget it\u201d solution. Servers need updates, maintenance, and support. All this is covered if you use Event Hubs. You won\u2019t have to worry about backend servers or patching. And, the product will be supported by Microsoft, meaning you can chase them if something goes wrong. Can you do this with open-source products like Kafka? No, you can\u2019t. How is Auzre EventHub Setup/Installed? All this while, I've been saying that Kafka installation is complex. It's not as complex as I've made it seem, though it is still more involved than setting up Event Hubs. For example, in HDInsight, Microsoft provides a ready-made template for Kafka setup, making it as simple as setting up Event Hubs. With HDInsight, everything is done through the browser, with all setup on the cloud. Additionally, there's Confluent Cloud on Azure (and other clouds) that further simplifies the process. Did you know you can have a kind of Azure-Kafka? These are actual Kafka servers, which can be easily set up using Azure HDInsight. This means you don\u2019t have to install the software on-prem or on Docker. It\u2019s like a hybrid solution. You use a bit of Azure (their servers, etc.) but still have actual Kafka. Also, Confluent Company provides Kafka on Azure MarketPlace. All installation methods Installation Method Details On Your Own Servers Manual Installation Set up Kafka on your own servers when you want full control. Docker Run Kafka inside a Docker container for easy management and portability. Kubernetes Deploy Kafka on a Kubernetes cluster when you need easy scaling. On the Cloud Virtual Machines Install Kafka on cloud VMs when using cloud infrastructure. Managed Services Confluent Cloud Use Confluent Cloud for fully managed Kafka when you want hassle-free management. Amazon MSK Opt for Amazon MSK if you're on AWS and need managed Kafka. Azure Event Hubs Choose Azure Event Hubs for a Kafka-compatible service on Azure. Hybrid Deploy Kafka on both servers and cloud when you need both on-premises and cloud. For Developers (Local Use) Docker Run Kafka locally inside Docker for development and testing. Confluent Platform Use Confluent Platform for an easy local setup with extra tools. Can you replace Kafka fully with Azure Event Hubs? If you have a streaming data source, you can replace Kafka with Azure Event Hubs instead. Event Hubs can handle the ingestion, streaming, and processing of real-time data. You won\u2019t need Kafka at all. Here\u2019s how open-source tools map to Azure services for streaming projects: Kafka - Azure Event Hubs Kafka Streams - Azure Stream Analytics Kafka Connect - Azure Data Factory With these, there will not be a need for Kafka in the entire project.Look at the table below, it shows open-source products and their Azure counterpart. Open-Source Tool Azure Equivalent Functionality Kafka (Streaming Data Platform) Azure Event Hubs Can replace Kafka for ingesting and streaming large volumes of real-time data. Kafka Streams (Stream Processing) Azure Stream Analytics Can replace Kafka Streams for real-time data processing and transformation as data flows through Event Hubs. Kafka Connect (Data Integration) Azure Data Factory Can replace Kafka Connect by connecting and transforming data across different sources. Schema Registry (Data Schema Management) Azure Event Grid / Azure Schema Registry Can replace Schema Registry for managing and ensuring compatibility of event schemas. Kafka Topics (Data Segmentation) Event Hubs Partitions Can replace Kafka Topics by segmenting data streams for organization and scaling. Monitoring and Management Azure Monitor / Azure Metrics Can replace custom Kafka monitoring tools for managing and monitoring streaming data.","title":"EventHubs Vs Kafka"},{"location":"StreamProcessing/2.0.1_EventHubs_Vs_Kafka/#azure-event-hubs-vs-kafka-a-quick-comparison","text":"","title":"Azure Event Hubs Vs Kafka: A Quick Comparison"},{"location":"StreamProcessing/2.0.1_EventHubs_Vs_Kafka/#what-is-azure-event-hubs","text":"Azure Event Hubs is an event ingestion and stream processing service. Sounds complex? Well, it can receive data from millions of events (like posts from a viral hashtag) and process them, then maybe send them to be stored in a database. It\u2019s almost the same as Kafka, the open-source and free product, and also similar to Amazon Kinesis and Google Pub/Sub. I mentioned Event Hubs is similar to Kafka. That means Azure Event Hubs is not Kafka. Kafka is an open-source platform that anyone can set up and run. On the other hand, Azure Event Hubs is a proprietary product by Microsoft. However, Event Hubs offers a Kafka-compatible endpoint, meaning you can run your existing Kafka work without any code changes. Again, Event Hubs is not Kafka. But it supports Kafka natively. \u201cNatively\u201d is the key part.","title":"What is Azure Event Hubs?"},{"location":"StreamProcessing/2.0.1_EventHubs_Vs_Kafka/#azure-event-hubs-vs-kafka","text":"Who manages the platform? With Kafka, you have full control since you\u2019re running it yourself. With Azure Event Hubs, Microsoft manages the service (backend servers, etc.), so you don\u2019t have to worry about maintenance or scaling. How easy is it? Event Hubs is easier if you want a managed service without the hassle of setup. Kafka requires you to handle everything from setup to scaling. Event Hubs: There are no servers, disks, or networks to manage and monitor and no brokers to consider or configure, ever. You create a namespace, which is an endpoint with a fully qualified domain name, and then you create Event Hubs (topics) within that namespace. Integration: Event Hubs integrates smoothly with other Azure services, making it a good choice if you\u2019re already in the Azure ecosystem.","title":"Azure Event Hubs Vs Kafka"},{"location":"StreamProcessing/2.0.1_EventHubs_Vs_Kafka/#why-use-azure-event-hubs","text":"Open-source Kafka on-premises is appealing because you don\u2019t pay for licensing. But it\u2019s not a \u201cset it and forget it\u201d solution. Servers need updates, maintenance, and support. All this is covered if you use Event Hubs. You won\u2019t have to worry about backend servers or patching. And, the product will be supported by Microsoft, meaning you can chase them if something goes wrong. Can you do this with open-source products like Kafka? No, you can\u2019t.","title":"Why Use Azure Event Hubs?"},{"location":"StreamProcessing/2.0.1_EventHubs_Vs_Kafka/#how-is-auzre-eventhub-setupinstalled","text":"All this while, I've been saying that Kafka installation is complex. It's not as complex as I've made it seem, though it is still more involved than setting up Event Hubs. For example, in HDInsight, Microsoft provides a ready-made template for Kafka setup, making it as simple as setting up Event Hubs. With HDInsight, everything is done through the browser, with all setup on the cloud. Additionally, there's Confluent Cloud on Azure (and other clouds) that further simplifies the process. Did you know you can have a kind of Azure-Kafka? These are actual Kafka servers, which can be easily set up using Azure HDInsight. This means you don\u2019t have to install the software on-prem or on Docker. It\u2019s like a hybrid solution. You use a bit of Azure (their servers, etc.) but still have actual Kafka. Also, Confluent Company provides Kafka on Azure MarketPlace.","title":"How is Auzre EventHub Setup/Installed?"},{"location":"StreamProcessing/2.0.1_EventHubs_Vs_Kafka/#all-installation-methods","text":"Installation Method Details On Your Own Servers Manual Installation Set up Kafka on your own servers when you want full control. Docker Run Kafka inside a Docker container for easy management and portability. Kubernetes Deploy Kafka on a Kubernetes cluster when you need easy scaling. On the Cloud Virtual Machines Install Kafka on cloud VMs when using cloud infrastructure. Managed Services Confluent Cloud Use Confluent Cloud for fully managed Kafka when you want hassle-free management. Amazon MSK Opt for Amazon MSK if you're on AWS and need managed Kafka. Azure Event Hubs Choose Azure Event Hubs for a Kafka-compatible service on Azure. Hybrid Deploy Kafka on both servers and cloud when you need both on-premises and cloud. For Developers (Local Use) Docker Run Kafka locally inside Docker for development and testing. Confluent Platform Use Confluent Platform for an easy local setup with extra tools.","title":"All installation methods"},{"location":"StreamProcessing/2.0.1_EventHubs_Vs_Kafka/#can-you-replace-kafka-fully-with-azure-event-hubs","text":"If you have a streaming data source, you can replace Kafka with Azure Event Hubs instead. Event Hubs can handle the ingestion, streaming, and processing of real-time data. You won\u2019t need Kafka at all. Here\u2019s how open-source tools map to Azure services for streaming projects: Kafka - Azure Event Hubs Kafka Streams - Azure Stream Analytics Kafka Connect - Azure Data Factory With these, there will not be a need for Kafka in the entire project.Look at the table below, it shows open-source products and their Azure counterpart. Open-Source Tool Azure Equivalent Functionality Kafka (Streaming Data Platform) Azure Event Hubs Can replace Kafka for ingesting and streaming large volumes of real-time data. Kafka Streams (Stream Processing) Azure Stream Analytics Can replace Kafka Streams for real-time data processing and transformation as data flows through Event Hubs. Kafka Connect (Data Integration) Azure Data Factory Can replace Kafka Connect by connecting and transforming data across different sources. Schema Registry (Data Schema Management) Azure Event Grid / Azure Schema Registry Can replace Schema Registry for managing and ensuring compatibility of event schemas. Kafka Topics (Data Segmentation) Event Hubs Partitions Can replace Kafka Topics by segmenting data streams for organization and scaling. Monitoring and Management Azure Monitor / Azure Metrics Can replace custom Kafka monitoring tools for managing and monitoring streaming data.","title":"Can you replace Kafka fully with Azure Event Hubs?"},{"location":"StreamProcessing/2.0.2_Project_Hello_EventHubs/","text":"Overview Components in the project Data Sources: Streaming data from IoT devices or social media feeds. (Simulated in Event Hubs) Ingestion: Azure Event Hubs for capturing real-time data. Processing: Azure Databricks for stream processing using Structured Streaming. Storage: Processed data stored Azure Data Lake (Delta Format). Visualisation: Data visualized using Power BI. To get started, the first thing would be to create a Event Hubs Service. This service can contain multiple Event Hub. Event Hubs - Can contain multiple Event Hub. Reference Projects This project is based on this tutorial. Stream processing with Azure Databricks","title":"Overview"},{"location":"StreamProcessing/2.0.2_Project_Hello_EventHubs/#overview","text":"","title":"Overview"},{"location":"StreamProcessing/2.0.2_Project_Hello_EventHubs/#components-in-the-project","text":"Data Sources: Streaming data from IoT devices or social media feeds. (Simulated in Event Hubs) Ingestion: Azure Event Hubs for capturing real-time data. Processing: Azure Databricks for stream processing using Structured Streaming. Storage: Processed data stored Azure Data Lake (Delta Format). Visualisation: Data visualized using Power BI. To get started, the first thing would be to create a Event Hubs Service. This service can contain multiple Event Hub. Event Hubs - Can contain multiple Event Hub.","title":"Components in the project"},{"location":"StreamProcessing/2.0.2_Project_Hello_EventHubs/#reference-projects","text":"This project is based on this tutorial. Stream processing with Azure Databricks","title":"Reference Projects"},{"location":"StreamProcessing/2.0.3_EventHubsLocalEmulator/","text":"Azure Event Hubs Local Emulator - End to End Introduction Want to learn Azure Event Hubs but don\u2019t have an Azure account or free credits? No credit card? Worried about the charges? No problem! In this article, I\u2019ll show you how to use the Azure Event Hubs Emulator, a setup recently released by Microsoft. It runs two Docker containers to create a fake Azure Event Hubs environment right on your local machine. Once it\u2019s set up, you can develop and learn Event Hubs without needing any Azure connection or login. It\u2019s completely separate from any account, and the best part? It\u2019s all local, so you can run everything even without an internet connection. The setup is pretty easy, they have provided a powershell script which creates the docker setup. After that its just connect-and-code. What You Need Here\u2019s what you need on your machine: Docker Desktop : If you don\u2019t have it, grab it from their website. VS Code and Python : You\u2019ll need these, along with Jupyter Notebook and a few plugins. If something doesn\u2019t work, just install the necessary plugin. Setting It Up Get the Emulator : Head over to the GitHub page and download the zip file. Unzip and Get Ready : Unzip the file to a folder on your computer. Start Docker : Launch Docker Desktop. Run the Script : Open PowerShell as an admin (right-click and choose \u201cRun as administrator\u201d). Run this command to allow scripts to run: powershell Start-Process powershell -Verb RunAs -ArgumentList 'Set-ExecutionPolicy Bypass \u2013Scope CurrentUser' Navigate to \\EventHub-Emulator\\Scripts\\Windows in the unzipped folder and run: powershell .\\LaunchEmulator.ps1 You should be able to see two containers running. Lets test the Fake Local Event Hubs Now that your fake, free, local Azure Event Hubs environment is up and running, let\u2019s see if it actually works: Check the Logs : In Docker, click on the eventhubs-emulator container to view the logs. This will give you the connection info: Namespace : emulatorns1 Event Hub : eh1 Consumer Groups : cg1 and $default Run the Notebook : Instead of cluttering this guide with code, I\u2019ve put everything you need into a Jupyter notebook. Just download it here , and run it cell by cell. The setup is straightforward, and the code should run smoothly. Resources Here are a couple of links you might find useful: GitHub: Azure Event Hubs Emulator Installer Blog: Introducing Local Emulator for Azure Event Hubs That's it! Now you have your own local setup to practice with Azure Event Hubs without any of the usual hassles. Enjoy! If you have questions, reach out to me at das.d@hotmail.com","title":"Event Hubs Emulator - End to End"},{"location":"StreamProcessing/2.0.3_EventHubsLocalEmulator/#azure-event-hubs-local-emulator-end-to-end","text":"","title":"Azure Event Hubs Local Emulator - End to End"},{"location":"StreamProcessing/2.0.3_EventHubsLocalEmulator/#introduction","text":"Want to learn Azure Event Hubs but don\u2019t have an Azure account or free credits? No credit card? Worried about the charges? No problem! In this article, I\u2019ll show you how to use the Azure Event Hubs Emulator, a setup recently released by Microsoft. It runs two Docker containers to create a fake Azure Event Hubs environment right on your local machine. Once it\u2019s set up, you can develop and learn Event Hubs without needing any Azure connection or login. It\u2019s completely separate from any account, and the best part? It\u2019s all local, so you can run everything even without an internet connection. The setup is pretty easy, they have provided a powershell script which creates the docker setup. After that its just connect-and-code.","title":"Introduction"},{"location":"StreamProcessing/2.0.3_EventHubsLocalEmulator/#what-you-need","text":"Here\u2019s what you need on your machine: Docker Desktop : If you don\u2019t have it, grab it from their website. VS Code and Python : You\u2019ll need these, along with Jupyter Notebook and a few plugins. If something doesn\u2019t work, just install the necessary plugin.","title":"What You Need"},{"location":"StreamProcessing/2.0.3_EventHubsLocalEmulator/#setting-it-up","text":"Get the Emulator : Head over to the GitHub page and download the zip file. Unzip and Get Ready : Unzip the file to a folder on your computer. Start Docker : Launch Docker Desktop. Run the Script : Open PowerShell as an admin (right-click and choose \u201cRun as administrator\u201d). Run this command to allow scripts to run: powershell Start-Process powershell -Verb RunAs -ArgumentList 'Set-ExecutionPolicy Bypass \u2013Scope CurrentUser' Navigate to \\EventHub-Emulator\\Scripts\\Windows in the unzipped folder and run: powershell .\\LaunchEmulator.ps1 You should be able to see two containers running.","title":"Setting It Up"},{"location":"StreamProcessing/2.0.3_EventHubsLocalEmulator/#lets-test-the-fake-local-event-hubs","text":"Now that your fake, free, local Azure Event Hubs environment is up and running, let\u2019s see if it actually works: Check the Logs : In Docker, click on the eventhubs-emulator container to view the logs. This will give you the connection info: Namespace : emulatorns1 Event Hub : eh1 Consumer Groups : cg1 and $default Run the Notebook : Instead of cluttering this guide with code, I\u2019ve put everything you need into a Jupyter notebook. Just download it here , and run it cell by cell. The setup is straightforward, and the code should run smoothly.","title":"Lets test the Fake Local Event Hubs"},{"location":"StreamProcessing/2.0.3_EventHubsLocalEmulator/#resources","text":"Here are a couple of links you might find useful: GitHub: Azure Event Hubs Emulator Installer Blog: Introducing Local Emulator for Azure Event Hubs That's it! Now you have your own local setup to practice with Azure Event Hubs without any of the usual hassles. Enjoy! If you have questions, reach out to me at das.d@hotmail.com","title":"Resources"},{"location":"StreamProcessing/2.0_Azure_EventHubs/","text":"What is Azure Event Hubs? It's like a bridge between Twitter and your Spark cluster. It buffers and forwards the large volume of data that flows from the source to a processing application. The confusing \"Event Hubs can stream\" statement As per Microsoft, \"Azure Event Hubs is a cloud-native data streaming service that can stream millions of events.\" Anyone reading this would think Event Hubs streams, i.e., generates some 'live content.' No, it doesn't. Twitter streams. Netflix streams. Azure Event Hubs does not. The term \"stream\" means Event Hubs handles the streams, temporarily stores them for some time, and then sends them to Spark (or something similar). Why? If there is no bridge, city would flood. Hence, without the Azure Event Hubs(bridge) So, \"stream\" here means \"handle and forward\" large volumes of data from the source to the destination, not that Event Hubs is the source of the data itself.","title":"EventHubs"},{"location":"StreamProcessing/2.0_Azure_EventHubs/#what-is-azure-event-hubs","text":"It's like a bridge between Twitter and your Spark cluster. It buffers and forwards the large volume of data that flows from the source to a processing application.","title":"What is Azure Event Hubs?"},{"location":"StreamProcessing/2.0_Azure_EventHubs/#the-confusing-event-hubs-can-stream-statement","text":"As per Microsoft, \"Azure Event Hubs is a cloud-native data streaming service that can stream millions of events.\" Anyone reading this would think Event Hubs streams, i.e., generates some 'live content.' No, it doesn't. Twitter streams. Netflix streams. Azure Event Hubs does not. The term \"stream\" means Event Hubs handles the streams, temporarily stores them for some time, and then sends them to Spark (or something similar). Why? If there is no bridge, city would flood. Hence, without the Azure Event Hubs(bridge) So, \"stream\" here means \"handle and forward\" large volumes of data from the source to the destination, not that Event Hubs is the source of the data itself.","title":"The confusing \"Event Hubs can stream\" statement"},{"location":"StreamProcessing/4_EventProcessingChoices/","text":"How many different Workflows can you have for Real-Time Data Processing? Open-Source Workflow Azure Workflow Databricks Workflow Google Cloud Workflow AWS Workflow Hybrid Workflow with Confluent and Snowflake Azure with Databricks and Synapse Analytics On-Premise with Cloud Integration How many different Workflows can you have for Real-Time Data Processing? Real-time data processing can be quite confusing because there are so many tools and services. But let\u2019s make it simple. We\u2019ll look at different ways to handle real-time events, like tweets, and see how they work with open-source tools, Azure, Aws, Google Cloud and Databricks. Open-Source Workflow In this setup, everything is open-source, giving you full control over the technology stack. [Twitter Users] -> [Kafka (Event Broker)] -> [Apache Flink/Spark Streaming (Event Processing)] -> [Apache Hudi/Iceberg (Storage)] Here\u2019s how it works: - Kafka: Acts as the middleman, receiving and managing events (tweets). - Apache Flink/Spark Streaming: These tools process the data in real-time, filtering or enriching tweets as needed. - Apache Hudi/Iceberg: These storage solutions handle storing the processed data, offering features like time travel and ACID transactions, similar to Delta Lake but in the open-source world. Azure Workflow This workflow leverages Azure\u2019s managed services for a streamlined, cloud-based solution. [Twitter Users] -> [Azure Event Hubs (Event Broker)] -> [Azure Stream Analytics (Event Processing)] -> [Azure Data Lake Storage + Delta Lake (Storage)] Here\u2019s the breakdown: - Azure Event Hubs: This is Azure\u2019s version of Kafka, handling real-time data ingestion. - Azure Stream Analytics: Processes the tweets as they come in, performing tasks like filtering and aggregation. - Azure Data Lake Storage + Delta Lake: Stores the processed data in a scalable and efficient way, allowing for further analysis and querying. Databricks Workflow This workflow combines the power of Databricks with Delta Lake for advanced analytics and machine learning. [Twitter Users] -> [Kafka (Event Broker)] -> [Databricks (Event Processing & Advanced Analytics)] -> [Delta Lake (Storage with Databricks)] Here\u2019s how it functions: - Kafka: As usual, Kafka receives and queues the events. - Databricks: Handles the heavy lifting of real-time processing and advanced analytics, including machine learning if needed. - Delta Lake: Integrated with Databricks, Delta Lake stores the data efficiently, allowing for complex queries and historical data analysis. Google Cloud Workflow If you\u2019re into Google Cloud, this setup might be right for you. [Twitter Users] -> [Google Cloud Pub/Sub (Event Broker)] -> [Google Dataflow (Event Processing)] -> [Google BigQuery (Storage)] Google Cloud Pub/Sub: Like Kafka, but for Google Cloud, handling event distribution. Google Dataflow: Processes the data using Apache Beam under the hood, perfect for both stream and batch processing. Google BigQuery: A serverless data warehouse where you can store and analyze all your processed data. AWS Workflow Here\u2019s a combination using Amazon Web Services for a fully managed experience. [Twitter Users] -> [Amazon Kinesis (Event Broker)] -> [AWS Lambda or Kinesis Data Analytics (Event Processing)] -> [Amazon S3 + AWS Glue (Storage & ETL)] Amazon Kinesis: Manages real-time data streams like Kafka. AWS Lambda or Kinesis Data Analytics: Lambda handles event-driven processing, while Kinesis Data Analytics can process streams using SQL. Amazon S3 + AWS Glue: S3 stores the data, and Glue can be used for ETL (Extract, Transform, Load) operations and cataloging. Hybrid Workflow with Confluent and Snowflake This setup combines a managed Kafka service with Snowflake\u2019s cloud data platform. [Twitter Users] -> [Confluent Cloud (Kafka as a Service)] -> [Kafka Streams or KSQL (Event Processing)] -> [Snowflake (Storage & Analytics)] Confluent Cloud: A managed Kafka service, making Kafka easier to deploy and scale. Kafka Streams or KSQL: These tools allow for processing streams directly within Kafka. Snowflake: A powerful cloud data platform for storing and analyzing your processed data. Azure with Databricks and Synapse Analytics This combination leverages Azure\u2019s data services for powerful analytics. [Twitter Users] -> [Azure Event Hubs (Event Broker)] -> [Databricks (Event Processing & Machine Learning)] -> [Azure Synapse Analytics + Delta Lake (Storage & Data Warehousing)] Azure Event Hubs: Captures the events in real-time. Databricks: Processes the data and can apply machine learning models. Azure Synapse Analytics + Delta Lake: Synapse provides advanced analytics and data warehousing, with Delta Lake ensuring reliable storage. On-Premise with Cloud Integration If you\u2019re starting on-premise but want to integrate with the cloud, here\u2019s an option: [Twitter Users] -> [Kafka (Event Broker)] -> [Apache Flink/Spark Streaming (Event Processing)] -> [On-Premises HDFS + Cloud Storage (e.g., AWS S3 or Azure Data Lake Storage) with Delta Lake (Storage)] Kafka: Manages your events locally. Apache Flink/Spark Streaming: Processes the events on-premise. On-Premises HDFS + Cloud Storage: You can store the data locally on HDFS or integrate it with cloud storage services like S3 or Azure Data Lake Storage, using Delta Lake for additional features like ACID transactions.","title":"Stream Processing Product Combination"},{"location":"StreamProcessing/4_EventProcessingChoices/#how-many-different-workflows-can-you-have-for-real-time-data-processing","text":"Real-time data processing can be quite confusing because there are so many tools and services. But let\u2019s make it simple. We\u2019ll look at different ways to handle real-time events, like tweets, and see how they work with open-source tools, Azure, Aws, Google Cloud and Databricks.","title":"How many different Workflows can you have for Real-Time Data Processing?"},{"location":"StreamProcessing/4_EventProcessingChoices/#open-source-workflow","text":"In this setup, everything is open-source, giving you full control over the technology stack. [Twitter Users] -> [Kafka (Event Broker)] -> [Apache Flink/Spark Streaming (Event Processing)] -> [Apache Hudi/Iceberg (Storage)] Here\u2019s how it works: - Kafka: Acts as the middleman, receiving and managing events (tweets). - Apache Flink/Spark Streaming: These tools process the data in real-time, filtering or enriching tweets as needed. - Apache Hudi/Iceberg: These storage solutions handle storing the processed data, offering features like time travel and ACID transactions, similar to Delta Lake but in the open-source world.","title":"Open-Source Workflow"},{"location":"StreamProcessing/4_EventProcessingChoices/#azure-workflow","text":"This workflow leverages Azure\u2019s managed services for a streamlined, cloud-based solution. [Twitter Users] -> [Azure Event Hubs (Event Broker)] -> [Azure Stream Analytics (Event Processing)] -> [Azure Data Lake Storage + Delta Lake (Storage)] Here\u2019s the breakdown: - Azure Event Hubs: This is Azure\u2019s version of Kafka, handling real-time data ingestion. - Azure Stream Analytics: Processes the tweets as they come in, performing tasks like filtering and aggregation. - Azure Data Lake Storage + Delta Lake: Stores the processed data in a scalable and efficient way, allowing for further analysis and querying.","title":"Azure Workflow"},{"location":"StreamProcessing/4_EventProcessingChoices/#databricks-workflow","text":"This workflow combines the power of Databricks with Delta Lake for advanced analytics and machine learning. [Twitter Users] -> [Kafka (Event Broker)] -> [Databricks (Event Processing & Advanced Analytics)] -> [Delta Lake (Storage with Databricks)] Here\u2019s how it functions: - Kafka: As usual, Kafka receives and queues the events. - Databricks: Handles the heavy lifting of real-time processing and advanced analytics, including machine learning if needed. - Delta Lake: Integrated with Databricks, Delta Lake stores the data efficiently, allowing for complex queries and historical data analysis.","title":"Databricks Workflow"},{"location":"StreamProcessing/4_EventProcessingChoices/#google-cloud-workflow","text":"If you\u2019re into Google Cloud, this setup might be right for you. [Twitter Users] -> [Google Cloud Pub/Sub (Event Broker)] -> [Google Dataflow (Event Processing)] -> [Google BigQuery (Storage)] Google Cloud Pub/Sub: Like Kafka, but for Google Cloud, handling event distribution. Google Dataflow: Processes the data using Apache Beam under the hood, perfect for both stream and batch processing. Google BigQuery: A serverless data warehouse where you can store and analyze all your processed data.","title":"Google Cloud Workflow"},{"location":"StreamProcessing/4_EventProcessingChoices/#aws-workflow","text":"Here\u2019s a combination using Amazon Web Services for a fully managed experience. [Twitter Users] -> [Amazon Kinesis (Event Broker)] -> [AWS Lambda or Kinesis Data Analytics (Event Processing)] -> [Amazon S3 + AWS Glue (Storage & ETL)] Amazon Kinesis: Manages real-time data streams like Kafka. AWS Lambda or Kinesis Data Analytics: Lambda handles event-driven processing, while Kinesis Data Analytics can process streams using SQL. Amazon S3 + AWS Glue: S3 stores the data, and Glue can be used for ETL (Extract, Transform, Load) operations and cataloging.","title":"AWS Workflow"},{"location":"StreamProcessing/4_EventProcessingChoices/#hybrid-workflow-with-confluent-and-snowflake","text":"This setup combines a managed Kafka service with Snowflake\u2019s cloud data platform. [Twitter Users] -> [Confluent Cloud (Kafka as a Service)] -> [Kafka Streams or KSQL (Event Processing)] -> [Snowflake (Storage & Analytics)] Confluent Cloud: A managed Kafka service, making Kafka easier to deploy and scale. Kafka Streams or KSQL: These tools allow for processing streams directly within Kafka. Snowflake: A powerful cloud data platform for storing and analyzing your processed data.","title":"Hybrid Workflow with Confluent and Snowflake"},{"location":"StreamProcessing/4_EventProcessingChoices/#azure-with-databricks-and-synapse-analytics","text":"This combination leverages Azure\u2019s data services for powerful analytics. [Twitter Users] -> [Azure Event Hubs (Event Broker)] -> [Databricks (Event Processing & Machine Learning)] -> [Azure Synapse Analytics + Delta Lake (Storage & Data Warehousing)] Azure Event Hubs: Captures the events in real-time. Databricks: Processes the data and can apply machine learning models. Azure Synapse Analytics + Delta Lake: Synapse provides advanced analytics and data warehousing, with Delta Lake ensuring reliable storage.","title":"Azure with Databricks and Synapse Analytics"},{"location":"StreamProcessing/4_EventProcessingChoices/#on-premise-with-cloud-integration","text":"If you\u2019re starting on-premise but want to integrate with the cloud, here\u2019s an option: [Twitter Users] -> [Kafka (Event Broker)] -> [Apache Flink/Spark Streaming (Event Processing)] -> [On-Premises HDFS + Cloud Storage (e.g., AWS S3 or Azure Data Lake Storage) with Delta Lake (Storage)] Kafka: Manages your events locally. Apache Flink/Spark Streaming: Processes the events on-premise. On-Premises HDFS + Cloud Storage: You can store the data locally on HDFS or integrate it with cloud storage services like S3 or Azure Data Lake Storage, using Delta Lake for additional features like ACID transactions.","title":"On-Premise with Cloud Integration"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration/","text":"Spark Amazon Kinesis Integration in 7 Steps Using VS Code Extra Note Some Kinesis and Kafka Concepts 1. Streaming Context 2. Topic Name / Stream Name / Kafka Topic 3. Consumption Model 4. Consumption Duration 5. Storage Levels 6. Kinesis Storage Level is Mandatory Summary Spark Amazon Kinesis Integration in 7 Steps Using VS Code Create a Project in VS Code : Open Visual Studio Code and create a new folder for your project. Initialize your project by creating a build.sbt file (for SBT) or pom.xml (for Maven) to manage dependencies. If you\u2019re using SBT, your build.sbt should include the necessary configurations and dependencies for Spark and Kinesis. Add Spark Kinesis Jars : In your build.sbt or pom.xml , add dependencies for spark-streaming-kinesis-asl and any other necessary libraries. For example, in build.sbt : scala libraryDependencies += \"org.apache.spark\" %% \"spark-streaming-kinesis-asl\" % \"2.4.8\" Run sbt update or mvn install to download the dependencies. Initialize Streaming Context : In your main Scala or Java file, initialize the StreamingContext . Example: scala val conf = new SparkConf().setAppName(\"KinesisSparkIntegration\").setMaster(\"local[*]\") val ssc = new StreamingContext(conf, Seconds(10)) Initialize Kinesis Utils : Use KinesisUtils.createStream to create a DStream from the Kinesis stream. Example: scala val kinesisStream = KinesisUtils.createStream( ssc, \"KinesisAppName\", \"KinesisStreamName\", \"kinesis.us-east-1.amazonaws.com\", \"us-east-1\", InitialPositionInStream.LATEST, Seconds(10), StorageLevel.MEMORY_AND_DISK_2 ) Byte Array Deserialization : Convert the byte array data from Kinesis into a more usable format, such as a string or JSON. Example: scala val stringStream = kinesisStream.map(record => new String(record)) Print the Stream : Use the print() action to output the contents of the stream for testing and debugging. Example: scala stringStream.print() Start the Stream : Start the streaming context and keep the application running. Example: scala ssc.start() ssc.awaitTermination() Extra Note Extensions : Install the Scala and SBT extensions for VS Code to improve code highlighting, auto-completion, and build management. Terminal : Use the integrated terminal in VS Code to run sbt or mvn commands. Debugging : Configure VS Code to debug Spark jobs by setting up a launch configuration in the launch.json file. Some users use Eclipse or IntelliJ and not VS Code. Some Kinesis and Kafka Concepts 1. Streaming Context Definition : In Apache Spark, StreamingContext is the main entry point for all Spark Streaming functionality. It is used to define a streaming computation by specifying the sources of streaming data (like Kafka or Kinesis), the transformations to apply to this data, and the output operations. Purpose : It manages the execution of the streaming job, setting the batch interval (how often the data is processed) and initiating the actual processing of data streams. 2. Topic Name / Stream Name / Kafka Topic Topic Name (Kafka) : A Kafka Topic is a category or feed name to which records are published. It is the basic building block of Kafka's messaging system. Topics are partitioned and replicated across Kafka brokers for scalability and fault tolerance. Stream Name (Kinesis) : Similar to Kafka topics, Stream Name in Amazon Kinesis refers to the name of a data stream where data records are continuously ingested. Data streams in Kinesis are sharded, which is akin to partitions in Kafka. 3. Consumption Model Definition : The consumption model defines how messages are read from the streaming data source (like Kafka or Kinesis). Earliest : This model instructs the consumer to start reading from the earliest available data in the stream or topic. In Kafka, this means starting from the earliest offset, and in Kinesis, from the beginning of the shard. Latest : This model instructs the consumer to start reading only new messages arriving after the consumer starts, ignoring older messages. In Kafka, this means starting from the latest offset, and in Kinesis, from the end of the shard. 4. Consumption Duration Definition : This refers to the period during which data is consumed from the stream. It could be defined as the length of time the consumer stays active, or it could be tied to how long the consumer is configured to keep processing data (e.g., indefinitely, until the end of a batch, or a specified time frame). 5. Storage Levels Definition : In Spark, storage levels determine how RDDs (Resilient Distributed Datasets) are stored in memory or on disk. These levels help optimize performance by controlling the persistence and redundancy of data. Types of Storage Levels : There are 8 primary storage levels in Spark: MEMORY_ONLY : Store RDDs in memory only. If the data doesn't fit in memory, it won't be stored. MEMORY_ONLY_SER : Store RDDs in memory in a serialized format. Useful to reduce memory usage. MEMORY_AND_DISK : Store RDDs in memory and spill to disk if memory is insufficient. MEMORY_AND_DISK_SER : Store RDDs in memory in a serialized format, and spill to disk if needed. DISK_ONLY : Store RDDs only on disk. MEMORY_ONLY_2 : Same as MEMORY_ONLY but with replication for fault tolerance. MEMORY_AND_DISK_2 : Same as MEMORY_AND_DISK but with replication. OFF_HEAP : Store RDDs in off-heap memory (outside the Java heap). Useful for certain memory configurations. Kinesis Storage Level : When integrating Spark with Kinesis, specifying a storage level is mandatory. Typically, you'd choose a storage level like MEMORY_AND_DISK_2 to ensure data resiliency across Spark nodes, given that streaming data is often critical and needs to be preserved even if some nodes fail. 6. Kinesis Storage Level is Mandatory Explanation : When using KinesisUtils.createStream in Spark Streaming, you must specify a storage level. This is to ensure that the data fetched from Kinesis is appropriately cached or stored, allowing for fault tolerance and reprocessing if necessary. The choice of storage level impacts how the data is handled, either being kept in memory, disk, or both, depending on your setup. Summary Streaming Context : Main entry point for Spark Streaming. Topic Name/Stream Name : Identifiers for data streams in Kafka (Topic) or Kinesis (Stream Name). Consumption Model : Determines how messages are consumed (Earliest or Latest). Consumption Duration : Time frame or condition for consuming data. Storage Levels : Defines how RDDs are persisted in Spark, with 8 levels available. Kinesis Storage Level : Must be specified when integrating Kinesis with Spark to ensure data is stored properly.","title":"5 AmazonKinesisSparkIntegration"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration/#spark-amazon-kinesis-integration-in-7-steps-using-vs-code","text":"Create a Project in VS Code : Open Visual Studio Code and create a new folder for your project. Initialize your project by creating a build.sbt file (for SBT) or pom.xml (for Maven) to manage dependencies. If you\u2019re using SBT, your build.sbt should include the necessary configurations and dependencies for Spark and Kinesis. Add Spark Kinesis Jars : In your build.sbt or pom.xml , add dependencies for spark-streaming-kinesis-asl and any other necessary libraries. For example, in build.sbt : scala libraryDependencies += \"org.apache.spark\" %% \"spark-streaming-kinesis-asl\" % \"2.4.8\" Run sbt update or mvn install to download the dependencies. Initialize Streaming Context : In your main Scala or Java file, initialize the StreamingContext . Example: scala val conf = new SparkConf().setAppName(\"KinesisSparkIntegration\").setMaster(\"local[*]\") val ssc = new StreamingContext(conf, Seconds(10)) Initialize Kinesis Utils : Use KinesisUtils.createStream to create a DStream from the Kinesis stream. Example: scala val kinesisStream = KinesisUtils.createStream( ssc, \"KinesisAppName\", \"KinesisStreamName\", \"kinesis.us-east-1.amazonaws.com\", \"us-east-1\", InitialPositionInStream.LATEST, Seconds(10), StorageLevel.MEMORY_AND_DISK_2 ) Byte Array Deserialization : Convert the byte array data from Kinesis into a more usable format, such as a string or JSON. Example: scala val stringStream = kinesisStream.map(record => new String(record)) Print the Stream : Use the print() action to output the contents of the stream for testing and debugging. Example: scala stringStream.print() Start the Stream : Start the streaming context and keep the application running. Example: scala ssc.start() ssc.awaitTermination()","title":"Spark Amazon Kinesis Integration in 7 Steps Using VS Code"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration/#extra-note","text":"Extensions : Install the Scala and SBT extensions for VS Code to improve code highlighting, auto-completion, and build management. Terminal : Use the integrated terminal in VS Code to run sbt or mvn commands. Debugging : Configure VS Code to debug Spark jobs by setting up a launch configuration in the launch.json file. Some users use Eclipse or IntelliJ and not VS Code.","title":"Extra Note"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration/#some-kinesis-and-kafka-concepts","text":"","title":"Some Kinesis and Kafka Concepts"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration/#1-streaming-context","text":"Definition : In Apache Spark, StreamingContext is the main entry point for all Spark Streaming functionality. It is used to define a streaming computation by specifying the sources of streaming data (like Kafka or Kinesis), the transformations to apply to this data, and the output operations. Purpose : It manages the execution of the streaming job, setting the batch interval (how often the data is processed) and initiating the actual processing of data streams.","title":"1. Streaming Context"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration/#2-topic-name-stream-name-kafka-topic","text":"Topic Name (Kafka) : A Kafka Topic is a category or feed name to which records are published. It is the basic building block of Kafka's messaging system. Topics are partitioned and replicated across Kafka brokers for scalability and fault tolerance. Stream Name (Kinesis) : Similar to Kafka topics, Stream Name in Amazon Kinesis refers to the name of a data stream where data records are continuously ingested. Data streams in Kinesis are sharded, which is akin to partitions in Kafka.","title":"2. Topic Name / Stream Name / Kafka Topic"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration/#3-consumption-model","text":"Definition : The consumption model defines how messages are read from the streaming data source (like Kafka or Kinesis). Earliest : This model instructs the consumer to start reading from the earliest available data in the stream or topic. In Kafka, this means starting from the earliest offset, and in Kinesis, from the beginning of the shard. Latest : This model instructs the consumer to start reading only new messages arriving after the consumer starts, ignoring older messages. In Kafka, this means starting from the latest offset, and in Kinesis, from the end of the shard.","title":"3. Consumption Model"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration/#4-consumption-duration","text":"Definition : This refers to the period during which data is consumed from the stream. It could be defined as the length of time the consumer stays active, or it could be tied to how long the consumer is configured to keep processing data (e.g., indefinitely, until the end of a batch, or a specified time frame).","title":"4. Consumption Duration"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration/#5-storage-levels","text":"Definition : In Spark, storage levels determine how RDDs (Resilient Distributed Datasets) are stored in memory or on disk. These levels help optimize performance by controlling the persistence and redundancy of data. Types of Storage Levels : There are 8 primary storage levels in Spark: MEMORY_ONLY : Store RDDs in memory only. If the data doesn't fit in memory, it won't be stored. MEMORY_ONLY_SER : Store RDDs in memory in a serialized format. Useful to reduce memory usage. MEMORY_AND_DISK : Store RDDs in memory and spill to disk if memory is insufficient. MEMORY_AND_DISK_SER : Store RDDs in memory in a serialized format, and spill to disk if needed. DISK_ONLY : Store RDDs only on disk. MEMORY_ONLY_2 : Same as MEMORY_ONLY but with replication for fault tolerance. MEMORY_AND_DISK_2 : Same as MEMORY_AND_DISK but with replication. OFF_HEAP : Store RDDs in off-heap memory (outside the Java heap). Useful for certain memory configurations. Kinesis Storage Level : When integrating Spark with Kinesis, specifying a storage level is mandatory. Typically, you'd choose a storage level like MEMORY_AND_DISK_2 to ensure data resiliency across Spark nodes, given that streaming data is often critical and needs to be preserved even if some nodes fail.","title":"5. Storage Levels"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration/#6-kinesis-storage-level-is-mandatory","text":"Explanation : When using KinesisUtils.createStream in Spark Streaming, you must specify a storage level. This is to ensure that the data fetched from Kinesis is appropriately cached or stored, allowing for fault tolerance and reprocessing if necessary. The choice of storage level impacts how the data is handled, either being kept in memory, disk, or both, depending on your setup.","title":"6. Kinesis Storage Level is Mandatory"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration/#summary","text":"Streaming Context : Main entry point for Spark Streaming. Topic Name/Stream Name : Identifiers for data streams in Kafka (Topic) or Kinesis (Stream Name). Consumption Model : Determines how messages are consumed (Earliest or Latest). Consumption Duration : Time frame or condition for consuming data. Storage Levels : Defines how RDDs are persisted in Spark, with 8 levels available. Kinesis Storage Level : Must be specified when integrating Kinesis with Spark to ensure data is stored properly.","title":"Summary"},{"location":"Synapse-ADF/1.0_SynapseConcepts/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Synapse Analytics Core Concepts Here are the building blocks of Azure Synapse Analytics Synapse Workspace It's the main portal of Synapse. It is present in a particular region. It has a ADLS G2 account linked and a folder there. It is always under a resource group. A Synapse workspace must be connected to a storage account (ADLS Gen2) and a file system (container inside that ADLS Gen2). Synapse will ask you to choose/create a new ADLS account and a container inside it To see the storage account linked to your Synapase workspace, go to Data Tab then Linked Tab. Linked Services Linked services in Synapase and ADF are just cconnection strings. Go to Manage Tab, External connections contains Linked Services section in Synapase and In Azure Data Factory to open Linked Services screen. The link services window makes it super easy to create connection strings. Now, you just have to select the irmspa Synapse SQL Pools Don't get confused with the term pool . Synapse gives you two SQL products: A serverless MSSQL database A dedicated MSSQL database. For serverless, all characters are fictitious. It's not like the old-school MSSQL where data stays inside in SQL's own format. For serverless, it's mainly data stored in ADLS folders. Question: So, serverless is just a query engine with no actual tables, master db, etc., like MSSQL? Answer: Yes, it has master db, tables, views, schemas, etc., but all the tables and databases there are fictitious. They are made-up showpieces derived from ADLS files. For instance, if you create a database using the serverless SQL pool: This is what you will see. Notice that everything is just a shell. The data is external: However, with the dedicated MSSQL, everything is real and traditional. It is a SQL warehouse. Hence, it old name was SQL Data Warehouse. The Dedicated pool is just a fancy name. Serverless SQL Pool The only thing real here is the SQL query engine. All data are fictiocious. Just a query running engine. All data is external. On-demand: Only pay for the queries you run. It stays online, but don\u2019t worry. You don\u2019t pay anything until you run something. Doesn\u2019t have its own storage: Doesn\u2019t store anything. It only runs queries in ADLS, etc. Cheap: Very cheap. $0 if you don\u2019t run a single SQL query. Dedicated Pool (AKA SQL DW) A full SQL Warehouse (a large SQL server) that you own. This means there is a traditional, old-school SQL database with real, dedicated storage, not just some abstract storage solution using ADLS (no insults to Serverless Pool ;-). It\u2019s the poor man\u2019s engine). Full-blown SQL Warehouse : Just a few years ago, it was called SQL Data Warehouse. Own local storage, not international calls to ADLS : It has its own storage, just like SQL Server. No, it\u2019s not ADLS; it\u2019s real SQL storage. Once on, you pay by the hour : Since it\u2019s dedicated, Microsoft covers the hardware costs for your dedicated SQL server. Whether you use it or not, you pay by the hour, and it\u2019s quite expensive. Run it for a day, and your full trial subscription might be gone. Synapse Spark Pool Synapse gives you a ready-to-use Apache Spark environment called a Serverless Spark Pool. It's like the Serverless SQL Pool, meaning you only pay when you use it. In the background, it's a Spark cluster, but it's called a Pool. In Databricks, you create a Spark cluster for a Spark environment. In Synapse, you create a Spark Pool. The end result is the same. Points to Remember: - It's just an Apache Spark cluster behind the scenes. - Use it to run Spark jobs or Spark queries. - You can write Spark logic in PySpark, SparkSQL, Scala, or even C#. - There are two ways to run Spark code in Synapse: - Spark Notebooks: Like Jupyter notebooks. - Spark Job Definitions: For running batch Spark jobs using jar files. Pipelines Pipelines in Synapse are the same as Azure Data Factory Pipelines. They are 100% identical. An activity is a task you want to perform. A pipeline is a group of activities, like copying data or running a notebook. Points to Remember: - Syapse Pipelines is actually ADF. - Pipelines are collections of activities (tasks like copying data, running a notebook, etc.). - Data Flows: A type of activity that lets you create transformations without code using graphics. It runs on Spark behind the scenes . - Triggers: To execute pipelines on a schedule. Integration Datasets These are pointers to your data. They are required when you create an activity like a Copy activity. Tell me where the data is? In a blob storage. What is the format? CSV. Tell me the connection string (Linked service) to that file/folder. You create it from the Data section in Synapse. Integration runtime Copying data is one of the main activities in Synapse and ADF. The main tool used for this is Integration Runtime . It's like a bridge that connects Synapse and ADF with data from on-premises and external sources. There are 3 types of Integration Runtime: Azure Integration Runtime: This is used to copy data within Azure or between cloud services. Self-hosted Integration Runtime: This is the bridge to copy data from your local machine to the Azure cloud. It is a software you install on your local computer. Azure-SSIS Integration Runtime: This allows you to lift and shift your SSIS packages to Azure. The Databases Types in Synapse Before we dive into this, let's ask ourselves: How many types of pools are there in Synapse? Even though I don't like the term \"pool\" (and I can't use \"server\"), we have to use it. There are three types: Serverless SQL, Dedicated SQL, and Serverless Spark pool. So, it's simple\u2014there are three types of databases, one for each: Serverless SQL Database : Created using the Serverless SQL pool. Dedicated SQL Database : Created using the Dedicated SQL pool. Note: This is essentially a data warehouse. Spark Database : v1 [Lake Database] : Created using a serverless Spark pool and PySpark, called a Spark [Lake] database. v2 [Delta Lake Database/Lakehouse] : Similar to v1, but the format of the .parquet files is Delta. So, does Apache Spark have database connections? Yes, of course. How else would it run SparkSQL and what about that magic command %%sql? It has robust database capabilities.","title":"SynapseConcepts"},{"location":"Synapse-ADF/1.0_SynapseConcepts/#synapse-analytics-core-concepts","text":"Here are the building blocks of Azure Synapse Analytics","title":"Synapse Analytics Core Concepts"},{"location":"Synapse-ADF/1.0_SynapseConcepts/#synapse-workspace","text":"It's the main portal of Synapse. It is present in a particular region. It has a ADLS G2 account linked and a folder there. It is always under a resource group. A Synapse workspace must be connected to a storage account (ADLS Gen2) and a file system (container inside that ADLS Gen2). Synapse will ask you to choose/create a new ADLS account and a container inside it To see the storage account linked to your Synapase workspace, go to Data Tab then Linked Tab.","title":"Synapse Workspace"},{"location":"Synapse-ADF/1.0_SynapseConcepts/#linked-services","text":"Linked services in Synapase and ADF are just cconnection strings. Go to Manage Tab, External connections contains Linked Services section in Synapase and In Azure Data Factory to open Linked Services screen. The link services window makes it super easy to create connection strings. Now, you just have to select the irmspa","title":"Linked Services"},{"location":"Synapse-ADF/1.0_SynapseConcepts/#synapse-sql-pools","text":"Don't get confused with the term pool . Synapse gives you two SQL products: A serverless MSSQL database A dedicated MSSQL database. For serverless, all characters are fictitious. It's not like the old-school MSSQL where data stays inside in SQL's own format. For serverless, it's mainly data stored in ADLS folders. Question: So, serverless is just a query engine with no actual tables, master db, etc., like MSSQL? Answer: Yes, it has master db, tables, views, schemas, etc., but all the tables and databases there are fictitious. They are made-up showpieces derived from ADLS files. For instance, if you create a database using the serverless SQL pool: This is what you will see. Notice that everything is just a shell. The data is external: However, with the dedicated MSSQL, everything is real and traditional. It is a SQL warehouse. Hence, it old name was SQL Data Warehouse. The Dedicated pool is just a fancy name.","title":"Synapse SQL Pools"},{"location":"Synapse-ADF/1.0_SynapseConcepts/#serverless-sql-pool","text":"The only thing real here is the SQL query engine. All data are fictiocious. Just a query running engine. All data is external. On-demand: Only pay for the queries you run. It stays online, but don\u2019t worry. You don\u2019t pay anything until you run something. Doesn\u2019t have its own storage: Doesn\u2019t store anything. It only runs queries in ADLS, etc. Cheap: Very cheap. $0 if you don\u2019t run a single SQL query.","title":"Serverless SQL Pool"},{"location":"Synapse-ADF/1.0_SynapseConcepts/#dedicated-pool-aka-sql-dw","text":"A full SQL Warehouse (a large SQL server) that you own. This means there is a traditional, old-school SQL database with real, dedicated storage, not just some abstract storage solution using ADLS (no insults to Serverless Pool ;-). It\u2019s the poor man\u2019s engine). Full-blown SQL Warehouse : Just a few years ago, it was called SQL Data Warehouse. Own local storage, not international calls to ADLS : It has its own storage, just like SQL Server. No, it\u2019s not ADLS; it\u2019s real SQL storage. Once on, you pay by the hour : Since it\u2019s dedicated, Microsoft covers the hardware costs for your dedicated SQL server. Whether you use it or not, you pay by the hour, and it\u2019s quite expensive. Run it for a day, and your full trial subscription might be gone.","title":"Dedicated Pool (AKA SQL DW)"},{"location":"Synapse-ADF/1.0_SynapseConcepts/#synapse-spark-pool","text":"Synapse gives you a ready-to-use Apache Spark environment called a Serverless Spark Pool. It's like the Serverless SQL Pool, meaning you only pay when you use it. In the background, it's a Spark cluster, but it's called a Pool. In Databricks, you create a Spark cluster for a Spark environment. In Synapse, you create a Spark Pool. The end result is the same. Points to Remember: - It's just an Apache Spark cluster behind the scenes. - Use it to run Spark jobs or Spark queries. - You can write Spark logic in PySpark, SparkSQL, Scala, or even C#. - There are two ways to run Spark code in Synapse: - Spark Notebooks: Like Jupyter notebooks. - Spark Job Definitions: For running batch Spark jobs using jar files.","title":"Synapse Spark Pool"},{"location":"Synapse-ADF/1.0_SynapseConcepts/#pipelines","text":"Pipelines in Synapse are the same as Azure Data Factory Pipelines. They are 100% identical. An activity is a task you want to perform. A pipeline is a group of activities, like copying data or running a notebook. Points to Remember: - Syapse Pipelines is actually ADF. - Pipelines are collections of activities (tasks like copying data, running a notebook, etc.). - Data Flows: A type of activity that lets you create transformations without code using graphics. It runs on Spark behind the scenes . - Triggers: To execute pipelines on a schedule.","title":"Pipelines"},{"location":"Synapse-ADF/1.0_SynapseConcepts/#integration-datasets","text":"These are pointers to your data. They are required when you create an activity like a Copy activity. Tell me where the data is? In a blob storage. What is the format? CSV. Tell me the connection string (Linked service) to that file/folder. You create it from the Data section in Synapse.","title":"Integration Datasets"},{"location":"Synapse-ADF/1.0_SynapseConcepts/#integration-runtime","text":"Copying data is one of the main activities in Synapse and ADF. The main tool used for this is Integration Runtime . It's like a bridge that connects Synapse and ADF with data from on-premises and external sources. There are 3 types of Integration Runtime: Azure Integration Runtime: This is used to copy data within Azure or between cloud services. Self-hosted Integration Runtime: This is the bridge to copy data from your local machine to the Azure cloud. It is a software you install on your local computer. Azure-SSIS Integration Runtime: This allows you to lift and shift your SSIS packages to Azure.","title":"Integration runtime"},{"location":"Synapse-ADF/1.0_SynapseConcepts/#the-databases-types-in-synapse","text":"Before we dive into this, let's ask ourselves: How many types of pools are there in Synapse? Even though I don't like the term \"pool\" (and I can't use \"server\"), we have to use it. There are three types: Serverless SQL, Dedicated SQL, and Serverless Spark pool. So, it's simple\u2014there are three types of databases, one for each: Serverless SQL Database : Created using the Serverless SQL pool. Dedicated SQL Database : Created using the Dedicated SQL pool. Note: This is essentially a data warehouse. Spark Database : v1 [Lake Database] : Created using a serverless Spark pool and PySpark, called a Spark [Lake] database. v2 [Delta Lake Database/Lakehouse] : Similar to v1, but the format of the .parquet files is Delta. So, does Apache Spark have database connections? Yes, of course. How else would it run SparkSQL and what about that magic command %%sql? It has robust database capabilities.","title":"The Databases Types in Synapse"},{"location":"Synapse-ADF/1.1_Pools/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Use Azure Synapse Serverless SQL Pool to Query Files in a Data Lake What are Serverless SQL pools and what they are capable of? Serverless SQL pools are less expensive SQL databases. Compared to Dedicated SQL pool which is more expensive and you have to pay it if you use it or not. Serverless sql pools are good for on-demand data query. Query CSV, JSON, and Parquet Files Using a Serverless SQL Pool Querying CSV Files: SELECT TOP 100 * FROM OPENROWSET( BULK 'https://mydatalake.blob.core.windows.net/data/files/*.csv', FORMAT = 'csv', PARSER_VERSION = '2.0' ) WITH ( product_id INT, product_name VARCHAR(20) COLLATE Latin1_General_100_BIN2_UTF8, list_price DECIMAL(5,2) ) AS rows Querying JSON Files: SELECT doc FROM OPENROWSET( BULK 'https://mydatalake.blob.core.windows.net/data/files/*.json', FORMAT = 'csv', FIELDTERMINATOR ='0x0b', FIELDQUOTE = '0x0b', ROWTERMINATOR = '0x0b' ) WITH (doc NVARCHAR(MAX)) as rows Querying Parquet Files: SELECT * FROM OPENROWSET( BULK 'https://mydatalake.blob.core.windows.net/data/orders/year=*/month=*/*.*', FORMAT = 'parquet' ) AS orders WHERE orders.filepath(1) = '2020' AND orders.filepath(2) IN ('1','2'); Create External Database Objects in a Serverless SQL Pool Creating a Database: CREATE DATABASE SalesDB COLLATE Latin1_General_100_BIN2_UTF8 Creating an External Data Source: CREATE EXTERNAL DATA SOURCE files WITH ( LOCATION = 'https://mydatalake.blob.core.windows.net/data/files/' ) Using External Data Source in Queries: SELECT * FROM OPENROWSET( BULK 'orders/*.csv', DATA_SOURCE = 'files', FORMAT = 'csv', PARSER_VERSION = '2.0' ) AS orders Creating a Database Scoped Credential: CREATE DATABASE SCOPED CREDENTIAL sqlcred WITH IDENTITY = 'SHARED ACCESS SIGNATURE', SECRET = 'sv=xxx...'; GO CREATE EXTERNAL DATA SOURCE secureFiles WITH ( LOCATION = 'https://mydatalake.blob.core.windows.net/data/secureFiles/', CREDENTIAL = sqlcred ); GO Creating an External File Format: CREATE EXTERNAL FILE FORMAT CsvFormat WITH ( FORMAT_TYPE = DELIMITEDTEXT, FORMAT_OPTIONS ( FIELD_TERMINATOR = ',', STRING_DELIMITER = '\"' ) ); GO Creating an External Table: CREATE EXTERNAL TABLE dbo.products ( product_id INT, product_name VARCHAR(20), list_price DECIMAL(5,2) ) WITH ( DATA_SOURCE = files, LOCATION = 'products/*.csv', FILE_FORMAT = CsvFormat ); GO -- Query the table SELECT * FROM dbo.products; How to transform data using a serverless SQL pool Use a CREATE EXTERNAL TABLE AS SELECT (CETAS) Statement to Transform Data Steps: 1. Create External Data Source: sql CREATE EXTERNAL DATA SOURCE files WITH ( LOCATION = 'https://mydatalake.blob.core.windows.net/data/files/', TYPE = BLOB_STORAGE, -- For serverless SQL pool CREDENTIAL = storageCred ); Create Database Scoped Credential: ```sql CREATE DATABASE SCOPED CREDENTIAL storagekeycred WITH IDENTITY='SHARED ACCESS SIGNATURE', SECRET = 'sv=xxx...'; CREATE EXTERNAL DATA SOURCE secureFiles WITH ( LOCATION = 'https://mydatalake.blob.core.windows.net/data/secureFiles/', CREDENTIAL = storagekeycred ); ``` Create External File Format: sql CREATE EXTERNAL FILE FORMAT ParquetFormat WITH ( FORMAT_TYPE = PARQUET, DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec' ); Use CETAS to Transform Data: sql CREATE EXTERNAL TABLE SpecialOrders WITH ( LOCATION = 'special_orders/', DATA_SOURCE = files, FILE_FORMAT = ParquetFormat ) AS SELECT OrderID, CustomerName, OrderTotal FROM OPENROWSET( BULK 'sales_orders/*.csv', DATA_SOURCE = 'files', FORMAT = 'CSV', PARSER_VERSION = '2.0', HEADER_ROW = TRUE ) AS source_data WHERE OrderType = 'Special Order'; Encapsulate a CETAS Statement in a Stored Procedure Benefits: - Reduces network traffic. - Provides a security boundary. - Eases maintenance. - Improves performance. Example Stored Procedure: CREATE PROCEDURE usp_special_orders_by_year @order_year INT AS BEGIN -- Drop the table if it already exists IF EXISTS ( SELECT * FROM sys.external_tables WHERE name = 'SpecialOrders' ) DROP EXTERNAL TABLE SpecialOrders -- Create external table with special orders -- from the specified year CREATE EXTERNAL TABLE SpecialOrders WITH ( LOCATION = 'special_orders/', DATA_SOURCE = files, FILE_FORMAT = ParquetFormat ) AS SELECT OrderID, CustomerName, OrderTotal FROM OPENROWSET( BULK 'sales_orders/*.csv', DATA_SOURCE = 'files', FORMAT = 'CSV', PARSER_VERSION = '2.0', HEADER_ROW = TRUE ) AS source_data WHERE OrderType = 'Special Order' AND YEAR(OrderDate) = @order_year; END Include a Data Transformation Stored Procedure in a Pipeline Pipeline Activities: 1. Delete Activity: Deletes the target folder for the transformed data in the data lake if it already exists. 2. Stored Procedure Activity: Connects to your serverless SQL pool and runs the stored procedure that encapsulates your CETAS operation. Example Pipeline Steps: - Delete Target Folder: - Use a Delete activity to remove the existing folder. - Run Stored Procedure: - Use a Stored procedure activity to execute the usp_special_orders_by_year procedure. Benefits: - Schedules operations to run at specific times or based on events (e.g., new files added to the source storage location). Create a Lake Database in Azure Synapse Analytics Understand Lake Database Concepts and Components Traditional Relational Database: - Schema composed of tables, views, and other objects. - Tables define entities with attributes as columns, enforcing data types, nullability, key uniqueness, and referential integrity. - Data is tightly coupled with table definitions, requiring all manipulations to be through the database system. Data Lake: - No fixed schema; data stored in structured, semi-structured, or unstructured files. - Analysts can work directly with files using various tools, without relational database constraints. Lake Database: - Provides a relational metadata layer over files in a data lake. - Includes definitions for tables, column names, data types, and relationships. - Tables reference files in the data lake, allowing SQL queries and relational semantics. - Data storage is decoupled from the database schema, offering more flexibility. Components: - Schema: Define tables and relationships using data modeling principles. - Storage: Data stored in Parquet or CSV files in the data lake, managed independently of database tables. - Compute: Use serverless SQL pools or Apache Spark pools to query and manipulate data. Describe Database Templates in Azure Synapse Analytics Lake Database Designer: - Start with a new lake database on the Data page. - Choose a template from the gallery or start with a blank database. - Add and customize tables using the visual database designer interface. Creating Tables: - Specify the type and location of files for storing underlying data. - Create tables from existing files in the data lake. - Store database files in a consistent format within the same root folder. Database Designer Interface: - Drag-and-drop surface for editing tables and relationships. - Specify names, storage settings, key usage, nullability, and data types for columns. - Define relationships between key columns in tables. - Publish the database when the schema is ready for use. Create a Lake Database Steps: 1. Create a Lake Database: - Use the lake database designer in Azure Synapse Studio. - Add a new lake database on the Data page. - Select a template or start with a blank database. - Add and customize tables using the visual interface. Specify Table Settings: Define the type and location of files for data storage. Create tables from existing files in the data lake. Ensure all database files are in a consistent format within the same root folder. Database Designer Features: Drag-and-drop interface to edit tables and relationships. Define schema by specifying table names, storage settings, columns, and relationships. Publish the database to start using it. Use a Lake Database Using a Serverless SQL Pool: - Query lake database tables using a serverless SQL pool in a SQL script. - Example: ```sql USE RetailDB; GO SELECT CustomerID, FirstName, LastName FROM Customer ORDER BY LastName; ``` No need for OPENROWSET function; the serverless SQL pool handles file mapping. Using an Apache Spark Pool: - Work with lake database tables using Spark SQL in an Apache Spark pool. - Example to insert a new record: sql %%sql INSERT INTO `RetailDB`.`Customer` VALUES (123, 'John', 'Yang') - Example to query the table: sql %%sql SELECT * FROM `RetailDB`.`Customer` WHERE CustomerID = 123 Transform Data with Spark in Azure Synapse Analytics Use Apache Spark to Modify and Save Dataframes Modify and Save Dataframes: - Load Data: Use spark.read to load data into a dataframe. python order_details = spark.read.csv('/orders/*.csv', header=True, inferSchema=True) display(order_details.limit(5)) Transform Data: Use dataframe methods and Spark functions for transformations. ```python from pyspark.sql.functions import split, col Create new columns and remove the original column transformed_df = order_details.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1)) transformed_df = transformed_df.drop(\"CustomerName\") display(transformed_df.limit(5)) ``` Save Data: Save the transformed dataframe to the data lake. python transformed_df.write.mode(\"overwrite\").parquet('/transformed_data/orders.parquet') print(\"Transformed data saved!\") Partition Data Files for Improved Performance and Scalability Partitioning Data: - Create Derived Field and Partition Data: ```python from pyspark.sql.functions import year, col # Load source data df = spark.read.csv('/orders/*.csv', header=True, inferSchema=True) # Add Year column dated_df = df.withColumn(\"Year\", year(col(\"OrderDate\"))) # Partition by year dated_df.write.partitionBy(\"Year\").mode(\"overwrite\").parquet(\"/data\") ``` Filtering Partitioned Data: - Read Partitioned Data: python orders_2020 = spark.read.parquet('/partitioned_data/Year=2020') display(orders_2020.limit(5)) Transform Data with SQL Define Tables and Views: - Save Dataframe as an External Table: python order_details.write.saveAsTable('sales_orders', format='parquet', mode='overwrite', path='/sales_orders_table') Query and Transform Data Using SQL: - Create Derived Columns and Save Results: ```python # Create derived columns sql_transform = spark.sql(\"SELECT *, YEAR(OrderDate) AS Year, MONTH(OrderDate) AS Month FROM sales_orders\") # Save the results sql_transform.write.partitionBy(\"Year\",\"Month\").saveAsTable('transformed_orders', format='parquet', mode='overwrite', path='/transformed_orders_table') ``` Query the Metastore: - Query the Transformed Data: ```sql %%sql SELECT * FROM transformed_orders WHERE Year = 2021 AND Month = 1 ``` Drop Tables: - Clean Up Metastore: ```sql %%sql DROP TABLE transformed_orders; DROP TABLE sales_orders; ``` Background Here we will see how to handle data usig Spark in Synapse. Let's get started First thing we would need is a Spark Pool. The serverless Spark is similar to Serverless SQL Pool. That means, only when it is working you need to pay money. No need to worry when you are not proccessing anything. You wont be charged a penny. The spark pool decides the number of nodes in the actual spark cluster in the background. If you want to work with Spark, you must create a Serverless Spark pool. In Azure Databricks and in general Spark enviorments you create a spark cluster. But, in synapse you create a Serverless Spark Pool. This pool manages the Spark cluster for you. It will ahve auto pause etc. Background Here I will show you how to run query using Serverless SQL pool. Every SA workspae comes with a built-in serverless SQL pool. Its just an engine to run your SQL queries with no own storage. Just an engine. Its built-in/serverless/online/Auto Using Serverless SQL Pool Let's get started Firs let's upload some data. We know every SA workspace is connected to a default ADLS folder. Let's upload a csv file to it. Let's run the script SELECT * FROM OPENROWSET( BULK 'abfss://contusendel@adlsusendel.dfs.core.windows.net/customers-100.csv', FORMAT = 'csv', HEADER_ROW = TRUE, PARSER_VERSION = '2.0' )AS [result] SELECT Country, Count(*) FROM OPENROWSET( BULK 'abfss://contusendel@adlsusendel.dfs.core.windows.net/customers-100.csv', FORMAT = 'csv', HEADER_ROW = TRUE, PARSER_VERSION = '2.0' )AS [result] GROUP by Country Using Dedicated SQL Pool","title":"Pools"},{"location":"Synapse-ADF/1.1_Pools/#use-azure-synapse-serverless-sql-pool-to-query-files-in-a-data-lake","text":"","title":"Use Azure Synapse Serverless SQL Pool to Query Files in a Data Lake"},{"location":"Synapse-ADF/1.1_Pools/#what-are-serverless-sql-pools-and-what-they-are-capable-of","text":"Serverless SQL pools are less expensive SQL databases. Compared to Dedicated SQL pool which is more expensive and you have to pay it if you use it or not. Serverless sql pools are good for on-demand data query.","title":"What are Serverless SQL pools and what they are capable of?"},{"location":"Synapse-ADF/1.1_Pools/#query-csv-json-and-parquet-files-using-a-serverless-sql-pool","text":"Querying CSV Files: SELECT TOP 100 * FROM OPENROWSET( BULK 'https://mydatalake.blob.core.windows.net/data/files/*.csv', FORMAT = 'csv', PARSER_VERSION = '2.0' ) WITH ( product_id INT, product_name VARCHAR(20) COLLATE Latin1_General_100_BIN2_UTF8, list_price DECIMAL(5,2) ) AS rows Querying JSON Files: SELECT doc FROM OPENROWSET( BULK 'https://mydatalake.blob.core.windows.net/data/files/*.json', FORMAT = 'csv', FIELDTERMINATOR ='0x0b', FIELDQUOTE = '0x0b', ROWTERMINATOR = '0x0b' ) WITH (doc NVARCHAR(MAX)) as rows Querying Parquet Files: SELECT * FROM OPENROWSET( BULK 'https://mydatalake.blob.core.windows.net/data/orders/year=*/month=*/*.*', FORMAT = 'parquet' ) AS orders WHERE orders.filepath(1) = '2020' AND orders.filepath(2) IN ('1','2');","title":"Query CSV, JSON, and Parquet Files Using a Serverless SQL Pool"},{"location":"Synapse-ADF/1.1_Pools/#create-external-database-objects-in-a-serverless-sql-pool","text":"Creating a Database: CREATE DATABASE SalesDB COLLATE Latin1_General_100_BIN2_UTF8 Creating an External Data Source: CREATE EXTERNAL DATA SOURCE files WITH ( LOCATION = 'https://mydatalake.blob.core.windows.net/data/files/' ) Using External Data Source in Queries: SELECT * FROM OPENROWSET( BULK 'orders/*.csv', DATA_SOURCE = 'files', FORMAT = 'csv', PARSER_VERSION = '2.0' ) AS orders Creating a Database Scoped Credential: CREATE DATABASE SCOPED CREDENTIAL sqlcred WITH IDENTITY = 'SHARED ACCESS SIGNATURE', SECRET = 'sv=xxx...'; GO CREATE EXTERNAL DATA SOURCE secureFiles WITH ( LOCATION = 'https://mydatalake.blob.core.windows.net/data/secureFiles/', CREDENTIAL = sqlcred ); GO Creating an External File Format: CREATE EXTERNAL FILE FORMAT CsvFormat WITH ( FORMAT_TYPE = DELIMITEDTEXT, FORMAT_OPTIONS ( FIELD_TERMINATOR = ',', STRING_DELIMITER = '\"' ) ); GO Creating an External Table: CREATE EXTERNAL TABLE dbo.products ( product_id INT, product_name VARCHAR(20), list_price DECIMAL(5,2) ) WITH ( DATA_SOURCE = files, LOCATION = 'products/*.csv', FILE_FORMAT = CsvFormat ); GO -- Query the table SELECT * FROM dbo.products;","title":"Create External Database Objects in a Serverless SQL Pool"},{"location":"Synapse-ADF/1.1_Pools/#how-to-transform-data-using-a-serverless-sql-pool","text":"","title":"How to transform data using a serverless SQL pool"},{"location":"Synapse-ADF/1.1_Pools/#use-a-create-external-table-as-select-cetas-statement-to-transform-data","text":"Steps: 1. Create External Data Source: sql CREATE EXTERNAL DATA SOURCE files WITH ( LOCATION = 'https://mydatalake.blob.core.windows.net/data/files/', TYPE = BLOB_STORAGE, -- For serverless SQL pool CREDENTIAL = storageCred ); Create Database Scoped Credential: ```sql CREATE DATABASE SCOPED CREDENTIAL storagekeycred WITH IDENTITY='SHARED ACCESS SIGNATURE', SECRET = 'sv=xxx...'; CREATE EXTERNAL DATA SOURCE secureFiles WITH ( LOCATION = 'https://mydatalake.blob.core.windows.net/data/secureFiles/', CREDENTIAL = storagekeycred ); ``` Create External File Format: sql CREATE EXTERNAL FILE FORMAT ParquetFormat WITH ( FORMAT_TYPE = PARQUET, DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec' ); Use CETAS to Transform Data: sql CREATE EXTERNAL TABLE SpecialOrders WITH ( LOCATION = 'special_orders/', DATA_SOURCE = files, FILE_FORMAT = ParquetFormat ) AS SELECT OrderID, CustomerName, OrderTotal FROM OPENROWSET( BULK 'sales_orders/*.csv', DATA_SOURCE = 'files', FORMAT = 'CSV', PARSER_VERSION = '2.0', HEADER_ROW = TRUE ) AS source_data WHERE OrderType = 'Special Order';","title":"Use a CREATE EXTERNAL TABLE AS SELECT (CETAS) Statement to Transform Data"},{"location":"Synapse-ADF/1.1_Pools/#encapsulate-a-cetas-statement-in-a-stored-procedure","text":"Benefits: - Reduces network traffic. - Provides a security boundary. - Eases maintenance. - Improves performance. Example Stored Procedure: CREATE PROCEDURE usp_special_orders_by_year @order_year INT AS BEGIN -- Drop the table if it already exists IF EXISTS ( SELECT * FROM sys.external_tables WHERE name = 'SpecialOrders' ) DROP EXTERNAL TABLE SpecialOrders -- Create external table with special orders -- from the specified year CREATE EXTERNAL TABLE SpecialOrders WITH ( LOCATION = 'special_orders/', DATA_SOURCE = files, FILE_FORMAT = ParquetFormat ) AS SELECT OrderID, CustomerName, OrderTotal FROM OPENROWSET( BULK 'sales_orders/*.csv', DATA_SOURCE = 'files', FORMAT = 'CSV', PARSER_VERSION = '2.0', HEADER_ROW = TRUE ) AS source_data WHERE OrderType = 'Special Order' AND YEAR(OrderDate) = @order_year; END","title":"Encapsulate a CETAS Statement in a Stored Procedure"},{"location":"Synapse-ADF/1.1_Pools/#include-a-data-transformation-stored-procedure-in-a-pipeline","text":"Pipeline Activities: 1. Delete Activity: Deletes the target folder for the transformed data in the data lake if it already exists. 2. Stored Procedure Activity: Connects to your serverless SQL pool and runs the stored procedure that encapsulates your CETAS operation. Example Pipeline Steps: - Delete Target Folder: - Use a Delete activity to remove the existing folder. - Run Stored Procedure: - Use a Stored procedure activity to execute the usp_special_orders_by_year procedure. Benefits: - Schedules operations to run at specific times or based on events (e.g., new files added to the source storage location).","title":"Include a Data Transformation Stored Procedure in a Pipeline"},{"location":"Synapse-ADF/1.1_Pools/#create-a-lake-database-in-azure-synapse-analytics","text":"","title":"Create a Lake Database in Azure Synapse Analytics"},{"location":"Synapse-ADF/1.1_Pools/#understand-lake-database-concepts-and-components","text":"Traditional Relational Database: - Schema composed of tables, views, and other objects. - Tables define entities with attributes as columns, enforcing data types, nullability, key uniqueness, and referential integrity. - Data is tightly coupled with table definitions, requiring all manipulations to be through the database system. Data Lake: - No fixed schema; data stored in structured, semi-structured, or unstructured files. - Analysts can work directly with files using various tools, without relational database constraints. Lake Database: - Provides a relational metadata layer over files in a data lake. - Includes definitions for tables, column names, data types, and relationships. - Tables reference files in the data lake, allowing SQL queries and relational semantics. - Data storage is decoupled from the database schema, offering more flexibility. Components: - Schema: Define tables and relationships using data modeling principles. - Storage: Data stored in Parquet or CSV files in the data lake, managed independently of database tables. - Compute: Use serverless SQL pools or Apache Spark pools to query and manipulate data.","title":"Understand Lake Database Concepts and Components"},{"location":"Synapse-ADF/1.1_Pools/#describe-database-templates-in-azure-synapse-analytics","text":"Lake Database Designer: - Start with a new lake database on the Data page. - Choose a template from the gallery or start with a blank database. - Add and customize tables using the visual database designer interface. Creating Tables: - Specify the type and location of files for storing underlying data. - Create tables from existing files in the data lake. - Store database files in a consistent format within the same root folder. Database Designer Interface: - Drag-and-drop surface for editing tables and relationships. - Specify names, storage settings, key usage, nullability, and data types for columns. - Define relationships between key columns in tables. - Publish the database when the schema is ready for use.","title":"Describe Database Templates in Azure Synapse Analytics"},{"location":"Synapse-ADF/1.1_Pools/#create-a-lake-database","text":"Steps: 1. Create a Lake Database: - Use the lake database designer in Azure Synapse Studio. - Add a new lake database on the Data page. - Select a template or start with a blank database. - Add and customize tables using the visual interface. Specify Table Settings: Define the type and location of files for data storage. Create tables from existing files in the data lake. Ensure all database files are in a consistent format within the same root folder. Database Designer Features: Drag-and-drop interface to edit tables and relationships. Define schema by specifying table names, storage settings, columns, and relationships. Publish the database to start using it.","title":"Create a Lake Database"},{"location":"Synapse-ADF/1.1_Pools/#use-a-lake-database","text":"Using a Serverless SQL Pool: - Query lake database tables using a serverless SQL pool in a SQL script. - Example: ```sql USE RetailDB; GO SELECT CustomerID, FirstName, LastName FROM Customer ORDER BY LastName; ``` No need for OPENROWSET function; the serverless SQL pool handles file mapping. Using an Apache Spark Pool: - Work with lake database tables using Spark SQL in an Apache Spark pool. - Example to insert a new record: sql %%sql INSERT INTO `RetailDB`.`Customer` VALUES (123, 'John', 'Yang') - Example to query the table: sql %%sql SELECT * FROM `RetailDB`.`Customer` WHERE CustomerID = 123","title":"Use a Lake Database"},{"location":"Synapse-ADF/1.1_Pools/#transform-data-with-spark-in-azure-synapse-analytics","text":"","title":"Transform Data with Spark in Azure Synapse Analytics"},{"location":"Synapse-ADF/1.1_Pools/#use-apache-spark-to-modify-and-save-dataframes","text":"Modify and Save Dataframes: - Load Data: Use spark.read to load data into a dataframe. python order_details = spark.read.csv('/orders/*.csv', header=True, inferSchema=True) display(order_details.limit(5)) Transform Data: Use dataframe methods and Spark functions for transformations. ```python from pyspark.sql.functions import split, col","title":"Use Apache Spark to Modify and Save Dataframes"},{"location":"Synapse-ADF/1.1_Pools/#create-new-columns-and-remove-the-original-column","text":"transformed_df = order_details.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1)) transformed_df = transformed_df.drop(\"CustomerName\") display(transformed_df.limit(5)) ``` Save Data: Save the transformed dataframe to the data lake. python transformed_df.write.mode(\"overwrite\").parquet('/transformed_data/orders.parquet') print(\"Transformed data saved!\")","title":"Create new columns and remove the original column"},{"location":"Synapse-ADF/1.1_Pools/#partition-data-files-for-improved-performance-and-scalability","text":"Partitioning Data: - Create Derived Field and Partition Data: ```python from pyspark.sql.functions import year, col # Load source data df = spark.read.csv('/orders/*.csv', header=True, inferSchema=True) # Add Year column dated_df = df.withColumn(\"Year\", year(col(\"OrderDate\"))) # Partition by year dated_df.write.partitionBy(\"Year\").mode(\"overwrite\").parquet(\"/data\") ``` Filtering Partitioned Data: - Read Partitioned Data: python orders_2020 = spark.read.parquet('/partitioned_data/Year=2020') display(orders_2020.limit(5))","title":"Partition Data Files for Improved Performance and Scalability"},{"location":"Synapse-ADF/1.1_Pools/#transform-data-with-sql","text":"Define Tables and Views: - Save Dataframe as an External Table: python order_details.write.saveAsTable('sales_orders', format='parquet', mode='overwrite', path='/sales_orders_table') Query and Transform Data Using SQL: - Create Derived Columns and Save Results: ```python # Create derived columns sql_transform = spark.sql(\"SELECT *, YEAR(OrderDate) AS Year, MONTH(OrderDate) AS Month FROM sales_orders\") # Save the results sql_transform.write.partitionBy(\"Year\",\"Month\").saveAsTable('transformed_orders', format='parquet', mode='overwrite', path='/transformed_orders_table') ``` Query the Metastore: - Query the Transformed Data: ```sql %%sql SELECT * FROM transformed_orders WHERE Year = 2021 AND Month = 1 ``` Drop Tables: - Clean Up Metastore: ```sql %%sql DROP TABLE transformed_orders; DROP TABLE sales_orders; ```","title":"Transform Data with SQL"},{"location":"Synapse-ADF/1.1_Pools/#background","text":"Here we will see how to handle data usig Spark in Synapse.","title":"Background"},{"location":"Synapse-ADF/1.1_Pools/#lets-get-started","text":"First thing we would need is a Spark Pool. The serverless Spark is similar to Serverless SQL Pool. That means, only when it is working you need to pay money. No need to worry when you are not proccessing anything. You wont be charged a penny. The spark pool decides the number of nodes in the actual spark cluster in the background. If you want to work with Spark, you must create a Serverless Spark pool. In Azure Databricks and in general Spark enviorments you create a spark cluster. But, in synapse you create a Serverless Spark Pool. This pool manages the Spark cluster for you. It will ahve auto pause etc.","title":"Let's get started"},{"location":"Synapse-ADF/1.1_Pools/#background_1","text":"Here I will show you how to run query using Serverless SQL pool. Every SA workspae comes with a built-in serverless SQL pool. Its just an engine to run your SQL queries with no own storage. Just an engine. Its built-in/serverless/online/Auto","title":"Background"},{"location":"Synapse-ADF/1.1_Pools/#using-serverless-sql-pool","text":"","title":"Using Serverless SQL Pool"},{"location":"Synapse-ADF/1.1_Pools/#lets-get-started_1","text":"Firs let's upload some data. We know every SA workspace is connected to a default ADLS folder. Let's upload a csv file to it.","title":"Let's get started"},{"location":"Synapse-ADF/1.1_Pools/#lets-run-the-script","text":"SELECT * FROM OPENROWSET( BULK 'abfss://contusendel@adlsusendel.dfs.core.windows.net/customers-100.csv', FORMAT = 'csv', HEADER_ROW = TRUE, PARSER_VERSION = '2.0' )AS [result] SELECT Country, Count(*) FROM OPENROWSET( BULK 'abfss://contusendel@adlsusendel.dfs.core.windows.net/customers-100.csv', FORMAT = 'csv', HEADER_ROW = TRUE, PARSER_VERSION = '2.0' )AS [result] GROUP by Country","title":"Let's run the script"},{"location":"Synapse-ADF/1.1_Pools/#using-dedicated-sql-pool","text":"","title":"Using Dedicated SQL Pool"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Pipelines Pipeline = Workflow = Collection of Activities Activities Activities = Steps/Tasks in a Pipeline Copy data Activity Copy data step/task copies data from here to there in the cloud. It's a data import tool . This is one of the most important activities in pipelines. Some projects have only this step doing all the work. Copy data tool The Copy data tool is a GUI to make the Copy data activity easier. Using this GUI you can create a pipeline using just Next, Next. It has two type of tasks. Built-in copy task This method is enough for almost all cases. It's the original method. Just click 'next' using the tool, and ADF will take care of everything else. But, this is suitable for situations where we have fixed sources and fixed destinations. Example: Copying Data between two Azure SQL Databases. Azure Blob storage to Azure SQL Databases etc. Metadata-driven copy task This method is for very large and complex situations. For example, copying hundreds of tables from an on-prem SQL Server to Azure Data Lake Storage (ADLS), with the list of tables and their destinations managed in a control table. Example: Imagine you have an on-premises SQL Server with hundreds of tables. You want to copy these tables to ADLS. Instead of creating individual copy tasks for each table, you create a control table that lists all the source tables and their corresponding destinations. ADF then uses this control table to dynamically copy each table to the specified destination. Data flow Activity This is also the most important activity in Pipelines. Using this you create the transformation work. There are two types of Data flow activities: Mapping Data Flows: This is the common data flow activity which we usually use. Here we use a GUI to create data transformation steps. Azure runs these on a behind-the-scenes Spark cluster managed by Azure. Learn more . Wrangling Data Flows: Use Excel-like Power Query for data preparation, integrating with Power Query Online and using Spark for execution. Now, if you want to bypass ADF and do your own coding, manage your own env etc you you can use these activities. HDInsight Activities: Use Hive, Pig, MapReduce, or Spark on your HDInsight cluster in Azure. Databricks Activities: Notebook Activity Jar Activity Python Activity Custom Activity: Execute custom code or scripts. Note: Data Flow activities run on Apache Spark clusters. Microsoft makes things easy by adding a GUI, but Dataflows are essentially Spark activities. Other Activities Azure ML Studio (Classic) Activity: Execute machine learning pipelines. Stored Procedure Activity: Execute a stored procedure in a database. Custom Activity: Execute custom code or scripts. Control Flow Activities These activities control the execution flow of a pipeline. If Condition Activity: Execute different paths based on conditions. For Each Activity: Iterate over a collection of items. Until Activity: Repeat an activity until a condition is met. Wait Activity: Pause the pipeline execution for a specified duration. Additional Activities Web Activity: Make HTTP requests. Azure Function Activity: Invoke an Azure Function. Execute Pipeline Activity: Call another pipeline. Integration Runtime It's the infrastructure part. It provides the hardware and running environment, and it connects to on-premises systems or local laptops. Linked Service These are connection strings or configurations that define how to connect to data sources or compute environments. Datasets These represent the data that gets processed in the pipeline.","title":"ETL Pipelines"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines/#pipelines","text":"Pipeline = Workflow = Collection of Activities","title":"Pipelines"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines/#activities","text":"Activities = Steps/Tasks in a Pipeline","title":"Activities"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines/#copy-data-activity","text":"Copy data step/task copies data from here to there in the cloud. It's a data import tool . This is one of the most important activities in pipelines. Some projects have only this step doing all the work.","title":"Copy data Activity"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines/#copy-data-tool","text":"The Copy data tool is a GUI to make the Copy data activity easier. Using this GUI you can create a pipeline using just Next, Next. It has two type of tasks.","title":"Copy data tool"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines/#built-in-copy-task","text":"This method is enough for almost all cases. It's the original method. Just click 'next' using the tool, and ADF will take care of everything else. But, this is suitable for situations where we have fixed sources and fixed destinations. Example: Copying Data between two Azure SQL Databases. Azure Blob storage to Azure SQL Databases etc.","title":"Built-in copy task"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines/#metadata-driven-copy-task","text":"This method is for very large and complex situations. For example, copying hundreds of tables from an on-prem SQL Server to Azure Data Lake Storage (ADLS), with the list of tables and their destinations managed in a control table. Example: Imagine you have an on-premises SQL Server with hundreds of tables. You want to copy these tables to ADLS. Instead of creating individual copy tasks for each table, you create a control table that lists all the source tables and their corresponding destinations. ADF then uses this control table to dynamically copy each table to the specified destination.","title":"Metadata-driven copy task"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines/#data-flow-activity","text":"This is also the most important activity in Pipelines. Using this you create the transformation work. There are two types of Data flow activities: Mapping Data Flows: This is the common data flow activity which we usually use. Here we use a GUI to create data transformation steps. Azure runs these on a behind-the-scenes Spark cluster managed by Azure. Learn more . Wrangling Data Flows: Use Excel-like Power Query for data preparation, integrating with Power Query Online and using Spark for execution. Now, if you want to bypass ADF and do your own coding, manage your own env etc you you can use these activities. HDInsight Activities: Use Hive, Pig, MapReduce, or Spark on your HDInsight cluster in Azure. Databricks Activities: Notebook Activity Jar Activity Python Activity Custom Activity: Execute custom code or scripts. Note: Data Flow activities run on Apache Spark clusters. Microsoft makes things easy by adding a GUI, but Dataflows are essentially Spark activities.","title":"Data flow Activity"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines/#other-activities","text":"Azure ML Studio (Classic) Activity: Execute machine learning pipelines. Stored Procedure Activity: Execute a stored procedure in a database. Custom Activity: Execute custom code or scripts.","title":"Other Activities"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines/#control-flow-activities","text":"These activities control the execution flow of a pipeline. If Condition Activity: Execute different paths based on conditions. For Each Activity: Iterate over a collection of items. Until Activity: Repeat an activity until a condition is met. Wait Activity: Pause the pipeline execution for a specified duration.","title":"Control Flow Activities"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines/#additional-activities","text":"Web Activity: Make HTTP requests. Azure Function Activity: Invoke an Azure Function. Execute Pipeline Activity: Call another pipeline.","title":"Additional Activities"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines/#integration-runtime","text":"It's the infrastructure part. It provides the hardware and running environment, and it connects to on-premises systems or local laptops.","title":"Integration Runtime"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines/#linked-service","text":"These are connection strings or configurations that define how to connect to data sources or compute environments.","title":"Linked Service"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines/#datasets","text":"These represent the data that gets processed in the pipeline.","title":"Datasets"},{"location":"Synapse-ADF/1.4_Copy-data-tool/","text":"Table of contents {: .text-delta } 1. TOC {:toc} ADF Built-in copy task scenarios The ADF has a buil-in Copy task. This is a very powerful tool. There are some projects where this tool alone did the major task. In industry, here are some common scenarios where the Copy data tool is the best choice. 1. Copying Data Between Azure SQL Databases You have two Azure SQL Databases, and you need to copy data from a table in the first database to a table in the second database. The schema is the same, and you want to transfer data regularly, such as every day. Example Use Case: - Source: Azure SQL Database (Table: SalesData) - Destination: Azure SQL Database (Table: SalesDataBackup) - Steps: Use the built-in copy task to set up a daily data copy job. 2. Copying Data from Azure Blob Storage to Azure SQL Database You have CSV files stored in Azure Blob Storage and you need to load this data into a table in an Azure SQL Database. Example Use Case: - Source: Azure Blob Storage (CSV file: customer_data.csv) - Destination: Azure SQL Database (Table: Customers) - Steps: Use the built-in copy task to map the CSV columns to the SQL table columns and set up a regular copy job. 3. Copying Data from On-Premises SQL Server to Azure Data Lake Storage (ADLS) You have an on-premises SQL Server database and you need to copy data to Azure Data Lake Storage for further analysis. Example Use Case: - Source: On-Premises SQL Server (Table: EmployeeRecords) - Destination: Azure Data Lake Storage (Folder: EmployeeData) - Steps: Set up a self-hosted integration runtime, then use the built-in copy task to move data from the on-premises server to ADLS. 4. Copying Data from One Azure Blob Storage Container to Another You want to copy files from one container in Azure Blob Storage to another container, perhaps for archival purposes. Example Use Case: - Source: Azure Blob Storage (Container: raw-data) - Destination: Azure Blob Storage (Container: archived-data) - Steps: Use the built-in copy task to set up the copy job and configure it to run on a schedule. 5. Copying Data from Azure Table Storage to Azure SQL Database You have data in Azure Table Storage and you want to migrate this data to an Azure SQL Database. Example Use Case: - Source: Azure Table Storage (Table: Orders) - Destination: Azure SQL Database (Table: Orders) - Steps: Use the built-in copy task to configure the source and destination, mapping the columns appropriately. 6. Copying Data from REST API to Azure SQL Database You need to fetch data from a REST API and load it into an Azure SQL Database table. Example Use Case: - Source: REST API (Endpoint: https://api.example.com/data) - Destination: Azure SQL Database (Table: ApiData) - Steps: Use the built-in copy task to connect to the REST API, transform the JSON data as needed, and load it into the SQL table. 7. Copying Data from Azure Cosmos DB to Azure SQL Database You have data in Azure Cosmos DB and need to transfer it to an Azure SQL Database for reporting purposes. Example Use Case: - Source: Azure Cosmos DB (Collection: Users) - Destination: Azure SQL Database (Table: Users) - Steps: Use the built-in copy task to map the Cosmos DB documents to the SQL table columns.","title":"ADF Copy task - When to use"},{"location":"Synapse-ADF/1.4_Copy-data-tool/#adf-built-in-copy-task-scenarios","text":"The ADF has a buil-in Copy task. This is a very powerful tool. There are some projects where this tool alone did the major task. In industry, here are some common scenarios where the Copy data tool is the best choice.","title":"ADF Built-in copy task scenarios"},{"location":"Synapse-ADF/1.4_Copy-data-tool/#1-copying-data-between-azure-sql-databases","text":"You have two Azure SQL Databases, and you need to copy data from a table in the first database to a table in the second database. The schema is the same, and you want to transfer data regularly, such as every day. Example Use Case: - Source: Azure SQL Database (Table: SalesData) - Destination: Azure SQL Database (Table: SalesDataBackup) - Steps: Use the built-in copy task to set up a daily data copy job.","title":"1. Copying Data Between Azure SQL Databases"},{"location":"Synapse-ADF/1.4_Copy-data-tool/#2-copying-data-from-azure-blob-storage-to-azure-sql-database","text":"You have CSV files stored in Azure Blob Storage and you need to load this data into a table in an Azure SQL Database. Example Use Case: - Source: Azure Blob Storage (CSV file: customer_data.csv) - Destination: Azure SQL Database (Table: Customers) - Steps: Use the built-in copy task to map the CSV columns to the SQL table columns and set up a regular copy job.","title":"2. Copying Data from Azure Blob Storage to Azure SQL Database"},{"location":"Synapse-ADF/1.4_Copy-data-tool/#3-copying-data-from-on-premises-sql-server-to-azure-data-lake-storage-adls","text":"You have an on-premises SQL Server database and you need to copy data to Azure Data Lake Storage for further analysis. Example Use Case: - Source: On-Premises SQL Server (Table: EmployeeRecords) - Destination: Azure Data Lake Storage (Folder: EmployeeData) - Steps: Set up a self-hosted integration runtime, then use the built-in copy task to move data from the on-premises server to ADLS.","title":"3. Copying Data from On-Premises SQL Server to Azure Data Lake Storage (ADLS)"},{"location":"Synapse-ADF/1.4_Copy-data-tool/#4-copying-data-from-one-azure-blob-storage-container-to-another","text":"You want to copy files from one container in Azure Blob Storage to another container, perhaps for archival purposes. Example Use Case: - Source: Azure Blob Storage (Container: raw-data) - Destination: Azure Blob Storage (Container: archived-data) - Steps: Use the built-in copy task to set up the copy job and configure it to run on a schedule.","title":"4. Copying Data from One Azure Blob Storage Container to Another"},{"location":"Synapse-ADF/1.4_Copy-data-tool/#5-copying-data-from-azure-table-storage-to-azure-sql-database","text":"You have data in Azure Table Storage and you want to migrate this data to an Azure SQL Database. Example Use Case: - Source: Azure Table Storage (Table: Orders) - Destination: Azure SQL Database (Table: Orders) - Steps: Use the built-in copy task to configure the source and destination, mapping the columns appropriately.","title":"5. Copying Data from Azure Table Storage to Azure SQL Database"},{"location":"Synapse-ADF/1.4_Copy-data-tool/#6-copying-data-from-rest-api-to-azure-sql-database","text":"You need to fetch data from a REST API and load it into an Azure SQL Database table. Example Use Case: - Source: REST API (Endpoint: https://api.example.com/data) - Destination: Azure SQL Database (Table: ApiData) - Steps: Use the built-in copy task to connect to the REST API, transform the JSON data as needed, and load it into the SQL table.","title":"6. Copying Data from REST API to Azure SQL Database"},{"location":"Synapse-ADF/1.4_Copy-data-tool/#7-copying-data-from-azure-cosmos-db-to-azure-sql-database","text":"You have data in Azure Cosmos DB and need to transfer it to an Azure SQL Database for reporting purposes. Example Use Case: - Source: Azure Cosmos DB (Collection: Users) - Destination: Azure SQL Database (Table: Users) - Steps: Use the built-in copy task to map the Cosmos DB documents to the SQL table columns.","title":"7. Copying Data from Azure Cosmos DB to Azure SQL Database"},{"location":"Synapse-ADF/1.5_IntegrationRuntime/","text":"Integration Runtime in Azure Synapse Analytics and Azure Data Factory What is Integration Runtime? Types of Integration Runtime 1. Azure Integration Runtime: 2. Self-hosted Integration Runtime: 3. Azure-SSIS Integration Runtime: All connectivity tools in Azure ecosystem Self-hosted IR vs On-premises Data Gateway Summary Integration Runtime in Azure Synapse Analytics and Azure Data Factory What is Integration Runtime? Copying data is one of the main activity in Synapse and ADF. The main tool to do this Integration Runtime is used. It's like a bridge that connects Synapse and ADF with data from on-premises and external sources. However, it's more than just a bridge. It provides the CPU and memory needed for copying and transforming data. It also offers an environment to run your SSIS packages. It can scale up or down if more power is needed. Plus, it ensures that your data transfer is secure. Where is IR used? It is used in ADF/Synapse pipelines. Types of Integration Runtime There are three types of Integration runtime. 1. Azure Integration Runtime: A cloud-based compute resource for running data integration and transformation tasks in Azure Data Factory and Azure Synapse Analytics. Managed Service : It is fully managed by Microsoft, running in the cloud. Data Movement : Primarily handles data movement within the cloud or between cloud services. Activity Dispatch : Manages and executes data transformation activities in the cloud. Network Access : Has limitations accessing on-premises resources directly due to network boundaries and security considerations. How to create? Navigate to Azure Synapse Analytics or ADF. Select the Manage tab. Under Integration Runtimes, click on + New. Choose Azure, and follow the setup wizard. 2. Self-hosted Integration Runtime: A tool for connecting and moving data between on-premises sources and Azure cloud services. Installed On-Premises : Deployed within your on-premises network or in a virtual machine that has network access to your on-premises data sources. Bridge for Connectivity : Acts as a secure bridge between on-premises data sources and the cloud. Data Movement : Facilitates secure and efficient data transfer from on-premises sources to the cloud and vice versa. Security : Ensures secure data transfer using encryption and secure authentication methods. How to create? Download the Self-hosted IR installer from the Azure portal. Install the runtime on a machine within your network. Register the self-hosted IR with the Azure Data Factory or Synapse workspace. Configure network settings to allow secure data movement. Set up high availability by adding multiple nodes. 3. Azure-SSIS Integration Runtime: A service to to run SQL Server Integration Services (SSIS) packages in the Azure cloud. Allows you to lift and shift SSIS packages to the cloud. Provides a fully managed environment for running SSIS packages in Azure. How to create? Navigate to the ADF or Synapse workspace. Go to the Manage tab, and click on Integration Runtimes. Choose Azure-SSIS and follow the creation wizard. Select the pricing tier and node size. Configure the custom setup by installing necessary components. Connect to the SSISDB or create a new one in Azure SQL Database. All connectivity tools in Azure ecosystem Product Purpose Usage From To On-premises Data Gateway Connects cloud services with on-premises data sources. Power BI, Power Apps, Power Automate, Azure Analysis Services, Logic Apps On-premises data sources Cloud services Self-hosted IR For data movement and transformation between on-premises and cloud services. Azure Data Factory (ADF) and Azure Synapse Analytics On-premises data sources ADF and Synapse Azure IR Executes data flows, data movement, and transformation activities within Azure cloud. Data integration and ETL tasks within Azure cloud Cloud data sources Synapse and other cloud services Azure Synapse Link Provides live data copying from operational stores to Synapse for real-time analytics. Near real-time data replication and analytics Operational data stores (e.g., Azure Cosmos DB) Synapse PolyBase Allows querying of external data as if it were in Synapse. Data virtualization and querying External data sources (e.g., Azure Blob Storage, ADLS, SQL Server, Oracle, Hadoop) Synapse Linked Services It is like a connection string. Managing connections to storage accounts, databases, and other services External resources Synapse and other Azure services Self-hosted IR vs On-premises Data Gateway The on-premise gateway is quite similar to the self-hosted IR. Are they the same? Can they be used interchangeably? The table below provides the answers: Feature Self-hosted Integration Runtime On-premises Data Gateway Purpose Facilitates data integration for Azure Data Factory and Synapse Pipelines. Connects on-premises data sources to Power BI, Power Apps, Power Automate, and Logic Apps. Supported Services Azure Data Factory, Azure Synapse Analytics. Power BI, Power Apps, Power Automate, Logic Apps. Data Transfer Capabilities Handles ETL processes, data movement, and SSIS execution. Enables real-time connectivity for reporting and app development. Installation Download and install the .msi file on your machine Same, the .exe needs to be downloaded to your local computer and installed. Just like good-old .exe installation. Security Secure data transfer with encrypted communication. Secure data transfer with encryption and local network connectivity. Summary Azure Integration Runtime (IR) is a key tool in ADF and Azure Synapse Analytics. It helps move and transform data between different places, like from your on-premises servers to the cloud or within the cloud itself. Azure IR provides the computing power needed to handle these tasks efficiently. Without it, moving and preparing data for analysis would be difficult, slow, and require a lot more manual work.","title":"Integration Runtime"},{"location":"Synapse-ADF/1.5_IntegrationRuntime/#integration-runtime-in-azure-synapse-analytics-and-azure-data-factory","text":"","title":"Integration Runtime in Azure Synapse Analytics and Azure Data Factory"},{"location":"Synapse-ADF/1.5_IntegrationRuntime/#what-is-integration-runtime","text":"Copying data is one of the main activity in Synapse and ADF. The main tool to do this Integration Runtime is used. It's like a bridge that connects Synapse and ADF with data from on-premises and external sources. However, it's more than just a bridge. It provides the CPU and memory needed for copying and transforming data. It also offers an environment to run your SSIS packages. It can scale up or down if more power is needed. Plus, it ensures that your data transfer is secure. Where is IR used? It is used in ADF/Synapse pipelines.","title":"What is Integration Runtime?"},{"location":"Synapse-ADF/1.5_IntegrationRuntime/#types-of-integration-runtime","text":"There are three types of Integration runtime.","title":"Types of Integration Runtime"},{"location":"Synapse-ADF/1.5_IntegrationRuntime/#1-azure-integration-runtime","text":"A cloud-based compute resource for running data integration and transformation tasks in Azure Data Factory and Azure Synapse Analytics. Managed Service : It is fully managed by Microsoft, running in the cloud. Data Movement : Primarily handles data movement within the cloud or between cloud services. Activity Dispatch : Manages and executes data transformation activities in the cloud. Network Access : Has limitations accessing on-premises resources directly due to network boundaries and security considerations. How to create? Navigate to Azure Synapse Analytics or ADF. Select the Manage tab. Under Integration Runtimes, click on + New. Choose Azure, and follow the setup wizard.","title":"1. Azure Integration Runtime:"},{"location":"Synapse-ADF/1.5_IntegrationRuntime/#2-self-hosted-integration-runtime","text":"A tool for connecting and moving data between on-premises sources and Azure cloud services. Installed On-Premises : Deployed within your on-premises network or in a virtual machine that has network access to your on-premises data sources. Bridge for Connectivity : Acts as a secure bridge between on-premises data sources and the cloud. Data Movement : Facilitates secure and efficient data transfer from on-premises sources to the cloud and vice versa. Security : Ensures secure data transfer using encryption and secure authentication methods. How to create? Download the Self-hosted IR installer from the Azure portal. Install the runtime on a machine within your network. Register the self-hosted IR with the Azure Data Factory or Synapse workspace. Configure network settings to allow secure data movement. Set up high availability by adding multiple nodes.","title":"2. Self-hosted Integration Runtime:"},{"location":"Synapse-ADF/1.5_IntegrationRuntime/#3-azure-ssis-integration-runtime","text":"A service to to run SQL Server Integration Services (SSIS) packages in the Azure cloud. Allows you to lift and shift SSIS packages to the cloud. Provides a fully managed environment for running SSIS packages in Azure. How to create? Navigate to the ADF or Synapse workspace. Go to the Manage tab, and click on Integration Runtimes. Choose Azure-SSIS and follow the creation wizard. Select the pricing tier and node size. Configure the custom setup by installing necessary components. Connect to the SSISDB or create a new one in Azure SQL Database.","title":"3. Azure-SSIS Integration Runtime:"},{"location":"Synapse-ADF/1.5_IntegrationRuntime/#all-connectivity-tools-in-azure-ecosystem","text":"Product Purpose Usage From To On-premises Data Gateway Connects cloud services with on-premises data sources. Power BI, Power Apps, Power Automate, Azure Analysis Services, Logic Apps On-premises data sources Cloud services Self-hosted IR For data movement and transformation between on-premises and cloud services. Azure Data Factory (ADF) and Azure Synapse Analytics On-premises data sources ADF and Synapse Azure IR Executes data flows, data movement, and transformation activities within Azure cloud. Data integration and ETL tasks within Azure cloud Cloud data sources Synapse and other cloud services Azure Synapse Link Provides live data copying from operational stores to Synapse for real-time analytics. Near real-time data replication and analytics Operational data stores (e.g., Azure Cosmos DB) Synapse PolyBase Allows querying of external data as if it were in Synapse. Data virtualization and querying External data sources (e.g., Azure Blob Storage, ADLS, SQL Server, Oracle, Hadoop) Synapse Linked Services It is like a connection string. Managing connections to storage accounts, databases, and other services External resources Synapse and other Azure services","title":"All connectivity tools in Azure ecosystem"},{"location":"Synapse-ADF/1.5_IntegrationRuntime/#self-hosted-ir-vs-on-premises-data-gateway","text":"The on-premise gateway is quite similar to the self-hosted IR. Are they the same? Can they be used interchangeably? The table below provides the answers: Feature Self-hosted Integration Runtime On-premises Data Gateway Purpose Facilitates data integration for Azure Data Factory and Synapse Pipelines. Connects on-premises data sources to Power BI, Power Apps, Power Automate, and Logic Apps. Supported Services Azure Data Factory, Azure Synapse Analytics. Power BI, Power Apps, Power Automate, Logic Apps. Data Transfer Capabilities Handles ETL processes, data movement, and SSIS execution. Enables real-time connectivity for reporting and app development. Installation Download and install the .msi file on your machine Same, the .exe needs to be downloaded to your local computer and installed. Just like good-old .exe installation. Security Secure data transfer with encrypted communication. Secure data transfer with encryption and local network connectivity.","title":"Self-hosted IR vs On-premises Data Gateway"},{"location":"Synapse-ADF/1.5_IntegrationRuntime/#summary","text":"Azure Integration Runtime (IR) is a key tool in ADF and Azure Synapse Analytics. It helps move and transform data between different places, like from your on-premises servers to the cloud or within the cloud itself. Azure IR provides the computing power needed to handle these tasks efficiently. Without it, moving and preparing data for analysis would be difficult, slow, and require a lot more manual work.","title":"Summary"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse/","text":"Table of contents {: .text-delta } 1. TOC {:toc} The Databases Types in Synapse Before we dive into this, let's ask ourselves: How many types of pools are there in Synapse?. There are three types : Serverless SQL, Dedicated SQL, and Serverless Spark pool. So, it's simple\u2014there are three types of databases, one for each: Serverless SQL Database : Created using the Serverless SQL pool. Dedicated SQL Database : Created using the Dedicated SQL pool. Note: This is essentially a data warehouse. Spark Database[Lake Database] : Spark DB v1 : Created using a serverless Spark pool and Spark notebook, called a Spark [Lake] database. Spark DB v2(Lakehouse, Delta) : Similar to v1, but the format of the .parquet files is Delta. So, does Apache Spark have database connections? Yes, of course. How else would it run SparkSQL and what about that magic command %%sql? It has robust database capabilities. Spark Databases Spark DB v1 - Lake Database A Lake database is a type of DB created by a Spark Pool using a Spark notebook. It\u2019s all virtual; it's not a good-old SQL database but an illusion. The actual data is stored in the ADLS container as folders\u2014one database, one folder. Let's see this in action. First, the show always starts with a Spark pool. The Serverless Spark pool doesn't come pre-created like the built-in SQL pool. So, let's create one: Next, open a PySpark notebook and run this command: %%sql CREATE DATABASE BhutuSpark -- Or, spark.sql(\"CREATE DATABASE BhutuSpark\") What Happens after the CREATE DB Command? Initially, in a fresh workspace, the Workspace in the Data Tab is completely empty, even if you have an inbuilt SQL pool. A few minutes after the Spark SQL command to create the database completes, you will see a Lake database structure appearing under the Data workspace. Initially, this will be completely empty. Also, you will notice two databases: the one you created and a default database. Synapse creates a folder in the connected ADLS container. In the UI, you will see a database structure (tables, views, users, etc.), but in the background, every Spark database is actually a folder in the ADLS container. When you create tables inside the database, there will again be subfolders inside the parent folder. Now, let's run a command to create an empty table for simplicity: %%sql CREATE TABLE bhutuspark.BhutuSparkTable ( id INT, name STRING ) USING parquet -- For Pyspark, put inside spark.sql(\"the command above\") This is what you will see in the Data Workspace. Notice it created a table. What happens after the CREATE TABLE command? Serverless SQL and Serverless Spark Pools create their own virtual databases. All data is virtual. What does this mean? The data is not stored inside them like old-school databases; the data is actually stored in your ADLS container. Let's see this: It creates a folder structure like this: ContainerName / synapse / workspaces / WorkSpaceName / warehouse / DBName.db / TableName What happens after the INSERT data command? Now, let's put some data inside the table: INSERT INTO bhutuspark.BhutuSparkTable VALUES (1, 'Alice'), (2, 'Bob') -- You can use pyspark also, spark.sql(\"the above command\") What will you see? You will see .parquet files inside the table folder. Also, note there are two parquet files. We will just note this for now. Summary A Lake database, or Spark database, is an illusion-DB created using a Spark pool and a notebook. The actual data is stored in Azure Data Lake Storage (ADLS) as folders. Each Spark database is one folder, and each table is a subfolder inside that folder. The version 1 of Spark tables only supports Insert not update or delete. Spark DB v2 - Delta Lake/Lakehouse Managed and External Spark Tables In Spark, there are two main types of tables you can create: Managed Tables and External Tables . Let\u2019s explore what these terms mean and see some examples using both SQL and PySpark. Note: Spark tables are inherently external since their data is stored in an ADLS container, unlike traditional SQL servers. So, when you hear \"External Table,\" it means truly external \u2013 not inside the default ADLS container, but in a location you specify, like another container or S3. Don't get confused; external means external-external! Managed Tables A managed table is a Spark SQL table where Spark manages both the metadata and the data. When you create a managed table, Spark will create a folder for the table here: warehouse / DBName.db / TableName Full path of the folder will be: DefaultSynapseADLSContainer / synapse / workspaces / WorkSpaceName / warehouse / DBName.db / TableName Note: Don't get excited by hearing \"Internal.\" Here, nothing is internal unlike old-school MSSQL servers. All data is stored outside in some container. \"Internal\" only means that the container is the default, well-known location. Summary: 1. Data is stored in a Spark-connected ADLS container. 2. Spark handles the storage. 3. Dropping a managed table deletes both the table metadata and the data. 4. DO NOT provide the LOCATION field > Table becomes EXTERNAL. External Tables All Spark tables store data externally. But, an external table is where the data is really external. Meaning, the .parquet/.csv files aren't saved in the default location like: DefaultSynapseADLSContainer / synapse / workspaces / WorkSpaceName / warehouse / DBName.db / TableName Instead, it stays where it is. Say in some other container or S3, etc. Here Spark creates the shell, but the data is really far away. Summary: 1. You manage the storage location. 2. Data is stored in a user-specified location. 3. Dropping the table only removes the metadata, not the data. 4. The LOCATION field in SQL is a must How to create Managed and External Spark Tables Empty Managed Table SparkSQL: CREATE TABLE dbName.mngd_movies ( movieName STRING, ) USING parquet; -- Format: parquet, delta csv, etc. -- WARNING: DO NOT USE LOCATION. Becomes, external PySpark-saveAsTable: # Step 1: Define the schema for the table schema = StructType([ StructField(\"movieName\", StringType(), True) ]) # Step 2: Create an empty DataFrame using the schema empty_df = spark.createDataFrame([], schema) # Step 3: Write the empty DataFrame to a managed table empty_df.write.mode('overwrite').saveAsTable('dbName.mngd_movies') Empty External Table SparkSQL: CREATE TABLE dbName.mngd_movies ( movieName STRING, ) USING parquet; -- Format: parquet, delta csv, etc. LOCATION 'abfss://your-container@your-storage-account.dfs.core.windows.net/your-path/'; -- WARNING: USE LOCATION. Else, managed Managed Tables From Existing Data PySpark-saveAsTable: # Step 1: Read the CSV file into a Spark DataFrame csv_file_path = 'abfss://your-container@your-storage-account.dfs.core.windows.net/movies.csv' movies_df = spark.read.csv(csv_file_path, header=True, inferSchema=True) # Step 2: Create a managed table movies_df.write.saveAsTable('mngd_movies') SparkSQL: -- Read the CSV file and create a temporary view CREATE OR REPLACE TEMPORARY VIEW temp_movies USING csv OPTIONS ( path 'abfss://your-container@your-storage-account.dfs.core.windows.net/movies.csv', header 'true' ); -- Create a managed table from the temporary view CREATE TABLE dbName.mngd_movies AS SELECT * FROM temp_movies; External Tables From Existing Data SparkSQL: CREATE TABLE dbName.ext_movies ( movieName STRING ) USING csv OPTIONS (header 'true') LOCATION 'abfss://your-container@your-storage-account.dfs.core.windows.net/movies.csv'; DESCRIBE EXTENDED TABLENAME In order to see the details of the table a very useful command is. THis tells if the table is managed and where is the location of the folder for this table in the attached ADLS container. DESCRIBE EXTENDED TABLEAME","title":"Types of DB in Synapse"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse/#the-databases-types-in-synapse","text":"Before we dive into this, let's ask ourselves: How many types of pools are there in Synapse?. There are three types : Serverless SQL, Dedicated SQL, and Serverless Spark pool. So, it's simple\u2014there are three types of databases, one for each: Serverless SQL Database : Created using the Serverless SQL pool. Dedicated SQL Database : Created using the Dedicated SQL pool. Note: This is essentially a data warehouse. Spark Database[Lake Database] : Spark DB v1 : Created using a serverless Spark pool and Spark notebook, called a Spark [Lake] database. Spark DB v2(Lakehouse, Delta) : Similar to v1, but the format of the .parquet files is Delta. So, does Apache Spark have database connections? Yes, of course. How else would it run SparkSQL and what about that magic command %%sql? It has robust database capabilities.","title":"The Databases Types in Synapse"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse/#spark-databases","text":"","title":"Spark Databases"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse/#spark-db-v1-lake-database","text":"A Lake database is a type of DB created by a Spark Pool using a Spark notebook. It\u2019s all virtual; it's not a good-old SQL database but an illusion. The actual data is stored in the ADLS container as folders\u2014one database, one folder. Let's see this in action. First, the show always starts with a Spark pool. The Serverless Spark pool doesn't come pre-created like the built-in SQL pool. So, let's create one: Next, open a PySpark notebook and run this command: %%sql CREATE DATABASE BhutuSpark -- Or, spark.sql(\"CREATE DATABASE BhutuSpark\")","title":"Spark DB v1 - Lake Database"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse/#what-happens-after-the-create-db-command","text":"Initially, in a fresh workspace, the Workspace in the Data Tab is completely empty, even if you have an inbuilt SQL pool. A few minutes after the Spark SQL command to create the database completes, you will see a Lake database structure appearing under the Data workspace. Initially, this will be completely empty. Also, you will notice two databases: the one you created and a default database. Synapse creates a folder in the connected ADLS container. In the UI, you will see a database structure (tables, views, users, etc.), but in the background, every Spark database is actually a folder in the ADLS container. When you create tables inside the database, there will again be subfolders inside the parent folder. Now, let's run a command to create an empty table for simplicity: %%sql CREATE TABLE bhutuspark.BhutuSparkTable ( id INT, name STRING ) USING parquet -- For Pyspark, put inside spark.sql(\"the command above\") This is what you will see in the Data Workspace. Notice it created a table.","title":"What Happens after the CREATE DB Command?"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse/#what-happens-after-the-create-table-command","text":"Serverless SQL and Serverless Spark Pools create their own virtual databases. All data is virtual. What does this mean? The data is not stored inside them like old-school databases; the data is actually stored in your ADLS container. Let's see this: It creates a folder structure like this: ContainerName / synapse / workspaces / WorkSpaceName / warehouse / DBName.db / TableName","title":"What happens after the CREATE TABLE command?"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse/#what-happens-after-the-insert-data-command","text":"Now, let's put some data inside the table: INSERT INTO bhutuspark.BhutuSparkTable VALUES (1, 'Alice'), (2, 'Bob') -- You can use pyspark also, spark.sql(\"the above command\") What will you see? You will see .parquet files inside the table folder. Also, note there are two parquet files. We will just note this for now.","title":"What happens after the INSERT data command?"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse/#summary","text":"A Lake database, or Spark database, is an illusion-DB created using a Spark pool and a notebook. The actual data is stored in Azure Data Lake Storage (ADLS) as folders. Each Spark database is one folder, and each table is a subfolder inside that folder. The version 1 of Spark tables only supports Insert not update or delete.","title":"Summary"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse/#spark-db-v2-delta-lakelakehouse","text":"","title":"Spark DB v2 - Delta Lake/Lakehouse"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse/#managed-and-external-spark-tables","text":"In Spark, there are two main types of tables you can create: Managed Tables and External Tables . Let\u2019s explore what these terms mean and see some examples using both SQL and PySpark. Note: Spark tables are inherently external since their data is stored in an ADLS container, unlike traditional SQL servers. So, when you hear \"External Table,\" it means truly external \u2013 not inside the default ADLS container, but in a location you specify, like another container or S3. Don't get confused; external means external-external!","title":"Managed and External Spark Tables"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse/#managed-tables","text":"A managed table is a Spark SQL table where Spark manages both the metadata and the data. When you create a managed table, Spark will create a folder for the table here: warehouse / DBName.db / TableName Full path of the folder will be: DefaultSynapseADLSContainer / synapse / workspaces / WorkSpaceName / warehouse / DBName.db / TableName Note: Don't get excited by hearing \"Internal.\" Here, nothing is internal unlike old-school MSSQL servers. All data is stored outside in some container. \"Internal\" only means that the container is the default, well-known location. Summary: 1. Data is stored in a Spark-connected ADLS container. 2. Spark handles the storage. 3. Dropping a managed table deletes both the table metadata and the data. 4. DO NOT provide the LOCATION field > Table becomes EXTERNAL.","title":"Managed Tables"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse/#external-tables","text":"All Spark tables store data externally. But, an external table is where the data is really external. Meaning, the .parquet/.csv files aren't saved in the default location like: DefaultSynapseADLSContainer / synapse / workspaces / WorkSpaceName / warehouse / DBName.db / TableName Instead, it stays where it is. Say in some other container or S3, etc. Here Spark creates the shell, but the data is really far away. Summary: 1. You manage the storage location. 2. Data is stored in a user-specified location. 3. Dropping the table only removes the metadata, not the data. 4. The LOCATION field in SQL is a must","title":"External Tables"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse/#how-to-create-managed-and-external-spark-tables","text":"","title":"How to create Managed and External Spark Tables"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse/#empty-managed-table","text":"SparkSQL: CREATE TABLE dbName.mngd_movies ( movieName STRING, ) USING parquet; -- Format: parquet, delta csv, etc. -- WARNING: DO NOT USE LOCATION. Becomes, external PySpark-saveAsTable: # Step 1: Define the schema for the table schema = StructType([ StructField(\"movieName\", StringType(), True) ]) # Step 2: Create an empty DataFrame using the schema empty_df = spark.createDataFrame([], schema) # Step 3: Write the empty DataFrame to a managed table empty_df.write.mode('overwrite').saveAsTable('dbName.mngd_movies')","title":"Empty Managed Table"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse/#empty-external-table","text":"SparkSQL: CREATE TABLE dbName.mngd_movies ( movieName STRING, ) USING parquet; -- Format: parquet, delta csv, etc. LOCATION 'abfss://your-container@your-storage-account.dfs.core.windows.net/your-path/'; -- WARNING: USE LOCATION. Else, managed","title":"Empty External Table"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse/#managed-tables-from-existing-data","text":"PySpark-saveAsTable: # Step 1: Read the CSV file into a Spark DataFrame csv_file_path = 'abfss://your-container@your-storage-account.dfs.core.windows.net/movies.csv' movies_df = spark.read.csv(csv_file_path, header=True, inferSchema=True) # Step 2: Create a managed table movies_df.write.saveAsTable('mngd_movies') SparkSQL: -- Read the CSV file and create a temporary view CREATE OR REPLACE TEMPORARY VIEW temp_movies USING csv OPTIONS ( path 'abfss://your-container@your-storage-account.dfs.core.windows.net/movies.csv', header 'true' ); -- Create a managed table from the temporary view CREATE TABLE dbName.mngd_movies AS SELECT * FROM temp_movies;","title":"Managed Tables From Existing Data"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse/#external-tables-from-existing-data","text":"SparkSQL: CREATE TABLE dbName.ext_movies ( movieName STRING ) USING csv OPTIONS (header 'true') LOCATION 'abfss://your-container@your-storage-account.dfs.core.windows.net/movies.csv';","title":"External Tables From Existing Data"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse/#describe-extended-tablename","text":"In order to see the details of the table a very useful command is. THis tells if the table is managed and where is the location of the folder for this table in the attached ADLS container. DESCRIBE EXTENDED TABLEAME","title":"DESCRIBE EXTENDED TABLENAME"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Synapse Lake Database, Lakehouse, and Delta Lake Synapse Lake Database & Synapse Lakehouse What is Lake Database? A Synapse Lake Database is a container for registered tables, whose data stays in ADLS as .parquet files . Here both the database and the tables are kind of psudo. Only the .parquet files are real and they stay away from the both. How to Create a Lake Database: 1. Create a Lake Database : - In Synapse Studio, go to the Data Tab -> click + -> Lake Database . Register the Parquets as Tables in the Lake Database : sql CREATE EXTERNAL TABLE [UberLakeDatabase].[BMWUS0BB] ( PassengerNo INT, PassengerName DATE ) WITH ( LOCATION = 'lakedatabase/cars/BMWUS0BB.parquet', DATA_SOURCE = [YourDataSource], FILE_FORMAT = [ParquetFormat] ); Lake Database is like Uber(ordinary): Lake Database is like Uber. Doesn't own any car. Just like Lake Database stores no data inside it. Registered Tables are like the Taxis. They've no relation with passengers. They just carry them. .parquet Files are the passengers. Each passenger is an entry in the Lake Database table. Spark/Synapse Pools are the drivers. CETAS Activity is like a new Driver registering his taxi with UBER. Lakehouse what is it? A Synapse Lakehouse is simply a Lake Database with tables that are of type Delta. The underlying files are still .parquet, but they are stored in Delta format, providing additional features like ACID transactions. Analogy: - Lakehouse is like Uber Premium. - Delta Tables offer premium services like ACID transactions, similar to the premium experience in Uber Premium. - Creation Process : The same as creating a Lake Database, but the .parquet files are saved in Delta format. How to Create a Lakehouse: Creating Delta Lake Files : Use Spark to write data in Delta format: python df.write.format(\"delta\").mode(\"overwrite\").save(\"ADLS Container\") Registering Delta Lake Files : Use CETAS to register Delta Lake files in the Lake Database: sql CREATE EXTERNAL TABLE [UberLakeDatabase].[AudiGK0VC] ( Name INT, MarriageDate DATE ) WITH ( LOCATION = 'lakedatabase/cars/AudiGK0VC.parquet', DATA_SOURCE = [YourDataSource], FILE_FORMAT = [DeltaFormat] ); Summary: A Lake Database is essentially: - Uber : Synapse Studio -> Data Tab \" -> + -> Lake Database - The cars : Plus registered CETAS tables pointing to Parquets in ADLS. - Passengers: The .parquet files. - The Driver : Spark servers(SQL pool) / Loveless SQL server(Synapse pool). Remember, these are like taxis, they have no connection with the passenger Synapse Lakehouse What is it? A Synapse Lakehouse is similar to a Lake Database, but it uses Delta Lake tables instead of regular Parquet files. Key Components: Summary: A Lakehouse is: - A Lake Database with files stored in Delta format. - Delta tables registered with CETAS in the Lake Database. Synapse Dedicated SQL Pool (Formerly SQL Warehouse) What is it? A Synapse Dedicated SQL Pool is a traditional data warehouse with real SQL tables that can handle large-scale, high-performance queries. Key Components: Creating a Dedicated SQL Pool : In Synapse Studio, create a dedicated SQL pool for high-capacity data storage. Creating Real Tables : Load data from Lake Database tables into dedicated SQL pool tables. Example: sql CREATE TABLE [DedicatedSales] WITH ( DISTRIBUTION = HASH(TransactionID), CLUSTERED COLUMNSTORE INDEX ) AS SELECT * FROM [SalesLakeDatabase].[SalesDelta]; Summary: A Dedicated SQL Pool is used for: - Creating real SQL tables from pseudo Lake Database tables for high-performance querying. Overall Summary Lake Database : Synapse -> \"Data Tab\" -> \"+\" -> \"Lake Database\" + CETAS tables pointing to Parquet files in ADLS. Delta Lake Tables : Parquet files saved as Delta format. Lakehouse : Delta Lake tables registered in a Lake Database. Warehouse : Real SQL tables in a Synapse Dedicated SQL Pool created from Lake Database tables for high-performance queries.","title":"Lake DB-Lakehouse-Delta Lake"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse/#synapse-lake-database-lakehouse-and-delta-lake","text":"","title":"Synapse Lake Database, Lakehouse, and Delta Lake"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse/#synapse-lake-database-synapse-lakehouse","text":"","title":"Synapse Lake Database &amp; Synapse Lakehouse"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse/#what-is-lake-database","text":"A Synapse Lake Database is a container for registered tables, whose data stays in ADLS as .parquet files . Here both the database and the tables are kind of psudo. Only the .parquet files are real and they stay away from the both. How to Create a Lake Database: 1. Create a Lake Database : - In Synapse Studio, go to the Data Tab -> click + -> Lake Database . Register the Parquets as Tables in the Lake Database : sql CREATE EXTERNAL TABLE [UberLakeDatabase].[BMWUS0BB] ( PassengerNo INT, PassengerName DATE ) WITH ( LOCATION = 'lakedatabase/cars/BMWUS0BB.parquet', DATA_SOURCE = [YourDataSource], FILE_FORMAT = [ParquetFormat] ); Lake Database is like Uber(ordinary): Lake Database is like Uber. Doesn't own any car. Just like Lake Database stores no data inside it. Registered Tables are like the Taxis. They've no relation with passengers. They just carry them. .parquet Files are the passengers. Each passenger is an entry in the Lake Database table. Spark/Synapse Pools are the drivers. CETAS Activity is like a new Driver registering his taxi with UBER.","title":"What is Lake Database?"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse/#lakehouse-what-is-it","text":"A Synapse Lakehouse is simply a Lake Database with tables that are of type Delta. The underlying files are still .parquet, but they are stored in Delta format, providing additional features like ACID transactions. Analogy: - Lakehouse is like Uber Premium. - Delta Tables offer premium services like ACID transactions, similar to the premium experience in Uber Premium. - Creation Process : The same as creating a Lake Database, but the .parquet files are saved in Delta format. How to Create a Lakehouse: Creating Delta Lake Files : Use Spark to write data in Delta format: python df.write.format(\"delta\").mode(\"overwrite\").save(\"ADLS Container\") Registering Delta Lake Files : Use CETAS to register Delta Lake files in the Lake Database: sql CREATE EXTERNAL TABLE [UberLakeDatabase].[AudiGK0VC] ( Name INT, MarriageDate DATE ) WITH ( LOCATION = 'lakedatabase/cars/AudiGK0VC.parquet', DATA_SOURCE = [YourDataSource], FILE_FORMAT = [DeltaFormat] );","title":"Lakehouse what is it?"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse/#summary","text":"A Lake Database is essentially: - Uber : Synapse Studio -> Data Tab \" -> + -> Lake Database - The cars : Plus registered CETAS tables pointing to Parquets in ADLS. - Passengers: The .parquet files. - The Driver : Spark servers(SQL pool) / Loveless SQL server(Synapse pool). Remember, these are like taxis, they have no connection with the passenger","title":"Summary:"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse/#synapse-lakehouse","text":"","title":"Synapse Lakehouse"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse/#what-is-it","text":"A Synapse Lakehouse is similar to a Lake Database, but it uses Delta Lake tables instead of regular Parquet files.","title":"What is it?"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse/#key-components","text":"","title":"Key Components:"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse/#summary_1","text":"A Lakehouse is: - A Lake Database with files stored in Delta format. - Delta tables registered with CETAS in the Lake Database.","title":"Summary:"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse/#synapse-dedicated-sql-pool-formerly-sql-warehouse","text":"","title":"Synapse Dedicated SQL Pool (Formerly SQL Warehouse)"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse/#what-is-it_1","text":"A Synapse Dedicated SQL Pool is a traditional data warehouse with real SQL tables that can handle large-scale, high-performance queries.","title":"What is it?"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse/#key-components_1","text":"Creating a Dedicated SQL Pool : In Synapse Studio, create a dedicated SQL pool for high-capacity data storage. Creating Real Tables : Load data from Lake Database tables into dedicated SQL pool tables. Example: sql CREATE TABLE [DedicatedSales] WITH ( DISTRIBUTION = HASH(TransactionID), CLUSTERED COLUMNSTORE INDEX ) AS SELECT * FROM [SalesLakeDatabase].[SalesDelta];","title":"Key Components:"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse/#summary_2","text":"A Dedicated SQL Pool is used for: - Creating real SQL tables from pseudo Lake Database tables for high-performance querying.","title":"Summary:"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse/#overall-summary","text":"Lake Database : Synapse -> \"Data Tab\" -> \"+\" -> \"Lake Database\" + CETAS tables pointing to Parquet files in ADLS. Delta Lake Tables : Parquet files saved as Delta format. Lakehouse : Delta Lake tables registered in a Lake Database. Warehouse : Real SQL tables in a Synapse Dedicated SQL Pool created from Lake Database tables for high-performance queries.","title":"Overall Summary"},{"location":"Synapse-ADF/1.8_ADF_SA_Evolution/","text":"Table of contents {: .text-delta } 1. TOC {:toc} How Azure Data Factory and Synapse analytics evolved over time Background Back in the older days, people didn't pay much attention to data engineering. But then, tools like SQL Server Integration Service made things a bit easier. Even though this tool let us bring in data from different sources, like files, its main job was to move data into SQL Server. Then Azure Data Factory showed up. Azure Data Factory Azure Data Factory(ADF), really has come a long way. It started off pretty simple, just helping move data from here to there in the cloud. But now? It's a full-on powerhouse, making complex data movement and transformations. Let's take a quick walk through how it got from point A to point B: Period Milestone Key Features Pre-2015 Before ADF Data integration handled by on-premises solutions like SSIS. Cloud-based data integration tools not yet mainstream. 2015 Initial Launch of ADF Launch of Azure Data Factory for orchestrating data workflows across various data stores. 2017-2018 Introduction of ADF V2 Visual Data Flows, Trigger-based Scheduling, Integration Runtime, Enhanced Monitoring and Management. 2018-Present Continuous Updates and Integration with Azure Services Regular feature updates, new connectors, integration with Azure Databricks, Azure Synapse Analytics, and Azure Machine Learning. 2019-Present Integration with Azure Synapse Analytics Unified analytics platform, combining data integration, big data, and analytics within Azure Synapse Analytics. 2020-Present Simplification and Advanced Capabilities User Experience Improvements, dynamic content, parameterization, enhanced security and management capabilities. Azure Synapse analytics Similar to ADF, Azure Synapse Analytics has come a long way too from its days as SQL Data Warehouse. It's now a one-stop shop for all things analytics. The table below shows some important milestones. Period Milestone Key Developments Pre-2019 SQL Data Warehouse Development and enhancements of SQL Server Data Warehouse capabilities, setting the stage for Azure Synapse Analytics. 2019 Azure Synapse Analytics Launch Rebranding of SQL Data Warehouse to Azure Synapse Analytics, introducing a unified analytics platform. 2020 Integrated Workspace Introduction of Synapse Studio, providing tools for data integration, exploration, and management within a unified workspace. 2021 Connectivity and Usability Enhancements Integration with Azure Purview for data governance, and expansion of on-demand query capabilities. 2022 Performance and Flexibility Improvements New features for performance optimization and enhanced data lake exploration tools. 2023 Expansion of Ecosystem Support Support for an open data ecosystem and introduction of advanced analytics functions. 2024 (Current) Ongoing Development Continuous enhancements focusing on integration, analytics capabilities, and performance optimizations.","title":"ADF & Synapse Evolution"},{"location":"Synapse-ADF/1.8_ADF_SA_Evolution/#how-azure-data-factory-and-synapse-analytics-evolved-over-time","text":"","title":"How Azure Data Factory and Synapse analytics evolved over time"},{"location":"Synapse-ADF/1.8_ADF_SA_Evolution/#background","text":"Back in the older days, people didn't pay much attention to data engineering. But then, tools like SQL Server Integration Service made things a bit easier. Even though this tool let us bring in data from different sources, like files, its main job was to move data into SQL Server. Then Azure Data Factory showed up.","title":"Background"},{"location":"Synapse-ADF/1.8_ADF_SA_Evolution/#azure-data-factory","text":"Azure Data Factory(ADF), really has come a long way. It started off pretty simple, just helping move data from here to there in the cloud. But now? It's a full-on powerhouse, making complex data movement and transformations. Let's take a quick walk through how it got from point A to point B: Period Milestone Key Features Pre-2015 Before ADF Data integration handled by on-premises solutions like SSIS. Cloud-based data integration tools not yet mainstream. 2015 Initial Launch of ADF Launch of Azure Data Factory for orchestrating data workflows across various data stores. 2017-2018 Introduction of ADF V2 Visual Data Flows, Trigger-based Scheduling, Integration Runtime, Enhanced Monitoring and Management. 2018-Present Continuous Updates and Integration with Azure Services Regular feature updates, new connectors, integration with Azure Databricks, Azure Synapse Analytics, and Azure Machine Learning. 2019-Present Integration with Azure Synapse Analytics Unified analytics platform, combining data integration, big data, and analytics within Azure Synapse Analytics. 2020-Present Simplification and Advanced Capabilities User Experience Improvements, dynamic content, parameterization, enhanced security and management capabilities.","title":"Azure Data Factory"},{"location":"Synapse-ADF/1.8_ADF_SA_Evolution/#azure-synapse-analytics","text":"Similar to ADF, Azure Synapse Analytics has come a long way too from its days as SQL Data Warehouse. It's now a one-stop shop for all things analytics. The table below shows some important milestones. Period Milestone Key Developments Pre-2019 SQL Data Warehouse Development and enhancements of SQL Server Data Warehouse capabilities, setting the stage for Azure Synapse Analytics. 2019 Azure Synapse Analytics Launch Rebranding of SQL Data Warehouse to Azure Synapse Analytics, introducing a unified analytics platform. 2020 Integrated Workspace Introduction of Synapse Studio, providing tools for data integration, exploration, and management within a unified workspace. 2021 Connectivity and Usability Enhancements Integration with Azure Purview for data governance, and expansion of on-demand query capabilities. 2022 Performance and Flexibility Improvements New features for performance optimization and enhanced data lake exploration tools. 2023 Expansion of Ecosystem Support Support for an open data ecosystem and introduction of advanced analytics functions. 2024 (Current) Ongoing Development Continuous enhancements focusing on integration, analytics capabilities, and performance optimizations.","title":"Azure Synapse analytics"},{"location":"Synapse-ADF/1.9_CETAS/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Background CETAS is very similar to SQL CREATE TABLE TableName command. But, the table is created in Azure Data Lake and is called an External Table. External here means it's not inside the Synapse SQL data warehouse (Fancy name: Dedicated SQL Pool). If you have Synapse, why not save the data in Synapse SQL warehouse? Because Synapse SQL warehouse storage is expensive and not meant for everyone to access. Give me some real examples Removing Patient's Identity and Sharing Their Data as ADLS Tables A hospital wants to share patient data stored in its Synapse SQL warehouse with researchers. Will they let the entire world access their Synapse warehouse? No way, that would land them in jail. Patient data can't be shared directly. So, they create CETAS in Azure Data Lake with data from their SQL warehouse. Benefits : Avoids jail time and helps the research world make new medicines. Inlet wants to store chip Sensor data and Share as Tables for university students Inlet company collects a huge amount of sensor data from chips in ADLS. They have an external university research team that wants to analyze it. The in-house team connects to Synapse using PySpark to clean the data. Then, using Synapse SQL (serverless), they create neat, report-ready CETAS tables in ADLS. The external university is given access to these tables only, keeping Inlet's internal Synapse warehouse out of the entire process. Hence, CETAS is an important command for creating external tables and storing the data from your SQL query permanently as tables in ADLS. CETAS Scenarios Pull Data from Synapse Warehouse and put It in ADLS CETAS Tables Suppose you are a data engineer with access to both the Azure Data Lake Gen2 account and the Synapse workspace. The patient records are present in the SQL warehouse, and you want to export this data into ADLS as a table. Get the access sorted Enable Managed Identity for Synapse Workspace: Here, we will use Managed Identity. Why? Because it's the simplest, and Azure handles everything for us. But there are other options for access too, like the Service Principal method and the SAS method. Go to your Synapse workspace in the Azure portal. Under the \"Identity\" section, ensure the \"System-assigned managed identity\" is enabled. Grant Access to Managed Identity on ADLS Gen2: Go to your ADLS Gen2 account in the Azure portal. Navigate to the \"Access Control (IAM)\" section. Click on \"Add role assignment.\" Assign the role \"Storage Blob Data Contributor\" to the managed identity of your Synapse workspace. Custom DB to store connection info - Serverless SQL only When using CETAS with a serverless SQL pool, don't use the built-in database for connection info, credentials, or file formats. Instead, create a new database to keep things organized. For a dedicated SQL pool, you can use the built-in database. Create Custom Database: sql CREATE DATABASE MyCustDbForCETASInfo; Use Custom Database: sql USE MyCustDbForCETASInfo; Pull-from-Warehouse and Put-in ADLS CETAS Next, create an external table in ADLS Gen2 using managed identity for authentication. Create Database Scoped Credential: ```sql USE MyCustDbForCETASInfo; CREATE DATABASE SCOPED CREDENTIAL MyADLSCredential WITH IDENTITY = 'Managed Identity'; ``` Create External Data Source: sql CREATE EXTERNAL DATA SOURCE MyADLS WITH ( TYPE = HADOOP, LOCATION = 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net', CREDENTIAL = MyADLSCredential ); Create External File Format: sql CREATE EXTERNAL FILE FORMAT ParquetFormat WITH ( FORMAT_TYPE = PARQUET ); Create and fill the CETAS Table: sql CREATE EXTERNAL TABLE SalesDataExternal WITH ( LOCATION = '/salesdata/', DATA_SOURCE = MyADLS, FILE_FORMAT = ParquetFormat ) AS SELECT * FROM SalesData; Pull Data from ADLS and Pur as ADLS CETAS Tables Like the Inlet company case study, let's say you have lots of .parquet files as data. You want to clean this data and create tables in ADLS itself using Synapse. To do this follow these steps, Get the access sorted The creation of managed identity etc have been left out. As I explained them before. Pull from ADLS & Put in ADLS as CETAS Create Database Scoped Credential: sql CREATE DATABASE SCOPED CREDENTIAL MyADLSCredential WITH IDENTITY = 'Managed Identity'; Create External Data Source: sql -- Create an external data source for the Azure storage account CREATE EXTERNAL DATA SOURCE MyADLS WITH ( TYPE = HADOOP, -- For dedicated SQL pool -- TYPE = BLOB_STORAGE, -- For serverless SQL pool LOCATION = 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net', CREDENTIAL = MyADLSCredential ); Create External File Format: sql CREATE EXTERNAL FILE FORMAT ParquetFormat WITH ( FORMAT_TYPE = PARQUET ); Query Data from ADLS: sql SELECT * FROM OPENROWSET( BULK 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net/path/to/data/', FORMAT = 'PARQUET' ) AS [result] Create the CETAS Table: sql CREATE EXTERNAL TABLE ProcessedDataExternal WITH ( LOCATION = '/processeddata/', DATA_SOURCE = MyADLS, FILE_FORMAT = ParquetFormat ) AS SELECT * FROM OPENROWSET( BULK 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net/path/to/data/', FORMAT = 'PARQUET' ) AS [result]; Service Principal Method and SAS Methods We used managed identity in our examples. Apart from Managed Identity, you have a couple of other options for connecting Synapse to Azure Data Lake Storage (ADLS): Service Principal Authentication: A service principal is like a special user for applications to access Azure resources. You create a service principal and give it the needed permissions on ADLS. Its like functional id . Shared Access Signature (SAS): A Shared Access Signature (SAS) allows limited access to your storage account for a specific time and with specific permissions. Here\u2019s how you can set up each method: Service Principal Authentication Setup Create a Service Principal: You can create a service principal using the Azure portal, Azure CLI, or PowerShell. Here is an example using Azure CLI: sh az ad sp create-for-rbac --name <service-principal-name> --role \"Storage Blob Data Contributor\" --scopes /subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.Storage/storageAccounts/<storage-account> Grant Access to the Service Principal on ADLS Gen2: Assign the necessary role to the service principal: sh az role assignment create --assignee <appId> --role \"Storage Blob Data Contributor\" --scope /subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.Storage/storageAccounts/<storage-account> Configure the External Data Source in Synapse: Use the service principal credentials in your SQL script: ```sql CREATE DATABASE SCOPED CREDENTIAL MyADLSCredential WITH IDENTITY = 'service-principal-id', SECRET = 'service-principal-password'; CREATE EXTERNAL DATA SOURCE MyADLS WITH ( TYPE = HADOOP, LOCATION = 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net', CREDENTIAL = MyADLSCredential ); ``` Shared Access Signature (SAS) Setup Generate a SAS Token: You can generate a SAS token through the Azure portal, Azure Storage Explorer, Azure CLI, or programmatically using Azure Storage SDKs. Here is an example using Azure CLI: sh az storage account generate-sas --permissions rwdlacup --account-name <storage-account> --services b --resource-types co --expiry <expiry-date> Configure the External Data Source in Synapse: Use the SAS token in your SQL script: ```sql CREATE DATABASE SCOPED CREDENTIAL MyADLSSASCredential WITH IDENTITY = 'SHARED ACCESS SIGNATURE', SECRET = 'sas-token'; CREATE EXTERNAL DATA SOURCE MyADLS WITH ( TYPE = HADOOP, LOCATION = 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net', CREDENTIAL = MyADLSSASCredential ); ``` Managed Identity is usually recommended because it's easy to use and secure, but Service Principal and SAS can be useful in certain situations where Managed Identity might not work. Alternative to CETAS? 1. Azure Synapse Spark Pools Spark capability, pyspark-notebooks comes with Synapse. You can use spark pools in Synapse. This can be more efficeint than JDBC for huge volume of data handling. from pyspark.sql import SparkSession # Initialize Spark session spark = SparkSession.builder \\ .appName(\"SynapseSparkPoolExample\") \\ .getOrCreate() # Read data from ADLS df = spark.read \\ .format(\"parquet\") \\ .load(\"abfss://myfilesystem@myadlsaccount.dfs.core.windows.net/path/to/data/\") # Process the data processed_df = df.filter(df['SalesDate'] >= '2023-01-01') # Write the processed data back to ADLS processed_df.write \\ .mode(\"overwrite\") \\ .format(\"parquet\") \\ .save(\"abfss://myfilesystem@myadlsaccount.dfs.core.windows.net/processeddata/\") # Register the Parquet files as a table in the Spark session spark.sql(\"CREATE EXTERNAL TABLE IF NOT EXISTS SalesDataExternal USING parquet LOCATION 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net/processeddata/'\") 2. Azure Data Factory (ADF) ADF is one of the core component of Synapse. It's main job is to tranfer data from here to there. This can be a preferred option to avoid lengthy coding etc. Typical steps to perform the activity would be: Create a pipeline : This is the workflow for the entire activity. Use Copy Activity : This is a very important activity to copy data and used frequently in pipelines. 3. PolyBase PolyBase is a technology for MSSQL Server. It allows you to query external data as if it were part of the database. In Synapse we created CETAS etc. We were already using Polybase technology. Let's test our knowledge What function is used to read the data in files stored in a data lake? FORMAT ROWSET OPENROWSET Answer: OPENROWSET What character in file path can be used to select all the file/folders that match rest of the path? & * / Answer: * Which external database object encapsulates the connection information to a file location in a data lake store? FILE FORMAT DATA SOURCE EXTERNAL TABLE Answer : DATA SOURCE Based on the given ferocity rankings for the animals, here's how you can create an output table:","title":"CETAS"},{"location":"Synapse-ADF/1.9_CETAS/#background","text":"CETAS is very similar to SQL CREATE TABLE TableName command. But, the table is created in Azure Data Lake and is called an External Table. External here means it's not inside the Synapse SQL data warehouse (Fancy name: Dedicated SQL Pool).","title":"Background"},{"location":"Synapse-ADF/1.9_CETAS/#if-you-have-synapse-why-not-save-the-data-in-synapse-sql-warehouse","text":"Because Synapse SQL warehouse storage is expensive and not meant for everyone to access.","title":"If you have Synapse, why not save the data in Synapse SQL warehouse?"},{"location":"Synapse-ADF/1.9_CETAS/#give-me-some-real-examples","text":"","title":"Give me some real examples"},{"location":"Synapse-ADF/1.9_CETAS/#removing-patients-identity-and-sharing-their-data-as-adls-tables","text":"A hospital wants to share patient data stored in its Synapse SQL warehouse with researchers. Will they let the entire world access their Synapse warehouse? No way, that would land them in jail. Patient data can't be shared directly. So, they create CETAS in Azure Data Lake with data from their SQL warehouse. Benefits : Avoids jail time and helps the research world make new medicines.","title":"Removing Patient's Identity and Sharing Their Data as ADLS Tables"},{"location":"Synapse-ADF/1.9_CETAS/#inlet-wants-to-store-chip-sensor-data-and-share-as-tables-for-university-students","text":"Inlet company collects a huge amount of sensor data from chips in ADLS. They have an external university research team that wants to analyze it. The in-house team connects to Synapse using PySpark to clean the data. Then, using Synapse SQL (serverless), they create neat, report-ready CETAS tables in ADLS. The external university is given access to these tables only, keeping Inlet's internal Synapse warehouse out of the entire process. Hence, CETAS is an important command for creating external tables and storing the data from your SQL query permanently as tables in ADLS.","title":"Inlet wants to store chip Sensor data and Share as Tables for university students"},{"location":"Synapse-ADF/1.9_CETAS/#cetas-scenarios","text":"","title":"CETAS Scenarios"},{"location":"Synapse-ADF/1.9_CETAS/#pull-data-from-synapse-warehouse-and-put-it-in-adls-cetas-tables","text":"Suppose you are a data engineer with access to both the Azure Data Lake Gen2 account and the Synapse workspace. The patient records are present in the SQL warehouse, and you want to export this data into ADLS as a table.","title":"Pull Data from Synapse Warehouse and put It in ADLS CETAS Tables"},{"location":"Synapse-ADF/1.9_CETAS/#get-the-access-sorted","text":"Enable Managed Identity for Synapse Workspace: Here, we will use Managed Identity. Why? Because it's the simplest, and Azure handles everything for us. But there are other options for access too, like the Service Principal method and the SAS method. Go to your Synapse workspace in the Azure portal. Under the \"Identity\" section, ensure the \"System-assigned managed identity\" is enabled. Grant Access to Managed Identity on ADLS Gen2: Go to your ADLS Gen2 account in the Azure portal. Navigate to the \"Access Control (IAM)\" section. Click on \"Add role assignment.\" Assign the role \"Storage Blob Data Contributor\" to the managed identity of your Synapse workspace.","title":"Get the access sorted"},{"location":"Synapse-ADF/1.9_CETAS/#custom-db-to-store-connection-info-serverless-sql-only","text":"When using CETAS with a serverless SQL pool, don't use the built-in database for connection info, credentials, or file formats. Instead, create a new database to keep things organized. For a dedicated SQL pool, you can use the built-in database. Create Custom Database: sql CREATE DATABASE MyCustDbForCETASInfo; Use Custom Database: sql USE MyCustDbForCETASInfo;","title":"Custom DB to store connection info - Serverless SQL only"},{"location":"Synapse-ADF/1.9_CETAS/#pull-from-warehouse-and-put-in-adls-cetas","text":"Next, create an external table in ADLS Gen2 using managed identity for authentication. Create Database Scoped Credential: ```sql USE MyCustDbForCETASInfo; CREATE DATABASE SCOPED CREDENTIAL MyADLSCredential WITH IDENTITY = 'Managed Identity'; ``` Create External Data Source: sql CREATE EXTERNAL DATA SOURCE MyADLS WITH ( TYPE = HADOOP, LOCATION = 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net', CREDENTIAL = MyADLSCredential ); Create External File Format: sql CREATE EXTERNAL FILE FORMAT ParquetFormat WITH ( FORMAT_TYPE = PARQUET ); Create and fill the CETAS Table: sql CREATE EXTERNAL TABLE SalesDataExternal WITH ( LOCATION = '/salesdata/', DATA_SOURCE = MyADLS, FILE_FORMAT = ParquetFormat ) AS SELECT * FROM SalesData;","title":"Pull-from-Warehouse and Put-in ADLS CETAS"},{"location":"Synapse-ADF/1.9_CETAS/#pull-data-from-adls-and-pur-as-adls-cetas-tables","text":"Like the Inlet company case study, let's say you have lots of .parquet files as data. You want to clean this data and create tables in ADLS itself using Synapse. To do this follow these steps,","title":"Pull Data from ADLS and Pur as ADLS CETAS Tables"},{"location":"Synapse-ADF/1.9_CETAS/#get-the-access-sorted_1","text":"The creation of managed identity etc have been left out. As I explained them before.","title":"Get the access sorted"},{"location":"Synapse-ADF/1.9_CETAS/#pull-from-adls-put-in-adls-as-cetas","text":"Create Database Scoped Credential: sql CREATE DATABASE SCOPED CREDENTIAL MyADLSCredential WITH IDENTITY = 'Managed Identity'; Create External Data Source: sql -- Create an external data source for the Azure storage account CREATE EXTERNAL DATA SOURCE MyADLS WITH ( TYPE = HADOOP, -- For dedicated SQL pool -- TYPE = BLOB_STORAGE, -- For serverless SQL pool LOCATION = 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net', CREDENTIAL = MyADLSCredential ); Create External File Format: sql CREATE EXTERNAL FILE FORMAT ParquetFormat WITH ( FORMAT_TYPE = PARQUET ); Query Data from ADLS: sql SELECT * FROM OPENROWSET( BULK 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net/path/to/data/', FORMAT = 'PARQUET' ) AS [result] Create the CETAS Table: sql CREATE EXTERNAL TABLE ProcessedDataExternal WITH ( LOCATION = '/processeddata/', DATA_SOURCE = MyADLS, FILE_FORMAT = ParquetFormat ) AS SELECT * FROM OPENROWSET( BULK 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net/path/to/data/', FORMAT = 'PARQUET' ) AS [result];","title":"Pull from ADLS &amp; Put in ADLS as CETAS"},{"location":"Synapse-ADF/1.9_CETAS/#service-principal-method-and-sas-methods","text":"We used managed identity in our examples. Apart from Managed Identity, you have a couple of other options for connecting Synapse to Azure Data Lake Storage (ADLS): Service Principal Authentication: A service principal is like a special user for applications to access Azure resources. You create a service principal and give it the needed permissions on ADLS. Its like functional id . Shared Access Signature (SAS): A Shared Access Signature (SAS) allows limited access to your storage account for a specific time and with specific permissions. Here\u2019s how you can set up each method:","title":"Service Principal Method and SAS Methods"},{"location":"Synapse-ADF/1.9_CETAS/#service-principal-authentication-setup","text":"Create a Service Principal: You can create a service principal using the Azure portal, Azure CLI, or PowerShell. Here is an example using Azure CLI: sh az ad sp create-for-rbac --name <service-principal-name> --role \"Storage Blob Data Contributor\" --scopes /subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.Storage/storageAccounts/<storage-account> Grant Access to the Service Principal on ADLS Gen2: Assign the necessary role to the service principal: sh az role assignment create --assignee <appId> --role \"Storage Blob Data Contributor\" --scope /subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.Storage/storageAccounts/<storage-account> Configure the External Data Source in Synapse: Use the service principal credentials in your SQL script: ```sql CREATE DATABASE SCOPED CREDENTIAL MyADLSCredential WITH IDENTITY = 'service-principal-id', SECRET = 'service-principal-password'; CREATE EXTERNAL DATA SOURCE MyADLS WITH ( TYPE = HADOOP, LOCATION = 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net', CREDENTIAL = MyADLSCredential ); ```","title":"Service Principal Authentication Setup"},{"location":"Synapse-ADF/1.9_CETAS/#shared-access-signature-sas-setup","text":"Generate a SAS Token: You can generate a SAS token through the Azure portal, Azure Storage Explorer, Azure CLI, or programmatically using Azure Storage SDKs. Here is an example using Azure CLI: sh az storage account generate-sas --permissions rwdlacup --account-name <storage-account> --services b --resource-types co --expiry <expiry-date> Configure the External Data Source in Synapse: Use the SAS token in your SQL script: ```sql CREATE DATABASE SCOPED CREDENTIAL MyADLSSASCredential WITH IDENTITY = 'SHARED ACCESS SIGNATURE', SECRET = 'sas-token'; CREATE EXTERNAL DATA SOURCE MyADLS WITH ( TYPE = HADOOP, LOCATION = 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net', CREDENTIAL = MyADLSSASCredential ); ``` Managed Identity is usually recommended because it's easy to use and secure, but Service Principal and SAS can be useful in certain situations where Managed Identity might not work.","title":"Shared Access Signature (SAS) Setup"},{"location":"Synapse-ADF/1.9_CETAS/#alternative-to-cetas","text":"","title":"Alternative to CETAS?"},{"location":"Synapse-ADF/1.9_CETAS/#1-azure-synapse-spark-pools","text":"Spark capability, pyspark-notebooks comes with Synapse. You can use spark pools in Synapse. This can be more efficeint than JDBC for huge volume of data handling. from pyspark.sql import SparkSession # Initialize Spark session spark = SparkSession.builder \\ .appName(\"SynapseSparkPoolExample\") \\ .getOrCreate() # Read data from ADLS df = spark.read \\ .format(\"parquet\") \\ .load(\"abfss://myfilesystem@myadlsaccount.dfs.core.windows.net/path/to/data/\") # Process the data processed_df = df.filter(df['SalesDate'] >= '2023-01-01') # Write the processed data back to ADLS processed_df.write \\ .mode(\"overwrite\") \\ .format(\"parquet\") \\ .save(\"abfss://myfilesystem@myadlsaccount.dfs.core.windows.net/processeddata/\") # Register the Parquet files as a table in the Spark session spark.sql(\"CREATE EXTERNAL TABLE IF NOT EXISTS SalesDataExternal USING parquet LOCATION 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net/processeddata/'\")","title":"1. Azure Synapse Spark Pools"},{"location":"Synapse-ADF/1.9_CETAS/#2-azure-data-factory-adf","text":"ADF is one of the core component of Synapse. It's main job is to tranfer data from here to there. This can be a preferred option to avoid lengthy coding etc. Typical steps to perform the activity would be: Create a pipeline : This is the workflow for the entire activity. Use Copy Activity : This is a very important activity to copy data and used frequently in pipelines.","title":"2. Azure Data Factory (ADF)"},{"location":"Synapse-ADF/1.9_CETAS/#3-polybase","text":"PolyBase is a technology for MSSQL Server. It allows you to query external data as if it were part of the database. In Synapse we created CETAS etc. We were already using Polybase technology.","title":"3. PolyBase"},{"location":"Synapse-ADF/1.9_CETAS/#lets-test-our-knowledge","text":"What function is used to read the data in files stored in a data lake? FORMAT ROWSET OPENROWSET Answer: OPENROWSET What character in file path can be used to select all the file/folders that match rest of the path? & * / Answer: * Which external database object encapsulates the connection information to a file location in a data lake store? FILE FORMAT DATA SOURCE EXTERNAL TABLE Answer : DATA SOURCE Based on the given ferocity rankings for the animals, here's how you can create an output table:","title":"Let's test our knowledge"},{"location":"Synapse-ADF/2.0_Projects/","text":"layout: default title: Projects parent: Synapse-ADF nav_order: 2 has_children: true","title":"2.0 Projects"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS/","text":"Table of contents {: .text-delta } 1. TOC {:toc} A Simple Synapse Pipeline. Copy files from Laptop To ADLS We have a Synapse workspace and some CSV files on our laptop that we want to upload to ADLS . Here is the Microsoft recommended way to do it: Install SHIR on the laptop. Create a pipeline with a copy data activity. Run the pipeline. In Power Platform, the SHIR is replaced by the on-premise gateway. Both are software installed on your local machine, but one is for Synapse and the other is for Power Platform, Fabric, etc. Let's get our hands dirty and see how to do it. For the busy people An outline of all the activities typically involved in this process is provided in the table below. For more details, please continue reading the following sections. Step Action Definition 1: Open Synapse Workspace Synapse Workspace Every synapse instance has a workspace. This is the central place for all synapse activities. 2: Install SHIR on Laptop Self-hosted Integration Runtime (SHIR) A software installed on your laptop to enable data movement from on-premises to the cloud. More details on Integration Pipelines 3: Create Linked Services Linked Services These are connection strings to your data sources (local file system) and destinations (ADLS). 4: Define Datasets Datasets These are like file type. In this case the source and destination datasets are .csvs 5: Build the Pipeline Pipeline A pipline is a workflow, it contains many tasks like Copy data etc. 6: Add a Copy Data Activity Copy Data Activity This is a very important activity that takes care of the entire data copying. 7: Set Up Triggers Triggers This will tell what will trigger the pipeline Create and Configure an Integration Runtime The very first step is to install the integration runtime on the local machine. The IR is the backbone of the connection between the local folder and Synapse. In your synapse workspace go to Manage -> Integration -> Integration runtime . Click on New , then in the settings, you will have two install options. Choose an express setup . Express setup is a quicker option as it both installs and links the local IR environment with the synapse setup. If you prefer to do a manual setup, refer to to appenxis. Create Two Linked Services (Connection Strings) Next, we need to create two connection strings (also known as Linked Services): one to the local laptop's folder (source) and another to the ADLS (destination). Linked Service to Laptop's Folder In Synapse workspace, go to Manage -> Linked Services -> New . Select File System and provide a name for the linked service. Select the Integration Runtime we created earlier. Specify the path to the CSV files on your laptop and provide a user name and password which has read/write access to the folder. Here, sa is a local user which has read/write access to the folder. Go to the properties of the source folder and navigate to the security tab to check if the user has the appropriate permissions for the folder. A Common Error After setting up the linked service when you Test connection it may fail. This has nothing to do with the setup but a windows security feature which causes the issue. To resolve this, open Command Prompt as Administrator and run the following commands: shell cd C:\\Program Files\\Microsoft Integration Runtime\\5.0\\Shared .\\dmgcmd.exe -DisableLocalFolderPathValidation This will disable local folder path validation, and Test connection will pass this time. Linked Service to ADLS Navigate to Manage -> Linked Services -> New . Select Azure Data Lake Storage Gen2 . In Our case we will use AutoResolveIntegrationRuntime . Sometimes its a good choice. Create a Pipeline with Copy Data Activity Now that the linked services are configured, create a pipeline to copy data: Crate a New Pipeline in Syanpse Workspace In Synapse workspace, go to Integrate -> Pipelines -> New Pipeline . Add Copy Data Activity Drag and drop the Copy Data activity onto the pipeline canvas. Configure the Source Dataset etc Choose dataset: Go to the Source tab, then Files, select File System, and click Continue. Choose File Format: Now, you have to select the format of the source files. We have CSV, so we will select Delimited Text. Select Linked Service: Next, select the Linked Service which we created earlier. This is the connection string that connects to the Laptops folder. You will see the File path and other details appear. Choose First row as header, which is usually the case for all CSVs. Preview data: If successful, you can preview the data. It will load one of the files to show you how the data looks, displaying a well-formatted table. Note, how we have seleccted *.csv to load all the csv files in the folder. Configure the Sink Dataset Select Integration Dataset: Go to the Sink tab, then select Azure Data Lake Storage Gen2. Selct File Format: Now, we need to provide the format in which the data will be copied to the destination. For this, select DelimitedText . Select Linked Service & IR: Next, select the linked service which has the connection information to the container in ADLS where your data will be stored. You can choose any integration runtime. Here, I have chosen the default AutoResolveIntegrationRuntime as it is the simplest and comes factory-shipped with the Synapse workspace. Choose other properties: Once the sink dataset is configured, you can choose other properties like Copy behavior, etc. Execute the Pipeline Validate the Pipeline : Ensure all configurations are correct and validate the pipeline. Run the Pipeline : Execute the pipeline to start the data transfer from your laptop to ADLS. If it runs successfully you will see the data copied to your desired ADLS container. Appendix Manually Installating Integration Runtime The integration runtime can also be downloaded and installed separately from the Microsoft software store. Install it on your local machine. The steps are straightforward. Just click through the installation process. In the final step, you will need to register the Runtime by copying and pasting the authentication key from the Synapse portal.","title":"CopyData-LocalToADLS"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS/#a-simple-synapse-pipeline-copy-files-from-laptop-to-adls","text":"We have a Synapse workspace and some CSV files on our laptop that we want to upload to ADLS . Here is the Microsoft recommended way to do it: Install SHIR on the laptop. Create a pipeline with a copy data activity. Run the pipeline. In Power Platform, the SHIR is replaced by the on-premise gateway. Both are software installed on your local machine, but one is for Synapse and the other is for Power Platform, Fabric, etc. Let's get our hands dirty and see how to do it.","title":"A Simple Synapse Pipeline. Copy files from Laptop To ADLS"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS/#for-the-busy-people","text":"An outline of all the activities typically involved in this process is provided in the table below. For more details, please continue reading the following sections. Step Action Definition 1: Open Synapse Workspace Synapse Workspace Every synapse instance has a workspace. This is the central place for all synapse activities. 2: Install SHIR on Laptop Self-hosted Integration Runtime (SHIR) A software installed on your laptop to enable data movement from on-premises to the cloud. More details on Integration Pipelines 3: Create Linked Services Linked Services These are connection strings to your data sources (local file system) and destinations (ADLS). 4: Define Datasets Datasets These are like file type. In this case the source and destination datasets are .csvs 5: Build the Pipeline Pipeline A pipline is a workflow, it contains many tasks like Copy data etc. 6: Add a Copy Data Activity Copy Data Activity This is a very important activity that takes care of the entire data copying. 7: Set Up Triggers Triggers This will tell what will trigger the pipeline","title":"For the busy people"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS/#create-and-configure-an-integration-runtime","text":"The very first step is to install the integration runtime on the local machine. The IR is the backbone of the connection between the local folder and Synapse. In your synapse workspace go to Manage -> Integration -> Integration runtime . Click on New , then in the settings, you will have two install options. Choose an express setup . Express setup is a quicker option as it both installs and links the local IR environment with the synapse setup. If you prefer to do a manual setup, refer to to appenxis.","title":"Create and Configure an Integration Runtime"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS/#create-two-linked-services-connection-strings","text":"Next, we need to create two connection strings (also known as Linked Services): one to the local laptop's folder (source) and another to the ADLS (destination).","title":"Create Two Linked Services (Connection Strings)"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS/#linked-service-to-laptops-folder","text":"In Synapse workspace, go to Manage -> Linked Services -> New . Select File System and provide a name for the linked service. Select the Integration Runtime we created earlier. Specify the path to the CSV files on your laptop and provide a user name and password which has read/write access to the folder. Here, sa is a local user which has read/write access to the folder. Go to the properties of the source folder and navigate to the security tab to check if the user has the appropriate permissions for the folder.","title":"Linked Service to Laptop's Folder"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS/#a-common-error","text":"After setting up the linked service when you Test connection it may fail. This has nothing to do with the setup but a windows security feature which causes the issue. To resolve this, open Command Prompt as Administrator and run the following commands: shell cd C:\\Program Files\\Microsoft Integration Runtime\\5.0\\Shared .\\dmgcmd.exe -DisableLocalFolderPathValidation This will disable local folder path validation, and Test connection will pass this time.","title":"A Common Error"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS/#linked-service-to-adls","text":"Navigate to Manage -> Linked Services -> New . Select Azure Data Lake Storage Gen2 . In Our case we will use AutoResolveIntegrationRuntime . Sometimes its a good choice.","title":"Linked Service to ADLS"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS/#create-a-pipeline-with-copy-data-activity","text":"Now that the linked services are configured, create a pipeline to copy data:","title":"Create a Pipeline with Copy Data Activity"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS/#crate-a-new-pipeline-in-syanpse-workspace","text":"In Synapse workspace, go to Integrate -> Pipelines -> New Pipeline .","title":"Crate a New Pipeline in Syanpse Workspace"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS/#add-copy-data-activity","text":"Drag and drop the Copy Data activity onto the pipeline canvas.","title":"Add Copy Data Activity"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS/#configure-the-source-dataset-etc","text":"Choose dataset: Go to the Source tab, then Files, select File System, and click Continue. Choose File Format: Now, you have to select the format of the source files. We have CSV, so we will select Delimited Text. Select Linked Service: Next, select the Linked Service which we created earlier. This is the connection string that connects to the Laptops folder. You will see the File path and other details appear. Choose First row as header, which is usually the case for all CSVs. Preview data: If successful, you can preview the data. It will load one of the files to show you how the data looks, displaying a well-formatted table. Note, how we have seleccted *.csv to load all the csv files in the folder.","title":"Configure the Source Dataset etc"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS/#configure-the-sink-dataset","text":"Select Integration Dataset: Go to the Sink tab, then select Azure Data Lake Storage Gen2. Selct File Format: Now, we need to provide the format in which the data will be copied to the destination. For this, select DelimitedText . Select Linked Service & IR: Next, select the linked service which has the connection information to the container in ADLS where your data will be stored. You can choose any integration runtime. Here, I have chosen the default AutoResolveIntegrationRuntime as it is the simplest and comes factory-shipped with the Synapse workspace. Choose other properties: Once the sink dataset is configured, you can choose other properties like Copy behavior, etc.","title":"Configure the Sink Dataset"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS/#execute-the-pipeline","text":"Validate the Pipeline : Ensure all configurations are correct and validate the pipeline. Run the Pipeline : Execute the pipeline to start the data transfer from your laptop to ADLS. If it runs successfully you will see the data copied to your desired ADLS container.","title":"Execute the Pipeline"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS/#appendix","text":"","title":"Appendix"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS/#manually-installating-integration-runtime","text":"The integration runtime can also be downloaded and installed separately from the Microsoft software store. Install it on your local machine. The steps are straightforward. Just click through the installation process. In the final step, you will need to register the Runtime by copying and pasting the authentication key from the Synapse portal.","title":"Manually Installating Integration Runtime"},{"location":"Synapse-ADF/2.2_PySparkWarehouse/","text":"Table of contents {: .text-delta } 1. TOC {:toc} Create a Warehouse with Just a simple Pyspark setup I am sad; I have just Python and I managed to pip install PySpark. I want to create a decent warehouse on my laptop. Am I being impractical? I read somewhere a DW is for data at rest, spark is for data in motion. Well, no. Let's see what comes free with PySpark: Built-in Hive : PySpark comes with a small Hive setup plus a Derby database by default as the Hive metastore. This means you can create managed tables just like you did in Synapse Analytics. Automatic spark-warehouse folder : PySpark automatically creates a spark-warehouse directory to store table data. This directory is created in your working directory, and all managed tables are stored as Parquet files within this directory. Automatic .Parquet : When you create tables using Hive support in PySpark, the data is stored in Parquet format by default. Bonus! Delta Table Support : You just have to pip install delta, and then you can save the data in the enhanced Delta format, making it an even better warehouse. First, let me create a Spark session with Hive support. from pyspark.sql import SparkSession # Start Spark session with Hive support spark = SparkSession.builder \\ .appName(\"PySpark Data Warehouse with Hive Support\") \\ .enableHiveSupport() \\ .getOrCreate() Then let me create a database and an empty table inside it: # Create a database spark.sql(\"CREATE DATABASE IF NOT EXISTS dbHollywood\") # Switch to the new database spark.sql(\"USE dbHollywood\") # Create a managed table spark.sql(\"\"\" CREATE TABLE IF NOT EXISTS mngd_tbl_HollywoodFilms ( id INT, name STRING ) \"\"\") THis is how the folders are getting creatd in the spark-warehouse folder: Now, let me insert some data: spark.sql(\"USE dbHollywood\") # Insert data into the managed table spark.sql(\"\"\" INSERT INTO mngd_tbl_HollywoodFilms VALUES (1, 'Matrix'), (2, 'Inception') \"\"\") Wow, these parquet fiels were auto-craeted. I didnt mention any format. In hive support data is auto-stored as .parquet. Now, if I query the table in teh same session I see this: Now, let me stop the session and create a new session to see if things are just a one-night stand or a lifetime friendship. If I have no metatore I can't query using the table name, though the backend data might be present. Now, let me try to delete the table and then the database and see if the data goes away or not. Else, how can I call them managed tables? Here is the snapshot of the spark-warehouse folder. Only DB folder, table folder completely deleted. Now, let me delete the database(ignore the warning) The database parent folder completely gone: Using Delta Tables Now, I am very satisfied with my warehouse and I want to use Spark Tables v2. Which is Delta table. The difference is that in delta table you can update the fields and also spark tables are basic and delta acidic(haha). To have delta table feature in our plain old python envirnmetn. Let's instll delta-spark library: pip install delta-spark Then, we will create the session with both delta and hive support. Create a database, then a managed table with delta. Then we will insert some data and then append the data. from pyspark.sql import SparkSession from delta import configure_spark_with_delta_pip from pyspark.sql.types import StructType, StructField, IntegerType, StringType from pyspark.sql import Row # Configure Spark session with Delta Lake and Hive support spark = configure_spark_with_delta_pip( SparkSession.builder .appName(\"DeltaLakeExample\") .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") .enableHiveSupport() # Enable Hive support ).getOrCreate() # Create a database spark.sql(\"CREATE DATABASE IF NOT EXISTS Hollywood\") # Switch to the new database spark.sql(\"USE Hollywood\") # Create a managed Delta table spark.sql(\"\"\" CREATE TABLE IF NOT EXISTS mgd_delta_movies ( moviename STRING ) USING delta \"\"\") # Insert data using SQL spark.sql(\"\"\" INSERT INTO mgd_delta_movies (moviename) VALUES ('Titanic'), ('Inception') \"\"\") # Define schema schema = StructType([StructField(\"moviename\", StringType(), True)]) # Create DataFrame with defined schema data = [Row(name='Scream')] df = spark.createDataFrame(data, schema=schema) # Insert the DataFrame data into the managed Delta table df.write.format(\"delta\").mode(\"append\").saveAsTable(\"mgd_delta_movies\") How, do we know that the tables are delta and they are manged? Its simple We didn't give any path for during create table command. And the folder for the table is inside spark-warehouse/db_folder/ Thats not engouh? Lets run some commands dbas use to find out: spark.sql(\"DESCRIBE FORMATTED Hollywood.Mgd_Delta_Movies\").show(truncate=False) Also spark.sql(\"DESCRIBE DETAIL Hollywood.Mgd_Delta_Movies\").show(truncate=False)","title":"PysparkWarehouse"},{"location":"Synapse-ADF/2.2_PySparkWarehouse/#create-a-warehouse-with-just-a-simple-pyspark-setup","text":"I am sad; I have just Python and I managed to pip install PySpark. I want to create a decent warehouse on my laptop. Am I being impractical? I read somewhere a DW is for data at rest, spark is for data in motion. Well, no. Let's see what comes free with PySpark: Built-in Hive : PySpark comes with a small Hive setup plus a Derby database by default as the Hive metastore. This means you can create managed tables just like you did in Synapse Analytics. Automatic spark-warehouse folder : PySpark automatically creates a spark-warehouse directory to store table data. This directory is created in your working directory, and all managed tables are stored as Parquet files within this directory. Automatic .Parquet : When you create tables using Hive support in PySpark, the data is stored in Parquet format by default. Bonus! Delta Table Support : You just have to pip install delta, and then you can save the data in the enhanced Delta format, making it an even better warehouse. First, let me create a Spark session with Hive support. from pyspark.sql import SparkSession # Start Spark session with Hive support spark = SparkSession.builder \\ .appName(\"PySpark Data Warehouse with Hive Support\") \\ .enableHiveSupport() \\ .getOrCreate() Then let me create a database and an empty table inside it: # Create a database spark.sql(\"CREATE DATABASE IF NOT EXISTS dbHollywood\") # Switch to the new database spark.sql(\"USE dbHollywood\") # Create a managed table spark.sql(\"\"\" CREATE TABLE IF NOT EXISTS mngd_tbl_HollywoodFilms ( id INT, name STRING ) \"\"\") THis is how the folders are getting creatd in the spark-warehouse folder: Now, let me insert some data: spark.sql(\"USE dbHollywood\") # Insert data into the managed table spark.sql(\"\"\" INSERT INTO mngd_tbl_HollywoodFilms VALUES (1, 'Matrix'), (2, 'Inception') \"\"\") Wow, these parquet fiels were auto-craeted. I didnt mention any format. In hive support data is auto-stored as .parquet. Now, if I query the table in teh same session I see this: Now, let me stop the session and create a new session to see if things are just a one-night stand or a lifetime friendship. If I have no metatore I can't query using the table name, though the backend data might be present. Now, let me try to delete the table and then the database and see if the data goes away or not. Else, how can I call them managed tables? Here is the snapshot of the spark-warehouse folder. Only DB folder, table folder completely deleted. Now, let me delete the database(ignore the warning) The database parent folder completely gone:","title":"Create a Warehouse with Just a simple Pyspark setup"},{"location":"Synapse-ADF/2.2_PySparkWarehouse/#using-delta-tables","text":"Now, I am very satisfied with my warehouse and I want to use Spark Tables v2. Which is Delta table. The difference is that in delta table you can update the fields and also spark tables are basic and delta acidic(haha). To have delta table feature in our plain old python envirnmetn. Let's instll delta-spark library: pip install delta-spark Then, we will create the session with both delta and hive support. Create a database, then a managed table with delta. Then we will insert some data and then append the data. from pyspark.sql import SparkSession from delta import configure_spark_with_delta_pip from pyspark.sql.types import StructType, StructField, IntegerType, StringType from pyspark.sql import Row # Configure Spark session with Delta Lake and Hive support spark = configure_spark_with_delta_pip( SparkSession.builder .appName(\"DeltaLakeExample\") .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") .enableHiveSupport() # Enable Hive support ).getOrCreate() # Create a database spark.sql(\"CREATE DATABASE IF NOT EXISTS Hollywood\") # Switch to the new database spark.sql(\"USE Hollywood\") # Create a managed Delta table spark.sql(\"\"\" CREATE TABLE IF NOT EXISTS mgd_delta_movies ( moviename STRING ) USING delta \"\"\") # Insert data using SQL spark.sql(\"\"\" INSERT INTO mgd_delta_movies (moviename) VALUES ('Titanic'), ('Inception') \"\"\") # Define schema schema = StructType([StructField(\"moviename\", StringType(), True)]) # Create DataFrame with defined schema data = [Row(name='Scream')] df = spark.createDataFrame(data, schema=schema) # Insert the DataFrame data into the managed Delta table df.write.format(\"delta\").mode(\"append\").saveAsTable(\"mgd_delta_movies\") How, do we know that the tables are delta and they are manged? Its simple We didn't give any path for during create table command. And the folder for the table is inside spark-warehouse/db_folder/ Thats not engouh? Lets run some commands dbas use to find out: spark.sql(\"DESCRIBE FORMATTED Hollywood.Mgd_Delta_Movies\").show(truncate=False) Also spark.sql(\"DESCRIBE DETAIL Hollywood.Mgd_Delta_Movies\").show(truncate=False)","title":"Using Delta Tables"},{"location":"Synapse-ADF/2.3_ADF_RestAPI_Databricks/","text":"This project will clear three areas: How to ingest data from a REST API to ADLS Mounting ADLS on Databricks Transform data in ADLS using Databricks How to load the processed data into Synapse using ADF Polybase How to use Azure data factory triggers Resoruces used: ADF ADLS Azure Databricks(Transformation) Azure Synapse","title":"2.3 ADF RestAPI Databricks"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines/","text":"Table of contents {: .text-delta } 1. TOC {:toc} How to Monitor ADF Pipelines No matter how good your workflow is, errors will happen. Here are some ways to monitor your ADF pipelines. My favorite way is to set up email alerts with a specific subject line and severity. Alright, here are all the ways you can monitor your pipelines. Ways to Monitor ADF Pipelines Using Azure Portal Setting Alerts and Metrics Log Analytics and Azure Monitor Custom Monitoring Solutions 1. Using Azure Portal Monitoring Tab Go to ADF: Open the Azure portal and find your Data Factory instance. Monitor Tab: Click on the \"Monitor\" tab on the left side. This section shows details about pipeline runs, activity runs, and triggers. Pipeline Runs See Pipeline Runs: Here, you can see all pipeline runs. Filter by status (Succeeded, Failed, In Progress) and time range. Run Details: Click on a pipeline run to see detailed info, including the status of each activity in the pipeline. Activity Runs Activity Details: This section shows individual activities within each pipeline run. You can see input, output, and error messages for each activity. 2. Setting Alerts and Metrics Creating Alerts Go to Alerts: In your ADF instance, go to the \"Alerts\" section. Create Alert: Click on \"New alert rule\" to make a new alert. Set Alert: Choose the condition (e.g., pipeline failure), threshold, and notification method (e.g., email, SMS). Viewing Metrics Metrics: You can check metrics like pipeline run duration, activity duration, and trigger runs. Metrics help you understand performance and spot issues. 3. Log Analytics and Azure Monitor Enable Diagnostic Logs Diagnostic Settings: In your ADF instance, go to \"Diagnostic settings\" and turn on diagnostic logs. Send these logs to a Log Analytics workspace, Event Hub, or Storage Account. Log Analytics Workspace: If you use Log Analytics, you can run queries to analyze the logs. Using Log Analytics Query Logs: In the Log Analytics workspace, run queries to find details about pipeline runs, failures, and performance. Sample Query: kusto ADFPipelineRun | where Status == 'Failed' | project RunId, PipelineName, Start, End, Status, ErrorMessage Azure Monitor Integration: You can use Azure Monitor with ADF for centralized monitoring. Alerts: Create alerts based on log queries and metrics using Azure Monitor. 4. Custom Monitoring Solutions Using PowerShell Automation: Use PowerShell scripts to automate monitoring and reporting of pipeline runs. Sample Script: ```powershell $dataFactoryName = \"your_data_factory_name\" $resourceGroupName = \"your_resource_group_name\" $pipelineRuns = Get-AzDataFactoryV2PipelineRun -ResourceGroupName $resourceGroupName -DataFactoryName $dataFactoryName $failedRuns = $pipelineRuns | Where-Object { $_.Status -eq 'Failed' } foreach ($run in $failedRuns) { Write-Output \"Pipeline: $($run.PipelineName) | Run ID: $($run.RunId) | Status: $($run.Status)\" } ``` Using Logic Apps Automated Workflows: Create Logic Apps to handle pipeline monitoring tasks automatically, like sending notifications or triggering other workflows based on pipeline statuses.","title":"ADF Pipelines Monitoring"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines/#how-to-monitor-adf-pipelines","text":"No matter how good your workflow is, errors will happen. Here are some ways to monitor your ADF pipelines. My favorite way is to set up email alerts with a specific subject line and severity. Alright, here are all the ways you can monitor your pipelines.","title":"How to Monitor ADF Pipelines"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines/#ways-to-monitor-adf-pipelines","text":"Using Azure Portal Setting Alerts and Metrics Log Analytics and Azure Monitor Custom Monitoring Solutions","title":"Ways to Monitor ADF Pipelines"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines/#1-using-azure-portal","text":"","title":"1. Using Azure Portal"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines/#monitoring-tab","text":"Go to ADF: Open the Azure portal and find your Data Factory instance. Monitor Tab: Click on the \"Monitor\" tab on the left side. This section shows details about pipeline runs, activity runs, and triggers.","title":"Monitoring Tab"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines/#pipeline-runs","text":"See Pipeline Runs: Here, you can see all pipeline runs. Filter by status (Succeeded, Failed, In Progress) and time range. Run Details: Click on a pipeline run to see detailed info, including the status of each activity in the pipeline.","title":"Pipeline Runs"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines/#activity-runs","text":"Activity Details: This section shows individual activities within each pipeline run. You can see input, output, and error messages for each activity.","title":"Activity Runs"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines/#2-setting-alerts-and-metrics","text":"","title":"2. Setting Alerts and Metrics"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines/#creating-alerts","text":"Go to Alerts: In your ADF instance, go to the \"Alerts\" section. Create Alert: Click on \"New alert rule\" to make a new alert. Set Alert: Choose the condition (e.g., pipeline failure), threshold, and notification method (e.g., email, SMS).","title":"Creating Alerts"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines/#viewing-metrics","text":"Metrics: You can check metrics like pipeline run duration, activity duration, and trigger runs. Metrics help you understand performance and spot issues.","title":"Viewing Metrics"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines/#3-log-analytics-and-azure-monitor","text":"","title":"3. Log Analytics and Azure Monitor"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines/#enable-diagnostic-logs","text":"Diagnostic Settings: In your ADF instance, go to \"Diagnostic settings\" and turn on diagnostic logs. Send these logs to a Log Analytics workspace, Event Hub, or Storage Account. Log Analytics Workspace: If you use Log Analytics, you can run queries to analyze the logs.","title":"Enable Diagnostic Logs"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines/#using-log-analytics","text":"Query Logs: In the Log Analytics workspace, run queries to find details about pipeline runs, failures, and performance. Sample Query: kusto ADFPipelineRun | where Status == 'Failed' | project RunId, PipelineName, Start, End, Status, ErrorMessage","title":"Using Log Analytics"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines/#azure-monitor","text":"Integration: You can use Azure Monitor with ADF for centralized monitoring. Alerts: Create alerts based on log queries and metrics using Azure Monitor.","title":"Azure Monitor"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines/#4-custom-monitoring-solutions","text":"","title":"4. Custom Monitoring Solutions"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines/#using-powershell","text":"Automation: Use PowerShell scripts to automate monitoring and reporting of pipeline runs. Sample Script: ```powershell $dataFactoryName = \"your_data_factory_name\" $resourceGroupName = \"your_resource_group_name\" $pipelineRuns = Get-AzDataFactoryV2PipelineRun -ResourceGroupName $resourceGroupName -DataFactoryName $dataFactoryName $failedRuns = $pipelineRuns | Where-Object { $_.Status -eq 'Failed' } foreach ($run in $failedRuns) { Write-Output \"Pipeline: $($run.PipelineName) | Run ID: $($run.RunId) | Status: $($run.Status)\" } ```","title":"Using PowerShell"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines/#using-logic-apps","text":"Automated Workflows: Create Logic Apps to handle pipeline monitoring tasks automatically, like sending notifications or triggering other workflows based on pipeline statuses.","title":"Using Logic Apps"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy/","text":"Table of contents {: .text-delta } 1. TOC {:toc} How to Understand and Export Your Azure Data Factory Pipeline Viewing the Pipeline's JSON Code In Azure Data Factory (ADF), you can easily see the JSON code that represents your entire pipeline. To do this, simply click on the curly brackets {} icon. This will display the JSON structure of your pipeline, including all the activities, parameters, variables, and other settings. Limitations of the JSON Representation However, please note that the JSON file you get by clicking the {} icon does not include everything you might need to fully understand your pipeline. Here are some important components that are usually missing: Linked Services : If your pipeline connects to external resources like Azure Data Lake, Azure SQL, or Databricks, the configuration details for these linked services are not included in the pipeline's JSON. You will need to check each linked service's JSON definition separately. Referenced Pipelines : If your pipeline calls other pipelines using activities like ExecutePipeline , the details of those pipelines are not included. You will have to access their JSON files individually. Global Parameters : Any global parameters your pipeline uses are not fully defined within its JSON file. You'll need to look at the global parameter configurations separately. Triggers : Triggers that start your pipeline (like those based on schedules or events) are not included in the pipeline's JSON. They are managed separately in the ADF interface. Data Flows : If your pipeline contains data flows, the JSON only references them. You'll need to view the details of the data flows separately. Azure Integration Runtime Configurations : These settings are also referenced but not fully included in the pipeline's JSON. In short , while the JSON representation gives you a detailed view of your pipeline's structure, you need to explore these additional configurations and resources to fully understand how your pipeline works. How to Copy the Entire ADF Pipeline If you want to copy and export the entire pipeline along with all its related resources (like linked services, datasets, and any referenced pipelines), here's how you can do it: 1. Use ARM Template Export Azure Data Factory allows you to export your pipeline and all related resources as an Azure Resource Manager (ARM) Template . This method includes all dependencies and is the most comprehensive way to copy everything. Go to Your Data Factory : Open your Data Factory instance in the Azure Portal. Access the Manage Tab : Click on the \"Manage\" tab on the left side. Export ARM Template : Under the \"ARM template\" section, click on \"Export ARM template\" . Select All Components : Make sure to select all the components you want to export, such as pipelines, linked services, datasets, triggers, and global parameters. This will package everything into a single JSON-based ARM template. Download the Template : Save the template file to your computer. Deploy to Another Environment : You can import this template into another Data Factory by using the \"Deploy ARM template\" option in the \"Manage\" section of the target Data Factory, or by using PowerShell or the Azure CLI. 2. Use Git Integration for Source Control If your Azure Data Factory is connected to a Git repository like Azure DevOps or GitHub, all your pipeline components are already stored there. Clone the Repository : Download the repository to your computer. It contains all pipelines, datasets, linked services, triggers, and global parameters. Deploy to Another Environment : Use the files from the repository to deploy to another Azure Data Factory instance or to examine the structure locally. 3. Manually Export Pipeline and Related Resources Refer to the section Migrating Azure Data Factory Pipelines Without Rebuilding Them 4. Use Azure CLI or PowerShell You can also use Azure CLI or Azure PowerShell to export your Data Factory components. Using Azure CLI : az datafactory export-pipeline --factory-name <your_data_factory_name> \\ --resource-group <your_resource_group_name> \\ --name <your_pipeline_name> \\ --output-path <output_file_path> You can run similar commands to export linked services and datasets. 5. Deploy Everything to Another Environment Once you have your ARM template or exported JSON files: For ARM Templates : Use the \"Deploy ARM template\" option in the new Data Factory or deploy via Azure CLI or PowerShell. For Git Integration : Push the files to the repository associated with the target Data Factory. For Manual Exports : Recreate each component in the new Data Factory by importing their JSON files one by one. 6. Validate After Export After importing or deploying, make sure to: Validate the Pipeline : Check that all components are working correctly. Update Environment-Specific Settings : Adjust any settings that are specific to the new environment, such as service credentials, authentication details, or resource URLs. Summary To export your entire pipeline with all dependencies: Best Method : Use the ARM template export feature in Azure Data Factory. This method includes everything\u2014pipelines, datasets, linked services, triggers, and more. Alternative Method : Use Git integration for version control and easy transfer between environments. Manual Method : If necessary, export the pipeline and its components individually. Migrating Azure Data Factory Pipelines Without Rebuilding Them Transferring data pipelines and their related resources from one Azure Data Factory (ADF) to another can be done smoothly without the need to recreate anything. Here's how you can achieve this: Method 1: Moving a Single Pipeline to Another Data Factory Access Your Source Data Factory : Log in to the ADF instance where your existing pipelines are located. Select the Pipeline : Navigate to the specific pipeline you want to migrate. Download Support Files : Hover over the pipeline. Click on the action menu (three dots). Choose \"Download Support Files\" . Extract the Files : Unzip the downloaded folder. You'll find all the support files, including the pipeline, datasets, linked services, triggers, and integration runtimes. For Multiple Pipelines : If you need to transfer more pipelines, repeat steps 2 to 4 for each one and organize the files accordingly. Clone Your Git Repository : Use git clone to create a local copy of your Azure DevOps Git repository. Create a new branch from your main branch, for example, import_pipelines . Check the Branch Contents : Look into your new branch import_pipelines to see the existing folders and files. Place the Downloaded Resources : Copy the extracted files into the corresponding folders in your repository: datasets \u2192 place dataset files here. DataFactoryPipelines \u2192 place pipeline files here. trigger \u2192 place trigger files here. linkedService \u2192 place linked service files here. Commit and Push Changes : Use Git commands or a GUI tool to commit your changes. Push the import_pipelines branch to the develop branch of the target Data Factory. Publish in the New Data Factory : After pushing, go to the target ADF and publish the changes. Your migrated pipeline should now appear there. Method 2: Transferring All Pipelines to Another Data Factory Open Azure DevOps : Log in to your Azure DevOps account. Navigate to the Pipelines section. Create a CI/CD Pipeline : Set up a Continuous Integration/Continuous Deployment pipeline. Configure it to deploy resources from your develop branch to the desired environment (e.g., test ). Configure Deployment Settings : In the pipeline, specify the destination resource group and subscription where the new Data Factory resides. Provide the ARM (Azure Resource Manager) template from your source Data Factory. Run the Pipeline and Verify : Execute the pipeline to deploy the resources. Check the target Data Factory to ensure all pipelines and resources have been transferred.","title":"Export ADF Pipeline"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy/#how-to-understand-and-export-your-azure-data-factory-pipeline","text":"","title":"How to Understand and Export Your Azure Data Factory Pipeline"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy/#viewing-the-pipelines-json-code","text":"In Azure Data Factory (ADF), you can easily see the JSON code that represents your entire pipeline. To do this, simply click on the curly brackets {} icon. This will display the JSON structure of your pipeline, including all the activities, parameters, variables, and other settings.","title":"Viewing the Pipeline's JSON Code"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy/#limitations-of-the-json-representation","text":"However, please note that the JSON file you get by clicking the {} icon does not include everything you might need to fully understand your pipeline. Here are some important components that are usually missing: Linked Services : If your pipeline connects to external resources like Azure Data Lake, Azure SQL, or Databricks, the configuration details for these linked services are not included in the pipeline's JSON. You will need to check each linked service's JSON definition separately. Referenced Pipelines : If your pipeline calls other pipelines using activities like ExecutePipeline , the details of those pipelines are not included. You will have to access their JSON files individually. Global Parameters : Any global parameters your pipeline uses are not fully defined within its JSON file. You'll need to look at the global parameter configurations separately. Triggers : Triggers that start your pipeline (like those based on schedules or events) are not included in the pipeline's JSON. They are managed separately in the ADF interface. Data Flows : If your pipeline contains data flows, the JSON only references them. You'll need to view the details of the data flows separately. Azure Integration Runtime Configurations : These settings are also referenced but not fully included in the pipeline's JSON. In short , while the JSON representation gives you a detailed view of your pipeline's structure, you need to explore these additional configurations and resources to fully understand how your pipeline works.","title":"Limitations of the JSON Representation"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy/#how-to-copy-the-entire-adf-pipeline","text":"If you want to copy and export the entire pipeline along with all its related resources (like linked services, datasets, and any referenced pipelines), here's how you can do it:","title":"How to Copy the Entire ADF Pipeline"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy/#1-use-arm-template-export","text":"Azure Data Factory allows you to export your pipeline and all related resources as an Azure Resource Manager (ARM) Template . This method includes all dependencies and is the most comprehensive way to copy everything. Go to Your Data Factory : Open your Data Factory instance in the Azure Portal. Access the Manage Tab : Click on the \"Manage\" tab on the left side. Export ARM Template : Under the \"ARM template\" section, click on \"Export ARM template\" . Select All Components : Make sure to select all the components you want to export, such as pipelines, linked services, datasets, triggers, and global parameters. This will package everything into a single JSON-based ARM template. Download the Template : Save the template file to your computer. Deploy to Another Environment : You can import this template into another Data Factory by using the \"Deploy ARM template\" option in the \"Manage\" section of the target Data Factory, or by using PowerShell or the Azure CLI.","title":"1. Use ARM Template Export"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy/#2-use-git-integration-for-source-control","text":"If your Azure Data Factory is connected to a Git repository like Azure DevOps or GitHub, all your pipeline components are already stored there. Clone the Repository : Download the repository to your computer. It contains all pipelines, datasets, linked services, triggers, and global parameters. Deploy to Another Environment : Use the files from the repository to deploy to another Azure Data Factory instance or to examine the structure locally.","title":"2. Use Git Integration for Source Control"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy/#3-manually-export-pipeline-and-related-resources","text":"Refer to the section Migrating Azure Data Factory Pipelines Without Rebuilding Them","title":"3. Manually Export Pipeline and Related Resources"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy/#4-use-azure-cli-or-powershell","text":"You can also use Azure CLI or Azure PowerShell to export your Data Factory components. Using Azure CLI : az datafactory export-pipeline --factory-name <your_data_factory_name> \\ --resource-group <your_resource_group_name> \\ --name <your_pipeline_name> \\ --output-path <output_file_path> You can run similar commands to export linked services and datasets.","title":"4. Use Azure CLI or PowerShell"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy/#5-deploy-everything-to-another-environment","text":"Once you have your ARM template or exported JSON files: For ARM Templates : Use the \"Deploy ARM template\" option in the new Data Factory or deploy via Azure CLI or PowerShell. For Git Integration : Push the files to the repository associated with the target Data Factory. For Manual Exports : Recreate each component in the new Data Factory by importing their JSON files one by one.","title":"5. Deploy Everything to Another Environment"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy/#6-validate-after-export","text":"After importing or deploying, make sure to: Validate the Pipeline : Check that all components are working correctly. Update Environment-Specific Settings : Adjust any settings that are specific to the new environment, such as service credentials, authentication details, or resource URLs.","title":"6. Validate After Export"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy/#summary","text":"To export your entire pipeline with all dependencies: Best Method : Use the ARM template export feature in Azure Data Factory. This method includes everything\u2014pipelines, datasets, linked services, triggers, and more. Alternative Method : Use Git integration for version control and easy transfer between environments. Manual Method : If necessary, export the pipeline and its components individually.","title":"Summary"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy/#migrating-azure-data-factory-pipelines-without-rebuilding-them","text":"Transferring data pipelines and their related resources from one Azure Data Factory (ADF) to another can be done smoothly without the need to recreate anything. Here's how you can achieve this:","title":"Migrating Azure Data Factory Pipelines Without Rebuilding Them"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy/#method-1-moving-a-single-pipeline-to-another-data-factory","text":"Access Your Source Data Factory : Log in to the ADF instance where your existing pipelines are located. Select the Pipeline : Navigate to the specific pipeline you want to migrate. Download Support Files : Hover over the pipeline. Click on the action menu (three dots). Choose \"Download Support Files\" . Extract the Files : Unzip the downloaded folder. You'll find all the support files, including the pipeline, datasets, linked services, triggers, and integration runtimes. For Multiple Pipelines : If you need to transfer more pipelines, repeat steps 2 to 4 for each one and organize the files accordingly. Clone Your Git Repository : Use git clone to create a local copy of your Azure DevOps Git repository. Create a new branch from your main branch, for example, import_pipelines . Check the Branch Contents : Look into your new branch import_pipelines to see the existing folders and files. Place the Downloaded Resources : Copy the extracted files into the corresponding folders in your repository: datasets \u2192 place dataset files here. DataFactoryPipelines \u2192 place pipeline files here. trigger \u2192 place trigger files here. linkedService \u2192 place linked service files here. Commit and Push Changes : Use Git commands or a GUI tool to commit your changes. Push the import_pipelines branch to the develop branch of the target Data Factory. Publish in the New Data Factory : After pushing, go to the target ADF and publish the changes. Your migrated pipeline should now appear there.","title":"Method 1: Moving a Single Pipeline to Another Data Factory"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy/#method-2-transferring-all-pipelines-to-another-data-factory","text":"Open Azure DevOps : Log in to your Azure DevOps account. Navigate to the Pipelines section. Create a CI/CD Pipeline : Set up a Continuous Integration/Continuous Deployment pipeline. Configure it to deploy resources from your develop branch to the desired environment (e.g., test ). Configure Deployment Settings : In the pipeline, specify the destination resource group and subscription where the new Data Factory resides. Provide the ARM (Azure Resource Manager) template from your source Data Factory. Run the Pipeline and Verify : Execute the pipeline to deploy the resources. Check the target Data Factory to ensure all pipelines and resources have been transferred.","title":"Method 2: Transferring All Pipelines to Another Data Factory"},{"location":"Synapse-ADF/Q%26A/","text":"Table of contents {: .text-delta } 1. TOC {:toc} 100 Azure Synapse Analytics Questions Here, I've have put together a list of common questions about Azure Synapse Analytics - beginner to advanced levels. These questions are based on real-life situations and cover all the key features of Synapse. Each question comes with multiple-choice answers, and the correct answer is hidden. You can reveal the answer by highlighting the text. 1. What are the steps to create a new dedicated SQL pool in Azure Synapse Analytics? A) Navigate to the Synapse workspace -> Click on 'SQL pools' -> Click 'New' -> Provide a name and select performance level -> Click 'Review + create'. B) Navigate to Azure Portal -> Click on 'Resource Groups' -> Create new Resource Group -> Add SQL Pool. C) Open Synapse Studio -> Go to 'Workspace' -> Select 'Create SQL Pool' -> Configure settings. D) Use Azure CLI to run 'az sql pool create'. Answer: A) Navigate to the Synapse workspace -> Click on 'SQL pools' -> Click 'New' -> Provide a name and select performance level -> Click 'Review + create'. 2. Which built-in function can you use to load data from Azure Blob Storage into Azure Synapse? A) COPY B) BULK INSERT C) PolyBase D) Data Factory Answer: C) PolyBase 3. What is the basic SQL command to create a table in Synapse Analytics? A) CREATE NEW TABLE table_name (column1 datatype, column2 datatype, ...); B) CREATE TABLE table_name (column1 datatype, column2 datatype, ...); C) NEW TABLE table_name (column1 datatype, column2 datatype, ...); D) TABLE CREATE table_name (column1 datatype, column2 datatype, ...); Answer: B) CREATE TABLE table_name (column1 datatype, column2 datatype, ...); 4. How can you pause a dedicated SQL pool to save costs? A) In the Synapse workspace, go to the SQL pool you want to pause -> Click 'Pause'. B) Navigate to Azure Portal -> Click on 'Resource Groups' -> Select SQL Pool -> Click 'Pause'. C) Open Synapse Studio -> Go to 'Manage' -> Select 'Pause SQL Pool'. D) Use Azure CLI to run 'az sql pool pause'. Answer: A) In the Synapse workspace, go to the SQL pool you want to pause -> Click 'Pause'. 5. Which tool can you use for monitoring and troubleshooting performance in Synapse Analytics? A) Azure Monitor B) Synapse Studio C) Azure Advisor D) Log Analytics Answer: B) Synapse Studio 6. What feature should you enable to secure data in transit within Synapse Analytics? A) Advanced Threat Protection B) Firewall Rules C) Transparent Data Encryption (TDE) D) Always Encrypted Answer: C) Transparent Data Encryption (TDE) 7. What is the SQL command to create a view in Synapse Analytics? A) CREATE VIEW view_name AS SELECT column1, column2, ... FROM table_name; B) CREATE NEW VIEW view_name AS SELECT column1, column2, ... FROM table_name; C) VIEW CREATE view_name AS SELECT column1, column2, ... FROM table_name; D) CREATE TABLE view_name AS SELECT column1, column2, ... FROM table_name; Answer: A) CREATE VIEW view_name AS SELECT column1, column2, ... FROM table_name; 8. What is the SQL command to delete a table from your Synapse Analytics database? A) DELETE TABLE table_name; B) DROP TABLE table_name; C) REMOVE TABLE table_name; D) DESTROY TABLE table_name; Answer: B) DROP TABLE table_name; 9. What is a best practice for efficiently loading large datasets into Synapse Analytics? A) Use BULK INSERT B) Use Data Factory C) Use PolyBase or COPY statement D) Use SQL INSERT INTO Answer: C) Use PolyBase or COPY statement 10. What is the basic SQL command for joining two tables in Synapse Analytics? A) SELECT * FROM table1 INNER JOIN table2 ON table1.column = table2.column; B) SELECT * FROM table1 JOIN table2 ON table1.column = table2.column; C) SELECT * FROM table1, table2 WHERE table1.column = table2.column; D) SELECT * FROM table1 LEFT JOIN table2 ON table1.column = table2.column; Answer: B) SELECT * FROM table1 JOIN table2 ON table1.column = table2.column; 11. How can Azure Synapse help a retail company segment their customers based on purchase history? A) Using Synapse SQL pools to run clustering algorithms on customer data B) By creating a new SQL database C) By using Data Factory to transform the data D) By using Azure Monitor to track customer activities Answer: A) Using Synapse SQL pools to run clustering algorithms on customer data 12. Which features of Synapse Analytics can be utilized by a financial institution to detect fraudulent transactions in real-time? A) Synapse Pipelines to integrate with Azure Stream Analytics and Machine Learning models B) Using Azure Monitor C) Implementing SQL triggers D) Using Azure Functions Answer: A) Synapse Pipelines to integrate with Azure Stream Analytics and Machine Learning models 13. What is the SQL syntax to partition a table to improve query performance in Synapse Analytics? A) CREATE TABLE table_name (...) WITH (DISTRIBUTION = HASH(column_name)); B) CREATE PARTITIONED TABLE table_name (...) BY (column_name); C) PARTITION TABLE table_name (...) USING (HASH(column_name)); D) CREATE TABLE table_name (...) PARTITIONED BY (column_name); Answer: A) CREATE TABLE table_name (...) WITH (DISTRIBUTION = HASH(column_name)); 14. How do you implement row-level security in Synapse Analytics? A) By using security policies and predicates to filter data at the row level B) By creating separate tables for each user C) By using Azure RBAC D) By enabling Transparent Data Encryption (TDE) Answer: A) By using security policies and predicates to filter data at the row level 15. Which service can you use to automatically scale resources to handle a spike in data ingestion in Synapse Analytics? A) Synapse SQL pool's autoscale feature B) Azure Load Balancer C) Azure Auto Scale D) SQL Server Management Studio Answer: A) Synapse SQL pool's autoscale feature 16. What techniques can you use to optimize a frequently run query in Synapse Analytics? A) Indexing, distribution strategies, and partitioning B) Using temporary tables C) Creating a new database D) Increasing the database size Answer: A) Indexing, distribution strategies, and partitioning 17. What is the SQL command to grant access to a specific user to only one database in Synapse Analytics? A) GRANT CONNECT TO user_name; B) GRANT ALL TO user_name; C) GIVE ACCESS TO user_name; D) PROVIDE CONNECT TO user_name; Answer: A) GRANT CONNECT TO user_name; 18. How can you achieve data masking to protect sensitive data in Synapse Analytics? A) Using Dynamic Data Masking (DDM) B) Using Transparent Data Encryption (TDE) C) By encrypting the data at rest D) By using SQL triggers Answer: A) Using Dynamic Data Masking (DDM) 19. What is the SQL syntax for creating an external table to query data in Azure Data Lake? A) CREATE EXTERNAL TABLE table_name (...) WITH (LOCATION = '...', DATA_SOURCE = data_source_name, FILE_FORMAT = file_format_name); B) CREATE TABLE table_name EXTERNAL (...) WITH (LOCATION = '...', DATA_SOURCE = data_source_name); C) EXTERNAL CREATE TABLE table_name (...) USING (LOCATION = '...', DATA_SOURCE = data_source_name); D) CREATE EXTERNAL TABLE table_name (...) USING (LOCATION = '...', FILE_FORMAT = file_format_name); Answer: A) CREATE EXTERNAL TABLE table_name (...) WITH (LOCATION = '...', DATA_SOURCE = data_source_name, FILE_FORMAT = file_format_name); 20. What tool can you use for scheduling and managing regular data loads from an external source into Synapse Analytics? A) Azure Data Factory B) Azure DevOps C) Azure Monitor D) Azure Functions Answer: A) Azure Data Factory 21. Which service integrates with Synapse Analytics to implement a machine learning model? - A) Azure Machine Learning service - B) Azure Cognitive Services - C) Azure Functions - D) Azure Bot Service Answer: A) Azure Machine Learning service 22. Which tools can be used for setting up CI/CD pipelines for Synapse Analytics? A) Azure DevOps or GitHub Actions B) Azure Pipelines C) Azure Logic Apps D) Power Automate Answer: A) Azure DevOps or GitHub Actions 23. What feature can you use to encrypt data at rest in Synapse Analytics? A) Transparent Data Encryption (TDE) B) Always Encrypted C) SSL/TLS D) Row-Level Security Answer: A) Transparent Data Encryption (TDE) 24. Which language and environment would you use to execute a complex data transformation within Synapse? A) SQL or Spark within Synapse Studio B) Python within Azure Notebooks C) R within Azure ML Studio D) Java within Eclipse Answer: A) SQL or Spark within Synapse Studio 25. How can you integrate Synapse Analytics with Power BI? A) Connect Power BI to Synapse Analytics via the dedicated SQL pool connector B) Export data from Synapse Analytics to CSV and import into Power BI C) Use Azure Functions to transfer data to Power BI D) Use Data Factory to connect Power BI to Synapse Analytics Answer: A) Connect Power BI to Synapse Analytics via the dedicated SQL pool connector 26. What feature can you use to track data changes in your data warehouse within Synapse Analytics? A) Change Data Capture (CDC) B) SQL Triggers C) Data Audit D) Data Logs Answer: A) Change Data Capture (CDC) 27. Which component is best suited for real-time data processing in Synapse Analytics? A) Azure Synapse Data Explorer B) Azure Functions C) Azure Logic Apps D) Azure Databricks Answer: A) Azure Synapse Data Explorer 28. What tool can assist with data lineage tracking in Synapse Analytics? A) Azure Purview B) Azure Monitor C) Azure Log Analytics D) Azure Sentinel Answer: A) Azure Purview 29. Which Synapse feature allows for large-scale data analytics across various data sources? A) Synapse Pipelines B) Synapse SQL C) Azure Data Factory D) Azure Databricks Answer: A) Synapse Pipelines 30. How can you execute R and Python scripts in Synapse Analytics? A) Use Synapse Notebooks B) Use Azure Functions C) Use Azure Data Factory D) Use Azure ML Studio Answer: A) Use Synapse Notebooks 31. How can a healthcare company ensure data compliance and privacy for sensitive patient data stored in Synapse Analytics? A) Implementing Dynamic Data Masking and Row-Level Security B) Using Azure Monitor C) Implementing SQL triggers D) Using Azure Load Balancer Answer: A) Implementing Dynamic Data Masking and Row-Level Security 32. What feature in Synapse Analytics can help optimize query performance by distributing data across different nodes? A) Data Distribution B) Sharding C) Partitioning D) Replication Answer: A) Data Distribution 33. How can a financial services company perform complex time-series analysis on transaction data in Synapse Analytics? A) Using Spark Pools and time-series libraries B) Using Azure Logic Apps C) By running SQL scripts D) By exporting data to a third-party tool Answer: A) Using Spark Pools and time-series libraries 34. Which method allows you to automate data workflows and orchestrate data movement in and out of Synapse Analytics? A) Synapse Pipelines B) Azure Logic Apps C) Power Automate D) Azure Functions Answer: A) Synapse Pipelines 35. How can a manufacturing company analyze IoT sensor data in real-time using Synapse Analytics? A) Integrating Synapse with Azure Stream Analytics B) Using SQL triggers C) Storing data in Blob Storage D) Using Azure Functions Answer: A) Integrating Synapse with Azure Stream Analytics 36. How do you manage user permissions and access control in Synapse Analytics? A) Using Role-Based Access Control (RBAC) B) Creating separate databases C) Using SQL triggers D) By encrypting the data Answer: A) Using Role-Based Access Control (RBAC) 37. How can a retail company use Synapse Analytics to forecast sales trends? A) By integrating with Azure Machine Learning for predictive analytics B) By exporting data to Excel C) Using SQL scripts D) By creating new databases Answer: A) By integrating with Azure Machine Learning for predictive analytics 38. What is the best way to ensure data quality before loading it into Synapse Analytics? A) Using Data Flows in Synapse Pipelines to perform data cleansing and transformation B) Using SQL scripts C) By encrypting the data D) By storing data in Blob Storage Answer: A) Using Data Flows in Synapse Pipelines to perform data cleansing and transformation 39. How can you integrate data from various sources such as SQL databases, Blob Storage, and on-premises data into Synapse Analytics? A) Using Azure Data Factory with Synapse Pipelines B) Using SQL scripts C) Using Azure Functions D) Using Power Automate Answer: A) Using Azure Data Factory with Synapse Pipelines 40. Which feature allows a retail company to visualize and explore large datasets interactively within Synapse Analytics? A) Synapse Studio B) Azure Logic Apps C) Azure Monitor D) Azure DevOps Answer: A) Synapse Studio 41. How can you ensure high availability and disaster recovery for your Synapse Analytics environment? A) Implementing geo-redundant storage and failover groups B) Using SQL triggers C) Using Azure Monitor D) By encrypting the data Answer: A) Implementing geo-redundant storage and failover groups 42. What is the purpose of the Synapse SQL Serverless pool? A) To query data in data lakes without needing to provision dedicated resources B) To run continuous SQL scripts C) To monitor database performance D) To provide disaster recovery solutions Answer: A) To query data in data lakes without needing to provision dedicated resources 43. Which feature allows you to create and manage data integration pipelines within Synapse Analytics? A) Synapse Pipelines B) Azure Logic Apps C) Power Automate D) Azure Functions Answer: A) Synapse Pipelines 44. How can you optimize the performance of a data warehouse query in Synapse Analytics? A) By creating clustered columnstore indexes B) By increasing database size C) By running queries during off-peak hours D) By exporting data to a third-party tool Answer: A) By creating clustered columnstore indexes 45. What is the best practice for handling slowly changing dimensions in Synapse Analytics? A) Using a combination of SQL and Synapse Pipelines to track changes B) By creating new databases C) By using Azure Logic Apps D) By encrypting the data Answer: A) Using a combination of SQL and Synapse Pipelines to track changes 46. How can you implement real-time analytics on streaming data in Synapse Analytics? A) By integrating Synapse with Azure Stream Analytics B) Using SQL triggers C) By storing data in Blob Storage D) Using Azure Functions Answer: A) By integrating Synapse with Azure Stream Analytics 47. Which feature of Synapse Analytics can help you manage and control costs for your data warehouse? A) Autoscaling SQL pools B) Using SQL triggers C) By creating separate databases D) By encrypting the data Answer: A) Autoscaling SQL pools 48. What is the benefit of using dedicated SQL pools in Synapse Analytics? A) They provide optimized performance for large-scale analytics workloads B) They are always on and consume fewer resources C) They offer more security features D) They are easier to configure Answer: A) They provide optimized performance for large-scale analytics workloads 49. How can you automate the deployment of Synapse Analytics resources using infrastructure as code? A) Using Azure Resource Manager (ARM) templates B) By using Azure Monitor C) Using SQL triggers D) By encrypting the data Answer: A) Using Azure Resource Manager (ARM) templates 50. How can a global manufacturing company use Synapse Analytics to unify data from multiple regions for centralized analysis? A) By setting up a Synapse workspace with integrated data pipelines from each region B) By creating separate databases for each region C) Using Azure Logic Apps D) By storing data in Blob Storage Answer: A) By setting up a Synapse workspace with integrated data pipelines from each region 51. Which Azure service can be used alongside Synapse Analytics to provide data cataloging and governance capabilities? A) Azure Purview B) Azure Monitor C) Azure DevOps D) Azure Functions Answer: A) Azure Purview 52. How can you leverage Synapse Analytics to perform batch processing of large datasets? A) Using Synapse Pipelines with integrated Spark pools B) Using SQL triggers C) By creating new databases D) Using Power Automate Answer: A) Using Synapse Pipelines with integrated Spark pools 53. What is a common use case for using Synapse Studio in a data analytics workflow? A) Interactive data exploration and visualization B) SQL trigger management C) Database encryption configuration D) Network configuration Answer: A) Interactive data exploration and visualization 54. How can a company ensure their Synapse Analytics data warehouse is secure and compliant with industry standards? A) Implementing security best practices such as data encryption, access control, and monitoring B) Using SQL triggers C) By creating separate databases D) By using Azure Logic Apps Answer: A) Implementing security best practices such as data encryption, access control, and monitoring 55. What feature allows Synapse Analytics to handle complex ETL processes and data transformations? A) Synapse Pipelines with Data Flows B) SQL scripts C) Azure Functions D) Azure DevOps Answer: A) Synapse Pipelines with Data Flows 56. How can you monitor and troubleshoot Synapse Analytics performance issues? A) Using built-in monitoring tools in Synapse Studio and Azure Monitor B) Using SQL triggers C) By creating new databases D) By encrypting the data Answer: A) Using built-in monitoring tools in Synapse Studio and Azure Monitor 57. Which feature in Synapse Analytics helps you to seamlessly integrate data from on-premises and cloud sources? A) Data integration using Azure Data Factory with Synapse Pipelines B) SQL scripts C) Azure Logic Apps D) Power Automate Answer: A) Data integration using Azure Data Factory with Synapse Pipelines 58. What type of workloads are best suited for using dedicated SQL pools in Synapse Analytics? A) Large-scale analytical workloads B) Small transactional workloads C) Real-time streaming workloads D) Configuration management workloads Answer: A) Large-scale analytical workloads 59. How can a company use Synapse Analytics to support data science and machine learning initiatives? A) Integrating with Azure Machine Learning and leveraging Spark pools for data processing B) Using SQL triggers C) By creating new databases D) Using Azure Functions Answer: A) Integrating with Azure Machine Learning and leveraging Spark pools for data processing 60. Which tool can be used to create and manage complex data transformation workflows in Synapse Analytics? A) Synapse Pipelines B) Azure Monitor C) SQL Server Management Studio D) Azure DevOps Answer: A) Synapse Pipelines 61. What method can be used to optimize storage costs in Synapse Analytics? A) Using compression techniques and tiered storage options B) By increasing database size C) Using SQL triggers D) By creating separate databases Answer: A) Using compression techniques and tiered storage options 62. How can Synapse Analytics support a hybrid data architecture? A) By integrating with on-premises and cloud data sources using Synapse Pipelines B) By running SQL scripts only in the cloud C) By using Azure Logic Apps D) By creating separate databases Answer: A) By integrating with on-premises and cloud data sources using Synapse Pipelines 63. What feature in Synapse Analytics can be used to run interactive queries on large datasets without pre-provisioned resources? A) Synapse SQL Serverless pool B) Dedicated SQL pool C) Azure Functions D) SQL triggers Answer: A) Synapse SQL Serverless pool 64. Which tool can be used to automate the deployment and management of Synapse Analytics resources? A) Azure DevOps with CI/CD pipelines B) Azure Monitor C) SQL Server Management Studio D) Power Automate Answer: A) Azure DevOps with CI/CD pipelines 65. How can a financial company use Synapse Analytics to perform regulatory reporting? A) By using Synapse Pipelines to aggregate data and generate reports B) By exporting data to a third-party tool C) Using SQL triggers D) By creating separate databases Answer: A) By using Synapse Pipelines to aggregate data and generate reports 66. What is the benefit of using Spark pools in Synapse Analytics for big data processing? A) They provide a scalable and distributed environment for processing large datasets B) They offer more security features C) They are easier to configure D) They consume fewer resources Answer: A) They provide a scalable and distributed environment for processing large datasets 67. How can you integrate Synapse Analytics with other Azure services like Power BI and Azure ML? A) By using Synapse Studio connectors and integration features B) By using SQL triggers C) By creating separate databases D) By encrypting the data Answer: A) By using Synapse Studio connectors and integration features 68. What method can be used to ensure data quality in Synapse Analytics before analysis? A) Using Data Flows for data cleansing and transformation B) Using SQL scripts C) By increasing database size D) By storing data in Blob Storage Answer: A) Using Data Flows for data cleansing and transformation 69. How can you implement a data lakehouse architecture using Synapse Analytics? A) By combining Synapse SQL pools with Azure Data Lake Storage and Synapse Pipelines B) By using SQL triggers C) By creating new databases D) Using Azure Functions Answer: A) By combining Synapse SQL pools with Azure Data Lake Storage and Synapse Pipelines 70. What is the role of Synapse Studio in a data analytics workflow? A) It provides an integrated workspace for data exploration, preparation, management, and visualization B) It monitors network configuration C) It configures database encryption D) It manages SQL triggers Answer: A) It provides an integrated workspace for data exploration, preparation, management, and visualization 71. How can a healthcare company use Synapse Analytics to support population health management? A) By integrating with electronic health records (EHR) and using machine learning models for predictive analytics B) By exporting data to Excel C) Using SQL triggers D) By creating new databases Answer: A) By integrating with electronic health records (EHR) and using machine learning models for predictive analytics 72. How can Synapse Analytics help in real-time customer sentiment analysis for a retail company? A) By using Spark Streaming with Synapse Pipelines to analyze social media and customer feedback data B) By using SQL scripts C) By creating separate databases D) By storing data in Blob Storage Answer: A) By using Spark Streaming with Synapse Pipelines to analyze social media and customer feedback data 73. What is the best approach to handle data archiving and retention in Synapse Analytics? A) Implementing tiered storage options and lifecycle policies B) By using SQL triggers C) By increasing database size D) By creating new databases Answer: A) Implementing tiered storage options and lifecycle policies 74. How can you secure Synapse Analytics against unauthorized access and data breaches? A) By implementing Azure Active Directory integration and role-based access control (RBAC) B) By using SQL scripts C) By creating separate databases D) By using Azure Logic Apps Answer: A) By implementing Azure Active Directory integration and role-based access control (RBAC) 75. How can a logistics company optimize route planning and delivery schedules using Synapse Analytics? A) By integrating with Azure Machine Learning to develop predictive models B) By exporting data to a third-party tool C) Using SQL triggers D) By creating separate databases Answer: A) By integrating with Azure Machine Learning to develop predictive models 76. What feature in Synapse Analytics helps in managing data schema changes and version control? A) Schema management tools in Synapse Studio B) Using SQL triggers C) By creating new databases D) Using Power Automate Answer: A) Schema management tools in Synapse Studio 77. How can you perform sentiment analysis on customer reviews stored in Synapse Analytics? A) By using Azure Cognitive Services text analytics integrated with Synapse Pipelines B) By exporting data to Excel C) Using SQL scripts D) By creating separate databases Answer: A) By using Azure Cognitive Services text analytics integrated with Synapse Pipelines 78. How can Synapse Analytics support large-scale data migration from on-premises systems? A) By using Azure Data Migration Service and Synapse Pipelines B) By using SQL triggers C) By creating new databases D) By using Azure Functions Answer: A) By using Azure Data Migration Service and Synapse Pipelines 79. How can an e-commerce company personalize customer experiences using Synapse Analytics? A) By leveraging Synapse SQL and machine learning models to analyze customer behavior and preferences B) By exporting data to a third-party tool C) Using SQL scripts D) By creating separate databases Answer: A) By leveraging Synapse SQL and machine learning models to analyze customer behavior and preferences 80. What is the role of Data Flows in Synapse Analytics? A) To provide a visual interface for designing data transformation logic B) To manage SQL triggers C) To configure database encryption D) To monitor network configuration Answer: A) To provide a visual interface for designing data transformation logic 81. How can a company use Synapse Analytics to implement a single source of truth for their data? A) By centralizing data from various sources into a Synapse data warehouse and applying data governance practices B) By using SQL triggers C) By creating separate databases D) By exporting data to Excel Answer: A) By centralizing data from various sources into a Synapse data warehouse and applying data governance practices 82. What is the purpose of integrating Synapse Analytics with Azure Purview? A) To enhance data cataloging, governance, and lineage tracking B) To manage SQL triggers C) To configure database encryption D) To monitor network configuration Answer: A) To enhance data cataloging, governance, and lineage tracking 83. How can Synapse Analytics help a media company analyze viewer engagement data? A) By using Synapse SQL and Spark pools to process and analyze large volumes of viewer data B) By exporting data to a third-party tool C) Using SQL triggers D) By creating separate databases Answer: A) By using Synapse SQL and Spark pools to process and analyze large volumes of viewer data 84. What is the advantage of using dedicated SQL pools over serverless SQL pools in Synapse Analytics? A) Dedicated SQL pools provide better performance for high concurrency and complex queries B) Serverless SQL pools offer better security features C) Dedicated SQL pools consume fewer resources D) Serverless SQL pools are easier to configure Answer: A) Dedicated SQL pools provide better performance for high concurrency and complex queries 85. How can a financial institution detect anomalies in transaction data using Synapse Analytics? A) By integrating with Azure Machine Learning for anomaly detection models B) By exporting data to a third-party tool C) Using SQL triggers D) By creating separate databases Answer: A) By integrating with Azure Machine Learning for anomaly detection models 86. What is the best practice for loading historical data into Synapse Analytics for analysis? A) Using PolyBase or COPY statement to load large volumes of data efficiently B) By using SQL scripts C) By creating new databases D) By encrypting the data Answer: A) Using PolyBase or COPY statement to load large volumes of data efficiently 87. How can Synapse Analytics help in optimizing supply chain operations for a manufacturing company? A) By analyzing production and logistics data using Synapse SQL and machine learning models B) By exporting data to Excel C) Using SQL scripts D) By creating separate databases Answer: A) By analyzing production and logistics data using Synapse SQL and machine learning models 88. How can you automate the backup and recovery of Synapse Analytics data? A) By using Azure Backup and Recovery solutions B) Using SQL scripts C) By creating new databases D) Using Power Automate Answer: A) By using Azure Backup and Recovery solutions 89. How can a company ensure their Synapse Analytics data warehouse meets compliance requirements? A) By implementing data encryption, access control, and audit logging B) By using SQL triggers C) By creating separate databases D) By exporting data to a third-party tool Answer: A) By implementing data encryption, access control, and audit logging 90. How can Synapse Analytics be used to support customer segmentation and targeting for a marketing campaign? A) By using Synapse SQL to analyze customer data and identify segments based on behavior and demographics B) By exporting data to Excel C) Using SQL scripts D) By creating separate databases Answer: A) By using Synapse SQL to analyze customer data and identify segments based on behavior and demographics 91. What is the benefit of using Synapse Studio for collaborative data analytics projects? A) It provides a unified workspace for multiple users to collaborate on data preparation, management, and analysis B) It monitors network configuration C) It manages SQL triggers D) It configures database encryption Answer: A) It provides a unified workspace for multiple users to collaborate on data preparation, management, and analysis 92. How can Synapse Analytics be integrated with third-party BI tools for advanced reporting? A) By using data connectors and APIs to link Synapse data with BI tools like Tableau or Qlik B) By exporting data to Excel C) Using SQL scripts D) By creating separate databases Answer: A) By using data connectors and APIs to link Synapse data with BI tools like Tableau or Qlik 93. How can a retail company use Synapse Analytics to optimize inventory management? A) By analyzing sales and inventory data to predict demand and optimize stock levels B) By exporting data to a third-party tool C) Using SQL triggers D) By creating separate databases Answer: A) By analyzing sales and inventory data to predict demand and optimize stock levels 94. What feature in Synapse Analytics allows you to schedule and automate data processing tasks? A) Synapse Pipelines B) SQL Server Management Studio C) Azure Monitor D) Power Automate Answer: A) Synapse Pipelines 95. How can a company use Synapse Analytics to perform cross-regional data analysis? A) By setting up data replication and using Synapse SQL to query data from different regions B) By using SQL triggers C) By creating separate databases D) By exporting data to Excel Answer: A) By setting up data replication and using Synapse SQL to query data from different regions 96. How can you improve query performance in Synapse Analytics when dealing with large datasets? A) By optimizing distribution keys and creating columnstore indexes B) By increasing database size C) Using SQL triggers D) By creating separate databases Answer: A) By optimizing distribution keys and creating columnstore indexes 97. How can Synapse Analytics support predictive maintenance for industrial equipment? A) By integrating with IoT data sources and using machine learning models for predictive analytics B) By exporting data to Excel C) Using SQL scripts D) By creating separate databases Answer: A) By integrating with IoT data sources and using machine learning models for predictive analytics 98. What is the best practice for managing large-scale data transformations in Synapse Analytics? A) Using Data Flows and Spark pools for efficient data processing B) By using SQL triggers C) By increasing database size D) By creating new databases Answer: A) Using Data Flows and Spark pools for efficient data processing 99. How can Synapse Analytics help in developing a 360-degree view of the customer? A) By consolidating data from various sources and using analytics to provide insights into customer behavior B) By exporting data to a third-party tool C) Using SQL scripts D) By creating separate databases Answer: A) By consolidating data from various sources and using analytics to provide insights into customer behavior 100. How can Synapse Analytics be used to support real-time business intelligence for an e-commerce platform? A) By integrating with streaming data sources and using serverless SQL pools for real-time querying B) By using SQL triggers C) By creating separate databases D) By exporting data to Excel Answer: A) By integrating with streaming data sources and using serverless SQL pools for real-time querying","title":"100 Synapse FAQs"},{"location":"Synapse-ADF/Q%26A/#100-azure-synapse-analytics-questions","text":"Here, I've have put together a list of common questions about Azure Synapse Analytics - beginner to advanced levels. These questions are based on real-life situations and cover all the key features of Synapse. Each question comes with multiple-choice answers, and the correct answer is hidden. You can reveal the answer by highlighting the text.","title":"100 Azure Synapse Analytics Questions"},{"location":"Synapse-ADF/Q%26A/#1-what-are-the-steps-to-create-a-new-dedicated-sql-pool-in-azure-synapse-analytics","text":"A) Navigate to the Synapse workspace -> Click on 'SQL pools' -> Click 'New' -> Provide a name and select performance level -> Click 'Review + create'. B) Navigate to Azure Portal -> Click on 'Resource Groups' -> Create new Resource Group -> Add SQL Pool. C) Open Synapse Studio -> Go to 'Workspace' -> Select 'Create SQL Pool' -> Configure settings. D) Use Azure CLI to run 'az sql pool create'. Answer: A) Navigate to the Synapse workspace -> Click on 'SQL pools' -> Click 'New' -> Provide a name and select performance level -> Click 'Review + create'.","title":"1. What are the steps to create a new dedicated SQL pool in Azure Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#2-which-built-in-function-can-you-use-to-load-data-from-azure-blob-storage-into-azure-synapse","text":"A) COPY B) BULK INSERT C) PolyBase D) Data Factory Answer: C) PolyBase","title":"2. Which built-in function can you use to load data from Azure Blob Storage into Azure Synapse?"},{"location":"Synapse-ADF/Q%26A/#3-what-is-the-basic-sql-command-to-create-a-table-in-synapse-analytics","text":"A) CREATE NEW TABLE table_name (column1 datatype, column2 datatype, ...); B) CREATE TABLE table_name (column1 datatype, column2 datatype, ...); C) NEW TABLE table_name (column1 datatype, column2 datatype, ...); D) TABLE CREATE table_name (column1 datatype, column2 datatype, ...); Answer: B) CREATE TABLE table_name (column1 datatype, column2 datatype, ...);","title":"3. What is the basic SQL command to create a table in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#4-how-can-you-pause-a-dedicated-sql-pool-to-save-costs","text":"A) In the Synapse workspace, go to the SQL pool you want to pause -> Click 'Pause'. B) Navigate to Azure Portal -> Click on 'Resource Groups' -> Select SQL Pool -> Click 'Pause'. C) Open Synapse Studio -> Go to 'Manage' -> Select 'Pause SQL Pool'. D) Use Azure CLI to run 'az sql pool pause'. Answer: A) In the Synapse workspace, go to the SQL pool you want to pause -> Click 'Pause'.","title":"4. How can you pause a dedicated SQL pool to save costs?"},{"location":"Synapse-ADF/Q%26A/#5-which-tool-can-you-use-for-monitoring-and-troubleshooting-performance-in-synapse-analytics","text":"A) Azure Monitor B) Synapse Studio C) Azure Advisor D) Log Analytics Answer: B) Synapse Studio","title":"5. Which tool can you use for monitoring and troubleshooting performance in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#6-what-feature-should-you-enable-to-secure-data-in-transit-within-synapse-analytics","text":"A) Advanced Threat Protection B) Firewall Rules C) Transparent Data Encryption (TDE) D) Always Encrypted Answer: C) Transparent Data Encryption (TDE)","title":"6. What feature should you enable to secure data in transit within Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#7-what-is-the-sql-command-to-create-a-view-in-synapse-analytics","text":"A) CREATE VIEW view_name AS SELECT column1, column2, ... FROM table_name; B) CREATE NEW VIEW view_name AS SELECT column1, column2, ... FROM table_name; C) VIEW CREATE view_name AS SELECT column1, column2, ... FROM table_name; D) CREATE TABLE view_name AS SELECT column1, column2, ... FROM table_name; Answer: A) CREATE VIEW view_name AS SELECT column1, column2, ... FROM table_name;","title":"7. What is the SQL command to create a view in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#8-what-is-the-sql-command-to-delete-a-table-from-your-synapse-analytics-database","text":"A) DELETE TABLE table_name; B) DROP TABLE table_name; C) REMOVE TABLE table_name; D) DESTROY TABLE table_name; Answer: B) DROP TABLE table_name;","title":"8. What is the SQL command to delete a table from your Synapse Analytics database?"},{"location":"Synapse-ADF/Q%26A/#9-what-is-a-best-practice-for-efficiently-loading-large-datasets-into-synapse-analytics","text":"A) Use BULK INSERT B) Use Data Factory C) Use PolyBase or COPY statement D) Use SQL INSERT INTO Answer: C) Use PolyBase or COPY statement","title":"9. What is a best practice for efficiently loading large datasets into Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#10-what-is-the-basic-sql-command-for-joining-two-tables-in-synapse-analytics","text":"A) SELECT * FROM table1 INNER JOIN table2 ON table1.column = table2.column; B) SELECT * FROM table1 JOIN table2 ON table1.column = table2.column; C) SELECT * FROM table1, table2 WHERE table1.column = table2.column; D) SELECT * FROM table1 LEFT JOIN table2 ON table1.column = table2.column; Answer: B) SELECT * FROM table1 JOIN table2 ON table1.column = table2.column;","title":"10. What is the basic SQL command for joining two tables in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#11-how-can-azure-synapse-help-a-retail-company-segment-their-customers-based-on-purchase-history","text":"A) Using Synapse SQL pools to run clustering algorithms on customer data B) By creating a new SQL database C) By using Data Factory to transform the data D) By using Azure Monitor to track customer activities Answer: A) Using Synapse SQL pools to run clustering algorithms on customer data","title":"11. How can Azure Synapse help a retail company segment their customers based on purchase history?"},{"location":"Synapse-ADF/Q%26A/#12-which-features-of-synapse-analytics-can-be-utilized-by-a-financial-institution-to-detect-fraudulent-transactions-in-real-time","text":"A) Synapse Pipelines to integrate with Azure Stream Analytics and Machine Learning models B) Using Azure Monitor C) Implementing SQL triggers D) Using Azure Functions Answer: A) Synapse Pipelines to integrate with Azure Stream Analytics and Machine Learning models","title":"12. Which features of Synapse Analytics can be utilized by a financial institution to detect fraudulent transactions in real-time?"},{"location":"Synapse-ADF/Q%26A/#13-what-is-the-sql-syntax-to-partition-a-table-to-improve-query-performance-in-synapse-analytics","text":"A) CREATE TABLE table_name (...) WITH (DISTRIBUTION = HASH(column_name)); B) CREATE PARTITIONED TABLE table_name (...) BY (column_name); C) PARTITION TABLE table_name (...) USING (HASH(column_name)); D) CREATE TABLE table_name (...) PARTITIONED BY (column_name); Answer: A) CREATE TABLE table_name (...) WITH (DISTRIBUTION = HASH(column_name));","title":"13. What is the SQL syntax to partition a table to improve query performance in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#14-how-do-you-implement-row-level-security-in-synapse-analytics","text":"A) By using security policies and predicates to filter data at the row level B) By creating separate tables for each user C) By using Azure RBAC D) By enabling Transparent Data Encryption (TDE) Answer: A) By using security policies and predicates to filter data at the row level","title":"14. How do you implement row-level security in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#15-which-service-can-you-use-to-automatically-scale-resources-to-handle-a-spike-in-data-ingestion-in-synapse-analytics","text":"A) Synapse SQL pool's autoscale feature B) Azure Load Balancer C) Azure Auto Scale D) SQL Server Management Studio Answer: A) Synapse SQL pool's autoscale feature","title":"15. Which service can you use to automatically scale resources to handle a spike in data ingestion in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#16-what-techniques-can-you-use-to-optimize-a-frequently-run-query-in-synapse-analytics","text":"A) Indexing, distribution strategies, and partitioning B) Using temporary tables C) Creating a new database D) Increasing the database size Answer: A) Indexing, distribution strategies, and partitioning","title":"16. What techniques can you use to optimize a frequently run query in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#17-what-is-the-sql-command-to-grant-access-to-a-specific-user-to-only-one-database-in-synapse-analytics","text":"A) GRANT CONNECT TO user_name; B) GRANT ALL TO user_name; C) GIVE ACCESS TO user_name; D) PROVIDE CONNECT TO user_name; Answer: A) GRANT CONNECT TO user_name;","title":"17. What is the SQL command to grant access to a specific user to only one database in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#18-how-can-you-achieve-data-masking-to-protect-sensitive-data-in-synapse-analytics","text":"A) Using Dynamic Data Masking (DDM) B) Using Transparent Data Encryption (TDE) C) By encrypting the data at rest D) By using SQL triggers Answer: A) Using Dynamic Data Masking (DDM)","title":"18. How can you achieve data masking to protect sensitive data in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#19-what-is-the-sql-syntax-for-creating-an-external-table-to-query-data-in-azure-data-lake","text":"A) CREATE EXTERNAL TABLE table_name (...) WITH (LOCATION = '...', DATA_SOURCE = data_source_name, FILE_FORMAT = file_format_name); B) CREATE TABLE table_name EXTERNAL (...) WITH (LOCATION = '...', DATA_SOURCE = data_source_name); C) EXTERNAL CREATE TABLE table_name (...) USING (LOCATION = '...', DATA_SOURCE = data_source_name); D) CREATE EXTERNAL TABLE table_name (...) USING (LOCATION = '...', FILE_FORMAT = file_format_name); Answer: A) CREATE EXTERNAL TABLE table_name (...) WITH (LOCATION = '...', DATA_SOURCE = data_source_name, FILE_FORMAT = file_format_name);","title":"19. What is the SQL syntax for creating an external table to query data in Azure Data Lake?"},{"location":"Synapse-ADF/Q%26A/#20-what-tool-can-you-use-for-scheduling-and-managing-regular-data-loads-from-an-external-source-into-synapse-analytics","text":"A) Azure Data Factory B) Azure DevOps C) Azure Monitor D) Azure Functions Answer: A) Azure Data Factory","title":"20. What tool can you use for scheduling and managing regular data loads from an external source into Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#21-which-service-integrates-with","text":"Synapse Analytics to implement a machine learning model? - A) Azure Machine Learning service - B) Azure Cognitive Services - C) Azure Functions - D) Azure Bot Service Answer: A) Azure Machine Learning service","title":"21. Which service integrates with"},{"location":"Synapse-ADF/Q%26A/#22-which-tools-can-be-used-for-setting-up-cicd-pipelines-for-synapse-analytics","text":"A) Azure DevOps or GitHub Actions B) Azure Pipelines C) Azure Logic Apps D) Power Automate Answer: A) Azure DevOps or GitHub Actions","title":"22. Which tools can be used for setting up CI/CD pipelines for Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#23-what-feature-can-you-use-to-encrypt-data-at-rest-in-synapse-analytics","text":"A) Transparent Data Encryption (TDE) B) Always Encrypted C) SSL/TLS D) Row-Level Security Answer: A) Transparent Data Encryption (TDE)","title":"23. What feature can you use to encrypt data at rest in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#24-which-language-and-environment-would-you-use-to-execute-a-complex-data-transformation-within-synapse","text":"A) SQL or Spark within Synapse Studio B) Python within Azure Notebooks C) R within Azure ML Studio D) Java within Eclipse Answer: A) SQL or Spark within Synapse Studio","title":"24. Which language and environment would you use to execute a complex data transformation within Synapse?"},{"location":"Synapse-ADF/Q%26A/#25-how-can-you-integrate-synapse-analytics-with-power-bi","text":"A) Connect Power BI to Synapse Analytics via the dedicated SQL pool connector B) Export data from Synapse Analytics to CSV and import into Power BI C) Use Azure Functions to transfer data to Power BI D) Use Data Factory to connect Power BI to Synapse Analytics Answer: A) Connect Power BI to Synapse Analytics via the dedicated SQL pool connector","title":"25. How can you integrate Synapse Analytics with Power BI?"},{"location":"Synapse-ADF/Q%26A/#26-what-feature-can-you-use-to-track-data-changes-in-your-data-warehouse-within-synapse-analytics","text":"A) Change Data Capture (CDC) B) SQL Triggers C) Data Audit D) Data Logs Answer: A) Change Data Capture (CDC)","title":"26. What feature can you use to track data changes in your data warehouse within Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#27-which-component-is-best-suited-for-real-time-data-processing-in-synapse-analytics","text":"A) Azure Synapse Data Explorer B) Azure Functions C) Azure Logic Apps D) Azure Databricks Answer: A) Azure Synapse Data Explorer","title":"27. Which component is best suited for real-time data processing in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#28-what-tool-can-assist-with-data-lineage-tracking-in-synapse-analytics","text":"A) Azure Purview B) Azure Monitor C) Azure Log Analytics D) Azure Sentinel Answer: A) Azure Purview","title":"28. What tool can assist with data lineage tracking in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#29-which-synapse-feature-allows-for-large-scale-data-analytics-across-various-data-sources","text":"A) Synapse Pipelines B) Synapse SQL C) Azure Data Factory D) Azure Databricks Answer: A) Synapse Pipelines","title":"29. Which Synapse feature allows for large-scale data analytics across various data sources?"},{"location":"Synapse-ADF/Q%26A/#30-how-can-you-execute-r-and-python-scripts-in-synapse-analytics","text":"A) Use Synapse Notebooks B) Use Azure Functions C) Use Azure Data Factory D) Use Azure ML Studio Answer: A) Use Synapse Notebooks","title":"30. How can you execute R and Python scripts in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#31-how-can-a-healthcare-company-ensure-data-compliance-and-privacy-for-sensitive-patient-data-stored-in-synapse-analytics","text":"A) Implementing Dynamic Data Masking and Row-Level Security B) Using Azure Monitor C) Implementing SQL triggers D) Using Azure Load Balancer Answer: A) Implementing Dynamic Data Masking and Row-Level Security","title":"31. How can a healthcare company ensure data compliance and privacy for sensitive patient data stored in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#32-what-feature-in-synapse-analytics-can-help-optimize-query-performance-by-distributing-data-across-different-nodes","text":"A) Data Distribution B) Sharding C) Partitioning D) Replication Answer: A) Data Distribution","title":"32. What feature in Synapse Analytics can help optimize query performance by distributing data across different nodes?"},{"location":"Synapse-ADF/Q%26A/#33-how-can-a-financial-services-company-perform-complex-time-series-analysis-on-transaction-data-in-synapse-analytics","text":"A) Using Spark Pools and time-series libraries B) Using Azure Logic Apps C) By running SQL scripts D) By exporting data to a third-party tool Answer: A) Using Spark Pools and time-series libraries","title":"33. How can a financial services company perform complex time-series analysis on transaction data in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#34-which-method-allows-you-to-automate-data-workflows-and-orchestrate-data-movement-in-and-out-of-synapse-analytics","text":"A) Synapse Pipelines B) Azure Logic Apps C) Power Automate D) Azure Functions Answer: A) Synapse Pipelines","title":"34. Which method allows you to automate data workflows and orchestrate data movement in and out of Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#35-how-can-a-manufacturing-company-analyze-iot-sensor-data-in-real-time-using-synapse-analytics","text":"A) Integrating Synapse with Azure Stream Analytics B) Using SQL triggers C) Storing data in Blob Storage D) Using Azure Functions Answer: A) Integrating Synapse with Azure Stream Analytics","title":"35. How can a manufacturing company analyze IoT sensor data in real-time using Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#36-how-do-you-manage-user-permissions-and-access-control-in-synapse-analytics","text":"A) Using Role-Based Access Control (RBAC) B) Creating separate databases C) Using SQL triggers D) By encrypting the data Answer: A) Using Role-Based Access Control (RBAC)","title":"36. How do you manage user permissions and access control in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#37-how-can-a-retail-company-use-synapse-analytics-to-forecast-sales-trends","text":"A) By integrating with Azure Machine Learning for predictive analytics B) By exporting data to Excel C) Using SQL scripts D) By creating new databases Answer: A) By integrating with Azure Machine Learning for predictive analytics","title":"37. How can a retail company use Synapse Analytics to forecast sales trends?"},{"location":"Synapse-ADF/Q%26A/#38-what-is-the-best-way-to-ensure-data-quality-before-loading-it-into-synapse-analytics","text":"A) Using Data Flows in Synapse Pipelines to perform data cleansing and transformation B) Using SQL scripts C) By encrypting the data D) By storing data in Blob Storage Answer: A) Using Data Flows in Synapse Pipelines to perform data cleansing and transformation","title":"38. What is the best way to ensure data quality before loading it into Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#39-how-can-you-integrate-data-from-various-sources-such-as-sql-databases-blob-storage-and-on-premises-data-into-synapse-analytics","text":"A) Using Azure Data Factory with Synapse Pipelines B) Using SQL scripts C) Using Azure Functions D) Using Power Automate Answer: A) Using Azure Data Factory with Synapse Pipelines","title":"39. How can you integrate data from various sources such as SQL databases, Blob Storage, and on-premises data into Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#40-which-feature-allows-a-retail-company-to-visualize-and-explore-large-datasets-interactively-within-synapse-analytics","text":"A) Synapse Studio B) Azure Logic Apps C) Azure Monitor D) Azure DevOps Answer: A) Synapse Studio","title":"40. Which feature allows a retail company to visualize and explore large datasets interactively within Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#41-how-can-you-ensure-high-availability-and-disaster-recovery-for-your-synapse-analytics-environment","text":"A) Implementing geo-redundant storage and failover groups B) Using SQL triggers C) Using Azure Monitor D) By encrypting the data Answer: A) Implementing geo-redundant storage and failover groups","title":"41. How can you ensure high availability and disaster recovery for your Synapse Analytics environment?"},{"location":"Synapse-ADF/Q%26A/#42-what-is-the-purpose-of-the-synapse-sql-serverless-pool","text":"A) To query data in data lakes without needing to provision dedicated resources B) To run continuous SQL scripts C) To monitor database performance D) To provide disaster recovery solutions Answer: A) To query data in data lakes without needing to provision dedicated resources","title":"42. What is the purpose of the Synapse SQL Serverless pool?"},{"location":"Synapse-ADF/Q%26A/#43-which-feature-allows-you-to-create-and-manage-data-integration-pipelines-within-synapse-analytics","text":"A) Synapse Pipelines B) Azure Logic Apps C) Power Automate D) Azure Functions Answer: A) Synapse Pipelines","title":"43. Which feature allows you to create and manage data integration pipelines within Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#44-how-can-you-optimize-the-performance-of-a-data-warehouse-query-in-synapse-analytics","text":"A) By creating clustered columnstore indexes B) By increasing database size C) By running queries during off-peak hours D) By exporting data to a third-party tool Answer: A) By creating clustered columnstore indexes","title":"44. How can you optimize the performance of a data warehouse query in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#45-what-is-the-best-practice-for-handling-slowly-changing-dimensions-in-synapse-analytics","text":"A) Using a combination of SQL and Synapse Pipelines to track changes B) By creating new databases C) By using Azure Logic Apps D) By encrypting the data Answer: A) Using a combination of SQL and Synapse Pipelines to track changes","title":"45. What is the best practice for handling slowly changing dimensions in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#46-how-can-you-implement-real-time-analytics-on-streaming-data-in-synapse-analytics","text":"A) By integrating Synapse with Azure Stream Analytics B) Using SQL triggers C) By storing data in Blob Storage D) Using Azure Functions Answer: A) By integrating Synapse with Azure Stream Analytics","title":"46. How can you implement real-time analytics on streaming data in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#47-which-feature-of-synapse-analytics-can-help-you-manage-and-control-costs-for-your-data-warehouse","text":"A) Autoscaling SQL pools B) Using SQL triggers C) By creating separate databases D) By encrypting the data Answer: A) Autoscaling SQL pools","title":"47. Which feature of Synapse Analytics can help you manage and control costs for your data warehouse?"},{"location":"Synapse-ADF/Q%26A/#48-what-is-the-benefit-of-using-dedicated-sql-pools-in-synapse-analytics","text":"A) They provide optimized performance for large-scale analytics workloads B) They are always on and consume fewer resources C) They offer more security features D) They are easier to configure Answer: A) They provide optimized performance for large-scale analytics workloads","title":"48. What is the benefit of using dedicated SQL pools in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#49-how-can-you-automate-the-deployment-of-synapse-analytics-resources-using-infrastructure-as-code","text":"A) Using Azure Resource Manager (ARM) templates B) By using Azure Monitor C) Using SQL triggers D) By encrypting the data Answer: A) Using Azure Resource Manager (ARM) templates","title":"49. How can you automate the deployment of Synapse Analytics resources using infrastructure as code?"},{"location":"Synapse-ADF/Q%26A/#50-how-can-a-global-manufacturing-company-use-synapse-analytics-to-unify-data-from-multiple-regions-for-centralized-analysis","text":"A) By setting up a Synapse workspace with integrated data pipelines from each region B) By creating separate databases for each region C) Using Azure Logic Apps D) By storing data in Blob Storage Answer: A) By setting up a Synapse workspace with integrated data pipelines from each region","title":"50. How can a global manufacturing company use Synapse Analytics to unify data from multiple regions for centralized analysis?"},{"location":"Synapse-ADF/Q%26A/#51-which-azure-service-can-be-used-alongside-synapse-analytics-to-provide-data-cataloging-and-governance-capabilities","text":"A) Azure Purview B) Azure Monitor C) Azure DevOps D) Azure Functions Answer: A) Azure Purview","title":"51. Which Azure service can be used alongside Synapse Analytics to provide data cataloging and governance capabilities?"},{"location":"Synapse-ADF/Q%26A/#52-how-can-you-leverage-synapse-analytics-to-perform-batch-processing-of-large-datasets","text":"A) Using Synapse Pipelines with integrated Spark pools B) Using SQL triggers C) By creating new databases D) Using Power Automate Answer: A) Using Synapse Pipelines with integrated Spark pools","title":"52. How can you leverage Synapse Analytics to perform batch processing of large datasets?"},{"location":"Synapse-ADF/Q%26A/#53-what-is-a-common-use-case-for-using-synapse-studio-in-a-data-analytics-workflow","text":"A) Interactive data exploration and visualization B) SQL trigger management C) Database encryption configuration D) Network configuration Answer: A) Interactive data exploration and visualization","title":"53. What is a common use case for using Synapse Studio in a data analytics workflow?"},{"location":"Synapse-ADF/Q%26A/#54-how-can-a-company-ensure-their-synapse-analytics-data-warehouse-is-secure-and-compliant-with-industry-standards","text":"A) Implementing security best practices such as data encryption, access control, and monitoring B) Using SQL triggers C) By creating separate databases D) By using Azure Logic Apps Answer: A) Implementing security best practices such as data encryption, access control, and monitoring","title":"54. How can a company ensure their Synapse Analytics data warehouse is secure and compliant with industry standards?"},{"location":"Synapse-ADF/Q%26A/#55-what-feature-allows-synapse-analytics-to-handle-complex-etl-processes-and-data-transformations","text":"A) Synapse Pipelines with Data Flows B) SQL scripts C) Azure Functions D) Azure DevOps Answer: A) Synapse Pipelines with Data Flows","title":"55. What feature allows Synapse Analytics to handle complex ETL processes and data transformations?"},{"location":"Synapse-ADF/Q%26A/#56-how-can-you-monitor-and-troubleshoot-synapse-analytics-performance-issues","text":"A) Using built-in monitoring tools in Synapse Studio and Azure Monitor B) Using SQL triggers C) By creating new databases D) By encrypting the data Answer: A) Using built-in monitoring tools in Synapse Studio and Azure Monitor","title":"56. How can you monitor and troubleshoot Synapse Analytics performance issues?"},{"location":"Synapse-ADF/Q%26A/#57-which-feature-in-synapse-analytics-helps-you-to-seamlessly-integrate-data-from-on-premises-and-cloud-sources","text":"A) Data integration using Azure Data Factory with Synapse Pipelines B) SQL scripts C) Azure Logic Apps D) Power Automate Answer: A) Data integration using Azure Data Factory with Synapse Pipelines","title":"57. Which feature in Synapse Analytics helps you to seamlessly integrate data from on-premises and cloud sources?"},{"location":"Synapse-ADF/Q%26A/#58-what-type-of-workloads-are-best-suited-for-using-dedicated-sql-pools-in-synapse-analytics","text":"A) Large-scale analytical workloads B) Small transactional workloads C) Real-time streaming workloads D) Configuration management workloads Answer: A) Large-scale analytical workloads","title":"58. What type of workloads are best suited for using dedicated SQL pools in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#59-how-can-a-company-use-synapse-analytics-to-support-data-science-and-machine-learning-initiatives","text":"A) Integrating with Azure Machine Learning and leveraging Spark pools for data processing B) Using SQL triggers C) By creating new databases D) Using Azure Functions Answer: A) Integrating with Azure Machine Learning and leveraging Spark pools for data processing","title":"59. How can a company use Synapse Analytics to support data science and machine learning initiatives?"},{"location":"Synapse-ADF/Q%26A/#60-which-tool-can-be-used-to-create-and-manage-complex-data-transformation-workflows-in-synapse-analytics","text":"A) Synapse Pipelines B) Azure Monitor C) SQL Server Management Studio D) Azure DevOps Answer: A) Synapse Pipelines","title":"60. Which tool can be used to create and manage complex data transformation workflows in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#61-what-method-can-be-used-to-optimize-storage-costs-in-synapse-analytics","text":"A) Using compression techniques and tiered storage options B) By increasing database size C) Using SQL triggers D) By creating separate databases Answer: A) Using compression techniques and tiered storage options","title":"61. What method can be used to optimize storage costs in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#62-how-can-synapse-analytics-support-a-hybrid-data-architecture","text":"A) By integrating with on-premises and cloud data sources using Synapse Pipelines B) By running SQL scripts only in the cloud C) By using Azure Logic Apps D) By creating separate databases Answer: A) By integrating with on-premises and cloud data sources using Synapse Pipelines","title":"62. How can Synapse Analytics support a hybrid data architecture?"},{"location":"Synapse-ADF/Q%26A/#63-what-feature-in-synapse-analytics-can-be-used-to-run-interactive-queries-on-large-datasets-without-pre-provisioned-resources","text":"A) Synapse SQL Serverless pool B) Dedicated SQL pool C) Azure Functions D) SQL triggers Answer: A) Synapse SQL Serverless pool","title":"63. What feature in Synapse Analytics can be used to run interactive queries on large datasets without pre-provisioned resources?"},{"location":"Synapse-ADF/Q%26A/#64-which-tool-can-be-used-to-automate-the-deployment-and-management-of-synapse-analytics-resources","text":"A) Azure DevOps with CI/CD pipelines B) Azure Monitor C) SQL Server Management Studio D) Power Automate Answer: A) Azure DevOps with CI/CD pipelines","title":"64. Which tool can be used to automate the deployment and management of Synapse Analytics resources?"},{"location":"Synapse-ADF/Q%26A/#65-how-can-a-financial-company-use-synapse-analytics-to-perform-regulatory-reporting","text":"A) By using Synapse Pipelines to aggregate data and generate reports B) By exporting data to a third-party tool C) Using SQL triggers D) By creating separate databases Answer: A) By using Synapse Pipelines to aggregate data and generate reports","title":"65. How can a financial company use Synapse Analytics to perform regulatory reporting?"},{"location":"Synapse-ADF/Q%26A/#66-what-is-the-benefit-of-using-spark-pools-in-synapse-analytics-for-big-data-processing","text":"A) They provide a scalable and distributed environment for processing large datasets B) They offer more security features C) They are easier to configure D) They consume fewer resources Answer: A) They provide a scalable and distributed environment for processing large datasets","title":"66. What is the benefit of using Spark pools in Synapse Analytics for big data processing?"},{"location":"Synapse-ADF/Q%26A/#67-how-can-you-integrate-synapse-analytics-with-other-azure-services-like-power-bi-and-azure-ml","text":"A) By using Synapse Studio connectors and integration features B) By using SQL triggers C) By creating separate databases D) By encrypting the data Answer: A) By using Synapse Studio connectors and integration features","title":"67. How can you integrate Synapse Analytics with other Azure services like Power BI and Azure ML?"},{"location":"Synapse-ADF/Q%26A/#68-what-method-can-be-used-to-ensure-data-quality-in-synapse-analytics-before-analysis","text":"A) Using Data Flows for data cleansing and transformation B) Using SQL scripts C) By increasing database size D) By storing data in Blob Storage Answer: A) Using Data Flows for data cleansing and transformation","title":"68. What method can be used to ensure data quality in Synapse Analytics before analysis?"},{"location":"Synapse-ADF/Q%26A/#69-how-can-you-implement-a-data-lakehouse-architecture-using-synapse-analytics","text":"A) By combining Synapse SQL pools with Azure Data Lake Storage and Synapse Pipelines B) By using SQL triggers C) By creating new databases D) Using Azure Functions Answer: A) By combining Synapse SQL pools with Azure Data Lake Storage and Synapse Pipelines","title":"69. How can you implement a data lakehouse architecture using Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#70-what-is-the-role-of-synapse-studio-in-a-data-analytics-workflow","text":"A) It provides an integrated workspace for data exploration, preparation, management, and visualization B) It monitors network configuration C) It configures database encryption D) It manages SQL triggers Answer: A) It provides an integrated workspace for data exploration, preparation, management, and visualization","title":"70. What is the role of Synapse Studio in a data analytics workflow?"},{"location":"Synapse-ADF/Q%26A/#71-how-can-a-healthcare-company-use-synapse-analytics-to-support-population-health-management","text":"A) By integrating with electronic health records (EHR) and using machine learning models for predictive analytics B) By exporting data to Excel C) Using SQL triggers D) By creating new databases Answer: A) By integrating with electronic health records (EHR) and using machine learning models for predictive analytics","title":"71. How can a healthcare company use Synapse Analytics to support population health management?"},{"location":"Synapse-ADF/Q%26A/#72-how-can-synapse-analytics-help-in-real-time-customer-sentiment-analysis-for-a-retail-company","text":"A) By using Spark Streaming with Synapse Pipelines to analyze social media and customer feedback data B) By using SQL scripts C) By creating separate databases D) By storing data in Blob Storage Answer: A) By using Spark Streaming with Synapse Pipelines to analyze social media and customer feedback data","title":"72. How can Synapse Analytics help in real-time customer sentiment analysis for a retail company?"},{"location":"Synapse-ADF/Q%26A/#73-what-is-the-best-approach-to-handle-data-archiving-and-retention-in-synapse-analytics","text":"A) Implementing tiered storage options and lifecycle policies B) By using SQL triggers C) By increasing database size D) By creating new databases Answer: A) Implementing tiered storage options and lifecycle policies","title":"73. What is the best approach to handle data archiving and retention in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#74-how-can-you-secure-synapse-analytics-against-unauthorized-access-and-data-breaches","text":"A) By implementing Azure Active Directory integration and role-based access control (RBAC) B) By using SQL scripts C) By creating separate databases D) By using Azure Logic Apps Answer: A) By implementing Azure Active Directory integration and role-based access control (RBAC)","title":"74. How can you secure Synapse Analytics against unauthorized access and data breaches?"},{"location":"Synapse-ADF/Q%26A/#75-how-can-a-logistics-company-optimize-route-planning-and-delivery-schedules-using-synapse-analytics","text":"A) By integrating with Azure Machine Learning to develop predictive models B) By exporting data to a third-party tool C) Using SQL triggers D) By creating separate databases Answer: A) By integrating with Azure Machine Learning to develop predictive models","title":"75. How can a logistics company optimize route planning and delivery schedules using Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#76-what-feature-in-synapse-analytics-helps-in-managing-data-schema-changes-and-version-control","text":"A) Schema management tools in Synapse Studio B) Using SQL triggers C) By creating new databases D) Using Power Automate Answer: A) Schema management tools in Synapse Studio","title":"76. What feature in Synapse Analytics helps in managing data schema changes and version control?"},{"location":"Synapse-ADF/Q%26A/#77-how-can-you-perform-sentiment-analysis-on-customer-reviews-stored-in-synapse-analytics","text":"A) By using Azure Cognitive Services text analytics integrated with Synapse Pipelines B) By exporting data to Excel C) Using SQL scripts D) By creating separate databases Answer: A) By using Azure Cognitive Services text analytics integrated with Synapse Pipelines","title":"77. How can you perform sentiment analysis on customer reviews stored in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#78-how-can-synapse-analytics-support-large-scale-data-migration-from-on-premises-systems","text":"A) By using Azure Data Migration Service and Synapse Pipelines B) By using SQL triggers C) By creating new databases D) By using Azure Functions Answer: A) By using Azure Data Migration Service and Synapse Pipelines","title":"78. How can Synapse Analytics support large-scale data migration from on-premises systems?"},{"location":"Synapse-ADF/Q%26A/#79-how-can-an-e-commerce-company-personalize-customer-experiences-using-synapse-analytics","text":"A) By leveraging Synapse SQL and machine learning models to analyze customer behavior and preferences B) By exporting data to a third-party tool C) Using SQL scripts D) By creating separate databases Answer: A) By leveraging Synapse SQL and machine learning models to analyze customer behavior and preferences","title":"79. How can an e-commerce company personalize customer experiences using Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#80-what-is-the-role-of-data-flows-in-synapse-analytics","text":"A) To provide a visual interface for designing data transformation logic B) To manage SQL triggers C) To configure database encryption D) To monitor network configuration Answer: A) To provide a visual interface for designing data transformation logic","title":"80. What is the role of Data Flows in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#81-how-can-a-company-use-synapse-analytics-to-implement-a-single-source-of-truth-for-their-data","text":"A) By centralizing data from various sources into a Synapse data warehouse and applying data governance practices B) By using SQL triggers C) By creating separate databases D) By exporting data to Excel Answer: A) By centralizing data from various sources into a Synapse data warehouse and applying data governance practices","title":"81. How can a company use Synapse Analytics to implement a single source of truth for their data?"},{"location":"Synapse-ADF/Q%26A/#82-what-is-the-purpose-of-integrating-synapse-analytics-with-azure-purview","text":"A) To enhance data cataloging, governance, and lineage tracking B) To manage SQL triggers C) To configure database encryption D) To monitor network configuration Answer: A) To enhance data cataloging, governance, and lineage tracking","title":"82. What is the purpose of integrating Synapse Analytics with Azure Purview?"},{"location":"Synapse-ADF/Q%26A/#83-how-can-synapse-analytics-help-a-media-company-analyze-viewer-engagement-data","text":"A) By using Synapse SQL and Spark pools to process and analyze large volumes of viewer data B) By exporting data to a third-party tool C) Using SQL triggers D) By creating separate databases Answer: A) By using Synapse SQL and Spark pools to process and analyze large volumes of viewer data","title":"83. How can Synapse Analytics help a media company analyze viewer engagement data?"},{"location":"Synapse-ADF/Q%26A/#84-what-is-the-advantage-of-using-dedicated-sql-pools-over-serverless-sql-pools-in-synapse-analytics","text":"A) Dedicated SQL pools provide better performance for high concurrency and complex queries B) Serverless SQL pools offer better security features C) Dedicated SQL pools consume fewer resources D) Serverless SQL pools are easier to configure Answer: A) Dedicated SQL pools provide better performance for high concurrency and complex queries","title":"84. What is the advantage of using dedicated SQL pools over serverless SQL pools in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#85-how-can-a-financial-institution-detect-anomalies-in-transaction-data-using-synapse-analytics","text":"A) By integrating with Azure Machine Learning for anomaly detection models B) By exporting data to a third-party tool C) Using SQL triggers D) By creating separate databases Answer: A) By integrating with Azure Machine Learning for anomaly detection models","title":"85. How can a financial institution detect anomalies in transaction data using Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#86-what-is-the-best-practice-for-loading-historical-data-into-synapse-analytics-for-analysis","text":"A) Using PolyBase or COPY statement to load large volumes of data efficiently B) By using SQL scripts C) By creating new databases D) By encrypting the data Answer: A) Using PolyBase or COPY statement to load large volumes of data efficiently","title":"86. What is the best practice for loading historical data into Synapse Analytics for analysis?"},{"location":"Synapse-ADF/Q%26A/#87-how-can-synapse-analytics-help-in-optimizing-supply-chain-operations-for-a-manufacturing-company","text":"A) By analyzing production and logistics data using Synapse SQL and machine learning models B) By exporting data to Excel C) Using SQL scripts D) By creating separate databases Answer: A) By analyzing production and logistics data using Synapse SQL and machine learning models","title":"87. How can Synapse Analytics help in optimizing supply chain operations for a manufacturing company?"},{"location":"Synapse-ADF/Q%26A/#88-how-can-you-automate-the-backup-and-recovery-of-synapse-analytics-data","text":"A) By using Azure Backup and Recovery solutions B) Using SQL scripts C) By creating new databases D) Using Power Automate Answer: A) By using Azure Backup and Recovery solutions","title":"88. How can you automate the backup and recovery of Synapse Analytics data?"},{"location":"Synapse-ADF/Q%26A/#89-how-can-a-company-ensure-their-synapse-analytics-data-warehouse-meets-compliance-requirements","text":"A) By implementing data encryption, access control, and audit logging B) By using SQL triggers C) By creating separate databases D) By exporting data to a third-party tool Answer: A) By implementing data encryption, access control, and audit logging","title":"89. How can a company ensure their Synapse Analytics data warehouse meets compliance requirements?"},{"location":"Synapse-ADF/Q%26A/#90-how-can-synapse-analytics-be-used-to-support-customer-segmentation-and-targeting-for-a-marketing-campaign","text":"A) By using Synapse SQL to analyze customer data and identify segments based on behavior and demographics B) By exporting data to Excel C) Using SQL scripts D) By creating separate databases Answer: A) By using Synapse SQL to analyze customer data and identify segments based on behavior and demographics","title":"90. How can Synapse Analytics be used to support customer segmentation and targeting for a marketing campaign?"},{"location":"Synapse-ADF/Q%26A/#91-what-is-the-benefit-of-using-synapse-studio-for-collaborative-data-analytics-projects","text":"A) It provides a unified workspace for multiple users to collaborate on data preparation, management, and analysis B) It monitors network configuration C) It manages SQL triggers D) It configures database encryption Answer: A) It provides a unified workspace for multiple users to collaborate on data preparation, management, and analysis","title":"91. What is the benefit of using Synapse Studio for collaborative data analytics projects?"},{"location":"Synapse-ADF/Q%26A/#92-how-can-synapse-analytics-be-integrated-with-third-party-bi-tools-for-advanced-reporting","text":"A) By using data connectors and APIs to link Synapse data with BI tools like Tableau or Qlik B) By exporting data to Excel C) Using SQL scripts D) By creating separate databases Answer: A) By using data connectors and APIs to link Synapse data with BI tools like Tableau or Qlik","title":"92. How can Synapse Analytics be integrated with third-party BI tools for advanced reporting?"},{"location":"Synapse-ADF/Q%26A/#93-how-can-a-retail-company-use-synapse-analytics-to-optimize-inventory-management","text":"A) By analyzing sales and inventory data to predict demand and optimize stock levels B) By exporting data to a third-party tool C) Using SQL triggers D) By creating separate databases Answer: A) By analyzing sales and inventory data to predict demand and optimize stock levels","title":"93. How can a retail company use Synapse Analytics to optimize inventory management?"},{"location":"Synapse-ADF/Q%26A/#94-what-feature-in-synapse-analytics-allows-you-to-schedule-and-automate-data-processing-tasks","text":"A) Synapse Pipelines B) SQL Server Management Studio C) Azure Monitor D) Power Automate Answer: A) Synapse Pipelines","title":"94. What feature in Synapse Analytics allows you to schedule and automate data processing tasks?"},{"location":"Synapse-ADF/Q%26A/#95-how-can-a-company-use-synapse-analytics-to-perform-cross-regional-data-analysis","text":"A) By setting up data replication and using Synapse SQL to query data from different regions B) By using SQL triggers C) By creating separate databases D) By exporting data to Excel Answer: A) By setting up data replication and using Synapse SQL to query data from different regions","title":"95. How can a company use Synapse Analytics to perform cross-regional data analysis?"},{"location":"Synapse-ADF/Q%26A/#96-how-can-you-improve-query-performance-in-synapse-analytics-when-dealing-with-large-datasets","text":"A) By optimizing distribution keys and creating columnstore indexes B) By increasing database size C) Using SQL triggers D) By creating separate databases Answer: A) By optimizing distribution keys and creating columnstore indexes","title":"96. How can you improve query performance in Synapse Analytics when dealing with large datasets?"},{"location":"Synapse-ADF/Q%26A/#97-how-can-synapse-analytics-support-predictive-maintenance-for-industrial-equipment","text":"A) By integrating with IoT data sources and using machine learning models for predictive analytics B) By exporting data to Excel C) Using SQL scripts D) By creating separate databases Answer: A) By integrating with IoT data sources and using machine learning models for predictive analytics","title":"97. How can Synapse Analytics support predictive maintenance for industrial equipment?"},{"location":"Synapse-ADF/Q%26A/#98-what-is-the-best-practice-for-managing-large-scale-data-transformations-in-synapse-analytics","text":"A) Using Data Flows and Spark pools for efficient data processing B) By using SQL triggers C) By increasing database size D) By creating new databases Answer: A) Using Data Flows and Spark pools for efficient data processing","title":"98. What is the best practice for managing large-scale data transformations in Synapse Analytics?"},{"location":"Synapse-ADF/Q%26A/#99-how-can-synapse-analytics-help-in-developing-a-360-degree-view-of-the-customer","text":"A) By consolidating data from various sources and using analytics to provide insights into customer behavior B) By exporting data to a third-party tool C) Using SQL scripts D) By creating separate databases Answer: A) By consolidating data from various sources and using analytics to provide insights into customer behavior","title":"99. How can Synapse Analytics help in developing a 360-degree view of the customer?"},{"location":"Synapse-ADF/Q%26A/#100-how-can-synapse-analytics-be-used-to-support-real-time-business-intelligence-for-an-e-commerce-platform","text":"A) By integrating with streaming data sources and using serverless SQL pools for real-time querying B) By using SQL triggers C) By creating separate databases D) By exporting data to Excel Answer: A) By integrating with streaming data sources and using serverless SQL pools for real-time querying","title":"100. How can Synapse Analytics be used to support real-time business intelligence for an e-commerce platform?"}]}