<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Spark-To-ADLS-Connection - My Docs</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/fontawesome.min.css" rel="stylesheet">
        <link href="../../../css/brands.min.css" rel="stylesheet">
        <link href="../../../css/solid.min.css" rel="stylesheet">
        <link href="../../../css/v4-font-face.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" >
        <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" disabled>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../../..">My Docs</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar-collapse" aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="nav-item">
                                <a href="../../.." class="nav-link">Welcome to MkDocs</a>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Airflow</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../Airflow/1.0.0_AirFlow_Concepts/" class="dropdown-item">Airflow Concepts</a>
</li>
                                    
<li>
    <a href="../../../Airflow/1.0.1_A_Dags_Anatomy/" class="dropdown-item">A dags anatomy</a>
</li>
                                    
<li>
    <a href="../../../Airflow/1.0.2_Hello_Airflow/" class="dropdown-item">Hello-Airflow</a>
</li>
                                    
<li>
    <a href="../../../Airflow/1.0.3_Airflow_Dbt_Docker/" class="dropdown-item">Project Airflow dbt</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">DE Projects</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../DE-Projects/Csv-To-MSSQL/" class="dropdown-item">Python - CSV To MSSQL</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/CurrencyPredictor/" class="dropdown-item">CurrencyPredictor</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/Dbrk-E2E-AttritionProject/" class="dropdown-item">Problem Statement</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/Download-Haddop-Jars/" class="dropdown-item">Download JARs-Apache Maven Repo</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/FetchJsonWriteParquet/" class="dropdown-item">Json To Parquet Using Spark And Azure</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/InstallScala/" class="dropdown-item">Scala Install</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/JsonFlatAzureSDK/" class="dropdown-item">Flatten Json Using Azure SDK</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/LocalPython_AzureBlob/" class="dropdown-item">Local Python Code to Rearrange Files in a Azure Blob Container</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/Microsoft_OpenJDK/" class="dropdown-item">Microsoft OpenJDK</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/Project_MigrationToAzureBlob/" class="dropdown-item">CMS Migration to Azure Blob</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/Project_MongoCMS/" class="dropdown-item">CMS using MongoDB</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/Project_SSRS_SSIS_SharePoint/" class="dropdown-item">Project - ETL and Reporting Lending Org</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/Raw-Json-To-Hive/" class="dropdown-item">Raw Json To Hive</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/SPY_ETF_Buy_Recommender/" class="dropdown-item">SPY ETF Buy Recommender</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">AzureSkyWeather</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DE-Projects/AzureSkyWeather/HomeProjectAzureSkyWeather/" class="dropdown-item">Project AzureSkyWeather</a>
</li>
            
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">1 Ingestion</a>
    <ul class="dropdown-menu">
            
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">HttpTriggered</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/" class="dropdown-item">Part 1A - Using Azure HTTP-Triggered Function</a>
</li>
            
<li>
    <a href="../../../DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/" class="dropdown-item">Azure Functions Quickstart</a>
</li>
    </ul>
  </li>
            
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">TimerTriggered</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc/" class="dropdown-item">Part 1B - Using Azure Timer-Triggered Function</a>
</li>
    </ul>
  </li>
    </ul>
  </li>
            
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">2 Transformation</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/" class="dropdown-item">Solution Details</a>
</li>
    </ul>
  </li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">JsonValidator</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DE-Projects/JsonValidator/AzureFunction-ValidateJSOns/" class="dropdown-item">Azure Functions - Validate JSONs</a>
</li>
            
<li>
    <a href="../../../DE-Projects/JsonValidator/Python-ValidateJSONs/" class="dropdown-item">Validate JSON using Python</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">Sparkzure</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DE-Projects/Sparkzure/HomeProjectSparkzure/" class="dropdown-item">Project Sparkzure</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">StreamKraft</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DE-Projects/StreamKraft/HomeProjectStreamKraft/" class="dropdown-item">Project StreamKraft</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">DevOps</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../DevOps/1.1_Hello_GitHub_Actions_Workflow/" class="dropdown-item">Hello GitHub Actions Workflow</a>
</li>
                                    
<li>
    <a href="../../../DevOps/1.1_Sample_Workflows/" class="dropdown-item">Sample Workflows</a>
</li>
                                    
<li>
    <a href="../../../DevOps/1_GitHub-Concepts/" class="dropdown-item">GitHub Concepts</a>
</li>
                                    
<li>
    <a href="../../../DevOps/2.1_Self-Hosted_Agent_Windows/" class="dropdown-item">Self-Hosted-Agent-Windows</a>
</li>
                                    
<li>
    <a href="../../../DevOps/2.2_Self-Hosted_Agent_Windows_Container/" class="dropdown-item">Self-Hosted-Agent-Win-Container</a>
</li>
                                    
<li>
    <a href="../../../DevOps/2.3_Self-Hosted_Agent_Linux_Container/" class="dropdown-item">Self-Hosted-Agent-Ubuntu-Container</a>
</li>
                                    
<li>
    <a href="../../../DevOps/2_Azure-Pipelines/" class="dropdown-item">Azure-Pipelines</a>
</li>
                                    
<li>
    <a href="../../../DevOps/ADF_CICD/" class="dropdown-item">ADF-CI-CD</a>
</li>
                                    
<li>
    <a href="../../../DevOps/Biceps/" class="dropdown-item">What is Bicep?</a>
</li>
                                    
<li>
    <a href="../../../DevOps/CI-CD_in%20ADF/" class="dropdown-item">CI CD in ADF</a>
</li>
                                    
<li>
    <a href="../../../DevOps/GitScenarios/" class="dropdown-item">GitScenarios</a>
</li>
                                    
<li>
    <a href="../../../DevOps/JenkinsVsGitHubVsAzureDevOps/" class="dropdown-item">Jenkins Vs GitHub Vs Devops</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">DockerAndKubernetes</a>
                                <ul class="dropdown-menu">
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">AirflowDocker</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DockerAndKubernetes/AirflowDocker/1_AirflowDocker/" class="dropdown-item">Airflow</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">Kafka</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DockerAndKubernetes/Kafka/2_Confluent%20Kafka/" class="dropdown-item">Confluent Kafka</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">Mongodb</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DockerAndKubernetes/Mongodb/3_DockerMongodb/" class="dropdown-item">MongoDB</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">SparkHiveHadoop</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/" class="dropdown-item">PySpark</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/" class="dropdown-item">Bitnami Spark</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.3_VSCode_Docker_Connection/" class="dropdown-item">VSCode-Docker-Connection</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/" class="dropdown-item">Spark-Hive_MSSQL</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/" class="dropdown-item">Hadoop Cluster</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/" class="dropdown-item">bde2020-Hive-Hadoop-Presto</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.7_Hadoop_Hive_SingleNode_MySQL/" class="dropdown-item">Hive-Hadoop-MySQL-SingleNode</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.8_Hive-ApacheOfficial/" class="dropdown-item">Hive Official Setup</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.9.1_Hive_Concepts/" class="dropdown-item">Hive Concepts</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.9.2_Hadoop_Concepts/" class="dropdown-item">Hadoop Concepts</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/" class="dropdown-item">Docker Misc Concepts</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4_Spark-Hive-Hadoop/" class="dropdown-item">4 Spark Hive Hadoop</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">M365</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../M365/DocumentumToSharePoint/" class="dropdown-item">Documentum-SharePoint Online</a>
</li>
                                    
<li>
    <a href="../../../M365/LicensingExamples/" class="dropdown-item">M365 Licensing Examples</a>
</li>
                                    
<li>
    <a href="../../../M365/SPMT/" class="dropdown-item">SPMT - SharePoint Migration</a>
</li>
                                    
<li>
    <a href="../../../M365/SharePoint2007FarmUpgrade/" class="dropdown-item">SharePoint Farm Upgrade</a>
</li>
                                    
<li>
    <a href="../../../M365/SharePoint2016FarmUpgrade/" class="dropdown-item">SharePoint Farm 2016 Farm Setup</a>
</li>
                                    
<li>
    <a href="../../../M365/SharePointEvents/" class="dropdown-item">SharePoint Event Receiver</a>
</li>
                                    
<li>
    <a href="../../../M365/SharePointFarmConsolidation/" class="dropdown-item">SharePoint Farm Consolidation</a>
</li>
                                    
<li>
    <a href="../../../M365/SharePointFormsOrPowerApps/" class="dropdown-item">SharePoint Forms or PowerApps</a>
</li>
                                    
<li>
    <a href="../../../M365/SharePointMiniRole/" class="dropdown-item">SharePoint Mini Role</a>
</li>
                                    
<li>
    <a href="../../../M365/SharePointVersionEvolution/" class="dropdown-item">SharePoint Evolution</a>
</li>
                                    
<li>
    <a href="../../../M365/SharePointVsOtherECM/" class="dropdown-item">SharePoint vs Other ECMS</a>
</li>
                                    
<li>
    <a href="../../../M365/WSS3DocumentUpload/" class="dropdown-item">Project - C# - WSS 3 Bulk ingestion</a>
</li>
                                    
<li>
    <a href="../../../M365/oAuthSharePointPython/" class="dropdown-item">Python-oAuth-SharePointOnline</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Microsoft Fabric</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../Microsoft-Fabric/DataFactory/" class="dropdown-item">Pipelines&DataFlows</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/DataScience/" class="dropdown-item">Data Science on Fabric Overview</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/DataWareHouse/" class="dropdown-item">DataWareHouse</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/DirectLake/" class="dropdown-item">DirectLake|Fabric|PowerBI</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/E2EProject/" class="dropdown-item">E2EProject</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse/" class="dropdown-item">JSON-DeltaLake-OPG</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse/" class="dropdown-item">ETL-Load data into Lakehouse - Pyspark Notebook</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/FabricAdministration/" class="dropdown-item">Administration</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/FabricQ%26A/" class="dropdown-item">Fabric Q&A</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/FabricSparkStreaming/" class="dropdown-item">Spark Streaming</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/HelloMicrosoftFabric/" class="dropdown-item">Hello Fabric</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/InspectingDataframes/" class="dropdown-item">Whats in your df?</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/KQL/" class="dropdown-item">KQL</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/PandasVsSparkDf/" class="dropdown-item">PandasVsSparkDf</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/Pyspark_SparkSQL/" class="dropdown-item">Pyspark|SparkSQL CheatSheet</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/RealTimeAnalytics/" class="dropdown-item">Real-time Intelligence</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Misc</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../Misc/10_Fact_vs_Dimension_tables/" class="dropdown-item">Fact Vs Dimension Table</a>
</li>
                                    
<li>
    <a href="../../../Misc/1_GoogleCloudSeeUsage/" class="dropdown-item">Check Google Usage</a>
</li>
                                    
<li>
    <a href="../../../Misc/2_InstallVMWareFree/" class="dropdown-item">VMWare For Free</a>
</li>
                                    
<li>
    <a href="../../../Misc/3_Markdown/" class="dropdown-item">Markdown</a>
</li>
                                    
<li>
    <a href="../../../Misc/4_VSTrics/" class="dropdown-item">Visual Studio Code Tricks</a>
</li>
                                    
<li>
    <a href="../../../Misc/5_WhichDatastsToUse/" class="dropdown-item">Free Datasets for Data Practice</a>
</li>
                                    
<li>
    <a href="../../../Misc/6_RunningAppsInBg/" class="dropdown-item">Running Service in Background</a>
</li>
                                    
<li>
    <a href="../../../Misc/7_Azure_Budget/" class="dropdown-item">How to set a Azure Budget</a>
</li>
                                    
<li>
    <a href="../../../Misc/9_WhyUbuntuIsGood/" class="dropdown-item">Why Ubuntu is good to learn</a>
</li>
                                    
<li>
    <a href="../../../Misc/markdown_pdf_export_html/" class="dropdown-item">Markdown pdf export html</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">MarkdownColor.md</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../Misc/MarkdownColor.md/Color/" class="dropdown-item">Color Reference</a>
</li>
            
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">Images</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../Misc/MarkdownColor.md/images/Color/" class="dropdown-item">Color Palettes</a>
</li>
    </ul>
  </li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">MongoDB</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../MongoDB/HowMongoDBStoresFiles/" class="dropdown-item">How MongoDB Stores Data</a>
</li>
                                    
<li>
    <a href="../../../MongoDB/MongbDB_Vs_Atlas_VsCosmosDB/" class="dropdown-item">MongoDB Vs CosmosDB</a>
</li>
                                    
<li>
    <a href="../../../MongoDB/MongoDBCommands/" class="dropdown-item">MongoDB Commands</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">PowerPlatform</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../PowerPlatform/CalculationGroups/" class="dropdown-item">CalculationGroups</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/CustomConnectors/" class="dropdown-item">Building Custom Connectors</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/ECMCaptureFlow/" class="dropdown-item">Power Automate Or Kofax/Captiva?</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/EnableMicrosoftSyntex/" class="dropdown-item">Enable Microsoft Syntex on your Office 365 tenant</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/EnableSyntexOnYourDocumentLibrary/" class="dropdown-item">Create a model on SharePoint - Syntex</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/GoogleProviderPowerPages/" class="dropdown-item">Google Authentication</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/HealthClinicDataverseSecurity/" class="dropdown-item">HealthClinicDataverseSecurity</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/HelloDataverse/" class="dropdown-item">Hello Dataverse</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/HelloDynamics365/" class="dropdown-item">Dynamics 365 Ecosystem</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/HelloPowerPlatform/" class="dropdown-item">PowerPlatform Ecosystem</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/ModelDrivenApps/" class="dropdown-item">Model-Driven Apps</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/OnPremiseGateway/" class="dropdown-item">Onpremise Gateway</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/PowerAutomateIsWorkflowTeams/" class="dropdown-item">Workflow is Power Automate</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/PowerPlatformAdminCentral/" class="dropdown-item">PowerPlatformAdminCentral</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/PowerPlatformQ%26A/" class="dropdown-item">Pl-900-Q&A</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">DocumentIntelligence</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch/" class="dropdown-item">Integrate AI Search and Azure AI Document Intelligence</a>
</li>
            
<li>
    <a href="../../../PowerPlatform/DocumentIntelligence/AAIDI_Q%26A/" class="dropdown-item">AAIDI Q&A</a>
</li>
            
<li>
    <a href="../../../PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence/" class="dropdown-item">Azure AI Document Intelligence</a>
</li>
            
<li>
    <a href="../../../PowerPlatform/DocumentIntelligence/DocumentAutomation/" class="dropdown-item">Document automation base kit</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Python</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../Python/1.0_Sets/" class="dropdown-item">Sets</a>
</li>
                                    
<li>
    <a href="../../../Python/1.1.0_assert_methods/" class="dropdown-item">assert methods</a>
</li>
                                    
<li>
    <a href="../../../Python/1.1.1_decorators/" class="dropdown-item">decorators</a>
</li>
                                    
<li>
    <a href="../../../Python/1.1.2_argv/" class="dropdown-item">argv</a>
</li>
                                    
<li>
    <a href="../../../Python/1.1.3_Diff_And_Patch/" class="dropdown-item">diffAndpatch</a>
</li>
                                    
<li>
    <a href="../../../Python/1.1.3_error_handling/" class="dropdown-item">Error Handling</a>
</li>
                                    
<li>
    <a href="../../../Python/1.1.4_pdb/" class="dropdown-item">pdb</a>
</li>
                                    
<li>
    <a href="../../../Python/1.1.5_pyformat/" class="dropdown-item">format method</a>
</li>
                                    
<li>
    <a href="../../../Python/1.10_Func_Modl_Summary/" class="dropdown-item">LFM Summary</a>
</li>
                                    
<li>
    <a href="../../../Python/1.11_ifelifelse/" class="dropdown-item">ifelifelse</a>
</li>
                                    
<li>
    <a href="../../../Python/1.12_Operators/" class="dropdown-item">operators</a>
</li>
                                    
<li>
    <a href="../../../Python/1.13_For_Loops/" class="dropdown-item">for loops</a>
</li>
                                    
<li>
    <a href="../../../Python/1.14_enumerate/" class="dropdown-item">1.14 enumerate</a>
</li>
                                    
<li>
    <a href="../../../Python/1.15_range_function/" class="dropdown-item">range function</a>
</li>
                                    
<li>
    <a href="../../../Python/1.16_built_in_functions/" class="dropdown-item">1.16 built in functions</a>
</li>
                                    
<li>
    <a href="../../../Python/1.17_withStatement/" class="dropdown-item">with statement</a>
</li>
                                    
<li>
    <a href="../../../Python/1.18_unittest_pytest/" class="dropdown-item">pytest</a>
</li>
                                    
<li>
    <a href="../../../Python/1.19_if_name_main.md/" class="dropdown-item">if__name__main</a>
</li>
                                    
<li>
    <a href="../../../Python/1.1_Tuples/" class="dropdown-item">Tuples</a>
</li>
                                    
<li>
    <a href="../../../Python/1.2_Tuples_Advanced/" class="dropdown-item">Advanced Tuples</a>
</li>
                                    
<li>
    <a href="../../../Python/1.3_List/" class="dropdown-item">1.3 List</a>
</li>
                                    
<li>
    <a href="../../../Python/1.4_Dictionaries/" class="dropdown-item">1.4 Dictionaries</a>
</li>
                                    
<li>
    <a href="../../../Python/1.5_Lamda_Functions/" class="dropdown-item">Lamda Functions</a>
</li>
                                    
<li>
    <a href="../../../Python/1.9_Func_Modl_Lib/" class="dropdown-item">Library-Modules-Funcs</a>
</li>
                                    
<li>
    <a href="../../../Python/1_Python/" class="dropdown-item">Python</a>
</li>
                                    
<li>
    <a href="../../../Python/Linux/" class="dropdown-item">Essential Unix Commands</a>
</li>
                                    
<li>
    <a href="../../../Python/Pyspark/" class="dropdown-item">PySpark</a>
</li>
                                    
<li>
    <a href="../../../Python/PythonScripts/" class="dropdown-item">Python Sample Scripts</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">GraphAPIJupyter</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../Python/GraphAPIJupyter/GraphAPIUsingJuputer/" class="dropdown-item">Graph API - Juputer</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">SQL</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../SQL/1.0.0_dbt/" class="dropdown-item">dbt</a>
</li>
                                    
<li>
    <a href="../../../SQL/1.0.1_setup_simple_dbt_project/" class="dropdown-item">1.0.1 setup simple dbt project</a>
</li>
                                    
<li>
    <a href="../../../SQL/FlatFileSoure/" class="dropdown-item">FlatFileSoure</a>
</li>
                                    
<li>
    <a href="../../../SQL/InputAndOutputProperties/" class="dropdown-item">Understanding External Columns and Output Columns in SSIS</a>
</li>
                                    
<li>
    <a href="../../../SQL/MSSQL_Versions/" class="dropdown-item">SQL Server Versions</a>
</li>
                                    
<li>
    <a href="../../../SQL/Project_1-ETL-CSV-MSSQL/" class="dropdown-item">Project 1 - ETL Flat Files to MSSQL</a>
</li>
                                    
<li>
    <a href="../../../SQL/Project_2-UsingWebServicesInSSIS/" class="dropdown-item">Project 2 - Web Service SSIS Script Task</a>
</li>
                                    
<li>
    <a href="../../../SQL/SQL/" class="dropdown-item">Spark-SQL</a>
</li>
                                    
<li>
    <a href="../../../SQL/SQL_AdvancedTopics/" class="dropdown-item">SQL Advanced Topics</a>
</li>
                                    
<li>
    <a href="../../../SQL/SSIS/" class="dropdown-item">SQL Server Integration Services</a>
</li>
                                    
<li>
    <a href="../../../SQL/SSRS/" class="dropdown-item">SSRS</a>
</li>
                                    
<li>
    <a href="../../../SQL/Windows_Functions/" class="dropdown-item">Window Functions</a>
</li>
                                    
<li>
    <a href="../../../SQL/connecting-with-dbt/" class="dropdown-item">Connect Local dbt with MSSQL Server</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle active" aria-current="page" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Spark DataBricks</a>
                                <ul class="dropdown-menu">
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">1.0 Spark</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../1.0_Spark/1.0_Spark-Concepts/" class="dropdown-item">Spark</a>
</li>
            
<li>
    <a href="../../1.0_Spark/1.10_Scala_Cheatsheet/" class="dropdown-item">Scala Cheatsheet</a>
</li>
            
<li>
    <a href="../../1.0_Spark/1.11_Spark_Interview_Questions/" class="dropdown-item">Spark Interview Questions</a>
</li>
            
<li>
    <a href="../../1.0_Spark/1.12_Spark_Shuffle/" class="dropdown-item">Shuffle in Spark</a>
</li>
            
<li>
    <a href="../../1.0_Spark/1.13_SparkDatabaseTablesCatalogsMetastore/" class="dropdown-item">Spark DB-Tables-Metastore-Catalogs</a>
</li>
            
<li>
    <a href="../../1.0_Spark/1.14_Q%26A/" class="dropdown-item">Q&A</a>
</li>
            
<li>
    <a href="../../1.0_Spark/1.15_CommonPysparkTopics/" class="dropdown-item">PySpark Concepts I</a>
</li>
            
<li>
    <a href="../../1.0_Spark/1.16_ConnectingSparkToHive/" class="dropdown-item">Spark-Hive-Delta Connection</a>
</li>
            
<li>
    <a href="../../1.0_Spark/1.1_NarrowVsWideTransformation/" class="dropdown-item">Narrow_Vs_Wide_Transformation</a>
</li>
            
<li>
    <a href="../../1.0_Spark/1.2_SparkArchitecture/" class="dropdown-item">Spark Architecture</a>
</li>
            
<li>
    <a href="../../1.0_Spark/1.3_persist_and_cache/" class="dropdown-item">persist and cache</a>
</li>
            
<li>
    <a href="../../1.0_Spark/1.4_broadcastvariables/" class="dropdown-item">Broadcast Variables</a>
</li>
            
<li>
    <a href="../../1.0_Spark/1.5_DataSkewHandling/" class="dropdown-item">Data Skew in Spark</a>
</li>
            
<li>
    <a href="../../1.0_Spark/1.6_dropna_fillna_df_missing_val_handling/" class="dropdown-item">dropna and fillna</a>
</li>
            
<li>
    <a href="../../1.0_Spark/1.7_distinct_dropDuplicate_windowsFunc/" class="dropdown-item">Removing Duplicates - PySpark</a>
</li>
            
<li>
    <a href="../../1.0_Spark/1.8_Partition_Grouping/" class="dropdown-item">Partition And Bucket</a>
</li>
            
<li>
    <a href="../../1.0_Spark/1.9_RDD_Dataframe_Dataset/" class="dropdown-item">RDD-Dataframe-Dataset</a>
</li>
            
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">Install Pyspark Windows</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/" class="dropdown-item">Install-PySpark-Windows</a>
</li>
    </ul>
  </li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">2.0 Spark To ADLS</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="./" class="dropdown-item active" aria-current="page">Spark-To-ADLS-Connection</a>
</li>
            
<li>
    <a href="../2.1_Spark-To_ADLS_Summary/" class="dropdown-item">Spark-To-ADLS-Summary</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">3.0 Databricks</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../3.0_Databricks/3.0_Databricks_Concepts/" class="dropdown-item">Databricks</a>
</li>
            
<li>
    <a href="../../3.0_Databricks/3.1_Catalogs_And_Metastore/" class="dropdown-item">Hive Metastore and the hive_metastore folder</a>
</li>
            
<li>
    <a href="../../3.0_Databricks/3.2_AuthenticationMethods/" class="dropdown-item">Authentication Method</a>
</li>
            
<li>
    <a href="../../3.0_Databricks/3.3_Mount_ADLS_on_Databricks/" class="dropdown-item">Mount ADLS on Databricks</a>
</li>
            
<li>
    <a href="../../3.0_Databricks/3.4_Databricks_Secret_Scope/" class="dropdown-item">Secret Scope</a>
</li>
            
<li>
    <a href="../../3.0_Databricks/3.5_Databricks_SQL/" class="dropdown-item">CREATE TABLE USING</a>
</li>
            
<li>
    <a href="../../3.0_Databricks/3.6_DatabricksMagicCommands/" class="dropdown-item">Magic Commands</a>
</li>
            
<li>
    <a href="../../3.0_Databricks/3.7_DeltaLake_And_Lakehouse/" class="dropdown-item">Delta Lake And Lakehouse</a>
</li>
            
<li>
    <a href="../../3.0_Databricks/4.8_Databricks_ProjectA1/" class="dropdown-item">Project-A</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">4.0 Hive</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../4.0_Hive/Hive_Concepts/" class="dropdown-item">Hive Concepts</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">StreamProcessing</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../StreamProcessing/1.0_What_Is_Stream_Processing/" class="dropdown-item">1.0 What Is Stream Processing</a>
</li>
                                    
<li>
    <a href="../../../StreamProcessing/2.0.1_EventHubs_Vs_Kafka/" class="dropdown-item">EventHubs Vs Kafka</a>
</li>
                                    
<li>
    <a href="../../../StreamProcessing/2.0.2_Project_Hello_EventHubs/" class="dropdown-item">Overview</a>
</li>
                                    
<li>
    <a href="../../../StreamProcessing/2.0.3_EventHubsLocalEmulator/" class="dropdown-item">Event Hubs Emulator - End to End</a>
</li>
                                    
<li>
    <a href="../../../StreamProcessing/2.0_Azure_EventHubs/" class="dropdown-item">EventHubs</a>
</li>
                                    
<li>
    <a href="../../../StreamProcessing/4_EventProcessingChoices/" class="dropdown-item">Stream Processing Product Combination</a>
</li>
                                    
<li>
    <a href="../../../StreamProcessing/5_AmazonKinesisSparkIntegration/" class="dropdown-item">5 AmazonKinesisSparkIntegration</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Synapse ADF</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../Synapse-ADF/1.0_SynapseConcepts/" class="dropdown-item">SynapseConcepts</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/1.1_Pools/" class="dropdown-item">Pools</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/1.3_ETL%20Pipelines/" class="dropdown-item">ETL Pipelines</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/1.4_Copy-data-tool/" class="dropdown-item">ADF Copy task - When to use</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/1.5_IntegrationRuntime/" class="dropdown-item">Integration Runtime</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/1.6_DB_Types_In_Synapse/" class="dropdown-item">Types of DB in Synapse</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/1.7_SynapseLakeDBAndLakehouse/" class="dropdown-item">Lake DB-Lakehouse-Delta Lake</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/1.8_ADF_SA_Evolution/" class="dropdown-item">ADF & Synapse Evolution</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/1.9_CETAS/" class="dropdown-item">CETAS</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/2.0_Projects/" class="dropdown-item">2.0 Projects</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/2.1_Pipeline-Local-ADLS/" class="dropdown-item">CopyData-LocalToADLS</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/2.2_PySparkWarehouse/" class="dropdown-item">PysparkWarehouse</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/2.3_ADF_RestAPI_Databricks/" class="dropdown-item">2.3 ADF RestAPI Databricks</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/2.4_Monitor_ADF_Pipelines/" class="dropdown-item">ADF Pipelines Monitoring</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/2.5_ADF_Pipeline_Copy/" class="dropdown-item">Export ADF Pipeline</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/Q%26A/" class="dropdown-item">100 Synapse FAQs</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ms-md-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-bs-toggle="modal" data-bs-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../../1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../2.1_Spark-To_ADLS_Summary/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-bs-toggle="collapse" data-bs-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-body-tertiary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-bs-level="2"><a href="#table-of-contents" class="nav-link">Table of contents</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-bs-level="1"><a href="#project-sparkzure-part1-connecting-local-spark-to-azure-data-lake" class="nav-link">Project Sparkzure Part1 - Connecting Local Spark to Azure Data Lake</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="2"><a href="#overview" class="nav-link">Overview</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#kickstart-integrating-spark-with-azure-data-lake" class="nav-link">Kickstart: Integrating Spark with Azure Data Lake</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#common-errors" class="nav-link">Common Errors</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#appendix" class="nav-link">Appendix</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-bs-level="1"><a href="#project-sparkzure-part2-sorting-files-in-adls-container-using-standalone-spark" class="nav-link">Project Sparkzure Part2 - Sorting Files in ADLS Container Using Standalone Spark</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-bs-level="1"><a href="#understanding-spark-configuration-sparkjarspackages-vs-sparkjars" class="nav-link">Understanding Spark Configuration: spark.jars.packages vs spark.jars</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="2"><a href="#the-role-of-configsparkjarspackages" class="nav-link">The Role of .config('spark.jars.packages', '...')</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#the-utility-of-configsparkjars" class="nav-link">The Utility of .config('spark.jars', '...')</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#conclusion_1" class="nav-link">Conclusion</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h2 id="table-of-contents">Table of contents</h2>
<ul>
<li><a href="#project-sparkzure-part1---connecting-local-spark-to-azure-data-lake">Project Sparkzure Part1 - Connecting Local Spark to Azure Data Lake</a></li>
<li><a href="#overview">Overview</a></li>
<li><a href="#kickstart-integrating-spark-with-azure-data-lake">Kickstart: Integrating Spark with Azure Data Lake</a><ul>
<li><a href="#create-the-containerized-setup">Create the containerized setup</a></li>
<li><a href="#register-an-app-for-oauth-authentication">Register an App for OAuth Authentication</a></li>
<li><a href="#register-a-new-app-using-app-registration">Register a new App using App Registration</a></li>
<li><a href="#copy-ids-and-secret-from-the-app">Copy Ids and secret From The App</a></li>
<li><a href="#give-the-app-permission-to-the-container">Give the App Permission to the Container</a></li>
<li><a href="#access-adls-data-from-spark-using-oauth-authentication-and-service-principal">Access ADLS Data From Spark Using OAuth Authentication and Service Principal</a></li>
<li><a href="#open-vs-code-and-connect-to-the-container">Open VS Code and Connect To the Container</a></li>
<li><a href="#run-the-code">Run the code</a></li>
<li><a href="#access-data-in-adls-container-using-storage-accounts-access-key-method">Access data in ADLS container using Storage Account's Access Key Method</a></li>
</ul>
</li>
<li><a href="#common-errors">Common Errors</a><ul>
<li><a href="#authorizationpermissionmismatch-during-oauth-authenticaiton">AuthorizationPermissionMismatch During OAuth Authenticaiton</a></li>
</ul>
</li>
<li><a href="#appendix">Appendix</a><ul>
<li><a href="#why-does-spark-rely-on-hadoop-libraries-to-access-azure-data-lake-storage-adls">Why Does Spark Rely on Hadoop Libraries to Access Azure Data Lake Storage (ADLS)?</a></li>
<li><a href="#understanding-essential-jars-for-azure-data-lake-operations-with-spark">Understanding Essential JARs for Azure Data Lake Operations with Spark</a></li>
</ul>
</li>
<li><a href="#project-sparkzure-part2---sorting-files-in-adls-container-using-standalone-spark">Project Sparkzure Part2 - Sorting Files in ADLS Container Using Standalone Spark</a><ul>
<li><a href="#overview-of-the-article">Overview of the Article</a></li>
<li><a href="#my-environment">My Environment</a></li>
<li><a href="#the-scenario">The scenario</a></li>
<li><a href="#kickstart">Kickstart</a></li>
<li><a href="#environment-setup">Environment Setup</a><ul>
<li><a href="#download-the-jars-by-running-this-command-from-terminal">Download the jars by running this command from terminal:</a></li>
<li><a href="#copy-the-jars-to-the-spark_homejars-location">Copy the jars to the SPARK_HOME/Jars location</a></li>
</ul>
</li>
<li><a href="#run-the-spark-code">Run the Spark Code</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-1">Appendix</a></li>
<li><a href="#programmatic-options-for-creating-containers-sorting-files-etc">Programmatic options for Creating Containers, Sorting Files etc:</a></li>
</ul>
</li>
<li><a href="#understanding-spark-configuration-sparkjarspackages-vs-sparkjars">Understanding Spark Configuration: <code>spark.jars.packages</code> vs <code>spark.jars</code></a></li>
<li><a href="#the-role-of-configsparkjarspackages-">The Role of <code>.config('spark.jars.packages', '...')</code></a><ul>
<li><a href="#how-it-works">How it Works</a></li>
<li><a href="#example-usage">Example Usage</a></li>
</ul>
</li>
<li><a href="#the-utility-of-configsparkjars-">The Utility of <code>.config('spark.jars', '...')</code></a><ul>
<li><a href="#how-it-functions">How it Functions</a></li>
<li><a href="#example-implementation">Example Implementation</a></li>
</ul>
</li>
<li><a href="#conclusion-1">Conclusion</a></li>
</ul>
<hr />
<h1 id="project-sparkzure-part1-connecting-local-spark-to-azure-data-lake">Project Sparkzure Part1 - Connecting Local Spark to Azure Data Lake</h1>
<h2 id="overview">Overview</h2>
<p>Azure Databricks to Azure Data Lake is easy and straightforward. All the requied jars pre-installed in Databricks. All you need to do is to create a session and connect. However, connecting a local Spark instance to Azure Data Lake can be complicated, especially when managing JAR dependencies. In this project, I will show you how to connect your local Spark application to ADLS and run a Spark query using Visual Studio Code. The local Spark application will be hosted in a container, but it can also be hosted locally locally ;-)</p>
<h2 id="kickstart-integrating-spark-with-azure-data-lake">Kickstart: Integrating Spark with Azure Data Lake</h2>
<h3 id="create-the-containerized-setup">Create the containerized setup</h3>
<p>Our environment is set up inside a Docker container running Ubuntu on a Windows OS host. Within this container, Python 3 and Spark are installed. But the steps can be used in local environments as well.</p>
<ul>
<li><strong>Check the python version in the container and find out site-packages directory</strong></li>
<li>
<p>Often, systems have both Python 2.x and Python 3.x installed. Use the following commands to determine which versions are available:
     <code>bash
     python --version
     python3 --version</code>
    <img alt="Alt text" src="../image-12.png" /></p>
</li>
<li>
<p>Determine where PySpark is installed using <code>pip</code>. Your enviornment may have multiple python installation especially if its linux or in a docker. You need to find the right <code>site-packages</code> directory so that the packages are copied to right location. To find out run this command in docker terminal or normal command prompt: </p>
<p><code>bash
 pip3 show pyspark | grep Location</code>
 Alternatively, you can get the location by running the command:
 <code>bash
 python3 -c "from distutils.sysconfig import get_python_lib; print(get_python_lib())"</code>
   <img alt="Alt text" src="../image-13.png" /></p>
</li>
<li>
<p><strong>Install <code>wget</code></strong></p>
</li>
<li>
<p><code>wget</code> is a tool for downloading files from the internet. If you don’t have it in your environment you can get it using the given command:
     <code>bash
     apt-get update &amp;&amp; apt-get install -y wget</code></p>
</li>
<li>
<p><strong>Download Hadoop ADLS JARs</strong></p>
<ul>
<li>I've downloaded and placed the jars <a href="../Hadoop_Azure_Jars.zip">here</a>. Download and copy it to a desired location.</li>
<li>Alternatively, run the command below to download jars to your home directory</li>
</ul>
<p><code>bash
cd ~
wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.3.3/hadoop-azure-3.3.3.jar
wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure-datalake/3.3.3/hadoop-azure-datalake-3.3.3.jar
wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.3.3/hadoop-common-3.3.3.jar
wget https://repo1.maven.org/maven2/com/microsoft/azure/azure-storage/8.6.6/azure-storage-8.6.6.jar
wget https://repo1.maven.org/maven2/com/azure/azure-security-keyvault-secrets/4.3.0/azure-security-keyvault-secrets-4.3.0.jar
wget https://repo1.maven.org/maven2/com/azure/azure-identity/1.3.0/azure-identity-1.3.0.jar</code>
- After downloading, place the jars in any desired folder. These jars will be referenced during spark session creation.
- Alternatively you can use download the jars <a href="mavencoordinates.html">on-the-fly</a> using maven coordinates using <code>.config('spark.jars.packages', '...')</code></p>
</li>
</ul>
<h3 id="register-an-app-for-oauth-authentication">Register an App for OAuth Authentication</h3>
<p>If you want to access a file(say CSV) in <strong>Azure</strong> through <strong>OAuth</strong> authentication, you need to create an <strong>App registration</strong> and grant this app permission to the CSV. This registered App's identity is used by <strong>Spark</strong> to authenticate. The same principle applies in <strong>Databricks</strong>, where an app is already created, named <strong>AzureDatabricks</strong>. Follow the steps below to register the app and give it permission to the file.</p>
<h4 id="register-a-new-app-using-app-registration">Register a new App using App Registration</h4>
<ul>
<li>In the Azure Portal to search for <strong>App registrations</strong>', select it, and opt for '+ New registration'. </li>
</ul>
<p><img alt="Image-6" src="../image-6.png" /></p>
<ul>
<li>Give a name, say <strong><code>adlssparkapp</code></strong>, choose 'Accounts in this organizational directory only', keep the Redirect URI empty, and click <strong>Register</strong>. </li>
</ul>
<p><img alt="Image-7" src="../image-7.png" /></p>
<h4 id="copy-ids-and-secret-from-the-app">Copy Ids and secret From The App</h4>
<ul>
<li>After registration, jot down the <strong>Application ID</strong> and <strong>Directory ID</strong>. </li>
</ul>
<p><img alt="Image-8" src="../image-8.png" /></p>
<ul>
<li>Go to <strong>Manage</strong> &gt; <strong>Certificates &amp; secrets</strong>, select <strong>+ New client secret</strong>, label it <strong><code>SparkAppSecret</code></strong>, set an expiration, and click  'Add'. </li>
</ul>
<p><img alt="Image-9" src="../image-9.png" /></p>
<ul>
<li>Post-creation, make note of the <strong>one-time</strong> viewable secret value essential for the Spark-Azure handshake. </li>
</ul>
<p><img alt="Image-10" src="../image-10.png" /></p>
<h4 id="give-the-app-permission-to-the-container">Give the App Permission to the Container</h4>
<ul>
<li>
<p>Open the container, navigate to <strong>Access Control (IAM)</strong> &gt; <strong>Role assignments</strong>, click <strong>Add</strong> &gt; <strong>Add role assignment</strong>, select <strong>Storage Blob Contributor</strong>, 
  <img alt="Alt text" src="../image-21.png" /></p>
</li>
<li>
<p>Search for the app <strong><code>adlssparkapp</code></strong>, and click <strong>OK</strong>.</p>
</li>
</ul>
<p><img alt="Alt text" src="../image-22.png" /></p>
<h3 id="access-adls-data-from-spark-using-oauth-authentication-and-service-principal">Access ADLS Data From Spark Using OAuth Authentication and Service Principal</h3>
<p>With the app now registered and the key, ID, and secret in hand, we can proceed to execute the main code. Follow the steps outlined below to continue:</p>
<h4 id="open-vs-code-and-connect-to-the-container">Open VS Code and Connect To the Container</h4>
<ul>
<li><strong>Open VS Code</strong>: Launch Visual Studio Code and click the remote container icon at the bottom left.</li>
</ul>
<p><img alt="Image-15" src="../image-15.png" /></p>
<ul>
<li><strong>Attach to Container</strong>: From the top menu, choose "Attach to running container".</li>
</ul>
<p><img alt="Image-16" src="../image-16.png" /></p>
<ul>
<li><strong>Select Container</strong>: Pick the displayed running container. </li>
</ul>
<p><img alt="Image-17" src="../image-17.png" /></p>
<p>This action launches a new VS Code instance connected to that container.</p>
<p><img alt="Alt text" src="../image-18.png" /></p>
<ul>
<li>
<p><strong>Create Notebook</strong>: In this instance, create a .ipynb (Jupyter notebook) to execute the subsequent section's code.
   <img alt="Alt text" src="../image-19.png" /></p>
</li>
<li>
<p><strong>Connect to the python version where we copied the hadoop jars</strong>
   There could be multiple python versions in a linux enviornment. From VS Code choose the python version whcih has our jars</p>
</li>
</ul>
<p><img alt="Alt text" src="../image-20.png" /></p>
<h4 id="run-the-code">Run the code</h4>
<pre><code>Run the code below in the jupyter notebok:
</code></pre>
<pre><code class="language-python">from pyspark.sql import SparkSession
# Initialize a Spark session with necessary configurations for connecting to ADLS
#Offline version
spark = SparkSession.builder \
    .appName(&quot;ADLS Access&quot;) \
    .config(&quot;spark.jars&quot;, &quot;/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar&quot;) \
    .getOrCreate()

# Or using maven coordinates
# Online version
spark = SparkSession.builder \
    .appName(&quot;ADLS Access&quot;) \
    .config(&quot;spark.jars.packages&quot;, 
            &quot;org.apache.hadoop:hadoop-azure:3.3.3,&quot;
            &quot;org.apache.hadoop:hadoop-azure-datalake:3.3.3,&quot;
            &quot;org.apache.hadoop:hadoop-common:3.3.3&quot;) \
    .getOrCreate()


# Define credentials and storage account details for ADLS access
storage_account = &quot;&lt;The_Storage_Act_Name_Containing_Container&gt;&quot;
app_client_id = &quot;&lt;The_Client_ID_From_Registered_App&gt;&quot;
app_directory_tenant_id = &quot;&lt;The_Client_ID_From_Registered_App&gt;&quot;
app_client_secret = &quot;&lt;The_Secret_Value_From_Registered_App&gt;&quot;

# Configure Spark to use OAuth authentication for ADLS access
spark.conf.set(f&quot;fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net&quot;, &quot;OAuth&quot;)
spark.conf.set(f&quot;fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net&quot;, &quot;org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider&quot;)
spark.conf.set(f&quot;fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net&quot;, app_client_id)
spark.conf.set(f&quot;fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net&quot;, app_client_secret)
spark.conf.set(f&quot;fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net&quot;, f&quot;https://login.microsoftonline.com/{app_directory_tenant_id}/oauth2/token&quot;)

# Define the path to the dataset in ADLS and read the CSV file using Spark
path = &quot;abfss://&lt;containerName&gt;@&lt;storaegaccountname&gt;.dfs.core.windows.net/&lt;CSV_File_Name.csv&gt;&quot;
spark.read.format(&quot;csv&quot;).load(path).show()

</code></pre>
<h3 id="access-data-in-adls-container-using-storage-accounts-access-key-method">Access data in ADLS container using Storage Account's Access Key Method</h3>
<p>Another methods to access ADLS is using the Access key method. Here we get the access key from the storage account then use it to access the files inside it. To use this method, follow these steps:</p>
<ul>
<li><strong>Get the Access Keys from the Storage Account</strong></li>
<li>In your storage account, under the <code>Security + networking</code> section in the left sidebar, find and select <code>Access keys</code>.</li>
<li>
<p>You’ll be presented with two keys: <code>key1</code> and <code>key2</code>. Both keys can be used to authenticate, so choose one and copy it. This will be used in the subsequent steps.</p>
<p><img alt="Alt text" src="../image-23.png" /></p>
</li>
<li>
<p><strong>Execute the code</strong><br />
After getting the access key use this code. Replace your access key in the access key location:</p>
</li>
</ul>
<pre><code class="language-python"># Import the required module for creating a Spark session.
from pyspark.sql import SparkSession

# Initialize the Spark session. The builder pattern is utilized to configure the session.
# We set the application name to &quot;ADLS Access&quot; for identification in Spark UI.
# Necessary JAR files are specified for Spark to connect and interact with Azure Data Lake Storage (ADLS).
spark = SparkSession.builder \
    .appName(&quot;ADLS Access&quot;) \
    .config(&quot;spark.jars&quot;, &quot;/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar&quot;) \
    .getOrCreate()

# Specify the Azure storage account name and the associated access key for authentication purposes.
storage_account_name = &quot;&lt;The_Storage_Account_Name&gt;&quot;
storage_account_key = &quot;&lt;key1_or_key2&gt;&quot;

# Configure Spark to utilize AzureBlobFileSystem. This is essential for Azure Blob storage connectivity.
spark.conf.set(f&quot;fs.azure&quot;, &quot;org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem&quot;)

# Authenticate the Spark session by providing the access key for the specified Azure storage account.
spark.conf.set(f&quot;fs.azure.account.key.{storage_account_name}.dfs.core.windows.net&quot;, storage_account_key)

# Read the desired CSV file located in ADLS into a DataFrame (df) using Spark.
df = spark.read.csv(f&quot;abfss://&lt;container_name&gt;@{storage_account_name}.dfs.core.windows.net/&lt;filename.csv&gt;&quot;)
</code></pre>
<h2 id="common-errors">Common Errors</h2>
<h3 id="authorizationpermissionmismatch-during-oauth-authenticaiton">AuthorizationPermissionMismatch During OAuth Authenticaiton</h3>
<p>While executing the code you may encounter errors like:</p>
<pre><code>AuthorizationPermissionMismatch, &quot;This request is not authorized to perform this operation using this permission.&quot;
</code></pre>
<p>or</p>
<pre><code>java.nio.file.AccessDeniedException: Operation failed: &quot;This request is not authorized to perform this operation using this permission.&quot;, 403, HEAD, https://strgacweatherapp.dfs.core.windows.net/weather-timer/2023-10-19-09.json?upn=false&amp;action=getStatus&amp;timeout=90
    at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.checkException(AzureBlobFileSystem.java:1384)
</code></pre>
<p><img alt="Alt text" src="../image-11.png" /></p>
<p>OAuth uses a registered apps identity to connect. This app should have permission to the folder where the file resides.</p>
<p><img alt="Alt text" src="../image-25.png" /></p>
<h2 id="appendix">Appendix</h2>
<h3 id="why-does-spark-rely-on-hadoop-libraries-to-access-azure-data-lake-storage-adls">Why Does Spark Rely on Hadoop Libraries to Access Azure Data Lake Storage (ADLS)?</h3>
<p><strong>Long story short</strong>: In a standalone Spark setup, we use specific Hadoop JARs solely for connecting to ADLS. It's important to note that these are just JARs and don't represent the full Hadoop ecosystem. </p>
<p>Apache Spark is used for distributed data processing. But for data storage it relies on other systems like ADLS, S3 etc. But why, when connecting Spark to ADLS, do we bring Hadoop into the picture? Let’s find out.</p>
<p><strong>Spark's Core Functionality:</strong>
Spark is designed to process data, not to understand the intricacies of every storage system. It can pull data from various sources, but it doesn't always have native integrations for each one.</p>
<p><strong>Hadoop's Role:</strong>
Hadoop, primarily known for its distributed file system (HDFS), also <strong>offers connectors to diverse storage systems</strong>. Over time, it has become the standard bridge between storage solutions and big data tools.</p>
<p><strong>ADLS and Hadoop Integration:</strong>
When Microsoft developed ADLS, they provided a connector to the Hadoop FileSystem API. This approach made sense. Why reinvent the wheel when big data tools already communicate efficiently with HDFS via Hadoop's API?</p>
<p><strong>Conclusion</strong></p>
<p>HSpark uses Hadoop libraries to access ADLS due to the standardized and robust nature of the Hadoop FileSystem API. Microsoft integrated ADLS with this Hadoop API to ensure that ADLS would be compatible with a broad range of big data tools, such as Spark and Hive. This decision was to use the extensive community support of the Hadoop ecosystem and also allowed Microsoft to reuse what was already working In essence, the Hadoop API serves as a bridge between Spark and ADLS.</p>
<h3 id="understanding-essential-jars-for-azure-data-lake-operations-with-spark">Understanding Essential JARs for Azure Data Lake Operations with Spark</h3>
<ol>
<li><strong>hadoop-azure-3.3.3.jar</strong>:</li>
<li><strong>Description</strong>: This library provides support for Azure Blob Storage integration with Hadoop. It contains the <code>WASB</code> (Windows Azure Storage Blob) file system connector.</li>
<li>
<p><strong>Use-Cases</strong>: Reading/writing data from/to Azure Blob Storage (often ADLS Gen1) using Hadoop's FileSystem API.</p>
</li>
<li>
<p><strong>hadoop-azure-datalake-3.3.3.jar</strong>:</p>
</li>
<li><strong>Description</strong>: This is the Data Lake connector for Hadoop, providing support for ADLS Gen1.</li>
<li>
<p><strong>Use-Cases</strong>: If you're working with ADLS Gen1, this JAR lets Spark access the data lake using the Hadoop FileSystem API.</p>
</li>
<li>
<p><strong>hadoop-common-3.3.3.jar</strong>:</p>
</li>
<li><strong>Description</strong>: The core library for Hadoop, it contains common utilities and the Hadoop FileSystem API.</li>
<li>
<p><strong>Use-Cases</strong>: Fundamental for almost all Hadoop-related operations. It's the foundational library upon which other Hadoop components rely.</p>
</li>
<li>
<p><strong>azure-storage-8.6.6.jar</strong>:</p>
</li>
<li><strong>Description</strong>: Azure's storage SDK, facilitating interaction with Azure Storage services like Blob, Queue, and Table.</li>
<li>
<p><strong>Use-Cases</strong>: Interacting with Azure Blob Storage (and by extension, ADLS Gen2 which is built on Blob). It's essential for Spark to communicate and access Azure storage services.</p>
</li>
<li>
<p><strong>azure-security-keyvault-secrets-4.3.0.jar</strong>:</p>
</li>
<li><strong>Description</strong>: Provides capabilities to interact with Azure Key Vault's secrets. It facilitates fetching, setting, or managing secrets.</li>
<li>
<p><strong>Use-Cases</strong>: Whenever you need to securely access or manage secrets (like storage account keys or database connection strings) stored in Azure Key Vault from your Spark application.</p>
</li>
<li>
<p><strong>azure-identity-1.3.0.jar</strong>:</p>
</li>
<li><strong>Description</strong>: Azure SDK's identity library, providing various credentials classes for Azure Active Directory (AAD) token authentication.</li>
<li><strong>Use-Cases</strong>: Authenticating against Azure services using AAD-based credentials, especially when trying to securely access resources like Key Vault or ADLS Gen2.</li>
</ol>
<hr />
<h1 id="project-sparkzure-part2-sorting-files-in-adls-container-using-standalone-spark">Project Sparkzure Part2 - Sorting Files in ADLS Container Using Standalone Spark</h1>
<h3 id="overview-of-the-article">Overview of the Article</h3>
<p>In Part 1, we dived into accessing ADLS files with Pyspark and Hadoop Jars. Now, let's switch gears a bit. In this article, we'll explore how to sort—by creating containers and moving/renaming files—the content in an Azure Data Lake Container using just a Standalone Spark application. While there's always the route of Azure Data Factory, Databricks, or Azure Logic Apps, I want to spotlight this approach. Why? Because it's not only a viable alternative, but it also comes with the perk of being nearly cost-free compared to the other Azure services I mentioned.</p>
<h3 id="my-environment">My Environment</h3>
<ul>
<li><strong>Deployment Platform:</strong> Docker</li>
<li><strong>Operating System:</strong> Ubuntu</li>
<li><strong>Python Version:</strong> 3.8.10</li>
<li><strong>Development IDE:</strong> Visual Studio Code connected to the container</li>
<li><strong>Spark Setup:</strong> Standalone Spark installed as part of Pyspark(<code>pip install pyspark</code>)</li>
<li><strong>Spark Home</strong> /usr/local/lib/python3.8/dist-packages/pyspark/</li>
<li><strong>Jars Location</strong> /usr/local/lib/python3.8/dist-packages/pyspark/jars/</li>
</ul>
<h3 id="the-scenario">The scenario</h3>
<p>We have a container name "weather-timer" that contains JSON files formatted as <code>YYYY-10-22-12.json</code>. These files hold weather information retrieved from a web API. The files need to be sorted in the format  <code>year=yyyy/month=mm/day=dd/hour=hh.json</code>. This is a real-world requirement, as a structure like this can make partition pruning more efficient during query time if you're using a system like Apache Hive or Delta Lake.</p>
<h3 id="kickstart">Kickstart</h3>
<h4 id="environment-setup">Environment Setup</h4>
<p>For ADLS connectivity in standalone PySpark, we need to download these 3 super-important Jars:
- <code>hadoop-azure-&lt;version&gt;.jar</code>: Supports Azure Blob Storage and Spark integration.
- <code>hadoop-azure-datalake-&lt;version&gt;.jar</code>: For for ADLS access, including authentication features.
- <code>hadoop-common-&lt;version&gt;.jar</code>: Contains utilities for the other JARs.</p>
<h5 id="download-the-jars-by-running-this-command-from-terminal">Download the jars by running this command from terminal:</h5>
<p>Run teh following command in terminal. Note: The version of jars might change over time.</p>
<p>```bash
cd ~
wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.3.3/hadoop-azure-3.3.3.jar
wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure-datalake/3.3.3/hadoop-azure-datalake-3.3.3.jar
wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.3.3/hadoop-common-3.3.3.jar</p>
<pre><code>##### Copy the jars to the SPARK_HOME/Jars location

```bash
   cd ~  # assuming you downloaded the JARs in the home directory
   cp *.jar /usr/local/lib/python3.8/dist-packages/pyspark/jars/
</code></pre>
<h4 id="run-the-spark-code">Run the Spark Code</h4>
<p>This is the code for performing the sorting. It checks if there are containers. If not, it makes them. Before you run the code, make sure you replace placeholders like <code>&lt;YOUR_STORAGE_ACT_NAME&gt;</code>, <code>&lt;YOUR_REG_APP_CLIENT_ID&gt;</code>, <code>&lt;YOUR_REG_APP_TENANT_ID&gt;</code>, <code>&lt;YOUR_REG_APP_CLIENT_SECRET&gt;</code>, <code>&lt;YOUR_CONTAINER_NAME&gt;</code>, and paths to JAR files with the actual values.. Also, update any other settings to match your system.</p>
<pre><code class="language-python">
# Importing the necessary module for SparkSession from the PySpark library.
from pyspark.sql import SparkSession

#Note: The location of jars is where we copied them after downloadign with wget. Just 3 jars.
spark = SparkSession.builder \
    .appName(&quot;ADLS Access&quot;) \
    .config(&quot;spark.jars&quot;, 
            &quot;/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,&quot;\
            &quot;/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,&quot;\
            &quot;/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar&quot;) \
    .getOrCreate()


# Configuring PySpark for Azure Data Lake Storage (ADLS) Authentication using OAuth and Service Principal
# Credentials and configurations
storage_account_name = &quot;&lt;YOUR_STORAGE_ACT_NAME&gt;&quot;
regapp_client_id = &quot;&lt;YOUR_REG_APP_CLIENT_ID&gt;&quot; # Application (client) ID of the registered app
regapp_directory_id = &quot;&lt;YOUR_REG_APP_TENANT_ID&gt;&quot; # Directory (tenant) ID of the registered app
regapp_client_secret = &quot;&lt;YOUR_REG_APP_CLIENT_SECRET&gt;&quot;

# Set the authentication type to OAuth for the specified storage account---------------
spark.conf.set(f&quot;fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net&quot;, &quot;OAuth&quot;)

# Define the token provider type for OAuth. The 'ClientCredsTokenProvider' is specified for the client credentials flow.
spark.conf.set(f&quot;fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net&quot;, &quot;org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider&quot;)

# Provide the client ID (application ID) of the registered application in Azure Active Directory (AD).
spark.conf.set(f&quot;fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net&quot;, regapp_client_id)

# Set the client secret of the registered application. This acts as a password for the application to verify its identity.
spark.conf.set(f&quot;fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net&quot;, regapp_client_secret)

# Specify the OAuth 2.0 token endpoint, allowing the application to retrieve tokens for authentication.
spark.conf.set(f&quot;fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net&quot;, f&quot;https://login.microsoftonline.com/{regapp_directory_id}/oauth2/token&quot;)
#----------------------------------------------------------------------------------------
#---------Code to perform the sorting----------------------------------------------------
# Define the ADLS Gen2 base path
base_path = f&quot;abfss://&lt;YOUR_CONTAINER_NAME&gt;@&lt;YOUR_STORAGE_ACT_NAME&gt;.dfs.core.windows.net/&quot;

conf = spark._jsc.hadoopConfiguration()
conf.set(&quot;fs.abfss.impl&quot;, &quot;org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem&quot;)
uri = spark._jvm.java.net.URI
path_obj = spark._jvm.org.apache.hadoop.fs.Path(base_path)
file_system = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri(base_path), conf)

old_files = [status.getPath().toString() for status in file_system.globStatus(spark._jvm.org.apache.hadoop.fs.Path(base_path + &quot;*-*.json&quot;))]

# Diagnostic: Check the number of files fetched
print(f&quot;Number of files to be processed: {len(old_files)}&quot;)

# Test with a subset (for diagnostic purposes)
subset_of_files = old_files[:5]

for old_file_path in old_files:
    # Extract year, month, day, and hour from the old file path
    filename = old_file_path.split('/')[-1]
    year, month, day, hour = filename.split('-')[:4]

    # Construct the new directory structure based on the desired format
    new_directory = base_path + f&quot;year={year}/month={month}/day={day}/&quot;

    # Check if the directory exists; if not, create it
    if not file_system.exists(spark._jvm.org.apache.hadoop.fs.Path(new_directory)):
        file_system.mkdirs(spark._jvm.org.apache.hadoop.fs.Path(new_directory))

    # Construct the new file path
    new_file_path = new_directory + f&quot;hour={hour}&quot;

    # Diagnostic: Printing the move action
    print(f&quot;Moving {old_file_path} to {new_file_path}&quot;)

    # Rename (move) the file to the new path and check if it's successful
    success = file_system.rename(spark._jvm.org.apache.hadoop.fs.Path(old_file_path), spark._jvm.org.apache.hadoop.fs.Path(new_file_path))

    # Diagnostic: Check if the move was successful
    print(f&quot;Move success: {success}&quot;)

print(&quot;Files rearranged successfully for the subset!&quot;)
#---------Code to perform the sorting----------------------------------------------------

</code></pre>
<h3 id="conclusion">Conclusion</h3>
<p>There are many ways to organize files in a container, like using ADF, Databricks, or Logic Apps. But this way is good too because it's free, unlike some pretty-expensive options like Databricks. I shared this article to let us know there's another option out there. It shows how we perform such operation on Azure DataLake from an outside standalone application.</p>
<h3 id="appendix_1">Appendix</h3>
<h4 id="programmatic-options-for-creating-containers-sorting-files-etc">Programmatic options for Creating Containers, Sorting Files etc:</h4>
<p>If you want to compare what other programmatic options we have to peroform such operation, here is the comparison:</p>
<ol>
<li>
<p><strong>Reading/Writing Large Datasets</strong>: </p>
<ul>
<li><strong>Best Tool</strong>: PySpark.</li>
<li><strong>Reason</strong>: Spark is designed for distributed data processing. Reading and processing large datasets from ADLS Gen2 into Spark dataframes will be efficient.</li>
</ul>
</li>
<li>
<p><strong>Listing Files in a Container/Directory</strong>:</p>
<ul>
<li><strong>Best Tool</strong>: PySpark or Hadoop FileSystem API.</li>
<li><strong>Reason</strong>: PySpark provides simple methods to list files, but if you're already interfacing with Hadoop's FileSystem API for other tasks, it's also a good choice.</li>
</ul>
</li>
<li>
<p><strong>Renaming or Moving Files</strong>:</p>
<ul>
<li><strong>Best Tool</strong>: Hadoop FileSystem API.</li>
<li><strong>Reason</strong>: While this can be done with the Azure SDK, the Hadoop FileSystem API provides a more direct interface when working alongside Spark.</li>
</ul>
</li>
<li>
<p><strong>Creating Containers or Directories</strong>:</p>
<ul>
<li><strong>Best Tool</strong>: Azure SDK (<code>azure-storage-file-datalake</code>).</li>
<li><strong>Reason</strong>: Creating containers or directories is a simple storage management task. The Azure SDK provides direct methods to do this without unnecessary overhead.</li>
</ul>
</li>
<li>
<p><strong>Setting Permissions or Managing Access</strong>:</p>
<ul>
<li><strong>Best Tool</strong>: Azure SDK.</li>
<li><strong>Reason</strong>: Managing permissions or access control is more straightforward with the Azure SDK, which provides methods tailored for these tasks.</li>
</ul>
</li>
</ol>
<hr />
<h1 id="understanding-spark-configuration-sparkjarspackages-vs-sparkjars">Understanding Spark Configuration: <code>spark.jars.packages</code> vs <code>spark.jars</code></h1>
<p>Apache Spark offers robust options for integrating external libraries, crucial for expanding its native capabilities. Two such configurations often used are <code>spark.jars.packages</code> and <code>spark.jars</code>. Understanding the distinct roles and applications of these configurations can significantly enhance how you manage dependencies in your Spark applications.</p>
<h2 id="the-role-of-configsparkjarspackages">The Role of <code>.config('spark.jars.packages', '...')</code></h2>
<p>This configuration is quintessential when it comes to managing library dependencies via Maven coordinates. It's designed to streamline the process of including external libraries in your Spark application.</p>
<h3 id="how-it-works">How it Works</h3>
<ul>
<li><strong>Maven Coordinates</strong>: You specify the library using its Maven coordinates in the format <code>'groupId:artifactId:version'</code>.</li>
<li><strong>Automatic Download</strong>: Spark automates the download process, fetching the specified library from Maven Central or another configured Maven repository.</li>
<li><strong>Ease of Use</strong>: This method is particularly user-friendly, ensuring you're incorporating the correct library version without manually downloading the JAR files.</li>
</ul>
<h3 id="example-usage">Example Usage</h3>
<pre><code class="language-python">.config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0')
</code></pre>
<p>In this instance, Spark is instructed to download and include the Kafka connector compatible with Spark version 2.12 and version 3.3.0 of the library.</p>
<h2 id="the-utility-of-configsparkjars">The Utility of <code>.config('spark.jars', '...')</code></h2>
<p>Contrasting <code>spark.jars.packages</code>, the <code>spark.jars</code> configuration is utilized when directly referencing locally stored JAR files.</p>
<h3 id="how-it-functions">How it Functions</h3>
<ul>
<li><strong>Local File Path</strong>: You provide the absolute path to the JAR file already present on your system.</li>
<li><strong>No Automatic Download</strong>: Spark bypasses any downloading process, relying on the specified JAR file's presence in the given location.</li>
<li><strong>Custom or Offline Use</strong>: This approach is ideal for using custom library versions or in environments with restricted internet access.</li>
</ul>
<h3 id="example-implementation">Example Implementation</h3>
<pre><code class="language-python">.config('spark.jars', '/opt/shared-data/spark-sql-kafka-0-10_2.13-3.4.0.jar')
</code></pre>
<p>Here, Spark is directed to incorporate a Kafka connector JAR file located at <code>/opt/shared-data/spark-sql-kafka-0-10_2.13-3.4.0.jar</code>.</p>
<h2 id="conclusion_1">Conclusion</h2>
<p>In summary, <code>spark.jars.packages</code> is a hassle-free solution for incorporating libraries using Maven coordinates, automating the downloading and version management. In contrast, <code>spark.jars</code> is suited for scenarios where you have a local JAR file, offering more control over the specific version and source of the library being used. The choice between these configurations hinges on your project's requirements and operational environment, providing flexibility in managing your Spark application's dependencies.</p>
<p>© D Das<br />
📧 <a href="mailto:das.d@hotmail.com">das.d@hotmail.com</a> | <a href="mailto:ddasdocs@gmail.com">ddasdocs@gmail.com</a></p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../../../js/bootstrap.bundle.min.js"></script>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js"></script>
        <script src="../../../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
