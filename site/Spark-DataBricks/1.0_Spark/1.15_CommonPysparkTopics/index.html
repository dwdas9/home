<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>PySpark Concepts I - My Docs</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/fontawesome.min.css" rel="stylesheet">
        <link href="../../../css/brands.min.css" rel="stylesheet">
        <link href="../../../css/solid.min.css" rel="stylesheet">
        <link href="../../../css/v4-font-face.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" >
        <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" disabled>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../../..">My Docs</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar-collapse" aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="nav-item">
                                <a href="../../.." class="nav-link">Welcome to MkDocs</a>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Airflow</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../Airflow/1.0.0_AirFlow_Concepts/" class="dropdown-item">Airflow Concepts</a>
</li>
                                    
<li>
    <a href="../../../Airflow/1.0.1_A_Dags_Anatomy/" class="dropdown-item">A dags anatomy</a>
</li>
                                    
<li>
    <a href="../../../Airflow/1.0.2_Hello_Airflow/" class="dropdown-item">Hello-Airflow</a>
</li>
                                    
<li>
    <a href="../../../Airflow/1.0.3_Airflow_Dbt_Docker/" class="dropdown-item">Project Airflow dbt</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">DE Projects</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../DE-Projects/Csv-To-MSSQL/" class="dropdown-item">Python - CSV To MSSQL</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/CurrencyPredictor/" class="dropdown-item">CurrencyPredictor</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/Dbrk-E2E-AttritionProject/" class="dropdown-item">Problem Statement</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/Download-Haddop-Jars/" class="dropdown-item">Download JARs-Apache Maven Repo</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/FetchJsonWriteParquet/" class="dropdown-item">Json To Parquet Using Spark And Azure</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/InstallScala/" class="dropdown-item">Scala Install</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/JsonFlatAzureSDK/" class="dropdown-item">Flatten Json Using Azure SDK</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/LocalPython_AzureBlob/" class="dropdown-item">Local Python Code to Rearrange Files in a Azure Blob Container</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/Microsoft_OpenJDK/" class="dropdown-item">Microsoft OpenJDK</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/Project_MigrationToAzureBlob/" class="dropdown-item">CMS Migration to Azure Blob</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/Project_MongoCMS/" class="dropdown-item">CMS using MongoDB</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/Project_SSRS_SSIS_SharePoint/" class="dropdown-item">Project - ETL and Reporting Lending Org</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/Raw-Json-To-Hive/" class="dropdown-item">Raw Json To Hive</a>
</li>
                                    
<li>
    <a href="../../../DE-Projects/SPY_ETF_Buy_Recommender/" class="dropdown-item">SPY ETF Buy Recommender</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">AzureSkyWeather</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DE-Projects/AzureSkyWeather/HomeProjectAzureSkyWeather/" class="dropdown-item">Project AzureSkyWeather</a>
</li>
            
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">1 Ingestion</a>
    <ul class="dropdown-menu">
            
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">HttpTriggered</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc/" class="dropdown-item">Part 1A - Using Azure HTTP-Triggered Function</a>
</li>
            
<li>
    <a href="../../../DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions/" class="dropdown-item">Azure Functions Quickstart</a>
</li>
    </ul>
  </li>
            
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">TimerTriggered</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc/" class="dropdown-item">Part 1B - Using Azure Timer-Triggered Function</a>
</li>
    </ul>
  </li>
    </ul>
  </li>
            
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">2 Transformation</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details/" class="dropdown-item">Solution Details</a>
</li>
    </ul>
  </li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">JsonValidator</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DE-Projects/JsonValidator/AzureFunction-ValidateJSOns/" class="dropdown-item">Azure Functions - Validate JSONs</a>
</li>
            
<li>
    <a href="../../../DE-Projects/JsonValidator/Python-ValidateJSONs/" class="dropdown-item">Validate JSON using Python</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">Sparkzure</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DE-Projects/Sparkzure/HomeProjectSparkzure/" class="dropdown-item">Project Sparkzure</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">StreamKraft</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DE-Projects/StreamKraft/HomeProjectStreamKraft/" class="dropdown-item">Project StreamKraft</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">DevOps</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../DevOps/1.1_Hello_GitHub_Actions_Workflow/" class="dropdown-item">Hello GitHub Actions Workflow</a>
</li>
                                    
<li>
    <a href="../../../DevOps/1.1_Sample_Workflows/" class="dropdown-item">Sample Workflows</a>
</li>
                                    
<li>
    <a href="../../../DevOps/1_GitHub-Concepts/" class="dropdown-item">GitHub Concepts</a>
</li>
                                    
<li>
    <a href="../../../DevOps/2.1_Self-Hosted_Agent_Windows/" class="dropdown-item">Self-Hosted-Agent-Windows</a>
</li>
                                    
<li>
    <a href="../../../DevOps/2.2_Self-Hosted_Agent_Windows_Container/" class="dropdown-item">Self-Hosted-Agent-Win-Container</a>
</li>
                                    
<li>
    <a href="../../../DevOps/2.3_Self-Hosted_Agent_Linux_Container/" class="dropdown-item">Self-Hosted-Agent-Ubuntu-Container</a>
</li>
                                    
<li>
    <a href="../../../DevOps/2_Azure-Pipelines/" class="dropdown-item">Azure-Pipelines</a>
</li>
                                    
<li>
    <a href="../../../DevOps/ADF_CICD/" class="dropdown-item">ADF-CI-CD</a>
</li>
                                    
<li>
    <a href="../../../DevOps/Biceps/" class="dropdown-item">What is Bicep?</a>
</li>
                                    
<li>
    <a href="../../../DevOps/CI-CD_in%20ADF/" class="dropdown-item">CI CD in ADF</a>
</li>
                                    
<li>
    <a href="../../../DevOps/GitScenarios/" class="dropdown-item">GitScenarios</a>
</li>
                                    
<li>
    <a href="../../../DevOps/JenkinsVsGitHubVsAzureDevOps/" class="dropdown-item">Jenkins Vs GitHub Vs Devops</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">DockerAndKubernetes</a>
                                <ul class="dropdown-menu">
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">AirflowDocker</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DockerAndKubernetes/AirflowDocker/1_AirflowDocker/" class="dropdown-item">Airflow</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">Kafka</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DockerAndKubernetes/Kafka/2_Confluent%20Kafka/" class="dropdown-item">Confluent Kafka</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">Mongodb</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DockerAndKubernetes/Mongodb/3_DockerMongodb/" class="dropdown-item">MongoDB</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">SparkHiveHadoop</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.1_PySpark/" class="dropdown-item">PySpark</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.2_Bitnami_Spark_Cluster/" class="dropdown-item">Bitnami Spark</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.3_VSCode_Docker_Connection/" class="dropdown-item">VSCode-Docker-Connection</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.4_Spark_Hive_MSSQL/" class="dropdown-item">Spark-Hive_MSSQL</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.5_Hadoop_Cluster_Single_N_MultiNode/" class="dropdown-item">Hadoop Cluster</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.6_Hive_Hadooop_Postgres_Presto/" class="dropdown-item">bde2020-Hive-Hadoop-Presto</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.7_Hadoop_Hive_SingleNode_MySQL/" class="dropdown-item">Hive-Hadoop-MySQL-SingleNode</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.8_Hive-ApacheOfficial/" class="dropdown-item">Hive Official Setup</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.9.1_Hive_Concepts/" class="dropdown-item">Hive Concepts</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.9.2_Hadoop_Concepts/" class="dropdown-item">Hadoop Concepts</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4.9_DockerConcepts/" class="dropdown-item">Docker Misc Concepts</a>
</li>
            
<li>
    <a href="../../../DockerAndKubernetes/SparkHiveHadoop/4_Spark-Hive-Hadoop/" class="dropdown-item">4 Spark Hive Hadoop</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">M365</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../M365/DocumentumToSharePoint/" class="dropdown-item">Documentum-SharePoint Online</a>
</li>
                                    
<li>
    <a href="../../../M365/LicensingExamples/" class="dropdown-item">M365 Licensing Examples</a>
</li>
                                    
<li>
    <a href="../../../M365/SPMT/" class="dropdown-item">SPMT - SharePoint Migration</a>
</li>
                                    
<li>
    <a href="../../../M365/SharePoint2007FarmUpgrade/" class="dropdown-item">SharePoint Farm Upgrade</a>
</li>
                                    
<li>
    <a href="../../../M365/SharePoint2016FarmUpgrade/" class="dropdown-item">SharePoint Farm 2016 Farm Setup</a>
</li>
                                    
<li>
    <a href="../../../M365/SharePointEvents/" class="dropdown-item">SharePoint Event Receiver</a>
</li>
                                    
<li>
    <a href="../../../M365/SharePointFarmConsolidation/" class="dropdown-item">SharePoint Farm Consolidation</a>
</li>
                                    
<li>
    <a href="../../../M365/SharePointFormsOrPowerApps/" class="dropdown-item">SharePoint Forms or PowerApps</a>
</li>
                                    
<li>
    <a href="../../../M365/SharePointMiniRole/" class="dropdown-item">SharePoint Mini Role</a>
</li>
                                    
<li>
    <a href="../../../M365/SharePointVersionEvolution/" class="dropdown-item">SharePoint Evolution</a>
</li>
                                    
<li>
    <a href="../../../M365/SharePointVsOtherECM/" class="dropdown-item">SharePoint vs Other ECMS</a>
</li>
                                    
<li>
    <a href="../../../M365/WSS3DocumentUpload/" class="dropdown-item">Project - C# - WSS 3 Bulk ingestion</a>
</li>
                                    
<li>
    <a href="../../../M365/oAuthSharePointPython/" class="dropdown-item">Python-oAuth-SharePointOnline</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Microsoft Fabric</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../Microsoft-Fabric/DataFactory/" class="dropdown-item">Pipelines&DataFlows</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/DataScience/" class="dropdown-item">Data Science on Fabric Overview</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/DataWareHouse/" class="dropdown-item">DataWareHouse</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/DirectLake/" class="dropdown-item">DirectLake|Fabric|PowerBI</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/E2EProject/" class="dropdown-item">E2EProject</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse/" class="dropdown-item">JSON-DeltaLake-OPG</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse/" class="dropdown-item">ETL-Load data into Lakehouse - Pyspark Notebook</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/FabricAdministration/" class="dropdown-item">Administration</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/FabricQ%26A/" class="dropdown-item">Fabric Q&A</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/FabricSparkStreaming/" class="dropdown-item">Spark Streaming</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/HelloMicrosoftFabric/" class="dropdown-item">Hello Fabric</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/InspectingDataframes/" class="dropdown-item">Whats in your df?</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/KQL/" class="dropdown-item">KQL</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/PandasVsSparkDf/" class="dropdown-item">PandasVsSparkDf</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/Pyspark_SparkSQL/" class="dropdown-item">Pyspark|SparkSQL CheatSheet</a>
</li>
                                    
<li>
    <a href="../../../Microsoft-Fabric/RealTimeAnalytics/" class="dropdown-item">Real-time Intelligence</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Misc</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../Misc/10_Fact_vs_Dimension_tables/" class="dropdown-item">Fact Vs Dimension Table</a>
</li>
                                    
<li>
    <a href="../../../Misc/1_GoogleCloudSeeUsage/" class="dropdown-item">Check Google Usage</a>
</li>
                                    
<li>
    <a href="../../../Misc/2_InstallVMWareFree/" class="dropdown-item">VMWare For Free</a>
</li>
                                    
<li>
    <a href="../../../Misc/3_Markdown/" class="dropdown-item">Markdown</a>
</li>
                                    
<li>
    <a href="../../../Misc/4_VSTrics/" class="dropdown-item">Visual Studio Code Tricks</a>
</li>
                                    
<li>
    <a href="../../../Misc/5_WhichDatastsToUse/" class="dropdown-item">Free Datasets for Data Practice</a>
</li>
                                    
<li>
    <a href="../../../Misc/6_RunningAppsInBg/" class="dropdown-item">Running Service in Background</a>
</li>
                                    
<li>
    <a href="../../../Misc/7_Azure_Budget/" class="dropdown-item">How to set a Azure Budget</a>
</li>
                                    
<li>
    <a href="../../../Misc/9_WhyUbuntuIsGood/" class="dropdown-item">Why Ubuntu is good to learn</a>
</li>
                                    
<li>
    <a href="../../../Misc/markdown_pdf_export_html/" class="dropdown-item">Markdown pdf export html</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">MarkdownColor.md</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../Misc/MarkdownColor.md/Color/" class="dropdown-item">Color Reference</a>
</li>
            
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">Images</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../Misc/MarkdownColor.md/images/Color/" class="dropdown-item">Color Palettes</a>
</li>
    </ul>
  </li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">MongoDB</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../MongoDB/HowMongoDBStoresFiles/" class="dropdown-item">How MongoDB Stores Data</a>
</li>
                                    
<li>
    <a href="../../../MongoDB/MongbDB_Vs_Atlas_VsCosmosDB/" class="dropdown-item">MongoDB Vs CosmosDB</a>
</li>
                                    
<li>
    <a href="../../../MongoDB/MongoDBCommands/" class="dropdown-item">MongoDB Commands</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">PowerPlatform</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../PowerPlatform/CalculationGroups/" class="dropdown-item">CalculationGroups</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/CustomConnectors/" class="dropdown-item">Building Custom Connectors</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/ECMCaptureFlow/" class="dropdown-item">Power Automate Or Kofax/Captiva?</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/EnableMicrosoftSyntex/" class="dropdown-item">Enable Microsoft Syntex on your Office 365 tenant</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/EnableSyntexOnYourDocumentLibrary/" class="dropdown-item">Create a model on SharePoint - Syntex</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/GoogleProviderPowerPages/" class="dropdown-item">Google Authentication</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/HealthClinicDataverseSecurity/" class="dropdown-item">HealthClinicDataverseSecurity</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/HelloDataverse/" class="dropdown-item">Hello Dataverse</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/HelloDynamics365/" class="dropdown-item">Dynamics 365 Ecosystem</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/HelloPowerPlatform/" class="dropdown-item">PowerPlatform Ecosystem</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/ModelDrivenApps/" class="dropdown-item">Model-Driven Apps</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/OnPremiseGateway/" class="dropdown-item">Onpremise Gateway</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/PowerAutomateIsWorkflowTeams/" class="dropdown-item">Workflow is Power Automate</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/PowerPlatformAdminCentral/" class="dropdown-item">PowerPlatformAdminCentral</a>
</li>
                                    
<li>
    <a href="../../../PowerPlatform/PowerPlatformQ%26A/" class="dropdown-item">Pl-900-Q&A</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">DocumentIntelligence</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch/" class="dropdown-item">Integrate AI Search and Azure AI Document Intelligence</a>
</li>
            
<li>
    <a href="../../../PowerPlatform/DocumentIntelligence/AAIDI_Q%26A/" class="dropdown-item">AAIDI Q&A</a>
</li>
            
<li>
    <a href="../../../PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence/" class="dropdown-item">Azure AI Document Intelligence</a>
</li>
            
<li>
    <a href="../../../PowerPlatform/DocumentIntelligence/DocumentAutomation/" class="dropdown-item">Document automation base kit</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Python</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../Python/1.0_Sets/" class="dropdown-item">Sets</a>
</li>
                                    
<li>
    <a href="../../../Python/1.1.0_assert_methods/" class="dropdown-item">assert methods</a>
</li>
                                    
<li>
    <a href="../../../Python/1.1.1_decorators/" class="dropdown-item">decorators</a>
</li>
                                    
<li>
    <a href="../../../Python/1.1.2_argv/" class="dropdown-item">argv</a>
</li>
                                    
<li>
    <a href="../../../Python/1.1.3_Diff_And_Patch/" class="dropdown-item">diffAndpatch</a>
</li>
                                    
<li>
    <a href="../../../Python/1.1.3_error_handling/" class="dropdown-item">Error Handling</a>
</li>
                                    
<li>
    <a href="../../../Python/1.1.4_pdb/" class="dropdown-item">pdb</a>
</li>
                                    
<li>
    <a href="../../../Python/1.1.5_pyformat/" class="dropdown-item">format method</a>
</li>
                                    
<li>
    <a href="../../../Python/1.10_Func_Modl_Summary/" class="dropdown-item">LFM Summary</a>
</li>
                                    
<li>
    <a href="../../../Python/1.11_ifelifelse/" class="dropdown-item">ifelifelse</a>
</li>
                                    
<li>
    <a href="../../../Python/1.12_Operators/" class="dropdown-item">operators</a>
</li>
                                    
<li>
    <a href="../../../Python/1.13_For_Loops/" class="dropdown-item">for loops</a>
</li>
                                    
<li>
    <a href="../../../Python/1.14_enumerate/" class="dropdown-item">1.14 enumerate</a>
</li>
                                    
<li>
    <a href="../../../Python/1.15_range_function/" class="dropdown-item">range function</a>
</li>
                                    
<li>
    <a href="../../../Python/1.16_built_in_functions/" class="dropdown-item">1.16 built in functions</a>
</li>
                                    
<li>
    <a href="../../../Python/1.17_withStatement/" class="dropdown-item">with statement</a>
</li>
                                    
<li>
    <a href="../../../Python/1.18_unittest_pytest/" class="dropdown-item">pytest</a>
</li>
                                    
<li>
    <a href="../../../Python/1.19_if_name_main.md/" class="dropdown-item">if__name__main</a>
</li>
                                    
<li>
    <a href="../../../Python/1.1_Tuples/" class="dropdown-item">Tuples</a>
</li>
                                    
<li>
    <a href="../../../Python/1.2_Tuples_Advanced/" class="dropdown-item">Advanced Tuples</a>
</li>
                                    
<li>
    <a href="../../../Python/1.3_List/" class="dropdown-item">1.3 List</a>
</li>
                                    
<li>
    <a href="../../../Python/1.4_Dictionaries/" class="dropdown-item">1.4 Dictionaries</a>
</li>
                                    
<li>
    <a href="../../../Python/1.5_Lamda_Functions/" class="dropdown-item">Lamda Functions</a>
</li>
                                    
<li>
    <a href="../../../Python/1.9_Func_Modl_Lib/" class="dropdown-item">Library-Modules-Funcs</a>
</li>
                                    
<li>
    <a href="../../../Python/1_Python/" class="dropdown-item">Python</a>
</li>
                                    
<li>
    <a href="../../../Python/Linux/" class="dropdown-item">Essential Unix Commands</a>
</li>
                                    
<li>
    <a href="../../../Python/Pyspark/" class="dropdown-item">PySpark</a>
</li>
                                    
<li>
    <a href="../../../Python/PythonScripts/" class="dropdown-item">Python Sample Scripts</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">GraphAPIJupyter</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../../Python/GraphAPIJupyter/GraphAPIUsingJuputer/" class="dropdown-item">Graph API - Juputer</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">SQL</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../SQL/1.0.0_dbt/" class="dropdown-item">dbt</a>
</li>
                                    
<li>
    <a href="../../../SQL/1.0.1_setup_simple_dbt_project/" class="dropdown-item">1.0.1 setup simple dbt project</a>
</li>
                                    
<li>
    <a href="../../../SQL/FlatFileSoure/" class="dropdown-item">FlatFileSoure</a>
</li>
                                    
<li>
    <a href="../../../SQL/InputAndOutputProperties/" class="dropdown-item">Understanding External Columns and Output Columns in SSIS</a>
</li>
                                    
<li>
    <a href="../../../SQL/MSSQL_Versions/" class="dropdown-item">SQL Server Versions</a>
</li>
                                    
<li>
    <a href="../../../SQL/Project_1-ETL-CSV-MSSQL/" class="dropdown-item">Project 1 - ETL Flat Files to MSSQL</a>
</li>
                                    
<li>
    <a href="../../../SQL/Project_2-UsingWebServicesInSSIS/" class="dropdown-item">Project 2 - Web Service SSIS Script Task</a>
</li>
                                    
<li>
    <a href="../../../SQL/SQL/" class="dropdown-item">Spark-SQL</a>
</li>
                                    
<li>
    <a href="../../../SQL/SQL_AdvancedTopics/" class="dropdown-item">SQL Advanced Topics</a>
</li>
                                    
<li>
    <a href="../../../SQL/SSIS/" class="dropdown-item">SQL Server Integration Services</a>
</li>
                                    
<li>
    <a href="../../../SQL/SSRS/" class="dropdown-item">SSRS</a>
</li>
                                    
<li>
    <a href="../../../SQL/Windows_Functions/" class="dropdown-item">Window Functions</a>
</li>
                                    
<li>
    <a href="../../../SQL/connecting-with-dbt/" class="dropdown-item">Connect Local dbt with MSSQL Server</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle active" aria-current="page" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Spark DataBricks</a>
                                <ul class="dropdown-menu">
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">1.0 Spark</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../1.0_Spark-Concepts/" class="dropdown-item">Spark</a>
</li>
            
<li>
    <a href="../1.10_Scala_Cheatsheet/" class="dropdown-item">Scala Cheatsheet</a>
</li>
            
<li>
    <a href="../1.11_Spark_Interview_Questions/" class="dropdown-item">Spark Interview Questions</a>
</li>
            
<li>
    <a href="../1.12_Spark_Shuffle/" class="dropdown-item">Shuffle in Spark</a>
</li>
            
<li>
    <a href="../1.13_SparkDatabaseTablesCatalogsMetastore/" class="dropdown-item">Spark DB-Tables-Metastore-Catalogs</a>
</li>
            
<li>
    <a href="../1.14_Q%26A/" class="dropdown-item">Q&A</a>
</li>
            
<li>
    <a href="./" class="dropdown-item active" aria-current="page">PySpark Concepts I</a>
</li>
            
<li>
    <a href="../1.16_ConnectingSparkToHive/" class="dropdown-item">Spark-Hive-Delta Connection</a>
</li>
            
<li>
    <a href="../1.1_NarrowVsWideTransformation/" class="dropdown-item">Narrow_Vs_Wide_Transformation</a>
</li>
            
<li>
    <a href="../1.2_SparkArchitecture/" class="dropdown-item">Spark Architecture</a>
</li>
            
<li>
    <a href="../1.3_persist_and_cache/" class="dropdown-item">persist and cache</a>
</li>
            
<li>
    <a href="../1.4_broadcastvariables/" class="dropdown-item">Broadcast Variables</a>
</li>
            
<li>
    <a href="../1.5_DataSkewHandling/" class="dropdown-item">Data Skew in Spark</a>
</li>
            
<li>
    <a href="../1.6_dropna_fillna_df_missing_val_handling/" class="dropdown-item">dropna and fillna</a>
</li>
            
<li>
    <a href="../1.7_distinct_dropDuplicate_windowsFunc/" class="dropdown-item">Removing Duplicates - PySpark</a>
</li>
            
<li>
    <a href="../1.8_Partition_Grouping/" class="dropdown-item">Partition And Bucket</a>
</li>
            
<li>
    <a href="../1.9_RDD_Dataframe_Dataset/" class="dropdown-item">RDD-Dataframe-Dataset</a>
</li>
            
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">Install Pyspark Windows</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../Install-Pyspark-Windows/Install-Pyspark-Windows/" class="dropdown-item">Install-PySpark-Windows</a>
</li>
    </ul>
  </li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">2.0 Spark To ADLS</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../2.0_Spark_To_ADLS/2.0_Spark_To_ADLS/" class="dropdown-item">Spark-To-ADLS-Connection</a>
</li>
            
<li>
    <a href="../../2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary/" class="dropdown-item">Spark-To-ADLS-Summary</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">3.0 Databricks</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../3.0_Databricks/3.0_Databricks_Concepts/" class="dropdown-item">Databricks</a>
</li>
            
<li>
    <a href="../../3.0_Databricks/3.1_Catalogs_And_Metastore/" class="dropdown-item">Hive Metastore and the hive_metastore folder</a>
</li>
            
<li>
    <a href="../../3.0_Databricks/3.2_AuthenticationMethods/" class="dropdown-item">Authentication Method</a>
</li>
            
<li>
    <a href="../../3.0_Databricks/3.3_Mount_ADLS_on_Databricks/" class="dropdown-item">Mount ADLS on Databricks</a>
</li>
            
<li>
    <a href="../../3.0_Databricks/3.4_Databricks_Secret_Scope/" class="dropdown-item">Secret Scope</a>
</li>
            
<li>
    <a href="../../3.0_Databricks/3.5_Databricks_SQL/" class="dropdown-item">CREATE TABLE USING</a>
</li>
            
<li>
    <a href="../../3.0_Databricks/3.6_DatabricksMagicCommands/" class="dropdown-item">Magic Commands</a>
</li>
            
<li>
    <a href="../../3.0_Databricks/3.7_DeltaLake_And_Lakehouse/" class="dropdown-item">Delta Lake And Lakehouse</a>
</li>
            
<li>
    <a href="../../3.0_Databricks/4.8_Databricks_ProjectA1/" class="dropdown-item">Project-A</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">4.0 Hive</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../4.0_Hive/Hive_Concepts/" class="dropdown-item">Hive Concepts</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">StreamProcessing</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../StreamProcessing/1.0_What_Is_Stream_Processing/" class="dropdown-item">1.0 What Is Stream Processing</a>
</li>
                                    
<li>
    <a href="../../../StreamProcessing/2.0.1_EventHubs_Vs_Kafka/" class="dropdown-item">EventHubs Vs Kafka</a>
</li>
                                    
<li>
    <a href="../../../StreamProcessing/2.0.2_Project_Hello_EventHubs/" class="dropdown-item">Overview</a>
</li>
                                    
<li>
    <a href="../../../StreamProcessing/2.0.3_EventHubsLocalEmulator/" class="dropdown-item">Event Hubs Emulator - End to End</a>
</li>
                                    
<li>
    <a href="../../../StreamProcessing/2.0_Azure_EventHubs/" class="dropdown-item">EventHubs</a>
</li>
                                    
<li>
    <a href="../../../StreamProcessing/4_EventProcessingChoices/" class="dropdown-item">Stream Processing Product Combination</a>
</li>
                                    
<li>
    <a href="../../../StreamProcessing/5_AmazonKinesisSparkIntegration/" class="dropdown-item">5 AmazonKinesisSparkIntegration</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Synapse ADF</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../Synapse-ADF/1.0_SynapseConcepts/" class="dropdown-item">SynapseConcepts</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/1.1_Pools/" class="dropdown-item">Pools</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/1.3_ETL%20Pipelines/" class="dropdown-item">ETL Pipelines</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/1.4_Copy-data-tool/" class="dropdown-item">ADF Copy task - When to use</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/1.5_IntegrationRuntime/" class="dropdown-item">Integration Runtime</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/1.6_DB_Types_In_Synapse/" class="dropdown-item">Types of DB in Synapse</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/1.7_SynapseLakeDBAndLakehouse/" class="dropdown-item">Lake DB-Lakehouse-Delta Lake</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/1.8_ADF_SA_Evolution/" class="dropdown-item">ADF & Synapse Evolution</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/1.9_CETAS/" class="dropdown-item">CETAS</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/2.0_Projects/" class="dropdown-item">2.0 Projects</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/2.1_Pipeline-Local-ADLS/" class="dropdown-item">CopyData-LocalToADLS</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/2.2_PySparkWarehouse/" class="dropdown-item">PysparkWarehouse</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/2.3_ADF_RestAPI_Databricks/" class="dropdown-item">2.3 ADF RestAPI Databricks</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/2.4_Monitor_ADF_Pipelines/" class="dropdown-item">ADF Pipelines Monitoring</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/2.5_ADF_Pipeline_Copy/" class="dropdown-item">Export ADF Pipeline</a>
</li>
                                    
<li>
    <a href="../../../Synapse-ADF/Q%26A/" class="dropdown-item">100 Synapse FAQs</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ms-md-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-bs-toggle="modal" data-bs-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../1.14_Q%26A/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../1.16_ConnectingSparkToHive/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-bs-toggle="collapse" data-bs-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-body-tertiary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-bs-level="1"><a href="#some-common-pyspark-topics" class="nav-link">Some common Pyspark Topics</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="2"><a href="#just-pyspark-vs-real-spark" class="nav-link">Just PySpark vs Real Spark</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#finding-spark" class="nav-link">Finding Spark</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#frequently-asked-pyspark-questions" class="nav-link">Frequently asked Pyspark questions</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<ul>
<li><a href="#some-common-pyspark-topics">Some common Pyspark Topics</a></li>
<li><a href="#just-pyspark-vs-real-spark">Just PySpark vs Real Spark</a><ul>
<li><a href="#standalone-python-vs-anaconda-python">Standalone Python vs. Anaconda Python</a></li>
<li><a href="#standalone-python">Standalone Python</a></li>
<li><a href="#anaconda-python">Anaconda Python</a></li>
<li><a href="#pyspark-vs-full-apache-spark-installation">PySpark vs full Apache Spark Installation</a></li>
<li><a href="#pyspark-via-pip">PySpark via pip</a></li>
<li><a href="#full-apache-spark-installation">Full Apache Spark Installation</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li><a href="#finding-spark">Finding Spark</a></li>
<li><a href="#frequently-asked-pyspark-questions">Frequently asked Pyspark questions</a></li>
</ul>
<h1 id="some-common-pyspark-topics">Some common Pyspark Topics</h1>
<h2 id="just-pyspark-vs-real-spark">Just PySpark vs Real Spark</h2>
<p>Here, I'll try to clear up the often-muddled area between PySpark and Full Spark installations. We will also touch upon different types of python installations available.</p>
<h3 id="standalone-python-vs-anaconda-python">Standalone Python vs. Anaconda Python</h3>
<h4 id="standalone-python">Standalone Python</h4>
<p>Thi is the python you directly install from <a href="https://python.org">Python Software Foundation</a>. Choose this for a lightweight setup, specific version control, and when using Python for general-purpose programming.</p>
<h4 id="anaconda-python">Anaconda Python</h4>
<p>An open-source Python distribution for scientific computing and data science.Go for Anaconda for an easy-to-manage data science environment, especially when dealing with large datasets, machine learning, and analytics.</p>
<h3 id="pyspark-vs-full-apache-spark-installation">PySpark vs full Apache Spark Installation</h3>
<h4 id="pyspark-via-pip">PySpark via pip</h4>
<p>
    Many believe <code>pip install pyspark</code> installs the <strong style="color: blue;">entire Apache Spark framework</strong>. 
    <strong style="color: red; font-size: larger;">No, it does not.</strong> When you install PySpark via pip, it installs the 
    <strong style="color: green;">Python interface</strong> plus a <strong style="color: green;">minimal, standalone version of Apache Spark</strong> 
    that can run locally on your machine. This standalone version of Spark is what allows the 
    <strong style="color: orange;">simulation of a Spark cluster environment</strong> on your single computer. Here's a breakdown:
</p>

<ol>
<li>
<p><strong>Apache Spark in PySpark</strong>: </p>
<ul>
<li>The PySpark package installed via pip includes a lightweight, standalone Spark installation. This isn't the full-fledged, distributed Spark system typically used in large-scale setups but a minimal version that can run on a single machine.</li>
<li>When you execute PySpark code after installing it via pip, you're actually running this local version of Spark.</li>
</ul>
</li>
<li>
<p><strong>Local Mode Execution</strong>:</p>
<ul>
<li>In this "local mode," Spark operates as if it's a cluster but is actually just using the resources (like CPU and memory) of your single machine. It simulates the behavior of a Spark cluster, which in a full setup would distribute processing tasks across multiple nodes (machines).</li>
<li>This mode is incredibly useful for development, testing, and learning because it lets you write and test code that would normally run on a Spark cluster, without the need for setting up multiple machines.</li>
</ul>
</li>
<li>
<p><strong>The Spark Context</strong>:</p>
<ul>
<li>When your PySpark code runs, it initializes a "Spark context" within your Python script. This context is the primary connection to the Spark execution environment and allows your Python script to access Spark's functionalities.</li>
<li>In the pip-installed PySpark environment, this Spark context talks to the local Spark instance included in the PySpark package, not a remote or distributed cluster.</li>
</ul>
</li>
</ol>
<h4 id="full-apache-spark-installation">Full Apache Spark Installation</h4>
<p>Full Spark Installation involves setting up the complete Apache Spark framework, for building large-scale data processing applications, beyond the scope of PySpark alone. This is necessary for production-grade, large-scale data processing and when you need to harness the full power of Spark's distributed computing capabilities.</p>
<h3 id="conclusion">Conclusion</h3>
<p>To sum it up, <code>pip install pyspark</code> actually installs both the Python interface to Spark (PySpark) and a minimal, local-only version of Apache Spark itself. This setup allows you to run Spark jobs as if you had a Spark cluster, but it's all happening within your own computer. The local mode is a simulation of a distributed Spark environment, suitable for learning, development, and processing small data sets.</p>
<h2 id="finding-spark">Finding Spark</h2>
<p>When you install Spark using a package manager like <code>pip</code>, it places the Spark binaries and libraries inside the Python's <code>dist-packages</code> or <code>site-packages</code> directory. This is a different installation approach than downloading and extracting Spark directly from the Apache Spark website.</p>
<p>Here's what you need to know:</p>
<ol>
<li><strong>SPARK_HOME for pip-installed PySpark</strong>: If you've installed Spark via <code>pip</code>, then the equivalent <code>SPARK_HOME</code> would be <code>/usr/local/lib/python3.8/dist-packages/pyspark/</code>. This directory would contain all the necessary libraries and dependencies for Spark to run within a Python environment.</li>
<li><strong>Configuration and JARs</strong>: Configuration files, JARs, and other related resources would also be located within this directory structure. For instance, if you're looking for the location to place additional JAR files, it would likely be somewhere like <code>/usr/local/lib/python3.8/dist-packages/pyspark/jars/</code>.</li>
<li><strong>Using <code>spark-submit</code></strong>: The <code>spark-submit</code> script in this directory is tailored for the pip-installed Spark and ensures that it picks up the correct libraries and configurations from this location.</li>
</ol>
<p>In summary, if you've installed Spark using <code>pip</code> within your Docker container, then the <code>/usr/local/lib/python3.8/dist-packages/pyspark/</code> directory is your primary point of reference for Spark configurations, binaries, and libraries. This is distinct from a standalone Spark installation, which would typically reside directly under <code>/usr/local/</code> or another directory, separate from the Python packages directory.</p>
<hr />
<p>What if echo $SPARK_HOME or %SPARK_HOME% returns nothing and you are not sure where spark is installed? Even though you instaleld it?</p>
<p>Let's try a different approach.</p>
<p>Spark's binaries (like <code>spark-submit</code> and <code>spark-shell</code>) are often a good hint as to where Spark might be installed.</p>
<ol>
<li><strong>Search for Spark binaries</strong>:
   Execute the following command within your Docker container:</li>
</ol>
<p><code>bash
   find / -name "spark-submit" 2&gt;/dev/null</code></p>
<p>This command searches for the <code>spark-submit</code> binary, which is a common Spark binary. The directory containing this binary is likely your Spark installation directory.</p>
<ol>
<li>Once you've located the directory containing <code>spark-submit</code>, you'll typically find Spark's home directory one level up. For instance, if <code>spark-submit</code> is found in <code>/path/to/spark/bin/spark-submit</code>, then <code>/path/to/spark</code> is likely your <code>SPARK_HOME</code>.</li>
</ol>
<p>In my system it returned</p>
<p><code>bash
/usr/local/bin/spark-submit
/usr/local/lib/python3.8/dist-packages/pyspark/bin/spark-submit</code></p>
<p>How do I know where I have installed all the libraries or which is actually the right spark home?</p>
<p>The path <code>/usr/local/lib/python3.8/dist-packages/pyspark/bin/spark-submit</code> suggests that this is the location where PySpark was installed as a Python package, likely via <code>pip</code> or a similar package manager.</p>
<p>When you install Spark using a package manager like <code>pip</code>, it places the Spark binaries and libraries inside the Python's <code>dist-packages</code> or <code>site-packages</code> directory. This is a different installation approach than downloading and extracting Spark directly from the Apache Spark website.</p>
<p>Here's what you need to know:</p>
<ol>
<li><strong>SPARK_HOME for pip-installed PySpark</strong>: If you've installed Spark via <code>pip</code>, then the equivalent <code>SPARK_HOME</code> would be <code>/usr/local/lib/python3.8/dist-packages/pyspark/</code>. This directory would contain all the necessary libraries and dependencies for Spark to run within a Python environment.</li>
<li><strong>Configuration and JARs</strong>: Configuration files, JARs, and other related resources would also be located within this directory structure. For instance, if you're looking for the location to place additional JAR files, it would likely be somewhere like <code>/usr/local/lib/python3.8/dist-packages/pyspark/jars/</code>.</li>
<li><strong>Using <code>spark-submit</code></strong>: The <code>spark-submit</code> script in this directory is tailored for the pip-installed Spark and ensures that it picks up the correct libraries and configurations from this location.</li>
</ol>
<p>In summary, if you've installed Spark using <code>pip</code> within your Docker container, then the <code>/usr/local/lib/python3.8/dist-packages/pyspark/</code> directory is your primary point of reference for Spark configurations, binaries, and libraries. This is distinct from a standalone Spark installation, which would typically reside directly under <code>/usr/local/</code> or another directory, separate from the Python packages directory.</p>
<h2 id="frequently-asked-pyspark-questions">Frequently asked Pyspark questions</h2>
<p>In this section, I'll go over some common PySpark questions and their answers. These are basic questions that anyone using PySpark should be familiar with. So, let's get started :-)</p>
<p><span style="color: DeepSkyBlue; font-family: Segoe UI, sans-serif;"><strong>What is PySpark, how is different from Apache Spark?</strong></span></p>
<p>PySpark is the Python API for Apache Spark, allowing Python programmers to use Spark’s large-scale data processing capabilities. Apache Spark is a unified analytics engine for large-scale data processing, originally written in Scala. PySpark provides a similar interface to Spark but allows for Python programming syntax and libraries.</p>
<p><span style="color: Coral; font-family: Segoe UI, sans-serif;"><strong>What's different between RDD, DataFrame, and Dataset in PySpark.</strong></span></p>
<p>RDD (Resilient Distributed Dataset) is the fundamental data structure of Spark, representing an immutable, distributed collection of objects that can be processed in parallel. DataFrame is a distributed collection of data organized into named columns, similar to a table in a relational database but with richer optimizations under the hood. Dataset is a type-safe version of DataFrame available in Scala and Java, offering the benefits of RDDs with the optimization benefits of DataFrames.</p>
<p><span style="color: DarkOrchid; font-family: Segoe UI, sans-serif;"><strong>How do you create a SparkSession in PySpark?</strong></span></p>
<p>You can create a SparkSession using the <code>SparkSession.builder</code> method, often initializing it with configurations such as <code>appName</code> to name your application and <code>master</code> to specify the cluster manager. For example: <code>spark = SparkSession.builder.appName("MyApp").getOrCreate()</code>.</p>
<p><span style="color: Tomato; font-family: Segoe UI, sans-serif;"><strong>What are the advantages of using PySpark over traditional Python libraries like Pandas?</strong></span></p>
<p>PySpark provides distributed data processing capabilities, allowing for processing of large datasets that do not fit into memory on a single machine. It offers high-level APIs and supports complex ETL operations, real-time processing, and machine learning, unlike Pandas, which is limited by the memory of a single machine.</p>
<p><span style="color: MediumSeaGreen; font-family: Segoe UI, sans-serif;"><strong>What do you understand by lazy evaluation in PySpark.</strong></span></p>
<p>Lazy evaluation in PySpark means that execution will not start until an action is performed. Transformations in PySpark are lazy, meaning they define a series of operations on data but do not compute anything until the user calls an action. This allows Spark to optimize the execution plan for efficiency.</p>
<p><span style="color: SlateBlue; font-family: Segoe UI, sans-serif;"><strong>How can you read a CSV  in PySpark?</strong></span></p>
<p>To read a CSV file using PySpark, you can use the <code>spark.read.csv</code> method, specifying the path to the CSV file. Options can be set for things like delimiter, header presence, and schema inference. For example: <code>df = spark.read.csv("path/to/csv", header=True, inferSchema=True)</code>.</p>
<p><span style="color: Sienna; font-family: Segoe UI, sans-serif;"><strong>Explain the actions and transformations in PySpark with examples.</strong></span></p>
<p>Transformations in PySpark create new RDDs, DataFrames, or Datasets from existing ones and are lazily evaluated. Examples include <code>map</code>, <code>filter</code>, and <code>groupBy</code>. Actions, on the other hand, trigger computation and return results. Examples include <code>count</code>, <code>collect</code>, and <code>show</code>. For instance, <code>rdd.filter(lambda x: x &gt; 10)</code> is a transformation, while <code>rdd.count()</code> is an action</p>
<p><span style="color: RoyalBlue; font-family: Segoe UI, sans-serif;"><strong>What are the various ways to select columns in a PySpark DataFrame?</strong></span>
Columns in a PySpark DataFrame can be selected using the <code>select</code> method by specifying column names directly or using the <code>df["column_name"]</code> syntax. You can also use SQL expressions with the <code>selectExpr</code> method.</p>
<p><span style="color: Goldenrod; font-family: Segoe UI, sans-serif;"><strong>How do you handle missing or null values in PySpark DataFrames?</strong></span></p>
<p>Missing or null values in PySpark DataFrames can be handled using methods like <code>fillna</code> to replace nulls with specified values, <code>drop</code> to remove rows with null values, or <code>na.drop()</code> and <code>na.fill()</code> for more nuanced control.</p>
<p><span style="color: DarkSalmon; font-family: Segoe UI, sans-serif;"><strong>Explain the difference between map() and flatMap() functions in PySpark.</strong></span></p>
<p>The <code>map()</code> function applies a function to each element of an RDD, returning a new RDD with the results. <code>flatMap()</code>, on the other hand, applies a function to each element and then flattens the results into a new RDD. Essentially, <code>map()</code> returns elements one-to-one, while <code>flatMap()</code> can return 0 or more elements for each input.</p>
<p><span style="color: LightCoral; font-family: Segoe UI, sans-serif;"><strong>How do you perform joins in PySpark DataFrames?</strong></span></p>
<p>Joins in PySpark DataFrames are performed using the <code>join</code> method, specifying another DataFrame to join with, the key or condition to join on, and the type of join (e.g., inner, outer, left, right).</p>
<p><span style="color: CadetBlue; font-family: Segoe UI, sans-serif;"><strong>Explain the significance of caching in PySpark and how it's implemented.</strong></span></p>
<p>Caching in PySpark is significant for optimization, allowing intermediate results to be stored in memory for faster access in subsequent actions. It's implemented using the <code>cache()</code> or <code>persist()</code> methods on RDDs or DataFrames, which store the data in memory or more persistent storage levels.</p>
<p><span style="color: Chocolate; font-family: Segoe UI, sans-serif;"><strong>What are User Defined Functions (UDFs) in PySpark, and when would you use them?</strong></span></p>
<p>UDFs in PySpark allow you to extend the built-in functions by defining custom functions in Python, which can then be used in DataFrame operations. They are useful for applying complex transformations or business logic that are not covered by Spark’s built-in functions.</p>
<p><span style="color: DarkSlateGray; font-family: Segoe UI, sans-serif;"><strong>How do you aggregate data in PySpark?</strong></span></p>
<p>Data in PySpark can be aggregated using methods like <code>groupBy</code> followed by aggregation functions such as <code>count</code>, <code>sum</code>, <code>avg</code>, etc. For example, <code>df.groupBy("column_name").count()</code> would count the number of rows for each unique value in the specified column.</p>
<p><span style="color: Indigo; font-family: Segoe UI, sans-serif;"><strong>Explain window functions and their usage in PySpark.</strong></span></p>
<p>Window functions in PySpark operate on a group of rows (a window) while returning a value for each row in the dataset. They are useful for operations like running totals, moving averages, and ranking without having to group the dataset. They are defined using the <code>Window</code> class and applied with functions like <code>rank</code>, <code>row_number</code>, etc.</p>
<p><span style="color: MidnightBlue; font-family: Segoe UI, sans-serif;"><strong>What strategies would you employ for optimizing PySpark jobs?</strong></span></p>
<p>Strategies for optimizing PySpark jobs include broadcasting large lookup tables, partitioning data effectively, caching intermediate results, minimizing shuffles, and using efficient serialization formats. Adjusting Spark configurations to match the job's needs can also improve performance.</p>
<p><span style="color: OliveDrab; font-family: Segoe UI, sans-serif;"><strong>How does partitioning impact performance in PySpark?</strong></span></p>
<p>Proper partitioning in PySpark can significantly impact performance by ensuring that data is distributed evenly across nodes, reducing shuffles and improving parallel processing efficiency. Poor partitioning can lead to data skew and bottlenecks.</p>
<p><span style="color: DarkOliveGreen; font-family: Segoe UI, sans-serif;"><strong>Explain broadcast variables and their role in PySpark optimization.</strong></span></p>
<p>Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They are used to optimize performance in PySpark, especially when you have a large dataset that needs to be used across multiple nodes.</p>
<p><span style="color: SaddleBrown; font-family: Segoe UI, sans-serif;"><strong>How do you handle skewed data in PySpark?</strong></span></p>
<p>Handling skewed data in PySpark can involve strategies such as salting keys to distribute the data more evenly, repartitioning or coalescing, and custom partitioning schemes to avoid data skew and ensure balanced workload across nodes.</p>
<p><span style="color: Teal; font-family: Segoe UI, sans-serif;"><strong>Discuss the concept of accumulators in PySpark.</strong></span></p>
<p>Accumulators in PySpark are variables that are only “added” to through an associative and commutative operation and can be used to implement counters or sums. PySpark ensures they are updated correctly across tasks.</p>
<p><span style="color: DarkKhaki; font-family: Segoe UI, sans-serif;"><strong>How do you handle streaming in PySpark?</strong></span></p>
<p>Working with structured streaming involves defining a DataStreamReader or DataStreamWriter with a schema, reading streaming data from various sources (like Kafka, sockets, or files), applying transformations, and then writing the output to a sink (like a file system, console, or memory).</p>
<p><span style="color: Maroon; font-family: Segoe UI, sans-serif;"><strong>How can you handle schema evolution in PySpark?</strong></span></p>
<p>Schema evolution in PySpark can be handled by using options like <code>mergeSchema</code> in data sources that support schema merging (e.g., Parquet). It allows for the automatic merging of differing schemas in data files over time, accommodating the addition of new columns or changes in data types.</p>
<p><span style="color: DarkRed; font-family: Segoe UI, sans-serif;"><strong>Explain the difference between persist() and cache() in PySpark.</strong></span></p>
<p>Both <code>persist()</code> and <code>cache()</code> in PySpark are used to store the computation results of an RDD, DataFrame, or Dataset so they can be reused in subsequent actions. The difference is that <code>persist()</code> allows the user to specify the storage level (memory, disk, etc.), whereas <code>cache()</code> uses the default storage level (MEMORY_ONLY).</p>
<p><span style="color: DarkSlateBlue; font-family: Segoe UI, sans-serif;"><strong>How do you work with nested JSON data in PySpark?</strong></span></p>
<p>Working with nested JSON data in PySpark involves reading the JSON file into a DataFrame and then using functions like <code>explode</code> to flatten nested structures or <code>select</code> and <code>col</code> for accessing nested fields. PySpark's built-in functions for dealing with complex data types are also useful here.</p>
<p><span style="color: DarkCyan; font-family: Segoe UI, sans-serif;"><strong>What is the purpose of the PySpark MLlib library?</strong></span></p>
<p>The purpose of the PySpark MLlib library is to provide machine learning algorithms and utilities for classification, regression, clustering, collaborative filtering, dimensionality reduction, and underlying optimization primitives. It allows for scalable and efficient execution of ML tasks on big data.</p>
<p><span style="color: DarkTurquoise; font-family: Segoe UI, sans-serif;"><strong>How do you integrate PySpark with other Python libraries like NumPy and Pandas?</strong></span></p>
<p>Integration of PySpark with other Python libraries like NumPy and Pandas can be achieved through the use of PySpark's ability to convert DataFrames to and from Pandas DataFrames (<code>toPandas</code> and <code>createDataFrame</code> methods) and by using UDFs to apply functions that utilize these libraries on Spark DataFrames.</p>
<p><span style="color: DarkGoldenrod; font-family: Segoe UI, sans-serif;"><strong>Explain the process of deploying PySpark applications in a cluster.</strong></span></p>
<p>Deploying PySpark applications in a cluster involves packaging your application's code and dependencies, submitting the job to a cluster manager (like Spark Standalone, YARN, or Mesos) using the <code>spark-submit</code> script, and specifying configurations such as the number of executors, memory per executor, and the application's entry point.</p>
<p><span style="color: DarkSeaGreen; font-family: Segoe UI, sans-serif;"><strong>What are the best practices for writing efficient PySpark code?</strong></span></p>
<p>Best practices include using DataFrames for better optimization, avoiding UDFs when built-in functions are available, minimizing data shuffles, broadcasting large reference datasets, efficient data partitioning, and leveraging Spark's built-in functions for complex operations.</p>
<p><span style="color: DarkGray; font-family: Segoe UI, sans-serif;"><strong>How do you handle memory-related issues in PySpark?</strong></span></p>
<p>Handling memory-related issues involves optimizing Spark configurations such as executor memory, driver memory, and memory overhead. Tuning the size and number of partitions, avoiding large broadcast variables, and using disk storage when necessary can also help.</p>
<p><span style="color: DimGray; font-family: Segoe UI, sans-serif;"><strong>Explain the significance of the Catalyst optimizer in PySpark.</strong></span></p>
<p>The Catalyst optimizer is a key component of Spark SQL that improves the performance of SQL and DataFrame queries. It optimizes query execution by analyzing query plans and applying optimization rules, such as predicate pushdown and constant folding, to generate an efficient physical plan.</p>
<p><span style="color: DarkMagenta; font-family: Segoe UI, sans-serif;"><strong>What are some common errors you've encountered while working with PySpark, and how did you resolve them?</strong></span></p>
<p>Common errors include out-of-memory errors, task serialization issues, and data skew. Resolving these issues typically involves tuning Spark configurations, ensuring efficient data partitioning, and applying strategies to handle large datasets and skewed data.</p>
<p><span style="color: DarkBlue; font-family: Segoe UI, sans-serif;"><strong>How do you debug PySpark applications effectively?</strong></span></p>
<p>Effective debugging of PySpark applications involves checking Spark UI for detailed information on job execution, stages, and tasks, logging information at key points in the application, and using local mode for debugging simpler versions of the code.</p>
<p><span style="color: DarkGreen; font-family: Segoe UI, sans-serif;"><strong>Explain the streaming capabilities of PySpark.</strong></span></p>
<p>PySpark supports structured streaming, a high-level API for stream processing that allows users to express streaming computations the same way they would express batch computations on static data. It supports event-time processing, window functions, and stateful operations.</p>
<p><span style="color: FireBrick; font-family: Segoe UI, sans-serif;"><strong>Can you explain model evaluation and hyperparameter tuning in PySpark.</strong></span></p>
<p>Model evaluation and hyperparameter tuning in PySpark can be performed using the MLlib library, which offers tools like CrossValidator for cross-validation and ParamGridBuilder for building a grid of parameters to search over. Evaluation metrics are available for assessing model performance.</p>
<p><span style="color: DarkSlateGrey; font-family: Segoe UI, sans-serif;"><strong>Name some common methods or tools do you use for testing PySpark code?</strong></span></p>
<p>Testing PySpark code can involve using the <code>pyspark.sql.functions.col</code> for column operations, the DataFrame API for data manipulation, and third-party libraries like PyTest for writing test cases. Mocking data and simulating Spark behavior in a local environment are also common practices.</p>
<p><span style="color: DarkOrange; font-family: Segoe UI, sans-serif;"><strong>How do you ensure data quality and consistency in PySpark pipelines?</strong></span></p>
<p>Ensuring data quality and consistency involves implementing validation checks, using schema enforcement on DataFrames, employing data profiling and cleansing techniques, and maintaining data lineage and auditing processes.</p>
<p><span style="color: DarkViolet; font-family: Segoe UI, sans-serif;"><strong>How do you perform machine learning tasks using PySpark MLlib?</strong></span></p>
<p>Performing machine learning tasks with PySpark MLlib involves using its DataFrame-based API for constructing ML pipelines, utilizing transformers and estimators for preprocessing and model training, and applying built-in algorithms for classification, regression, clustering, etc.</p>
<p><span style="color: DarkOrchid; font-family: Segoe UI, sans-serif;"><strong>How do you handle large-scale machine learning with PySpark?</strong></span></p>
<p>Handling large-scale machine learning involves leveraging the distributed computing capabilities of Spark and MLlib, using algorithms optimized for parallel processing, effectively partitioning data, and tuning Spark resources to balance the workload across the cluster.</p>
<p><span style="color: DarkSalmon; font-family: Segoe UI, sans-serif;"><strong>What are the challenges one faces while implementing machine learning algorithms using PySpark?</strong></span></p>
<p>Challenges include dealing with data skewness, selecting the right algorithms that scale efficiently, managing resource allocation in a distributed environment, ensuring data quality, and integrating with other systems for real-time predictions.</p>
<hr />
<p>© D Das<br />
📧 <a href="mailto:das.d@hotmail.com">das.d@hotmail.com</a> | <a href="mailto:ddasdocs@gmail.com">ddasdocs@gmail.com</a></p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../../../js/bootstrap.bundle.min.js"></script>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js"></script>
        <script src="../../../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
