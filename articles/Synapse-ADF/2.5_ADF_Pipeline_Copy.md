---
layout: default
title: Export ADF Pipeline
parent: Synapse-ADF
nav_order: 2
---

<details open markdown="block">
  <summary>
    Table of contents
  </summary>
  {: .text-delta }
1. TOC
{:toc}
</details>

# How to Understand and Export Your Azure Data Factory Pipeline

## Viewing the Pipeline's JSON Code

In Azure Data Factory (ADF), you can easily see the JSON code that represents your entire pipeline. To do this, simply click on the curly brackets `{}` icon. This will display the JSON structure of your pipeline, including all the activities, parameters, variables, and other settings.

![Icon for viewing JSON code in Azure Data Factory](images/2024-09-15-03-16-44.png)

## Limitations of the JSON Representation

However, please note that the JSON file you get by clicking the `{}` icon does **not** include everything you might need to fully understand your pipeline. Here are some important components that are usually missing:

1. **Linked Services**: If your pipeline connects to external resources like Azure Data Lake, Azure SQL, or Databricks, the configuration details for these linked services are not included in the pipeline's JSON. You will need to check each linked service's JSON definition separately.

2. **Referenced Pipelines**: If your pipeline calls other pipelines using activities like `ExecutePipeline`, the details of those pipelines are not included. You will have to access their JSON files individually.

3. **Global Parameters**: Any global parameters your pipeline uses are not fully defined within its JSON file. You'll need to look at the global parameter configurations separately.

4. **Triggers**: Triggers that start your pipeline (like those based on schedules or events) are not included in the pipeline's JSON. They are managed separately in the ADF interface.

5. **Data Flows**: If your pipeline contains data flows, the JSON only references them. You'll need to view the details of the data flows separately.

6. **Azure Integration Runtime Configurations**: These settings are also referenced but not fully included in the pipeline's JSON.

**In short**, while the JSON representation gives you a detailed view of your pipeline's structure, you need to explore these additional configurations and resources to fully understand how your pipeline works.

## How to Copy the Entire ADF Pipeline

If you want to copy and export the entire pipeline along with all its related resources (like linked services, datasets, and any referenced pipelines), here's how you can do it:

### 1. Use ARM Template Export

Azure Data Factory allows you to export your pipeline and all related resources as an **Azure Resource Manager (ARM) Template**. This method includes all dependencies and is the most comprehensive way to copy everything.

- **Go to Your Data Factory**: Open your Data Factory instance in the Azure Portal.

- **Access the Manage Tab**: Click on the **"Manage"** tab on the left side.

- **Export ARM Template**: Under the **"ARM template"** section, click on **"Export ARM template"**.

- **Select All Components**: Make sure to select all the components you want to export, such as pipelines, linked services, datasets, triggers, and global parameters. This will package everything into a single JSON-based ARM template.

- **Download the Template**: Save the template file to your computer.

- **Deploy to Another Environment**: You can import this template into another Data Factory by using the **"Deploy ARM template"** option in the **"Manage"** section of the target Data Factory, or by using PowerShell or the Azure CLI.

### 2. Use Git Integration for Source Control

If your Azure Data Factory is connected to a Git repository like Azure DevOps or GitHub, all your pipeline components are already stored there.

- **Clone the Repository**: Download the repository to your computer. It contains all pipelines, datasets, linked services, triggers, and global parameters.

- **Deploy to Another Environment**: Use the files from the repository to deploy to another Azure Data Factory instance or to examine the structure locally.

### 3. Manually Export Pipeline and Related Resources

If you prefer not to use the ARM template or Git integration, you can manually export each component.

- **Export the Pipeline**: In the **Author** pane of Azure Data Factory Studio, select your pipeline. Click the three dots next to it and choose **"Export"** to get its JSON definition.

- **Export Other Components**: Similarly, export linked services, datasets, and triggers by selecting them and choosing the **"Export"** option.

**Note**: This method can be time-consuming because you'll need to export each component individually.

### 4. Use Azure CLI or PowerShell

You can also use Azure CLI or Azure PowerShell to export your Data Factory components.

**Using Azure CLI**:

```bash
az datafactory export-pipeline --factory-name <your_data_factory_name> \
                               --resource-group <your_resource_group_name> \
                               --name <your_pipeline_name> \
                               --output-path <output_file_path>
```

You can run similar commands to export linked services and datasets.

### 5. Deploy Everything to Another Environment

Once you have your ARM template or exported JSON files:

- **For ARM Templates**: Use the **"Deploy ARM template"** option in the new Data Factory or deploy via Azure CLI or PowerShell.

- **For Git Integration**: Push the files to the repository associated with the target Data Factory.

- **For Manual Exports**: Recreate each component in the new Data Factory by importing their JSON files one by one.

### 6. Validate After Export

After importing or deploying, make sure to:

- **Validate the Pipeline**: Check that all components are working correctly.

- **Update Environment-Specific Settings**: Adjust any settings that are specific to the new environment, such as service credentials, authentication details, or resource URLs.

## Summary

To export your **entire** pipeline with all dependencies:

- **Best Method**: Use the **ARM template export** feature in Azure Data Factory. This method includes everythingâ€”pipelines, datasets, linked services, triggers, and more.

- **Alternative Method**: Use **Git integration** for version control and easy transfer between environments.

- **Manual Method**: If necessary, export the pipeline and its components individually.

By following these steps, you'll ensure that you have all the necessary components to fully understand and move your pipeline to another system.