---
layout: default
title: persists in Spark
parent: Concepts
grand_parent: Spark-Databricks
nav_order: 5
---

<img src="images/custom-image-2024-07-09-19-28-59.png" alt="Warehouse Directory" style="border: 2px solid #ccc; box-shadow: 3px 3px 8px rgba(0, 0, 0, 0.2); border-radius: 10px;">

# Understanding `persist()` in Spark

In Spark, the `persist()` method is used to save a dataset (RDD or DataFrame) in memory or on disk, so you can use it multiple times without recalculating it every time. This helps speed up your work, especially if you need to use the same data repeatedly.

## Different Settings for `persist()`

The default setting for persist is  `MEMORY_ONLY`. Here are the other settings:

1. **MEMORY_ONLY**: Keeps data in memory (default). If there's not enough memory, it won't save some parts, and it will recalculate those parts when needed.
2. **MEMORY_AND_DISK**: Keeps data in memory, but if there's not enough memory, it saves some parts on disk.
3. **MEMORY_ONLY_SER**: Saves data in a more compact form in memory.
4. **MEMORY_AND_DISK_SER**: Saves data in a more compact form in memory, and on disk if needed.
5. **DISK_ONLY**: Saves data only on disk.
6. **OFF_HEAP**: Saves data in memory but outside the regular Java heap space (experimental).

## Example

Imagine you have a list of numbers, and you want to do some calculations on it multiple times. Here's how you can use `persist()`:

```python
from pyspark import SparkContext
from pyspark.sql import SparkSession

# Start Spark
sc = SparkContext("local", "Persist Example")
spark = SparkSession(sc)

# Create an RDD
numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
rdd = sc.parallelize(numbers)

# Persist the RDD in memory
rdd.persist()

# Do some actions
print(rdd.count())  # Count the numbers
print(rdd.collect())  # Collect all the numbers
```

In this example, `persist()` saves the `rdd` in memory, so if you do more actions like `count()` or `collect()`, it doesn't have to recalculate the data each time.

## Removing Saved Data:

When you don't need the saved data anymore, you can remove it from memory or disk using `unpersist()`:

```python
rdd.unpersist()
```

## Conclusion

Using `persist()` in Spark is like telling Spark to remember your data so it doesn't have to start from scratch every time you need it. This can make your work much faster, especially when working with large datasets. And remember, the default setting saves data in memory, but you can choose other options based on your needs.