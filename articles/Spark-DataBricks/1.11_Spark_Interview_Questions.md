---
layout: default
title: Spark Interview Questions
parent: Concepts
grand_parent: Spark-Databricks
nav_order: 12
---

- [Types of join strateries](#types-of-join-strateries)
- [Types of Joins](#types-of-joins)
- [Common optimizing techniques](#common-optimizing-techniques)
  - [1. **Caching and Persistence**](#1-caching-and-persistence)
  - [2. **Broadcast Variables**](#2-broadcast-variables)
  - [3. **Partitioning**](#3-partitioning)
  - [4. **Avoiding Shuffles**](#4-avoiding-shuffles)
  - [5. **Coalesce**](#5-coalesce)
  - [6. **Predicate Pushdown**](#6-predicate-pushdown)
  - [7. **Using the Right Join Strategy**](#7-using-the-right-join-strategy)
  - [8. **Tuning Spark Configurations**](#8-tuning-spark-configurations)
  - [9. **Using DataFrames/Datasets API**](#9-using-dataframesdatasets-api)
  - [10. **Vectorized Query Execution**](#10-vectorized-query-execution)
- [Different phases of Spark-SQL engine](#different-phases-of-spark-sql-engine)
- [Common reasons for analysis exception in Spark](#common-reasons-for-analysis-exception-in-spark)
- [Flow of how Spark works internally](#flow-of-how-spark-works-internally)
- [Explain DAG in Spark](#explain-dag-in-spark)
  - [What is DAG in Spark?](#what-is-dag-in-spark)
  - [Why do we need DAG in Spark?](#why-do-we-need-dag-in-spark)
  - [What would happen without a DAG?](#what-would-happen-without-a-dag)
  - [Example](#example)
- [Explain spark.sql.shuffle.partitions Variable](#explain-sparksqlshufflepartitions-variable)
  - [Purpose](#purpose)
  - [When It's Used](#when-its-used)
  - [How to Set It](#how-to-set-it)
  - [Example](#example-1)
  - [Why Adjust This Setting?](#why-adjust-this-setting)

## Types of join strateries

| **Join Strategy**                   | **Description**                                                                                                 | **Use Case**                                                                 | **Example**                                                      |
|-------------------------------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------|------------------------------------------------------------------|
| Shuffle Hash Join                   | Both DataFrames are shuffled based on the join keys, and then a hash join is performed.                         | Useful when both DataFrames are large and have a good distribution of keys.  | `spark.conf.set("spark.sql.join.preferSortMergeJoin", "false")`<br>`df1.join(df2, "key")` |
| Broadcast Hash Join                 | One of the DataFrames is small enough to fit in memory and is broadcasted to all worker nodes. A hash join is then performed. | Efficient when one DataFrame is much smaller than the other.                | `val broadcastDF = broadcast(df2)`<br>`df1.join(broadcastDF, "key")` |
| Sort-Merge Join                     | Both DataFrames are sorted on the join keys and then merged. This requires a shuffle if the data is not already sorted. | Suitable for large DataFrames when the join keys are sorted or can be sorted efficiently. | `spark.conf.set("spark.sql.join.preferSortMergeJoin", "true")`<br>`df1.join(df2, "key")` |
| Cartesian Join (Cross Join)         | Every row of one DataFrame is paired with every row of the other DataFrame.                                     | Generally not recommended due to its computational expense, but can be used for generating combinations of all rows. | `df1.crossJoin(df2)`                                              |
| Broadcast Nested Loop Join          | The smaller DataFrame is broadcasted, and a nested loop join is performed.                                      | Used when there are no join keys or the join condition is complex and cannot be optimized with hash or sort-merge joins. | `val broadcastDF = broadcast(df2)`<br>`df1.join(broadcastDF)`    |
| Shuffle-and-Replicate Nested Loop Join | Both DataFrames are shuffled and replicated to perform a nested loop join.                                      | Used for complex join conditions that cannot be handled by other join strategies. | `df1.join(df2, expr("complex_condition"))`                       |

## Types of Joins

| **Join Type**       | **Description**                                                                                     | **Example**                                         |
|---------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------|
| Inner Join          | Returns rows that have matching values in both DataFrames.                                          | `df1.join(df2, "key")`                              |
| Outer Join          | Returns all rows when there is a match in either DataFrame. Missing values are filled with nulls.    | `df1.join(df2, Seq("key"), "outer")`                |
| Left Outer Join     | Returns all rows from the left DataFrame, and matched rows from the right DataFrame.                 | `df1.join(df2, Seq("key"), "left_outer")`           |
| Right Outer Join    | Returns all rows from the right DataFrame, and matched rows from the left DataFrame.                 | `df1.join(df2, Seq("key"), "right_outer")`          |
| Left Semi Join      | Returns only the rows from the left DataFrame that have a match in the right DataFrame.              | `df1.join(df2, Seq("key"), "left_semi")`            |
| Left Anti Join      | Returns only the rows from the left DataFrame that do not have a match in the right DataFrame.       | `df1.join(df2, Seq("key"), "left_anti")`            |
| Cross Join          | Returns the Cartesian product of both DataFrames. Every row in the left DataFrame will be combined with every row in the right DataFrame. | `df1.crossJoin(df2)`                                |
| Self Join           | A join in which a DataFrame is joined with itself. This can be an inner, outer, left, or right join. | `df.join(df, df("key1") === df("key2"))`            |

## Common optimizing techniques


### 1. **Caching and Persistence**
- **What it is**: Storing data in memory for quick access.
- **When to use**: When you need to reuse the same data multiple times.
- **Example**: 
  ```scala
  val cachedData = df.cache()
  ```

### 2. **Broadcast Variables**
- **What it is**: Sending a small dataset to all worker nodes to avoid shuffling.
- **When to use**: When one dataset is much smaller than the other.
- **Example**: 
  ```scala
  val broadcastData = spark.broadcast(smallDF)
  largeDF.join(broadcastData.value, "key")
  ```

### 3. **Partitioning**
- **What it is**: Dividing data into smaller, manageable chunks.
- **When to use**: When dealing with large datasets to improve parallel processing.
- **Example**:
  ```scala
  val partitionedData = df.repartition(10, $"key")
  ```

### 4. **Avoiding Shuffles**
- **What it is**: Reducing the movement of data between nodes.
- **When to use**: To improve performance by minimizing network overhead.
- **Example**: Use `mapPartitions` instead of `groupBy` when possible.

### 5. **Coalesce**
- **What it is**: Reducing the number of partitions.
- **When to use**: When the data has become sparse after a transformation.
- **Example**:
  ```scala
  val coalescedData = df.coalesce(1)
  ```

### 6. **Predicate Pushdown**
- **What it is**: Filtering data as early as possible in the processing.
- **When to use**: To reduce the amount of data read and processed.
- **Example**: 
  ```scala
  val filteredData = df.filter($"column" > 10)
  ```

### 7. **Using the Right Join Strategy**
- **What it is**: Choosing the most efficient way to join two datasets.
- **When to use**: Based on the size and distribution of data.
- **Example**: Prefer broadcast joins for small datasets.

### 8. **Tuning Spark Configurations**
- **What it is**: Adjusting settings to optimize resource usage.
- **When to use**: To match the workload and cluster resources.
- **Example**: 
  ```scala
  spark.conf.set("spark.executor.memory", "4g")
  ```

### 9. **Using DataFrames/Datasets API**
- **What it is**: Leveraging the high-level APIs for optimizations.
- **When to use**: To benefit from Catalyst optimizer and Tungsten execution engine.
- **Example**:
  ```scala
  val df = spark.read.csv("data.csv")
  df.groupBy("column").count()
  ```

### 10. **Vectorized Query Execution**
- **What it is**: Processing multiple rows of data at a time.
- **When to use**: For high-performance operations on large datasets.
- **Example**: Use built-in SQL functions and DataFrame methods.

## Different phases of Spark-SQL engine

| **Phase**            | **Description**                                                        | **Details**                                                                                  | **Example**                                                |
|----------------------|------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|------------------------------------------------------------|
| Parsing              | Converting SQL queries into a logical plan.                            | The SQL query is parsed into an abstract syntax tree (AST).                                  | Converting `SELECT * FROM table WHERE id = 1` into an internal format. |
| Analysis             | Resolving references and verifying the logical plan.                   | Resolves column names, table names, and function names; checks for errors.                   | Ensuring that the table `table` and the column `id` exist in the database. |
| Optimization         | Improving the logical plan for better performance.                     | Transforms the logical plan using various optimization techniques; applies rules via Catalyst optimizer. | Reordering filters to reduce the amount of data processed early on. |
| Physical Planning    | Converting the logical plan into a physical plan.                      | Converts the optimized logical plan into one or more physical plans; selects the most efficient plan. | Deciding whether to use a hash join or a sort-merge join. |
| Code Generation      | Generating executable code from the physical plan.                     | Generates Java bytecode to execute the physical plan; this code runs on Spark executors.     | Creating code to perform join operations, filter data, and compute results. |
| Execution            | Running the generated code on the Spark cluster.                       | Distributes generated code across the Spark cluster; executed by Spark executors; results collected and returned. | Running join and filter operations on different nodes in the cluster and aggregating results. |

## Common reasons for analysis exception in Spark

Here are some common reasons why you might encounter an AnalysisException in Spark:

| **Reason**                         | **Description**                                             | **Example**                                |
|------------------------------------|-------------------------------------------------------------|--------------------------------------------|
| Non-Existent Column or Table       | Column or table specified does not exist.                   | Referring to a non-existent column `id`.   |
| Ambiguous Column Reference         | Same column name exists in multiple tables without qualification. | Joining two DataFrames with the same column name `id`. |
| Invalid SQL Syntax                 | SQL query has syntax errors.                                | Using incorrect SQL syntax like `SELCT`.   |
| Unsupported Operations             | Using an operation that Spark SQL does not support.         | Using an unsupported function.             |
| Schema Mismatch                    | Schema of the DataFrame does not match the expected schema. | Inserting data with different column types. |
| Missing File or Directory          | Specified file or directory does not exist.                 | Referring to a non-existent CSV file.      |
| Incorrect Data Type                | Operations expecting a specific data type are given the wrong type. | Performing a math operation on a string column. |

## Flow of how Spark works internally

| **Component/Step**                  | **Role/Process**                                                              | **Function/Example**                                           |
|-------------------------------------|-------------------------------------------------------------------------------|----------------------------------------------------------------|
| **Driver Program**                  | Entry point for the Spark application                                         | - Manages application lifecycle<br>- Defines RDD transformations and actions |
| **SparkContext**                    | Acts as the master of the Spark application                                   | - Connects to cluster manager<br>- Coordinates tasks                        |
| **Cluster Manager**                 | Manages the cluster of machines                                               | - Allocates resources to Spark applications<br>- Examples: YARN, Mesos, standalone |
| **Executors**                       | Worker nodes that run tasks and store data                                    | - Execute assigned code<br>- Return results to the driver<br>- Cache data in memory for quick access |
| **Spark Application Submission**    | Submitting the driver program to the cluster manager                          | - Example: Submitting a job using `spark-submit`                            |
| **SparkContext Initialization**     | Driver program initializes SparkContext                                       | - Example: `val sc = new SparkContext(conf)`                                |
| **Job Scheduling**                  | Driver program defines transformations and actions on RDDs/DataFrames         | - Example: `val rdd = sc.textFile("data.txt").map(line => line.split(" "))`  |
| **DAG (Directed Acyclic Graph) Creation** | Constructing a DAG of stages for the job                                      | - Stages are sets of tasks that can be executed in parallel<br>- Example: A series of `map` and `filter` transformations create a DAG |
| **Task Execution**                  | Dividing the DAG into stages, creating tasks, and sending them to executors   | - Tasks are distributed across executors<br>- Each executor processes a partition of the data |
| **Data Shuffling**                  | Exchanging data between nodes during operations like `reduceByKey`            | - Data is grouped by key across nodes<br>- Example: Shuffling data for aggregations |
| **Result Collection**               | Executors process the tasks and send the results back to the driver program   | - Example: Final results of `collect` or `count` are returned to the driver |
| **Job Completion**                  | Driver program completes the execution                                        | - Example: Driver terminates after executing `sc.stop()`                    |

## Explain DAG in Spark

### What is DAG in Spark?

- **DAG stands for Directed Acyclic Graph.**
- It is a series of steps representing the operations (like transformations and actions) to be performed on data.
- **Directed** means the operations flow in one direction.
- **Acyclic** means there are no cycles or loops in the graph.

### Why do we need DAG in Spark?

- **Optimizes Execution**: Spark can optimize the order of operations for better performance.
- **Fault Tolerance**: If a node fails, Spark can use the DAG to recompute only the lost data.
- **Parallel Processing**: DAG helps divide tasks into stages that can be executed in parallel across the cluster.

### What would happen without a DAG?

- **No Optimization**: Operations would be executed in the order they are written, potentially resulting in slower performance.
- **No Fault Tolerance**: Spark wouldn't know how to recompute lost data efficiently if a failure occurs.
- **Less Parallelism**: It would be harder to break down tasks into parallelizable units, reducing efficiency.

### Example

Imagine you have a simple Spark job:
```scala
val data = sc.textFile("file.txt")
val words = data.flatMap(line => line.split(" "))
val wordCounts = words.map(word => (word, 1)).reduceByKey(_ + _)
wordCounts.collect()
```

**DAG Construction**:
1. **Read Data**: `sc.textFile("file.txt")`
2. **Split Lines into Words**: `data.flatMap(...)`
3. **Map Words to Key-Value Pairs**: `words.map(...)`
4. **Reduce by Key**: `wordCounts.reduceByKey(...)`
5. **Collect Results**: `wordCounts.collect()`

The DAG helps Spark understand how to break down these steps into stages and tasks for optimized execution.

## Explain spark.sql.shuffle.partitions Variable

- **What it is**: A configuration setting in Spark that defines the default number of partitions used when shuffling data for wide transformations.
- **Default Value**: The default value is 200 partitions.

### Purpose

- **Optimize Performance**: Balances the workload across the cluster for efficient execution.
- **Control Data Distribution**: Helps manage how data is distributed and processed during shuffle operations.

### When It's Used

- **Wide Transformations**: Used during operations like `reduceByKey`, `groupByKey`, `join`, and any other operations that require data shuffling across the network.
- **SQL Queries**: Affects operations in Spark SQL that involve shuffling data, such as joins and aggregations.

### How to Set It

- **Setting via Configuration**: You can set this variable in your Spark configuration to control the number of shuffle partitions.
  ```scala
  spark.conf.set("spark.sql.shuffle.partitions", "number_of_partitions")
  ```
- **Setting via spark-submit**: You can also set it when submitting your Spark job.
  ```sh
  spark-submit --conf spark.sql.shuffle.partitions=number_of_partitions ...
  ```

### Example

1. **Default Setting**:
   ```scala
   val spark = SparkSession.builder.appName("Example").getOrCreate()
   println(spark.conf.get("spark.sql.shuffle.partitions"))  // Output: 200
   ```

2. **Custom Setting**:
   ```scala
   val spark = SparkSession.builder.appName("Example").getOrCreate()
   spark.conf.set("spark.sql.shuffle.partitions", "50")
   val df = spark.read.json("data.json")
   df.groupBy("column").count().show()
   ```

### Why Adjust This Setting?

- **Small Datasets**: Reduce the number of partitions to avoid creating too many small tasks, which can lead to overhead.
- **Large Datasets**: Increase the number of partitions to distribute the data more evenly and avoid large, unmanageable partitions that can slow down processing.