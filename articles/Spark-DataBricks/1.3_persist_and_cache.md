---
layout: default
title: persist() and cache()
parent: Concepts
grand_parent: Spark-Databricks
nav_order: 5
---
- [Understanding `persist()` in Spark](#understanding-persist-in-spark)
    - [Syntax of `persist()`](#syntax-of-persist)
    - [Storage Levels](#storage-levels)
    - [Different Values for StorageLevel](#different-values-for-storagelevel)
    - [Example](#example)
    - [Removing Saved Data](#removing-saved-data)
    - [When to use persist()](#when-to-use-persist)
    - [Conclusion](#conclusion)
- [Understanding `cache()` in Spark](#understanding-cache-in-spark)



<img src="images/custom-image-2024-07-09-19-28-59.png" style="
    border: 2px solid gray;
    border-radius: 6px;
    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.2);
    margin: 20px;
    padding: 1px;
    width: 500; /* Maintain aspect ratio */
    height: 400; /* Maintain aspect ratio */
    transition: transform 0.2s;
"/>

# Understanding `persist()` in Spark

In Spark, the `persist()` method is used to save a dataset (RDD or DataFrame) in memory or on disk, so you can use it multiple times without recalculating it every time. If you need to use the same data repeatedly, `persist()` can help speed up your work and add some fault tolerance.

### Syntax of `persist()`

| **Usage**        | **Syntax (DataFrame)**           | **Syntax (RDD)**                   |
|------------------|----------------------------------|------------------------------------|
| **No Argument** | `dfPersist = df.persist()`       | `rdd.persist()`                    |
| **With Argument** | `dfPersist = df.persist(StorageLevel.XXXXXXX)` | `rdd.persist(StorageLevel.XXXXXXX)` |

### Storage Levels

**Note**: The default storage level is different for DataFrames and RDDs:
- **DataFrames**: Default is `MEMORY_AND_DISK`. ([DataFrame API Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.persist.html))
- **RDDs**: Default is `MEMORY_ONLY`. ([RDD Persistence Documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence))

### Different Values for StorageLevel

StorageLevel values are available in the `pyspark.StorageLevel` class. Here is the complete list:

- **DISK_ONLY**: Store data on disk only.
- **DISK_ONLY_2**: Store data on disk with replication to two nodes.
- **DISK_ONLY_3**: Store data on disk with replication to three nodes.
- **MEMORY_AND_DISK**: Store data in memory and spill to disk if necessary.
- **MEMORY_AND_DISK_2**: Store data in memory and spill to disk if necessary, with replication to two nodes.
- **MEMORY_AND_DISK_SER**: Store data in JVM memory as serialized objects and spill to disk if necessary.
- **MEMORY_AND_DISK_DESER**: Store data in JVM memory as deserialized objects and spill to disk if necessary.
- **MEMORY_ONLY**: Store data as deserialized objects in JVM memory only.
- **MEMORY_ONLY_2**: Store data in memory only, with replication to two nodes.
- **NONE**: No storage level. (Note: This can't be used as an argument.)
- **OFF_HEAP**: Store data in off-heap memory (experimental).

**Note**: The official Spark documentation states that the default storage level for `RDD.persist()` is `MEMORY_ONLY`. For `df.persist()`, it is `MEMORY_AND_DISK`, and starting from version 3.4.0, it is `MEMORY_AND_DISK_DESER`. ([Spark Persistence Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.persist.html))

### Example

Imagine you have a list of numbers, and you want to do some calculations on it multiple times. Here’s how you can use `persist()`:

```python
from pyspark import SparkContext

# Initialize Spark Context
sc = SparkContext("local", "PersistExample")

# Create an RDD
numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
rdd = sc.parallelize(numbers)

# Persist the RDD in memory
rdd.persist()

# Do some actions
print(rdd.count())  # Count the numbers
print(rdd.collect())  # Collect all the numbers
# This will show the storage level being used
print(dfPersistMemoryOnly.storageLevel)  
```

In this example, `persist()` saves the `rdd` in memory, so if you do more actions like `count()` or `collect()`, it doesn’t have to recalculate the data each time.

### Removing Saved Data

When you don’t need the saved data anymore, you can remove it from memory or disk using `unpersist()`:

```python
rdd.unpersist()
```

### When to use persist()

So, we have learned about `persist()`. Does that mean you should use `persist()` every time you have a DataFrame (df)? No. Here are the situations where it is recommended to use it. 

1. **Running Multiple Machine Learning Models**:
   When you run multiple machine learning models on the same dataset, using `persist()` can save time.

2. **Interactive Data Analysis**:
   If you are using a notebook for interactive data analysis, where you need to understand the data by running multiple queries, `persist()` will make the results come faster.

3. **ETL Process**:
   In your ETL (Extract, Transform, Load) process, if you have a cleaned DataFrame and you are using the same DataFrame in many steps, using `persist()` can help.

4. **Graph Processing**:
   When processing graphs using libraries like GraphX, `persist()` can improve performance.

> Remember, persist() is a technique to improve the code and not something mandatory. If you want to improve your processing speed, you should look at techniques to better partiton your data, use broadcast variables for joins etc rater than relying on persist() to fix all your problems.

### Conclusion

Using `persist()` in Spark is like telling Spark to remember your data so it doesn’t have to start from scratch every time you need it. This can make your work much faster, especially when working with large datasets, and also adds fault tolerance.

# Understanding `cache()` in Spark

The `cache()` function is a shorthand method for `persist()` with the default storage level, which is `MEMORY_ONLY`. This means,

`cache()` is same as `persist(StorageLevel.MEMORY_ONLY)`