var __index = {"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Welcome to Das Digital Digest","text":"<p>Thanks for stopping by! This site shares lessons from my professional journey \u2014 all drawn from real projects and hands-on experience. It\u2019s meant to be practical and useful for real work, not just theory. For questions or suggestions feel free to reach me at das.d@hotmail.com. Happy reading!</p> <p>Early Life and Educaton</p> <p>I was born in West Bengal and grew up across different parts of India, experiences that shaped my academic and personal journey. My academic background includes a B.Sc. (Hons) in Physics with a minor in Mathematics from the University of Calcutta followed by a Master\u2019s in Computer Application from the West Bengal University of Technology.</p> <p>Family</p> <p>I grew up in a family where my father served in the armed forces and my mother managed the household. Because of my dad's transferable job we moved frequently across India. This gave me exposure to different cultures and languages from an early age and made me adaptable and comfortable in diverse environments.</p> <p>I am married and have a daughter. My wife works as a Senior Scientist and Senior Assistant Director with the government. She holds a bachelor\u2019s and master\u2019s degree from St. Xavier\u2019s College, Kolkata, and a Ph.D. in Biotechnology from Nanyang Technological University (NTU), Singapore.</p> <p>Professional Journey</p> <p>My professional journey began as a final-year intern at HCL Technologies. A few months after my internship, I was seconded to Singapore, and since then, I\u2019ve had the opportunity to work across Singapore, Canada, Amsterdam, and the US.</p> <p>Over the years, I\u2019ve worked for leading software product companies such as RWS (SDL Tridion), IBM, and Visa Inc., as well as major IT services firms like HCL Technologies and Tata Consultancy Services (TCS).  </p> <p>I have also worked with a Canadian federal organization for several years, Singapore\u2019s tax office \u2014 the Inland Revenue Authority of Singapore (IRAS) \u2014 and the retail giant IKEA.</p> <p>Most of my projects have been in the financial domain \u2014 including banking, insurance, and taxation \u2014 while some projects were in logistics, retail, oil and gas, media and pharma.</p>"},{"location":"AI-ML/Running_Stable_Diffusion_Locally.html","title":"Installing and Running Stable Diffusion Locally on Mac M3 Pro/Max","text":"<p>In this guide, I\u2019ll show you how to run Stable Diffusion on your Mac M3 Pro/Max using the AUTOMATIC1111 Web UI. The process is straightforward: clone the AUTOMATIC1111/stable-diffusion-webui repository, then run the provided <code>webui.sh</code> script.</p>"},{"location":"AI-ML/Running_Stable_Diffusion_Locally.html#prerequisites","title":"Prerequisites","text":"<p>You will need Git and Python 3.10.6. Other Python versions might work, but 3.10.6 is recommended to ensure all scripts and libraries run smoothly.</p> <p></p>"},{"location":"AI-ML/Running_Stable_Diffusion_Locally.html#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<p>Clone the AUTOMATIC1111/stable-diffusion-webui repository in a convenient folder. Below is a screenshot showing this in VS Code:</p> <p></p>"},{"location":"AI-ML/Running_Stable_Diffusion_Locally.html#step-2-select-the-python-interpreter","title":"Step 2: Select the Python Interpreter","text":"<p>In VS Code, set Python 3.10.6 as your interpreter. This ensures any commands or virtual environments you create use the correct version.</p> <p></p>"},{"location":"AI-ML/Running_Stable_Diffusion_Locally.html#step-3-open-the-terminal","title":"Step 3: Open the Terminal","text":"<p>Open the integrated Terminal in VS Code and navigate to the folder containing the cloned repository.</p> <p></p>"},{"location":"AI-ML/Running_Stable_Diffusion_Locally.html#step-4-optional-create-a-virtual-environment","title":"Step 4 (Optional): Create a Virtual Environment","text":"<p>The <code>webui.sh</code> script will automatically set up a virtual environment if you don\u2019t have one. However, you can create one manually:</p> <pre><code>python3 -m venv venv\nsource venv/bin/activate\n</code></pre> <p>After activating it, <code>(venv)</code> appears in your terminal prompt. Make sure you\u2019re using Python 3.10.6 for this step.</p>"},{"location":"AI-ML/Running_Stable_Diffusion_Locally.html#step-5-place-your-model-checkpoint","title":"Step 5: Place Your Model Checkpoint","text":"<p>Download a model from Hugging Face (for example, Stable Diffusion v1.5) and save the file (<code>.safetensors</code> or <code>.ckpt</code>) into:</p> <pre><code>stable-diffusion-webui/models/Stable-diffusion/\n</code></pre> <p>Your folder might look like this:</p> <p></p>"},{"location":"AI-ML/Running_Stable_Diffusion_Locally.html#step-6-run-the-webuish-script","title":"Step 6: Run the <code>webui.sh</code> Script","text":"<p>Make the startup script executable if needed, then run it:</p> <pre><code>chmod +x webui.sh\n./webui.sh\n</code></pre> <p></p> <p>When it runs: - Dependencies (like PyTorch) are installed if missing. - The GPU backend is automatically detected. On an Apple Silicon M3 Pro/Max, MPS (Metal Performance Shaders) is used.</p>"},{"location":"AI-ML/Running_Stable_Diffusion_Locally.html#step-7-access-the-web-ui","title":"Step 7: Access the Web UI","text":"<p>Once the setup completes, the terminal will display a local URL like:</p> <pre><code>Running on local URL:  http://127.0.0.1:7860\n</code></pre> <p>Open that address in your browser to see the AUTOMATIC1111 Stable Diffusion Web UI.</p> <p></p>"},{"location":"AI-ML/Running_Stable_Diffusion_Locally.html#using-the-automatic1111-web-ui","title":"Using the AUTOMATIC1111 Web UI","text":""},{"location":"AI-ML/Running_Stable_Diffusion_Locally.html#txt2img","title":"txt2img","text":"<p>Enter your prompt, pick settings like Sampling Steps or Method (Euler, DPM++, etc.), and click Generate to create an image.</p>"},{"location":"AI-ML/Running_Stable_Diffusion_Locally.html#img2img","title":"img2img","text":"<p>Upload an existing image and specify how different you want the result to be by adjusting Denoising Strength.</p>"},{"location":"AI-ML/Running_Stable_Diffusion_Locally.html#inpainting","title":"Inpainting","text":"<p>Mask sections of an image and let Stable Diffusion replace them based on your prompt.</p>"},{"location":"AI-ML/Running_Stable_Diffusion_Locally.html#settings-extensions","title":"Settings &amp; Extensions","text":"<p>Explore advanced options under Settings (paths, optimization, UI tweaks, etc.). In Extensions, you can install additional features (e.g., ControlNet, textual inversions, LoRAs).</p>"},{"location":"AI-ML/Running_Stable_Diffusion_Locally.html#wrap-up","title":"Wrap-Up","text":"<p>In my experience, the setup was smooth and worked on the very first try. Generating a simple image at default settings took about 40 seconds, while more detailed prompts produced amusingly random results. Even under heavy load, memory usage stayed below 40 GB on my system. Enjoy your local AI art station on your Mac M3 Pro/Max!</p>"},{"location":"AboutMe/AzureAndM365Projects.html","title":"AzureAndM365Projects","text":"Azure Migration and Modernization of ETL Processes and Infrastructure Project and TeamTech StackRole &amp; ResponsibilitiesKey Achievements <ul> <li>Project: ETL Modernization and Infrastructure Migration to Azure  </li> <li>Role: Solution Architect / Data Engineering Lead  </li> <li>Team: 15 members including Data Engineers, ETL Developers, Azure Infrastructure Specialists, and cross-functional stakeholders  </li> <li>Objective:  <ul> <li>Migrate legacy on-prem Windows Servers and SQL Server databases to Azure VM and Azure SQL PaaS.  </li> <li>Modernize SSIS ETL pipelines for document and metadata processing.  </li> <li>Enable scalable, automated, and highly available data workflows in Azure.  </li> <li>Reduce on-prem maintenance cost and operational risk.  </li> </ul> </li> </ul> <ul> <li>Azure VMs (Windows Server 2016)  </li> <li>Azure SQL Database (PaaS)  </li> <li>SSIS / SSRS  </li> <li>SQL Server Management Studio  </li> <li>Power BI  </li> <li>Docker (for legacy ECM apps)  </li> <li>SharePoint Online  </li> </ul> <ul> <li>Designed Azure-based architecture, including VM sizing, SQL PaaS, and high availability planning.  </li> <li>Migrated on-prem SSIS ETL packages to Azure VM-hosted SQL Server, optimizing for performance and reliability.  </li> <li>Re-engineered SSIS packages to process document metadata from legacy ECM systems efficiently.  </li> <li>Built Azure SQL tables and schemas to capture document and operational data.  </li> <li>Implemented automation for metadata extraction, validation, and ingestion from multiple sources.  </li> <li>Configured Azure DR and HA strategies, including geo-redundant backups and site recovery.  </li> <li>Conducted POCs for performance tuning of SSIS workflows and SSRS reporting.  </li> <li>Collaborated with cross-functional teams and vendors to provision Azure infrastructure and ensure secure access.  </li> <li>Developed technical documentation, deployment guides, and trained internal teams on new cloud-based ETL workflows.  </li> <li>Monitored pipeline performance post-migration and ensured data quality and integrity.  </li> </ul> <ul> <li>Successfully migrated all on-prem SSIS ETL workflows to Azure VMs and Azure SQL.  </li> <li>Reduced infrastructure maintenance cost by ~40% by leveraging Azure PaaS and VM-based ETL hosting.  </li> <li>Improved ETL execution performance by ~50% through optimized SSIS workflows and SQL tuning.  </li> <li>Enabled high availability and disaster recovery for mission-critical financial data.  </li> </ul>"},{"location":"AboutMe/DataEngineeringProjects.html","title":"DataEngineeringProjects","text":"Data &amp; ETL Migration to Azure Government Commercial Cloud (GCC) BackgroundTech StackResponsibilities <p>Currently I work as a Senior Manager / Data Engineer at IRAS (Inland Revenue Authority of Singapore), Ministry of Finance, Singapore. My day-to-day works involves migrating data from on-prem SQL Server to Azure Synapse Analytics (Dedicated &amp; Serverless) using Azure Data Factory and PySpark (Synapse Spark Pool). I also rewrite legacy SAS SQL scripts into PySpark for Synapse Spark Pool. Additionally, I am migrating 120+ Hadoop/Hive ETL workflows to Synapse Serverless with ADLS. I collaborate with Data Scientists and ML Engineers on data preparation and integration tasks.</p> <ul> <li>Project: Data and ETL Migration from On-Prem to Azure Government Commercial Cloud (GCC) </li> <li>Role: Senior Manager / Data Engineer  </li> <li>Team: 20+ members across Data Engineering, Digital Infrastructure, Data Science, and AI  </li> <li>Objective: <ul> <li>Migrate on-prem SQL tables to Synapse Analytics (Serverless Delta Lake tables and Dedicated Pool) using Azure Data Factory and PySpark (Synapse Spark Pool).  </li> <li>Rewrite SAS SQL scripts into PySpark for Synapse Spark Pool.  </li> <li>Transition Hadoop/Hive ETL workflows to Synapse Serverless with ADLS.  </li> <li>Collaborate with Data Scientists and ML Engineers on data preparation and integration tasks.  </li> </ul> </li> </ul> <ul> <li>Azure Government Commercial Cloud (GCC)  </li> <li>Azure Data Factory  </li> <li>Synapse Analytics (Dedicated &amp; Serverless)  </li> <li>ADLS  </li> <li>PySpark / Synapse Spark Pool  </li> <li>Hadoop / Hive</li> <li>Azure DevOps  </li> <li>GitHub Copilot  </li> <li>MLflow / Kedro  </li> </ul> <ul> <li>Migrated data from on-prem SQL tables to Azure Synapse (Dedicated &amp; Serverless) using ADF and PySpark.  </li> <li>(In-progress)Migrating 120+ Hadoop/Hive ETL workflows to Synapse-ADLS.  </li> <li>(In-progress)Converting legacy SAS SQL scripts into PySpark for Synapse Spark Pool.  </li> <li>Provided technical guidance on PySpark Script optimization and best practices.  </li> <li>Worked with Data Scientists to refine datasets for model development.  </li> <li>Provided CI/CD pipeline guidance using Azure DevOps.  </li> <li>Conducted PoC comparing Kedro and MLflow for MLOps.  </li> <li>Supported user access and security in Azure GCC. </li> <li>Worked with other government agency teams to onboard GitHub Copilot and worked with vendors to resolve security issues.  </li> </ul> Gen-AI Chatbot Prototype for a Retail Company BackgroundImplementation <ul> <li>Role: Assistant Manager / Data Engineer  </li> </ul> <p>Problem IKEA partnered with Sprinklr to build a GenAI assistant for their retail website. My role was to coordinate requirements, design the architecture, and provide technical guidance. The main challenge was bridging IKEA\u2019s complex multi-country API ecosystem (Singapore, Malaysia, Philippines, Thailand, Mexico) with Sprinklr\u2019s chatbot requirements. The chatbot needed to understand queries like \u201cshow me white wardrobes under $500\u201d and translate them into specific API calls with correct filters and category mappings.</p> <p>Solution I developed a FastAPI-based chatbot prototype that integrates directly with IKEA\u2019s live APIs. The system uses a multi-agent approach: GPT-3.5 first analyzes user intent (product vs. store info), extracts details (category, store, filters), and a second LLM pass formats API responses into conversational replies. The prototype maintains indexed mappings of product categories and store locations, dynamically processes filters (color, material, price), and demonstrates every integration point for stakeholders. It ran locally and handled live queries against IKEA\u2019s production APIs.</p> <p></p> <p>Results</p> <p>The prototype showed that a GenAI chatbot could be built directly on top of IKEA\u2019s APIs. It gave a clear architecture for Sprinklr to use. Before this, there was confusion on how to handle filters, categories, and store logic. The prototype removed that uncertainty and showed what users wanted and what Sprinklr was offering. With just the APIs and GPT-3.5, we built a working chatbot, and the project picked up momentum after that.</p> <p></p> <pre><code># FastAPI-based IKEA Gen-AI Chatbot Prototype (Dec 2024)\n# Handles queries like: \"Show me white wardrobes\", with dynamic filters and API integration\n\nimport os, json, logging\nfrom typing import Dict, List, Optional, Any\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport httpx\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nload_dotenv()\napp = FastAPI()\n\nMARKET = os.getenv(\"MARKET\", \"sg\")\nLANGUAGE = os.getenv(\"LANGUAGE\", \"en\")\nCLIENT_ID = os.getenv(\"CLIENT_ID\", \"aiseaapp\")\nVERSION = os.getenv(\"VERSION\", \"20240110\")\n\nclass APIEndpoints:\n    def __init__(self, market, language):\n        self.STORES = f'https://www.ikea.com/{market}/{language}/meta-data/navigation/stores.json'\n        self.SEARCH = f'https://sik.search.blue.cdtapps.com/{market}/{language}/search-result-page'\n        self.PRODUCT_LIST = f'https://sik.search.blue.cdtapps.com/{market}/{language}/product-list-page'\n\nclass DataManager:\n    def __init__(self):\n        self.stores = self._load_json('stores.json')\n        self.categories = self._load_json('product-categories.json')\n        self.store_index = self._index_stores()\n        self.category_index = self._index_categories()\n\n    def _load_json(self, filename):\n        try:\n            with open(filename, 'r') as f:\n                return json.load(f)\n        except Exception:\n            return {}\n\n    def _index_stores(self):\n        return {store['displayName'].lower(): {'id': store['id']} for store in self.stores}\n\n    def _index_categories(self):\n        return {cat['title'].lower(): {'id': cat['id']} for cat in self.categories}\n\n    def find_category(self, query):\n        query_terms = query.lower().split()\n        return [{\"id\": data[\"id\"], \"title\": name} for name, data in self.category_index.items()\n                if all(term in name for term in query_terms)]\n\n    def find_store(self, query):\n        return [{\"id\": data[\"id\"], \"name\": name}\n                for name, data in self.store_index.items()\n                if any(term in name for term in query.lower().split())]\n\nclass QueryProcessor:\n    def __init__(self, api_key):\n        self.client = OpenAI(api_key=api_key)\n        self.data_manager = DataManager()\n        self.api_endpoints = APIEndpoints(MARKET, LANGUAGE)\n\n    def analyze_query(self, query):\n        try:\n            response = self.client.chat.completions.create(\n                model=\"gpt-3.5-turbo\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"Analyze IKEA query and extract query_type, category, store, requirements as JSON.\"},\n                    {\"role\": \"user\", \"content\": query}\n                ]\n            )\n            return json.loads(response.choices[0].message.content)\n        except Exception:\n            return {}\n\n    async def make_api_call(self, endpoint, params=None):\n        base_params = {'c': CLIENT_ID, 'v': VERSION}\n        if params:\n            filters = params.pop('filters', {}) if 'filters' in params else {}\n            base_params.update(filters)\n            base_params.update(params)\n        async with httpx.AsyncClient() as client:\n            response = await client.get(endpoint, params=base_params)\n            response.raise_for_status()\n            return response.json()\n\n    async def process_query(self, query, filters=None):\n        analysis = self.analyze_query(query)\n        categories = self.data_manager.find_category(query)\n        stores = self.data_manager.find_store(query)\n        params = {}\n        if filters:\n            params['filters'] = filters\n        api_responses = []\n        if analysis.get('query_type') == 'store_info':\n            api_responses.append(await self.make_api_call(self.api_endpoints.STORES))\n        elif analysis.get('query_type') == 'product_search':\n            if categories:\n                for category in categories:\n                    search_params = {'category': category['id'], 'store': stores[0]['id'] if stores else '022', **params}\n                    api_responses.append(await self.make_api_call(self.api_endpoints.PRODUCT_LIST, search_params))\n            else:\n                search_params = {'q': query, 'types': 'PRODUCT', **params}\n                api_responses.append(await self.make_api_call(self.api_endpoints.SEARCH, search_params))\n        formatted_response = \"I apologize, but I couldn't retrieve the information you requested.\"\n        if api_responses:\n            try:\n                response = self.client.chat.completions.create(\n                    model=\"gpt-3.5-turbo\",\n                    messages=[\n                        {\"role\": \"system\", \"content\": \"Format IKEA data into helpful response.\"},\n                        {\"role\": \"user\", \"content\": f\"Query: {query}\\nData: {json.dumps(api_responses)}\"}\n                    ]\n                )\n                formatted_response = response.choices[0].message.content\n            except Exception:\n                pass\n        return {'query': query, 'analysis': analysis, 'response': formatted_response}\n\nclass ChatRequest(BaseModel):\n    query: str\n    filters: Optional[Dict[str, Any]] = None\n\n@app.post(\"/chat\")\nasync def chat(request: ChatRequest):\n    query = request.query\n    filters = request.filters\n    if not query:\n        raise HTTPException(status_code=400, detail=\"No query provided\")\n    processor = QueryProcessor(os.getenv('OPENAI_API_KEY'))\n    return await processor.process_query(query, filters)\n\nif __name__ == '__main__':\n    import uvicorn\n    uvicorn.run(app, host='127.0.0.1', port=5000)\n</code></pre> CI/CD Pipeline for Databricks Git Synchronization for IKANO Retail BackgroundImplementation <p>IKANO retail data platform runs on Azure with Databricks, ADF, DBT, and Power BI. The platform processes data across 5 countries (SG, MY, PH, TH, MX) with master pipelines running twice daily, orchestrating 50+ Databricks jobs.</p> <p>Issue: Databricks Git integration doesn't automatically sync when code is pushed to Azure DevOps. Developers had to manually pull changes in Databricks after every commit. Authentication tokens expired periodically, breaking deployments silently. This manual process was unreliable for a platform processing critical daily data flows.</p> <p>Solution: I designed an Azure DevOps pipeline that triggers on commits to the main branch. The pipeline uses the Databricks CLI to authenticate via service principal, update Git credentials with a fresh token, and sync the Databricks repo to the latest commit. This automated approach ensures Databricks always has the latest code without manual intervention.</p> <p>Pipeline Config <pre><code>trigger:\n- ar_iac_databricks\n\njobs:\n- job: 'dbt_code_deploy'\ndisplayName: 'Deploy code on Databricks repos'\ntimeoutInMinutes: 10\npool:\n    vmImage: ubuntu-latest\n\nvariables:\n- group: azure-env-prod\n- name: databricks_git_credentials_id\n    value: 'xxx'\n- name: databricks_repository_id\n    value: 'xxx'\n- name: branch_name\n    value: 'ar_iac_databricks'\n</code></pre></p> <p>Auth &amp; Sync Steps <pre><code>steps:\n- script: sudo apt update &amp;&amp; sudo apt install curl jq -y\ndisplayName: 'Install tools'\n\n- script: curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh\ndisplayName: 'Install databricks-cli'\n\n- script: | \n    echo '[databricks-profile]' &gt;&gt; ~/.databrickscfg\n    echo \"host=$(DATABRICKS_PROD_URL)\" &gt;&gt; ~/.databrickscfg\n    echo \"azure_tenant_id=$(AZURE_TENANT_ID)\" &gt;&gt; ~/.databrickscfg\n    echo \"azure_client_id=$(AZURE_CLIENT_ID)\" &gt;&gt; ~/.databrickscfg\n    echo \"azure_client_secret=$AZURE_CLIENT_SECRET\" &gt;&gt; ~/.databrickscfg\nenv:\n    AZURE_CLIENT_SECRET: $(AZURE_CLIENT_SECRET)\ndisplayName: 'Setup databricks config'\n\n- script: |\n    curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \\\n    https://login.microsoftonline.com/$(AZURE_TENANT_ID)/oauth2/v2.0/token \\\n    -d client_id=$(AZURE_CLIENT_ID) \\\n    -d grant_type=client_credentials \\\n    -d scope=xxx/.default \\\n    -d client_secret=$AZURE_CLIENT_SECRET | jq '.access_token' -r &gt; ~/.ado_token\ndisplayName: 'Get fresh token'\n\n- script: |\n    databricks git-credentials update \\\n    $(databricks_git_credentials_id) \\\n    \"AzureDevOpsServices\" \\\n    --personal-access-token \"$(cat ~/.ado_token)\" \\\n    --profile databricks-profile\ndisplayName: 'Update git credentials'\n\n- script: |\n    databricks repos update \\\n    $(databricks_repository_id) \\\n    --branch $(branch_name) \\\n    --profile databricks-profile\ndisplayName: 'Sync repository'\n</code></pre></p> Databricks, DBT, ADF and Gen-AI Solutions for a Retail Company Background &amp; Project Overview <ul> <li>Project: Databricks, DBT, ADF and Gen-AI Solutions for IKANO Retail</li> <li> <p>Role: Assistant Manager / Data Engineer</p> <p>Background: </p> <p>The platform was built on Azure, Databricks, ADF (for orchestration), DBT (for SQL-based transformations), and Power BI (for reporting). It ingested data from multiple sources including Dynamics MVBC, Dynamics MVCRM, SQL Server, SharePoint, Google BigQuery, and Yotpo.  </p> <p>Data ingestion was done using Databricks Autoloader with transformations handled in Databricks + DBT. Two master ADF pipelines ran twice daily (SEA and Mexico), each orchestrating 50+ Databricks jobs and running for 5+ hours.  </p> <p>The setup processed sales orders, CRM data, product and pricing references, POS transactions, inventory, promotions, and ad-hoc SharePoint extracts, enabling country-specific reporting and cross-market insights. Dedicated ingestion jobs per country (SG, MY, PH, TH, MX) applied local rules and formats but were centrally orchestrated by ADF and Databricks.  </p> <p>Master Pipeline: </p> <ul> <li>Central orchestrator for 50+ Databricks jobs across SEA and Mexico  </li> <li>Runs twice daily, handling ingestion, transformation, parameter passing, logging, and error handling  </li> <li>Integrates data from MVBC, MVCRM, Yotpo, web analytics, food outlets, and SharePoint </li> <li>Outputs transformed data into DBT models that build analytics-ready (gold) tables for reporting and dashboards  </li> </ul> <p>Responsibilities: </p> <ul> <li>BAU support and monitoring of ADF + Databricks + DBT pipelines  </li> <li>Day-to-day Azure/Databricks administration  </li> <li>Optimize ADF and Databricks workflows to reduce Azure costs  </li> <li>Plan migration of Hive-based workloads into Databricks Unity Catalog </li> <li>Build an internal knowledge platform for documentation and learning  </li> </ul> </li> </ul> Tech Stack <ul> <li>Azure (ADLS, ADF, Azure DevOps, ARM Templates)  </li> <li>Databricks (Repos, Delta Lake, Autoloader, Unity Catalog)  </li> <li>DBT (SQL/ELT transformations)  </li> <li>Power BI  </li> <li>Dynamics MVCRM / MVBC  </li> <li>SharePoint  </li> <li>Google BigQuery  </li> <li>REST APIs (Ikano, Dynamics, internal systems)  </li> <li>Weaviate (Vector Database)  </li> <li>LangChain  </li> <li>GPT-3.5 / GitHub Copilot  </li> <li>Terraform &amp; ARM Templates (infra automation, evaluation)  </li> <li>GitHub-Jekyll / MkDocs / GitBook (knowledge platform, evaluation)  </li> </ul> Azure Databricks Migration for ML Data Preparation \u2013 Environmental Research Lab Project and TeamTechnical WorkResults &amp; ImpactTech StackChallenges <ul> <li>Project: Water Quality Data Migration from SharePoint to Azure  </li> <li>Role: Data Engineer  </li> <li>Duration: Jan 2024 \u2013 Sep 2024  </li> <li>Team: 5 members (Data Engineer, Lab Technician, Azure Admin, Environmental Scientist, Data Analyst)  </li> <li>Domain: Environmental Research \u2013 Water Purification  </li> </ul> <p>Overview: Migrated water quality testing data to Azure Data Lake and set up Databricks pipelines for cleaning and preparing ML-ready datasets. Supported predictive modeling of water treatment effectiveness.</p> <ul> <li> <p>Migration: </p> <ul> <li>Copied 12,000+ CSV/Excel files (450GB) from SharePoint to Azure Data Lake using Azure Data Factory pipelines  </li> <li>Verified file integrity and organized folders using Python scripts </li> </ul> </li> <li> <p>Databricks Processing: </p> <ul> <li>Ingested new CSV files with Auto Loader </li> <li>Cleaned and standardized key measurements (pH, turbidity, dissolved oxygen, contaminants) using PySpark/DataFrame transformations </li> <li>Performed basic data quality checks (range validation, missing values)  </li> <li>Aggregated data for ML and reporting  </li> </ul> </li> <li> <p>Job Scheduling: </p> <ul> <li>Daily Databricks jobs using Workflows </li> <li>Basic alerts on failures via email/Slack </li> </ul> </li> </ul> <ul> <li>Reduced data preparation from 2\u20133 weeks to 2 days  </li> <li>Standardized units and flagged data errors  </li> <li>Enabled trend analysis and ML model development for water treatment  </li> </ul> Component Technology Source SharePoint / Lab CSV files Migration Azure Data Factory, Python scripts Storage Azure Data Lake Storage Gen2 Processing Azure Databricks (Auto Loader, PySpark) Format Delta Lake Orchestration Databricks Workflows <ul> <li>Variations in CSV formats \u2192 simple parsing and schema adjustments  </li> <li>Missing/incorrect values \u2192 validation and cleaning steps  </li> <li>Network drive connectivity \u2192 retry logic and batch transfers</li> </ul> Azure Databricks Migration for ML Data Preparation \u2013 Biotech Research Lab Project and Team <ul> <li>Project: Lab Data Migration from SharePoint to Azure  </li> <li>Role: Data Engineer  </li> <li>Team: 4 members (Data Engineer, Azure Admin, Lab Manager, Data Scientist)  </li> <li>Domain: Biotech Research Lab  </li> </ul> <p>Overview: Migrated 3 years of lab data from SharePoint to Azure Data Lake and set up Databricks pipelines for data cleaning, standardization, and ML-ready dataset preparation. Enabled faster data access and ML model development.</p> Technical Work <ul> <li>Migration: <ul> <li>Azure Data Factory pipelines to transfer 8,000+ Excel/CSV files (750GB)  </li> <li>Python scripts for validation, integrity checks, and folder organization  </li> </ul> </li> </ul> <ul> <li> <p>Data Processing: </p> <ul> <li>Databricks notebooks for format standardization, missing/outlier handling  </li> <li>Compound ID mapping, aggregation, and feature engineering for ML  </li> <li>Generated data quality reports and visual checks  </li> </ul> </li> <li> <p>ML Output: </p> <ul> <li>Compound screening: 45,000 records  </li> <li>Protein binding: 12,000 measurements  </li> <li>Cell viability: 38,000 data points  </li> <li>Combined feature matrix for multi-target prediction</li> </ul> </li> </ul> ImpactTech StackChallenges <ul> <li>Reduced data prep time from weeks to hours  </li> <li>Resolved 2,300+ duplicate records and standardized units  </li> <li>Filled ~15% missing data via interpolation  </li> <li>Delivered first ML prototype 2 months ahead of schedule  </li> </ul> Component Technology Source SharePoint 2016 Migration Azure Data Factory Storage Azure Data Lake Storage Gen2 Processing Azure Databricks (Python) Development Jupyter notebooks, pandas, numpy <ul> <li>Data inconsistencies: Multiple Excel templates \u2192 Flexible parsing logic  </li> <li>Missing context: Cross-referenced lab notebooks  </li> <li>Connectivity issues: SharePoint timeouts \u2192 Retry logic, batch processing</li> </ul>"},{"location":"AboutMe/SharePointAndM365Projects.html","title":"SharePointAndM365Projects","text":"SharePoint CSV Migration to SQL Server Background <p>The project involved migrating CSV and Excel files from SharePoint 2013 (on-premises) to SQL Server 2016 for a retail client, mainly to support SSRS reporting. Over 200 files, updated weekly or monthly (50KB\u201350MB, with inconsistent headers and delimiters), were consolidated into a centralized SQL Server data warehouse. I architected the solution, selected the products and integration approach, and coordinated with DBAs, network teams, SharePoint admins, and business users. The design included a staging database for raw imports, normalized production tables for cleaned data, and audit tables for ETL logging. Built on SSIS 2016 with Visual Studio/SSDT, the pipelines automated file ingestion, validation, and transformation, reducing manual effort from 2 days to 2 hours and significantly improving data quality and standardization. The platform established a single SQL Server repository for downstream SSRS reports and analytics, with SQL Agent jobs managing scheduled execution.</p> <pre><code>- **Project:** SharePoint CSV Migration to SQL Server  \n- **Role:**  Data Engineer / Architect  \n- **Team:** 6 members (SSIS Developers, DBAs, SharePoint Admins)\n</code></pre> SSIS Implementation <ul> <li> <p>Packages: Flat File Source reading CSV/Excel  </p> </li> <li> <p>Workflows: </p> <ul> <li>Foreach Loop containers for multiple files  </li> <li>Data Flow tasks with conversion, validation, and cleansing  </li> <li>Conditional Split to route valid and invalid records  </li> <li>Script Tasks for file archiving and custom processing  </li> </ul> </li> <li> <p>Data Validation: </p> <ul> <li>Row count checks before/after processing  </li> <li>Data type conversions  </li> <li>Business rule validation using Conditional Split  </li> <li>Duplicate detection and error handling  </li> </ul> </li> <li> <p>Configuration &amp; Logging: </p> <ul> <li>Package configurations for different environments  </li> <li>Dynamic file path variables  </li> <li>Logging and notification for monitoring execution  </li> </ul> </li> </ul> SSIS-based ETL Solution For a Multinational Bank Project OverviewArchitectureTech Stack <p>Background</p> <p>This project was for a leading multinational bank with its hub in Singapore and branches across Southeast Asia. The goal was to build an ETL framework to process structured XML data from the bank\u2019s document capture platform and integrate it with their existing SQL Server systems. SSIS was chosen as the ETL tool since it was already part of the SQL Server stack and required minimal changes to security or infrastructure.</p> <p>Project Details: - Role: Solution Architect / Onsite coordinator - Team Size: 12 members (ETL developers, database architects, business analysts, DBAs) - Domain: Banking &amp; Financial Services - Document Processing - Pilot Scope: Credit Card Applications &amp; Account Opening (with expansion roadmap)</p> <p>Responsibilities In this project, I worked as a solution architect and onsite coordinator, focusing on the design and development of the ETL framework. My role covered selecting the right SQL Server\u2013based tools, designing the core data integration approach, and building key SSIS workflows. I collaborated with business analysts to turn requirements into technical specifications, ensured data quality checks were in place, and tuned performance for high-volume processing. Alongside my own development work, I supported the offshore team by reviewing packages and providing technical guidance where needed.</p> <p>Overall System Architecture: <pre><code>graph TB\n    subgraph \"Document Sources\"\n        DOC[Banking Documents&lt;br/&gt;Credit Cards / Accounts&lt;br/&gt;Paper / Digital Forms]\n    end\n\n    subgraph \"Document Processing (Existing)\"\n        CAPTIVA[EMC Captiva&lt;br/&gt;Document Capture &amp; OCR]\n        XML[XML Files&lt;br/&gt;Network Shared Folder&lt;br/&gt;\\\\BankingData\\XMLExport\\]\n    end\n\n    subgraph \"Microsoft ETL Platform - SQL Server 2008\"\n        VS[Visual Studio 2008&lt;br/&gt;SSIS Package Development&lt;br/&gt;BI Development Studio]\n\n        subgraph \"SSIS ETL Framework\"\n            MASTER[Master Controller&lt;br/&gt;SSIS Parent Package&lt;br/&gt;Orchestration &amp; Scheduling]\n            CC[Credit Card ETL&lt;br/&gt;Risk Assessment&lt;br/&gt;Validation Rules]\n            AO[Account Opening ETL&lt;br/&gt;KYC Processing&lt;br/&gt;Compliance Checks]\n        end\n\n        SSMS[SQL Server Management Studio&lt;br/&gt;DB Administration&lt;br/&gt;Performance Monitoring]\n    end\n\n    subgraph \"SQL Server 2008 Database Infrastructure\"\n        STAGING[(Staging Database&lt;br/&gt;Raw XML Import)]\n        EDW[(Enterprise Data Warehouse&lt;br/&gt;Clean &amp; Processed Data)]\n        AUDIT[(Audit Database&lt;br/&gt;Process Logs &amp; Lineage)]\n    end\n\n    subgraph \"Monitoring &amp; Reporting\"\n        SSRS[ETL Monitoring Dashboards&lt;br/&gt;Business Reports]\n        ALERTS[SQL Server Agent&lt;br/&gt;Job Scheduling&lt;br/&gt;Email Alerts]\n    end\n\n    subgraph \"Regional Deployment\"\n        SG[SG Hub&lt;br/&gt;Primary Processing]\n        MY[MY Node&lt;br/&gt;Local Compliance]\n        TH[TH Node&lt;br/&gt;Local Compliance]\n        ID[ID Node&lt;br/&gt;Local Compliance]\n        PH[PH Node&lt;br/&gt;Local Compliance]\n    end\n\n    DOC --&gt; CAPTIVA\n    CAPTIVA --&gt; XML\n    XML --&gt; MASTER\n\n    VS -.-&gt;|Develops| MASTER\n    VS -.-&gt;|Develops| CC\n    VS -.-&gt;|Develops| AO\n    SSMS -.-&gt;|Manages| STAGING\n    SSMS -.-&gt;|Manages| EDW\n\n    MASTER --&gt; CC\n    MASTER --&gt; AO\n    CC --&gt; STAGING\n    AO --&gt; STAGING\n    STAGING --&gt; EDW\n    CC --&gt; AUDIT\n    AO --&gt; AUDIT\n\n    EDW --&gt; SSRS\n    AUDIT --&gt; SSRS\n    MASTER --&gt; ALERTS\n\n    EDW -.-&gt;|Replicates to| SG\n    EDW -.-&gt;|Replicates to| MY\n    EDW -.-&gt;|Replicates to| TH\n    EDW -.-&gt;|Replicates to| ID\n    EDW -.-&gt;|Replicates to| PH\n\n    style VS fill:#e1f5fe\n    style SSMS fill:#e1f5fe\n    style SSRS fill:#e1f5fe\n    style ALERTS fill:#e1f5fe\n    style MASTER fill:#fff3e0\n    style CC fill:#fff3e0\n    style AO fill:#fff3e0\n    style STAGING fill:#f3e5f5\n    style EDW fill:#f3e5f5\n    style AUDIT fill:#f3e5f5</code></pre> Data Quality We built in basic data quality checks at the ETL stage:</p> <pre><code>- XML schema validation (XSD)\n- Business rule checks (credit scoring, risk, compliance)\n- Required field and format validation\n- Reference data lookups (country codes, branch codes, products)\n- Audit trail to track records end-to-end\n</code></pre> <p>Performance Tuning To handle large volumes, we tuned SSIS for speed:</p> <pre><code>- Ran packages in parallel where servers allowed\n- Used bulk loading (OLE DB with batch commits)\n- Adjusted buffer sizes and row limits for available memory\n- Disabled/rebuilt indexes during heavy loads\n- Reused database connections to reduce overhead\n</code></pre> <ul> <li>SQL Server 2008 R2 (Database + SSIS)</li> <li>SSIS (ETL framework for XML extraction, validation, and loading)</li> <li>SQL Server Agent (job scheduling and automation)</li> <li>SQL Server Management Studio (database administration and queries)</li> <li>BIDS (SSIS package development)</li> <li>C# script tasks/components for XML parsing, schema validation, and file handling where out-of-the-box SSIS wasn\u2019t sufficient</li> <li>File System tasks in SSIS for handling file detection/movement</li> <li>SSRS for general reports and ETL monitoring dashboards</li> <li>Windows Server 2008 (application and database servers)</li> <li>SQL Server failover clustering (Was already implemented for high availability)</li> </ul> SSIS-based ETL Solution Architecture \u2013 Major Oil &amp; Gas Company Project OverviewTech StackSolution ArchitectureResponsibilities <p>Background:</p> <p>This project was for a major oil &amp; gas company. Their existing data capture system produced XML files\u2014mainly invoices and contracts\u2014from multiple countries (Angola, Egypt, ANZ, Iberia, Germany). I led the development of  ETLs in SSIS to extract, validate, and transform the XML, then load it into SQL Server for downstream consumers, including SAP and BI/reporting teams. SSIS was chosen because it was already in place, integrated with MSSQL and downstream systems, and required minimal changes to existing security and network setups.</p> <p>Implementation Approach:</p> <p>After gathering requirements, the XML schemas for invoices, contracts, and related documents were analyzed. The ETL was built in SSIS with reusable packages for parsing, validation, and staging. Custom C# libraries were used for handling complex XML structures, applying business rules, and data enrichment. A staging database managed raw data, and audit, monitoring, and alerting were implemented to track processing and handle errors. The processed data was then loaded into SQL Server for downstream systems including SAP.</p> <p>Business Value:</p> <p>During the initial phase, the solution automated processing of ~10k XML files per month with error handling and recovery. It provided clean, structured data for SAP and reporting teams, supported 24/7 processing with high availability. The solution was scalable, meaning additional ETLs could be added for new document types or departments.</p> <p>Project Details:</p> <ul> <li>Role: Solution Architect / Lead Data Engineer</li> <li>Team: 20 members (ETL developers, data engineers, QA specialists, business analysts)</li> <li>Domain: Oil &amp; Gas / Energy Sector</li> <li>Geographic Scope: Angola, West Africa, and regional operations</li> </ul> <p>Core Data Platform:</p> <ul> <li>Microsoft SQL Server 2008 R2 Enterprise - Primary database platform and data warehouse</li> <li>SQL Server Integration Services (SSIS) 2008 - ETL orchestration and data transformation</li> <li>SQL Server Management Studio 2008 - Database administration and development</li> <li>Windows Server 2008 R2 - Application server infrastructure</li> </ul> <p>Development &amp; Integration:</p> <ul> <li>Microsoft .NET Framework 4.0 - Custom component development and business logic</li> <li>Visual Studio 2008 - Integrated development environment</li> <li>C# Programming Language - Custom ETL components and utilities</li> <li>PowerShell v2 - Automation scripting and monitoring</li> </ul> <p>Supporting Technologies:</p> <ul> <li>Microsoft Excel - Business reporting and analytics dashboards</li> <li>Windows File System - Source file monitoring and archival</li> <li>SQL Server Agent - Job scheduling and workflow automation</li> <li>XML Schema Definition (XSD) - Data validation and structure enforcement</li> </ul> <pre><code>graph TB\n    subgraph \"Source Systems\"\n        FIELD_OPS[Field Operations&lt;br/&gt;Angola, Africa&lt;br/&gt;XML Files]\n        VENDORS[Vendor Systems&lt;br/&gt;Batch XML Files]\n        LEGACY[Legacy Systems&lt;br/&gt;Migration/Existing Data]\n    end\n\n    subgraph \"ETL Layer\"\n        FILE_MONITOR[File Monitor&lt;br/&gt;Automated Detection]\n        XML_PARSE[XML Parsing &amp; Validation]\n        TRANSFORM[Transformation &amp; Business Rules]\n        STAGING[Staging DB&lt;br/&gt;Temporary Storage&lt;br/&gt;Audit Tables]\n        SSIS_ENGINE[SSIS Orchestration]\n    end\n\n    subgraph \"Data Platform\"\n        SQL_SERVER[SQL Server 2008 R2&lt;br/&gt;DW &amp; ODS]\n    end\n\n    subgraph \"Consumers\"\n        SAP[SAP Team]\n        BI[BI &amp; Reporting]\n        OPS[Operations Teams]\n    end\n\n    FIELD_OPS --&gt; FILE_MONITOR\n    VENDORS --&gt; FILE_MONITOR\n    LEGACY --&gt; FILE_MONITOR\n\n    FILE_MONITOR --&gt; XML_PARSE\n    XML_PARSE --&gt; TRANSFORM\n    TRANSFORM --&gt; STAGING\n    STAGING --&gt; SSIS_ENGINE\n    SSIS_ENGINE --&gt; SQL_SERVER\n\n    SQL_SERVER --&gt; SAP\n    SQL_SERVER --&gt; BI\n    SQL_SERVER --&gt; OPS</code></pre> <p>I was the solution architect for the project. I defined the components and recommended the tech stack, analyzed the initial technical landscape, and looked at integration challenges and user requirements. I coordinated with the SAP team, DBAs, and the network team, and then designed the SSIS solution to meet the needs of the business and downstream systems.</p> SharePoint 2010 Data Ingestion - SDL Tridion Netherlands (Now RWS Group) B.V. Project and TeamTech StackRole &amp; Responsibilities <ul> <li>Client/Company: SDL Dev Labs (Now RWS Group)  </li> <li>Role: Technical Engineer  </li> <li>Team: 10 members (Product Team for ECM/WCM solutions) </li> <li>Domain: Web Content Management / Enterprise Content Management </li> <li>Objective: <ul> <li>Design and implement data ingestion workflows from digital documents to SharePoint CMS and Tridion WCM.  </li> <li>Ensure metadata extraction, content transformation, and proper publication pipeline.  </li> <li>Provide technical support to customers for seamless WCM/ECM operations.  </li> </ul> </li> </ul> <ul> <li>SDL Tridion Product Suite  </li> <li>SharePoint 2010 / SharePoint Designer 2010  </li> <li>SQL Server 2008  </li> <li>Visual Studio 2008  </li> <li>C#.NET, VBA  </li> </ul> <ul> <li>Developed end-to-end data ingestion workflows: digital documents \u2192 OCR-based metadata extraction \u2192 SharePoint CMS.  </li> <li>Built workflows for content lifecycle: initial creation in SharePoint \u2192 editing and publication in Tridion.  </li> <li>Provided ongoing technical support to customers and internal teams.  </li> <li>Ensured high quality and reliability of content workflows, improving customer satisfaction.  </li> <li>Received recognition for delivering excellent support and contributing to improved CSAT scores.  </li> </ul> SharePoint 2007 \u2013 Stanardalone to 6-Server Farm Migration Project OverviewTech StackSolution ArchitectureResponsibilitiesKey Achievements <p>Background:</p> <p>This project involved migrating a standalone SharePoint 2007 deployment to a 6-server farm for a logistics and supply chain company in Amsterdam. I designed the farm with redundancy and load balancing, set up SQL Server 2008 R2 Active/Passive failover clusters, and implemented the necessary network and load balancing infrastructure. Around 50 site collections were migrated with minimal downtime, and governance, security, and operational procedures were established.</p> <p>Implementation Approach</p> <p>The project involved migrating a standalone SharePoint 2007 deployment to a 6-server farm. The work started with planning and architecture design, including stakeholder workshops, analyzing the existing environment, defining growth requirements, and creating hardware and procurement plans. SQL Server 2008 R2 failover clusters were implemented with shared storage, backup, and recovery procedures, coordinating with networking and infrastructure teams. The SharePoint farm was deployed across multiple servers, with service applications distributed, F5 load balancer configured, and business requirements integrated. Site collections were migrated in phases using stsadm, with end-user validation, monitoring, and performance tracking. Operational procedures, monitoring dashboards, and alerting frameworks were established, and the production cutover was executed with infrastructure and business teams.</p> <p></p> <p>Project Details:</p> <ul> <li>Role: SharePoint and SQL Server Architect</li> <li>Location: Amsterdam, Netherlands (Client-side deployment)</li> <li>Team: 15+ members (infrastructure engineers, DBAs, SharePoint developers, network architects)</li> <li>Domain: Global Logistics &amp; Supply Chain</li> </ul> <p>Core Platform:</p> <ul> <li>SharePoint 2007 (MOSS) - Enterprise portal and collaboration platform</li> <li>SQL Server 2008 R2 Enterprise - Database engine with failover clustering</li> <li>Windows Server 2008 R2 - Database servers</li> <li>Windows Server 2003 R2 SP2 - SharePoint farm servers</li> <li>IIS 6.0 - Web server platform</li> </ul> <p>Infrastructure &amp; Hardware:</p> <ul> <li>F5 BIG-IP LTM - Hardware load balancer with SSL acceleration</li> <li>EMC SAN Storage - Shared storage with Fiber Channel 8Gbps</li> <li>Dell PowerEdge Servers - Enterprise server hardware</li> <li>VMware vSphere - Virtualization platform for supporting services</li> </ul> <p>Development &amp; Management:</p> <ul> <li>SharePoint Designer 2007 - Site customization and workflow design</li> <li>Visual Studio 2008 - Custom component development</li> <li>SQL Server Management Studio - Database administration</li> <li>Visual Source Safe 2005 - Source code management</li> </ul> <p>6-Server Farm Overview: <pre><code>graph TB\n    subgraph \"Load Balancing Tier\"\n        F5[F5 BIG-IP Load Balancer&lt;br/&gt;SSL Termination&lt;br/&gt;Health Monitoring]\n    end\n\n    subgraph \"Web Tier\"\n        WFE1[Web Front-End 1&lt;br/&gt;Windows 2003 R2&lt;br/&gt;IIS 6.0 + MOSS]\n        WFE2[Web Front-End 2&lt;br/&gt;Windows 2003 R2&lt;br/&gt;IIS 6.0 + MOSS]\n    end\n\n    subgraph \"Application Tier\"\n        APP1[Application Server 1&lt;br/&gt;Central Admin&lt;br/&gt;Search Services]\n        APP2[Application Server 2&lt;br/&gt;Excel Services&lt;br/&gt;InfoPath Services]\n    end\n\n    subgraph \"Database Tier\"\n        SQL_CLUSTER[SQL Server 2008 R2 Cluster&lt;br/&gt;Active/Passive Configuration&lt;br/&gt;Shared SAN Storage]\n    end\n\n    F5 --&gt; WFE1\n    F5 --&gt; WFE2\n    WFE1 --&gt; APP1\n    WFE1 --&gt; APP2\n    WFE2 --&gt; APP1\n    WFE2 --&gt; APP2\n    APP1 --&gt; SQL_CLUSTER\n    APP2 --&gt; SQL_CLUSTER\n    WFE1 --&gt; SQL_CLUSTER\n    WFE2 --&gt; SQL_CLUSTER</code></pre></p> <p>SQL Server Cluster Architecture: <pre><code>graph TB\n    subgraph \"SQL Server Failover Cluster\"\n        SQL_NODE1[SQL Node 1 - Active&lt;br/&gt;Windows 2008 R2&lt;br/&gt;32GB RAM, 8-Core CPU]\n        SQL_NODE2[SQL Node 2 - Passive&lt;br/&gt;Windows 2008 R2&lt;br/&gt;32GB RAM, 8-Core CPU]\n        SHARED_STORAGE[EMC SAN Storage&lt;br/&gt;RAID 10 Configuration&lt;br/&gt;1.5TB Capacity&lt;br/&gt;Fiber Channel 8Gbps]\n    end\n\n    subgraph \"Storage Layout\"\n        QUORUM[Quorum Disk&lt;br/&gt;1GB]\n        SYSTEM_DB[System Databases&lt;br/&gt;100GB]\n        CONTENT_DB[Content Databases&lt;br/&gt;500GB]\n        LOGS[Transaction Logs&lt;br/&gt;200GB]\n        BACKUP[Backup Storage&lt;br/&gt;700GB]\n    end\n\n    SQL_NODE1 -.-&gt;|Heartbeat| SQL_NODE2\n    SQL_NODE1 --&gt; SHARED_STORAGE\n    SQL_NODE2 --&gt; SHARED_STORAGE\n    SHARED_STORAGE --&gt; QUORUM\n    SHARED_STORAGE --&gt; SYSTEM_DB\n    SHARED_STORAGE --&gt; CONTENT_DB\n    SHARED_STORAGE --&gt; LOGS\n    SHARED_STORAGE --&gt; BACKUP</code></pre></p> <p>Network Infrastructure: <pre><code>graph TB\n    subgraph \"External Access\"\n        INTERNET[Internet Traffic]\n        FIREWALL[Corporate Firewall]\n    end\n\n    subgraph \"Load Balancing\"\n        F5_PRIMARY[F5 Primary Unit]\n        F5_SECONDARY[F5 Secondary Unit]\n    end\n\n    subgraph \"Network Core\"\n        CORE_SWITCH1[Core Switch 1&lt;br/&gt;Cisco Catalyst 6500]\n        CORE_SWITCH2[Core Switch 2&lt;br/&gt;Cisco Catalyst 6500]\n    end\n\n    subgraph \"VLANs\"\n        WEB_VLAN[Web Tier VLAN&lt;br/&gt;x.x.x.x/x]\n        APP_VLAN[App Tier VLAN&lt;br/&gt;x.x.x.x/x]\n        DB_VLAN[DB Tier VLAN&lt;br/&gt;x.x.x.x/x]\n    end\n\n    INTERNET --&gt; FIREWALL\n    FIREWALL --&gt; F5_PRIMARY\n    F5_PRIMARY -.-&gt;|Failover| F5_SECONDARY\n    F5_PRIMARY --&gt; CORE_SWITCH1\n    F5_SECONDARY --&gt; CORE_SWITCH2\n    CORE_SWITCH1 -.-&gt;|Redundancy| CORE_SWITCH2\n    CORE_SWITCH1 --&gt; WEB_VLAN\n    CORE_SWITCH1 --&gt; APP_VLAN\n    CORE_SWITCH1 --&gt; DB_VLAN</code></pre></p> <ul> <li>Designed and implemented 6-server SharePoint 2007 farm with redundancy and load balancing</li> <li>Worked with network team to configure SQL Server 2008 R2 Active/Passive failover cluster with shared SAN storage</li> <li>Worked with network team to set up F5 BIG-IP load balancer for SSL termination and health monitoring</li> <li>Planned and executed migration of 50+ site collections from standalone to farm environment</li> <li>Established governance, security policies, and operational procedures for farm management</li> <li>Coordinated with network, infrastructure, and DBA teams for seamless integration</li> <li>Provided post-migration support and optimization to ensure performance and reliability</li> <li>Conducted knowledge transfer sessions for internal IT teams on farm administration</li> </ul> <ul> <li>Successfully migrated 50+ site collections with minimal downtime (&lt;4 hours)</li> <li>Achieved 99.9% uptime through farm architecture and failover mechanisms</li> <li>Improved page load times by 30% through optimized SQL queries and indexing</li> <li>Received client commendation for seamless migration and effective knowledge transfer</li> </ul> SharePoint 2007 \u2013 Publishing Portal Content Transformation BackgroundTech StackArchitectureResponsibilities <p>Background: For a major Insurance provider, legacy intranet content was hosted on static HTML/ASP sites and dispersed repositories, resulting in:</p> <ul> <li>Manual content updates  </li> <li>Limited search capabilities  </li> <li>No multilingual support  </li> <li>Decentralized document and asset management  </li> </ul> <p>The client wanted to migrate to Microsoft Office SharePoint Server (MOSS) 2007 Publishing Portal to establish centralized content storage, structured publishing, multilingual support, and enterprise search capabilities.</p> <p>Project Details: </p> <ul> <li>Role: Senior Software Engineer  </li> <li>Team Size: 10 members (developers, designers, IT administrators)  </li> <li>Domain: Insurance Industry </li> </ul> <p>Solution Overview: <pre><code>graph TB\n    subgraph \"Legacy Systems\"\n        LegacyHTML[Static HTML/ASP Sites]\n        LegacyDocs[Dispersed Document Repositories]\n        LegacyAssets[Scattered Digital Assets]\n    end\n\n    subgraph \"Migration Process\"\n        Analysis[Content Analysis &amp; Transformation]\n        Cleanup[HTML Cleanup &amp; Standardization]\n        Migration[Content Migration &amp; Metadata Preservation]\n    end\n\n    subgraph \"MOSS 2007 Solution\"\n        Portal[SharePoint Publishing Portal]\n        Search[Enterprise Search]\n        Multilingual[Variations for Multilingual Support]\n        Forms[InfoPath Forms Integration]\n    end\n\n    LegacyHTML --&gt; Analysis\n    LegacyDocs --&gt; Analysis\n    LegacyAssets --&gt; Analysis\n\n    Analysis --&gt; Cleanup\n    Cleanup --&gt; Migration\n    Migration --&gt; Portal\n    Portal --&gt; Search\n    Portal --&gt; Multilingual\n    Portal --&gt; Forms</code></pre></p> <p>Core Platform:</p> <ul> <li>Microsoft Office SharePoint Server (MOSS) 2007 - Publishing Portal, Variations, Page Libraries, Picture Libraries, Document Libraries, Lists</li> <li>SQL Server 2005 Service Pack 3 - Active-Passive Cluster configuration</li> <li>Windows Server 2003 Service Pack 2 - SharePoint Farm deployment</li> <li>Internet Information Services (IIS) 6.0 - Web server platform</li> </ul> <p>Development &amp; Design Tools:</p> <ul> <li>SharePoint Designer 2007 - Master pages, page layouts, workflows</li> <li>Visual Studio 2005 Service Pack 1 - Custom development (C# .NET, HTML, CSS, jQuery)</li> <li>SQL Server Management Studio 2005 - Database administration</li> <li>Adobe Photoshop CS3 - Graphics and UI design</li> <li>InfoPath 2007 - Forms design and integration</li> </ul> <p>SharePoint Farm Architecture: <pre><code>graph TB\n    subgraph \"Load Balancing Layer\"\n        LB[Load Balancer]\n    end\n\n    subgraph \"Web Front-End Tier\"\n        WFE1[WFE Server 1&lt;br/&gt;Windows Server 2003 SP2&lt;br/&gt;IIS 6.0&lt;br/&gt;MOSS 2007]\n        WFE2[WFE Server 2&lt;br/&gt;Windows Server 2003 SP2&lt;br/&gt;IIS 6.0&lt;br/&gt;MOSS 2007]\n    end\n\n    subgraph \"Application Tier\"\n        APP[Application Server&lt;br/&gt;Windows Server 2003 SP2&lt;br/&gt;MOSS 2007&lt;br/&gt;SharePoint Services]\n    end\n\n    subgraph \"Database Tier\"\n        SQLActive[(SQL Server 2005 SP3&lt;br/&gt;Active Node&lt;br/&gt;Content Databases)]\n        SQLPassive[(SQL Server 2005 SP3&lt;br/&gt;Passive Node&lt;br/&gt;Failover Cluster)]\n    end\n\n    subgraph \"Client Access\"\n        Users[End Users&lt;br/&gt;IE 7]\n        Admins[SharePoint Admins&lt;br/&gt;Central Administration]\n    end\n\n    Users --&gt; LB\n    Admins --&gt; LB\n    LB --&gt; WFE1\n    LB --&gt; WFE2\n\n    WFE1 -.-&gt;|Service Calls| APP\n    WFE2 -.-&gt;|Service Calls| APP\n    APP -.-&gt;|Service Applications| WFE1\n    APP -.-&gt;|Service Applications| WFE2\n\n    WFE1 --&gt;|Content DB Access| SQLActive\n    WFE2 --&gt;|Content DB Access| SQLActive\n    APP --&gt;|Config/Admin DB| SQLActive\n\n    SQLActive -.-&gt;|Cluster Failover| SQLPassive\n    SQLPassive -.-&gt;|Cluster Failover| SQLActive</code></pre></p> <p>Content Migration Flow: <pre><code>graph LR\n    subgraph \"Legacy Content Sources\"\n        HTML[Static HTML Pages&lt;br/&gt;ASP Classic Sites]\n        Docs[Document Repositories&lt;br/&gt;File Shares&lt;br/&gt;Legacy Systems]\n        Images[Image Assets&lt;br/&gt;Media Files]\n        Lists[Events, Links, FAQs&lt;br/&gt;Static Lists]\n    end\n\n    subgraph \"Transformation Process\"\n        Extract[Content Extraction&lt;br/&gt;&amp; Analysis]\n        Transform[HTML Cleanup&lt;br/&gt;UTF Compliance&lt;br/&gt;Link Preservation]\n        Validate[Content Validation&lt;br/&gt;Metadata Mapping]\n    end\n\n    subgraph \"SharePoint Content Types\"\n        PageLib[Page Libraries&lt;br/&gt;Publishing Pages&lt;br/&gt;Master Pages &amp; Layouts]\n        DocLib[Document Libraries&lt;br/&gt;Version Control&lt;br/&gt;Metadata]\n        PicLib[Picture Libraries&lt;br/&gt;Image References&lt;br/&gt;Alt Text]\n        SPLists[SharePoint Lists&lt;br/&gt;Custom Columns&lt;br/&gt;Views]\n    end\n\n    HTML --&gt; Extract\n    Docs --&gt; Extract\n    Images --&gt; Extract\n    Lists --&gt; Extract\n\n    Extract --&gt; Transform\n    Transform --&gt; Validate\n\n    Validate --&gt; PageLib\n    Validate --&gt; DocLib\n    Validate --&gt; PicLib\n    Validate --&gt; SPLists</code></pre></p> <p>As one of the core members of a small SharePoint team, I contributed across all phases of the implementation, working closely with infra, application, and network teams to deliver the solution.</p> <p>Phase 1: Infrastructure Setup We configured a SQL Server 2005 SP3 Active-Passive cluster with shared storage for high availability, created content and configuration databases, and deployed a SharePoint 2007 farm with WFE and application servers. IIS 6.0 was configured and integrated with Active Directory for authentication, and load balancing was established in collaboration with the network team to ensure resilience and uptime.</p> <p>Phase 2: Content Analysis &amp; Migration I was actively involved in content transformation analysis, covering UTF compliance, HTML cleanup, link preservation, and metadata standardization. Our team developed migration strategies for different content types, moving HTML/ASP pages into publishing page libraries, images into picture libraries, documents into document libraries (with metadata intact), and restructuring events, links, and FAQs into SharePoint lists. These efforts streamlined management of previously scattered content and improved accessibility for end users.</p> <p>Phase 3: Customization &amp; Development On the customization side, I contributed to creating master pages and page layouts with SharePoint Designer, applying CSS styling and jQuery enhancements, and implementing branding elements. We also integrated InfoPath 2007 forms with workflows, enabled multilingual publishing through SharePoint Variations, and configured enterprise search with custom scopes and result types to enhance discoverability.</p> <p>Phase 4: Support &amp; Adoption To support long-term use, I helped document farm topology, migration procedures, and custom development work, as well as prepare deployment scripts and automation tools. I also provided operational support, trained content managers on publishing workflows, and worked on governance and maintenance procedures. These activities reduced content update times from hours to minutes, shortened approval cycles, and improved overall discoverability, contributing to smoother adoption across the business.</p> WSS 3.0 \u2013 Migration of Network Documents to SharePoint BackgroundTech StackRole &amp; Responsibilities <ul> <li>Project: Document Migration from Network Drives to SharePoint WSS 3.0  </li> <li>Role: Software Engineer  </li> <li>Team: 1 Project Manager + 3 Developers</li> <li>Domain: Financial Services / Banking</li> <li>Objective: <ul> <li>Migrate documents from network drives into SharePoint WSS 3.0.  </li> <li>Preserve key metadata (creation date, modification date, author) during migration.  </li> <li>Build a custom migration utility in C# to accelerate uploads.  </li> <li>Upgrade WSS 3.0 to SP2 as a prerequisite for future migration to MOSS 2007.  </li> </ul> </li> </ul> <ul> <li>SharePoint WSS 3.0 / SP2  </li> <li>Windows Server 2003 + IIS 6  </li> <li>Visual Studio 2005 (C#)  </li> <li>SQL Server 2005  </li> </ul> <ul> <li>Developed a custom C# migration tool to upload documents into WSS 3.0.  </li> <li>Collaborated with users during the migration process, resolving errors and enhancing the tool based on feedback.  </li> <li>Ensured document metadata integrity (author, creation date, modification date) during migration.  </li> <li>Supported manual pre-sorting of documents to improve migration efficiency.  </li> <li>Upgraded SharePoint WSS 3.0 to SP2 in preparation for transition to MOSS 2007.  </li> </ul>"},{"location":"Airflow/1.0.0_AirFlow_Concepts.html","title":"Concepts","text":""},{"location":"Airflow/1.0.0_AirFlow_Concepts.html#what-is-airflow","title":"What is Airflow?","text":"<p>Airflow, short for Air(Bnb)(Work)flow, is an open-source Linux-based platform to build, schedule, and monitor workflows. It\u2019s mainly used for data-related (ETL) workflows. The main strength of Airflow is that using it you can create workflows as code.</p> <p>Remember:  - Airflow is for Linux. It is meant for Linux ONLY. You might find some installation method to prove otherwise, but behind the scenes, there will always be a Linux kernel, Docker container, WSL, or something similar. And since most servers run on Linux, why bother trying to run workflows on Windows?  - Built on Python: In Airflow, Python is everywhere.</p>"},{"location":"Airflow/1.0.0_AirFlow_Concepts.html#airflow-core-components","title":"Airflow Core Components","text":"<ul> <li>Web Server: Provides the primary web interface for interacting with Airflow.</li> <li>Scheduler: Responsible for scheduling tasks.</li> <li>Meta Database: Stores information about tasks, their status, and other metadata.</li> <li>Trigger: Initiates tasks based on predefined conditions.</li> <li>Executor: Determines how and where tasks will be executed but doesn\u2019t run the tasks itself.</li> <li>Queue: A list of tasks waiting to be executed.</li> <li>Worker: The process that actually performs the tasks.</li> </ul> <p>To get a quick understanding of Airflow's concepts, you might want to check out this video.</p>"},{"location":"Airflow/1.0.0_AirFlow_Concepts.html#airflow-core-concepts","title":"Airflow Core Concepts","text":"<ul> <li> <p>DAG (Directed Acyclic Graph): This is the core of your workflow. A DAG is essentially a data pipeline and must be acyclic, meaning it cannot contain any loops.  </p> <p></p> <p></p> </li> <li> <p>Operator: An operator represents a task. Airflow offers a wide variety of operators, including those for Python to execute Python code.</p> </li> <li>Finding Operators: To explore available operators, visit the Astronomer Registry.</li> <li>Tasks/Task Instance: A task is a specific instance of an operator, representing the actual unit of work that gets executed.</li> <li>Workflow: The entire process defined by the DAG. Essentially, the \"DAG is the workflow.\"</li> </ul>"},{"location":"Airflow/1.0.0_AirFlow_Concepts.html#what-airflow-is-not","title":"What Airflow is Not","text":"<ul> <li>Not a Data Processing Framework: Airflow can't process large volumes of data by itself.</li> <li>Not a Real-Time Streaming Framework: For real-time streaming, tools like Kafka are more appropriate.</li> <li>Not a Data Storage System: Although Airflow uses databases for its operations, it is not meant for data storage.</li> </ul>"},{"location":"Airflow/1.0.0_AirFlow_Concepts.html#when-airflow-might-not-be-the-best-solution","title":"When Airflow Might Not Be the Best Solution:","text":"<ul> <li>High-Frequency Scheduling: If you need to schedule tasks every second, Airflow may not be suitable.</li> <li>Large Data Processing: Airflow is not designed to process large datasets directly. If you need to handle terabytes of data, it\u2019s better to trigger a Spark job from Airflow and let Spark do the heavy lifting.</li> <li>Real-Time Data Processing: Airflow is not ideal for real-time data processing; Kafka would be a better option.</li> <li>Simple Workflows: For straightforward workflows, Airflow might be overkill. Alternatives like ADF, cron jobs, or Power Automate may be more appropriate.</li> </ul>"},{"location":"Airflow/1.0.0_AirFlow_Concepts.html#different-type-of-airflow-setup","title":"Different type of Airflow Setup","text":""},{"location":"Airflow/1.0.0_AirFlow_Concepts.html#single-node-architecture","title":"Single-Node Architecture","text":"<p>In a single-node setup, all components of Airflow run on one machine.  </p> <p></p> <p>This architecture is ideal for smaller workflows and getting started with Airflow.</p>"},{"location":"Airflow/1.0.0_AirFlow_Concepts.html#multi-node-airflow-architecture","title":"Multi-Node Airflow Architecture","text":"<p>As your workflows grow, you might consider a multi-node architecture for better scalability and performance. </p>"},{"location":"Airflow/1.0.1_A_Dags_Anatomy.html","title":"DAG Anatomy","text":""},{"location":"Airflow/1.0.1_A_Dags_Anatomy.html#a-dags-anatomy","title":"A DAG's Anatomy","text":"<p>A DAG in Airflow is basically a Python file (.py file) that lives inside the <code>dags</code> folder. It has to be on the web server, and usually, this folder is mapped to the <code>/opt/airflow/dags</code> folder across all servers.</p>"},{"location":"Airflow/1.0.1_A_Dags_Anatomy.html#so-how-do-you-create-a-dag","title":"So, how do you create a DAG?","text":"<p>First you create a <code>blabla.py</code> python file inside the /dags folder. Then, you need to import the <code>DAG</code> object. This is how Airflow knows that your Python file is a DAG:</p> <p></p> <p>Then, you define the DAG itself using a unique name:</p> <p></p> <ul> <li>Unique DAG ID: This name has to be super unique across your entire Airflow setup.</li> <li>Start Date: You need to tell Airflow when to start running this DAG.</li> <li>Schedule Interval: This is where you define how often the DAG should run, usually with a cron expression.</li> <li>catchup=False This prevents Airflow from trying to catch up with all the past runs, which can save you from a lot of unnecessary DAG runs and give you more manual control.</li> </ul> <p>Finally, you would add your tasks under this DAG. To keep it simple we can just add <code>None</code>.</p> <p>So, total code would look like:</p> <pre><code>from airflow import DAG\nfrom datetime import datetime\n\nwith DAG('donald_kim', start_date=datetime(2022,1,1), schedule_interval='@daily', catchup=False) as dag:\n\nNone\n</code></pre>"},{"location":"Airflow/1.0.1_A_Dags_Anatomy.html#weve-created-our-base-lets-create-a-task","title":"We've created our base let's create a task","text":"<p>Now, we will create a task to create a table in Postgress.</p> <p>Firs step would be to import hte postgres operator:</p> <p><code>from airflow.providers.postgres.operators.postgres  import PostgresOperator</code></p> <p>Then we create a table. Inside it we create a task_id. It has to be unique.</p> <p>create_table = PostgresOperator(task_id='create_table', postgres_conn_id='postgres', sql=''' CREATE TABLE IF NOT EXISTS users(firstname text NOT NULL)''')</p>"},{"location":"Airflow/1.0.2_Hello_Airflow.html","title":"Setup Airflow on Docker and create a simple dag","text":"<p>This article serves as your first step into the Airflow fold. Here, I'll walk you through creating a standalone Airflow container with all necessary components, and then guide you in setting up your first DAG.</p> <p>Our envionrment will contain these components:</p> <ol> <li>A Windows laptop</li> <li>Docker Desktop installed</li> <li>Visual Studio Code</li> <li> <p>VS Code Remote Development pack - This lets you connect to the container and develop code locally.</p> <p></p> </li> </ol>"},{"location":"Airflow/1.0.2_Hello_Airflow.html#steps","title":"Steps","text":""},{"location":"Airflow/1.0.2_Hello_Airflow.html#1-create-the-airflow-container","title":"1. Create the Airflow Container","text":""},{"location":"Airflow/1.0.2_Hello_Airflow.html#download-airflow-docker-image","title":"Download Airflow Docker Image","text":"<p>Run the following command in your command prompt or power shell to pull the latest Airflow Docker image: <code>docker pull apache/airflow:latest</code></p>"},{"location":"Airflow/1.0.2_Hello_Airflow.html#create-a-docker-volume","title":"Create a Docker Volume","text":"<p>Execute this command to create a Docker volume named <code>airflow-volume</code> for data persistence: <code>docker volume create airflow-volume</code></p>"},{"location":"Airflow/1.0.2_Hello_Airflow.html#initialize-airflow-database","title":"Initialize Airflow Database","text":"<p>Initialize the Airflow database using the following two commands: <pre><code>docker run --rm --network dasnet -v airflow-volume:/opt/airflow apache/airflow:latest db init\n</code></pre> <pre><code>docker run --rm --network dasnet -v airflow-volume:/opt/airflow apache/airflow:latest users create  --username airflow  --firstname FIRST_NAME  --lastname LAST_NAME   --role Admin   --email admin@example.com   --password airflow\n</code></pre></p> <p>Note: I use a network dasnet. Hence --network part. You can totally remove the --network.</p>"},{"location":"Airflow/1.0.2_Hello_Airflow.html#start-the-airflow-webserver","title":"Start the Airflow Webserver","text":"<p>To start the Airflow webserver, use this command:</p> <pre><code>docker run -d --name airflow --network dasnet -p 8080:8080 -e AIRFLOW_UID=50000 -v airflow-volume:/opt/airflow apache/airflow:latest webserver\n</code></pre> <p>Note: I use a network <code>dasnet</code>, which is why I added the <code>--network</code> part. You can skip the <code>--network</code> if you don\u2019t need it. Also, 8080 is a very common port. If it clashes with any other apps on your laptop, you can change it to something like 8084:8084 or any other random number that might not cause a conflict.</p>"},{"location":"Airflow/1.0.2_Hello_Airflow.html#2-connect-to-your-container-from-vs-code","title":"2. Connect to Your Container from VS Code","text":"<p>After creating the container, connect to it using Visual Studio Code:</p> <ul> <li>Open VS Code and click on the \"Remote Explorer\" icon on the left sidebar.</li> <li>Click on the Remote Development icon (usually in the bottom left corner).</li> <li> <p>Select the Airflow container from the list to connect.</p> <p></p> </li> </ul>"},{"location":"Airflow/1.0.2_Hello_Airflow.html#3-create-our-first-dag-a-hello_worldpy-script","title":"3. Create our first dag - a <code>hello_world.py</code> Script","text":"<p>Now, let\u2019s create your first DAG inside the container:</p> <p>Note: For this example we are using a single-node Airflow container. As there is only one machine(container). The dags folder will be inside the /opt/airflow/dags inside the conatienr. Also, to make it simple we haven't mounted it anywhere on the local system.</p> <ul> <li>In VS Code, go to File &gt; New File.</li> <li>Select Python as the file type.</li> <li>Go to the <code>/opt/airflow/dags</code> directory.</li> <li> <p>Name the file <code>hello_world.py</code>.</p> <p></p> </li> <li> <p>Paste the following code into the file and save it:   ```python   # Contact: das.d@hotmail.com</p> </li> </ul> <p># Import necessary modules from Airflow and Python   from airflow import DAG  # All DAGs(.py files) MUST have this import.   from airflow.operators.python import PythonOperator  # Used to define tasks that run Python functions   from datetime import datetime  # This is always imported as you need to schedule the dag and it needs date and time</p> <p># Define a simple Python function that prints 'Hello World'   # This function will be the task executed by the DAG   def print_hello():       print('Hello World')</p> <p>#This block, the DAG instance dag, is what makes this code an actual DAG.   with DAG(       'hello_world',  # Unique identifier for the DAG; should be descriptive       start_date=datetime(2023, 1, 1),  # The date when the DAG will start running; set to a past date for immediate start       schedule_interval='@daily'  # How often the DAG should run; '@daily' means it runs once a day   ) as dag:</p> <pre><code>  # Define a task using PythonOperator\n  # Any task defined inside the 'with' becomes part of the dag\n  t1 = PythonOperator(\n      task_id='print_hello',  # 'task_id' is a unique identifier for the task within the DAG\n      python_callable=print_hello  # 'python_callable' is the function that the task will execute. The function to be executed when this task runs\n  )\n</code></pre> <p># The DAG and task are now defined   # Airflow will take care of scheduling and running the task based on the DAG's schedule ```</p>"},{"location":"Airflow/1.0.2_Hello_Airflow.html#4-start-the-airflow-scheduler","title":"4. Start the Airflow Scheduler","text":"<p>To run your DAG, you need to start the Airflow scheduler:</p> <ul> <li>Open the terminal in VS Code (which is already connected to the container).</li> <li>Run the following command:</li> </ul> <p><pre><code>airflow scheduler\n</code></pre> - The scheduler will start, and it will pick up your <code>hello_world</code> DAG to run it according to the schedule you've set (daily in this case).</p> <pre><code>&lt;img src=\"images/2024-08-29-00-18-11.png\" alt=\"Description of the image\" style=\"max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\"&gt;\n</code></pre>"},{"location":"Azure/Cloud_Accounts/ServicePrincipalAndManagedIdentity.html","title":"Service Principal and Managed Identity","text":""},{"location":"Azure/Cloud_Accounts/ServicePrincipalAndManagedIdentity.html#understanding-azure-service-principals-and-managed-identities","title":"Understanding Azure Service Principals and Managed Identities","text":"<p>In this article, I will explain the difference between a Service Principal and a Managed Identity in Azure, using comparision with how it was done in old days with on-premises Active Directory (AD).</p>"},{"location":"Azure/Cloud_Accounts/ServicePrincipalAndManagedIdentity.html#service-principal-like-a-service-account-in-on-prem-ad","title":"Service Principal: Like a Service Account in On-Prem AD","text":"<p>In old days: Remember when we used to create dedicated service accounts (also called Functional IDs) for running Windows services like SQL Server? Instead of using a regular user account, we'd create these special accounts so the service wasn't tied to any particular person and could be used across multiple servers. And we all experienced those dreaded scenarios when the service account password expired and suddenly all the Windows services running under that account went down across the environment!</p> <p>In Azure, this is a Service Principal: It's the same concept - a dedicated identity for an application or service, but in the cloud. You manually create it, manage its credentials, and control its lifecycle just like your AD service accounts.</p>"},{"location":"Azure/Cloud_Accounts/ServicePrincipalAndManagedIdentity.html#managed-identity-like-a-service-account","title":"Managed Identity: Like a Service Account++","text":"<p>In old days when a new server was added to the domain, the computer used to get a computer account, don't confuse it with the hostname. That computer account has an identity but you never see or manage its password - it's handled automatically in the background. They used to appear in AD as a computer console. They would have names like <code>SERVER01$</code>. They had passwords which were never seen by anyone and the computer itself handleded the authentication process using these credentials in the domain. You could reset the computer account if needed, which forced the computer to re-establish trust. But, never the password.</p> <p>In Azure, this is a Managed Identity, the diference beeing its not just for computers but also for applications. Managed identities are automatically created and managed by Azure, so you don't have to worry about creating, storing, or rotating credentials. They are tied to the Azure resource (like an App Service or VM) and can be given permission to access downstream resources, e.g. Azure SQL Database, without any passwords.</p>"},{"location":"Azure/Cloud_Accounts/ServicePrincipalAndManagedIdentity.html#real-world-examples","title":"Real-World Examples","text":""},{"location":"Azure/Cloud_Accounts/ServicePrincipalAndManagedIdentity.html#example-1-web-app-accessing-azure-sql-database","title":"Example 1: Web App Accessing Azure SQL Database","text":"<p>Let's say you have a .NET web application running in Azure App Service that needs to securely connect to an Azure SQL Database to retrieve customer data.</p>"},{"location":"Azure/Cloud_Accounts/ServicePrincipalAndManagedIdentity.html#option-1-using-a-service-principal","title":"Option 1: Using a Service Principal","text":"<p>This is like the old days when you created service accounts for applications:</p> <ol> <li>You create an application registration in Azure AD (creating a service principal)</li> <li>You generate a client secret (essentially a password)</li> <li>You store this secret in your App Service's configuration settings:    <pre><code>CONNECTION_STRING=\"Server=myserver.database.windows.net;User ID=app_client_id;Password=app_client_secret;\"\n</code></pre></li> <li>You assign permissions for this service principal to access the SQL Database</li> <li>Your application code uses this connection string to authenticate</li> <li>Every 90 days, you must rotate this secret manually and update your configuration</li> </ol>"},{"location":"Azure/Cloud_Accounts/ServicePrincipalAndManagedIdentity.html#option-2-using-a-managed-identity-modern-approach","title":"Option 2: Using a Managed Identity (Modern Approach)","text":"<p>This simplifies everything:</p> <ol> <li>Go to your App Service in Azure Portal, click on \"Identity\" in the left menu</li> <li>Toggle \"System assigned\" to \"On\" and save</li> <li>Go to your SQL Database and grant permissions to this managed identity</li> <li>Modify your application code to use managed identity authentication:    <pre><code>// Connection string has no secrets\nstring connectionString = \"Server=myserver.database.windows.net;Database=mydb;\";\n\n// The DefaultAzureCredential automatically uses the App Service's managed identity\nvar conn = new SqlConnection(connectionString);\nconn.AccessToken = (await new DefaultAzureCredential().GetTokenAsync(\n    new TokenRequestContext(new[] { \"https://database.windows.net/.default\" }))).Token;\nawait conn.OpenAsync();\n</code></pre></li> <li>That's it! No secrets to manage, store, or rotate</li> </ol> <p>Real-world benefit: When a developer leaves the company, there's no risk of them taking credentials with them because there are no credentials to take.</p>"},{"location":"Azure/Cloud_Accounts/ServicePrincipalAndManagedIdentity.html#example-2-accessing-azure-storage-blobs","title":"Example 2: Accessing Azure Storage Blobs","text":"<p>Let's say your App Service needs to read files from Azure Blob Storage:</p>"},{"location":"Azure/Cloud_Accounts/ServicePrincipalAndManagedIdentity.html#service-principal-approach-little-older-way","title":"Service Principal Approach (Little older way)","text":"<pre><code>// Store these credentials in app settings\nstring accountName = \"mystorageaccount\";\nstring accountKey = \"fj39f93jf93jf93j93jf93==\"; // Secret that must be managed\n\nvar storageAccount = new CloudStorageAccount(\n    new StorageCredentials(accountName, accountKey), \n    true);\n</code></pre>"},{"location":"Azure/Cloud_Accounts/ServicePrincipalAndManagedIdentity.html#managed-identity-approach-modern-way","title":"Managed Identity Approach (Modern Way)","text":"<p><pre><code>// No secrets needed\nvar blobServiceClient = new BlobServiceClient(\n    new Uri(\"https://mystorageaccount.blob.core.windows.net\"),\n    new DefaultAzureCredential());\n</code></pre> The managed identity is automatically used behind the scenes, with Azure handling all the authentication transparently. No passwords or secrets to manage, store, or rotate.</p>"},{"location":"Azure/Cloud_Accounts/ServicePrincipalAndManagedIdentity.html#conclusion","title":"Conclusion","text":"<p>Service Principals are like old-days Service accounts/Functional IDs. Managed Identities are like computer accounts in Active Directory, but for all computer resources (VMs, App Services, etc.).</p>"},{"location":"DE-Projects/Csv-To-MSSQL.html","title":"CSV to MSSQL","text":""},{"location":"DE-Projects/Csv-To-MSSQL.html#build-an-mssql-table-from-csv-schema-split-large-csvs-and-populate-rows-using-pandas","title":"Build an MSSQL table from CSV schema, split large CSVs, and populate rows using Pandas.","text":"<p>How to use python to create a MSSQL table from the schema of a csv file. Split the large csv file into manageable smaller chunks and upload the data from these segments into the SQL server table. The code is divided into three main parts:</p>"},{"location":"DE-Projects/Csv-To-MSSQL.html#part-1-creating-an-mssql-table-from-a-csv-schema","title":"Part 1: Creating an MSSQL Table from a CSV Schema","text":"<ol> <li>Establish a connection to an MSSQL Server using specified connection details.</li> <li>Read a large CSV file into a pandas DataFrame.</li> <li>Define a mapping of pandas data types to SQL Server data types.</li> <li>Create a list of column definitions with data types based on DataFrame's columns.</li> <li>Generate an SQL statement to create a new table in the MSSQL database using the column definitions.</li> <li>Execute the SQL statement to create the table.</li> </ol> <pre><code># Import necessary libraries\nimport pyodbc\nimport pandas as pd\n\n# Establish a connection to the SQL Server (replace placeholders with your server details)\nconn = pyodbc.connect('DRIVER={Your_ODBC_Driver};SERVER=Your_Server;DATABASE=Your_Database;UID=Your_Username;PWD=Your_Password')\n\n#Example: conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=localhost;DATABASE=Nyctaxi;UID=sa;PWD=Passw_r123')\n\n# Specify the path to your large CSV file\nlarge_csv_file = 'path_to_your_large_csv_file.csv'\n\n# Read the CSV file into a pandas DataFrame\ndf = pd.read_csv(large_csv_file)\n\n# Define a mapping of pandas data types to SQL Server data types\ndata_type_mapping = {\n    'int64': 'BIGINT',\n    'float64': 'FLOAT',\n    'object': 'VARCHAR(MAX)',  # Use VARCHAR(MAX) for string data\n    # Add more mappings as needed for other data types\n}\n\n# Create a list of column definitions with data types\ncolumn_definitions = [f'{col} {data_type_mapping[str(df[col].dtype)]}' for col in df.columns]\n\n# Create a SQL statement to create the table\ncreate_table_sql = f'''\n    CREATE TABLE YourTableName (\n        {', '.join(column_definitions)}\n    )\n'''\n\n# Execute the SQL statement to create the table\ncursor = conn.cursor()\ncursor.execute(create_table_sql)\ncursor.commit()\n\n# Close the database connection\nconn.close()\n</code></pre>"},{"location":"DE-Projects/Csv-To-MSSQL.html#part-2-splitting-the-large-csv-into-smaller-chunks","title":"Part 2: Splitting the Large CSV into Smaller Chunks","text":"<ol> <li>Define the desired number of rows per chunk to manage data processing.</li> <li>Read the large CSV file in chunks of the specified size.</li> <li>Specify a directory to save the CSV chunks.</li> <li>Iterate through the chunks and save them as separate CSV files in the directory.</li> </ol> <pre><code># Define the desired number of rows per chunk\nrows_per_chunk = 10000  # Adjust this number as needed\n\n# Read the large CSV file in chunks\nchunk_size = rows_per_chunk\nchunks = pd.read_csv(large_csv_file, chunksize=chunk_size)\n\n# Directory to save the CSV chunks\ncsv_chunks_directory = 'Path to the folder where split files will be placed'\n\n#Remember, for windows use C:/Users/rocky [forward slash] or C:\\\\Users\\\\rocky [double back slash]\n\n# Iterate through the chunks and save them as separate CSV files\nchunk_number = 0\nfor chunk in chunks:\n    chunk_number += 1\n    chunk.to_csv(f'{csv_chunks_directory}/chunk_{chunk_number}.csv', index=False)\n\nprint(f'Split into {chunk_number} chunks.')\n</code></pre>"},{"location":"DE-Projects/Csv-To-MSSQL.html#part-3-importing-data-from-split-files-with-error-handling","title":"Part 3: Importing Data from Split Files with Error Handling","text":"<ol> <li>Initialize a new database connection (separate from Part 1) as we have closed the previous connection.</li> <li>Define the directory where CSV chunks from Part 2 are located.</li> <li>Define a file to log rows with errors.</li> <li>Iterate through the CSV chunks in the directory.</li> <li>Read CSV data into a pandas DataFrame for each chunk.</li> <li>Prepare an SQL INSERT statement for the database table created in Part 1.</li> <li>Create a cursor for database operations.</li> <li>Iterate through the rows in the DataFrame and attempt to insert each row into the SQL Server table.</li> <li>Incorporate error handling to log errors during insertion and continue with the next row.</li> <li>Commit the transaction after processing each chunk of data.</li> <li>Close the error log file and the database connection.</li> </ol> <pre><code>import os\nimport pandas as pd\nimport pyodbc\n\n# Initialize your database connection. Refer to part one for details.\nconn = pyodbc.connect(\"your_connection_string_here\")\n\n#Example, #Example: conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=localhost;DATABASE=Nyctaxi;UID=sa;PWD=Passw_r123')\n\n# Define the directory where CSV chunks are located\ncsv_chunks_directory = \"your_csv_directory_here\"\n\n# Define a file to log rows with errors\nerror_log_file = \"error_log.txt\"\n\n# Open the error log file in append mode\nwith open(error_log_file, 'a') as error_log:\n    # Iterate through the CSV chunks and load them into the SQL Server table\n    for filename in os.listdir(csv_chunks_directory):\n        if filename.endswith(\".csv\"):\n            csv_file_path = os.path.join(csv_chunks_directory, filename)\n\n            # Read the CSV chunk into a pandas DataFrame\n            df = pd.read_csv(csv_file_path)\n\n            # Prepare an SQL INSERT statement\n            insert_sql = f\"INSERT INTO dbo.yellowtaxitrips ({', '.join(df.columns)}) VALUES ({', '.join(['?']*len(df.columns))})\"\n\n            # Create a cursor\n            cursor = conn.cursor()\n\n            # Iterate through the rows and insert them into the SQL Server table\n            for _, row in df.iterrows():\n                try:\n                    cursor.execute(insert_sql, tuple(row))\n                except pyodbc.Error as e:\n                    # Log the error and the problematic row to the error log file\n                    error_log.write(f\"Error: {e}\\n\")\n                    error_log.write(f\"Problematic Row: {row}\\n\")\n                    error_log.write(\"\\n\")  # Add a separator for readability\n                    continue  # Skip this row and continue with the next one\n\n            # Commit the transaction\n            conn.commit()\n\n    # Close the error log file\n    error_log.close()\n\n# Close the database connection\nconn.close()\n</code></pre>"},{"location":"DE-Projects/Csv-To-MSSQL.html#conclusion","title":"Conclusion","text":"<p>The script is effective for analyzing large datasets and predicting column types. However, for large files, it's not recommended to use it for splitting and importing. Instead, SSIS would be a better choice. SQL Server Bulk Import is the fastest method for well-defined files. Although SSIS is notably fast, the SQL Server Import and Export Data Wizard (an SSIS tool) crashes with large csv files(1 GB). It's advisable to use the SSIS studio in Visual Studio with the mainstream version.</p>"},{"location":"DE-Projects/CurrencyPredictor.html","title":"Currency Predictor","text":""},{"location":"DE-Projects/CurrencyPredictor.html#my-approach","title":"My Approach","text":""},{"location":"DE-Projects/CurrencyPredictor.html#1-data-collection","title":"1. Data Collection","text":"<ul> <li>Source Forex Data: I will use one of the free Forex APIs mentioned earlier (like Alpha Vantage, CurrencyLayer, or Free Forex API) to collect real-time and historical currency exchange rate data.</li> <li>Additional Data: I\u2019ll consider integrating other data sources like economic indicators (e.g., interest rates, inflation data), news feeds (using APIs like NewsAPI), and social sentiment (from Twitter API).</li> </ul>"},{"location":"DE-Projects/CurrencyPredictor.html#2-data-processing","title":"2. Data Processing","text":"<ul> <li>Stream Data to Kafka: I\u2019ll set up a Kafka pipeline to stream live Forex data into my system, ensuring that the data is continuously updated.</li> <li>Data Storage: I\u2019ll use a database like PostgreSQL or a time-series database like InfluxDB to store historical data for analysis.</li> </ul>"},{"location":"DE-Projects/CurrencyPredictor.html#3-ai-model-development","title":"3. AI Model Development","text":"<ul> <li>Feature Engineering: I\u2019ll extract useful features from the data, such as currency pair volatility, moving averages, or sentiment scores.</li> <li>Model Selection: I plan to use machine learning models like Random Forests, Gradient Boosting, or LSTM (Long Short-Term Memory) networks for predicting currency price trends.</li> <li>Training the Model: I\u2019ll use historical data to train my AI models to predict future currency movements, possibly using platforms like Azure Machine Learning or TensorFlow.</li> <li>Model Evaluation: I\u2019ll regularly evaluate the model\u2019s performance using metrics like accuracy, precision, and recall.</li> </ul>"},{"location":"DE-Projects/CurrencyPredictor.html#4-ai-powered-suggestions","title":"4. AI-Powered Suggestions","text":"<ul> <li>Decision Logic: Based on the model\u2019s predictions, I\u2019ll develop logic that suggests which currency to buy. This could be as simple as recommending currencies predicted to appreciate or a more complex strategy considering multiple factors.</li> <li>User Input: I\u2019ll allow users to input preferences or constraints (e.g., risk tolerance, preferred currency pairs).</li> </ul>"},{"location":"DE-Projects/CurrencyPredictor.html#5-dashboard-development","title":"5. Dashboard Development","text":"<ul> <li>Visualization: I\u2019ll use tools like Tableau, Power BI, or Grafana to create interactive visualizations of Forex trends, AI predictions, and suggested trades.</li> <li>Integration: I\u2019ll integrate the AI model\u2019s output into the dashboard to provide real-time trading recommendations.</li> <li>User Interface: I\u2019ll ensure the dashboard is user-friendly, displaying key metrics like predicted price changes, confidence levels, and suggested trades clearly.</li> </ul>"},{"location":"DE-Projects/CurrencyPredictor.html#6-deployment","title":"6. Deployment","text":"<ul> <li>Web Hosting: I\u2019ll host the dashboard on a cloud platform like Azure, AWS, or Google Cloud.</li> <li>Monitoring: I\u2019ll implement monitoring for both the data pipeline and AI models to ensure everything runs smoothly and the predictions remain accurate.</li> </ul>"},{"location":"DE-Projects/CurrencyPredictor.html#7-continuous-improvement","title":"7. Continuous Improvement","text":"<ul> <li>Feedback Loop: I\u2019ll collect user feedback and actual market outcomes to continuously improve the AI model.</li> <li>Model Retraining: I\u2019ll regularly retrain the model with new data to keep it up-to-date with the latest market conditions.</li> </ul> <p>In short: I\u2019ll be streaming real-time Forex data, processing it with AI to predict currency movements, and presenting these insights in a user-friendly dashboard that suggests which currencies to buy based on the model\u2019s predictions.</p>"},{"location":"DE-Projects/Dbrk-E2E-AttritionProject.html","title":"Problem Statement","text":"<p>Age analysis - people leaving at what age Department analysis Marital Status Attrition by edution Attrition by envionrment Other kpi</p>"},{"location":"DE-Projects/Download-Haddop-Jars.html","title":"Hadoop Jars","text":""},{"location":"DE-Projects/Download-Haddop-Jars.html#how-to-download-jar-files-from-apache-maven-repository","title":"How to Download JAR Files from Apache Maven Repository","text":"<p>Need to download JAR files like <code>hadoop-azure-x.x.x.jar</code> for your project? Don't worry, I'll show you how to get them from Apache Maven Repository. It's easy and doesn't take much time.</p>"},{"location":"DE-Projects/Download-Haddop-Jars.html#steps-to-download-jar-files","title":"Steps to Download JAR Files","text":""},{"location":"DE-Projects/Download-Haddop-Jars.html#go-to-apache-maven-repository","title":"Go to Apache Maven Repository","text":"<p>Open your browser and visit mvnrepository.com. This website has a lot of JAR files.</p>"},{"location":"DE-Projects/Download-Haddop-Jars.html#search-for-the-jar-file","title":"Search for the JAR File","text":"<p>In the website, there will be a search box. Type the name of the JAR file you are looking for (like <code>hadoop-azure</code>) and press enter. You will see a list of JAR files.</p> <p></p>"},{"location":"DE-Projects/Download-Haddop-Jars.html#pick-the-version-you-want","title":"Pick the Version You Want","text":"<p>After you click on the JAR file name, you'll see different versions. Click on the version you need.</p> <p></p>"},{"location":"DE-Projects/Download-Haddop-Jars.html#download-the-jar-file","title":"Download the JAR File","text":"<p>In the page for your chosen version, look for the \"Files\" section. There will be a link for a <code>.jar</code> file. Click on this link to start downloading the file.    </p>"},{"location":"DE-Projects/Download-Haddop-Jars.html#how-to-use-the-jars","title":"How to Use the JARs","text":"<p>Once downloaded, you can store the JAR files in a location of your choice, for example, <code>C:\\spark_jars\\</code>. You can reference these JARs in your code when needed. For instance, if you are trying to access Azure Data Lake Storage (ADLS) from a local Spark installation on a Windows machine, you can set up your Spark session like this:</p>"},{"location":"DE-Projects/Download-Haddop-Jars.html#from-pysparksql-import-sparksession-spark-sparksessionbuilder-appnameadls-access-configsparkjars-cspark_jarshadoop-azure-333jarcspark_jarshadoop-azure-datalake-333jarcspark_jarshadoop-common-333jar-getorcreate","title":"<pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"ADLS Access\") \\\n    .config(\"spark.jars\", \"C:\\\\spark_jars\\\\hadoop-azure-3.3.3.jar;C:\\\\spark_jars\\\\hadoop-azure-datalake-3.3.3.jar;C:\\\\spark_jars\\\\hadoop-common-3.3.3.jar\") \\\n    .getOrCreate()\n</code></pre>","text":"<p>\u00a9 D Das \ud83d\udce7 das.d@hotmail.com |</p>"},{"location":"DE-Projects/FetchJsonWriteParquet.html","title":"To Parquet","text":""},{"location":"DE-Projects/FetchJsonWriteParquet.html#json-transformation-using-spark-and-azure","title":"Json Transformation Using Spark And Azure","text":"<p>In this article, I'll guide you on how to retrieve JSON data from an online API using Spark and Hadoop tools. We'll learn to split the data and save it in a 'silver' level. Then, we'll take the data from 'silver', partition it again, and store it in a *'gold'* level as Parquet files. This process is in line with the lakehouse architecture standards too. </p>"},{"location":"DE-Projects/FetchJsonWriteParquet.html#prerequisites","title":"Prerequisites","text":"<p>You'll need the following:</p> <ul> <li>Azure Subscription: For using Azure services.</li> <li>Azure Data Lake Storage Gen2: Two contaienrs, silver and gold inside a storage account.</li> <li>Python with Pyspark : To develop and test this code a simple pyspark environment will do.</li> <li>Hadoop ADLS Jars: You need to download the jars using tools like wget and store it and then refrence it in spark configuration. To know more refer to my article.</li> </ul>"},{"location":"DE-Projects/FetchJsonWriteParquet.html#script-breakdown","title":"Script Breakdown","text":""},{"location":"DE-Projects/FetchJsonWriteParquet.html#initialize-spark-session","title":"Initialize Spark Session","text":"<p>The script starts by creating a spark session by passing the downloaded hadoop jars for ADLS. The authentication method used is OAuth with service princiapl. Note, the registered app should have storage blob contributor role at the storage account level(preferred) or individually for silver and bronze. See appendix below.</p> <pre><code># Author: Das, Purpose: Fetch and Partition Jsons\nfrom pyspark.sql import SparkSession\nimport requests\n\n# Note: Chek your actual client ID, tenant ID, and client secret value. Your's will be diffferent\nstorage_account_name = \"TheStorageAccountNameHoldingSilverAndGoldContainers\"\nregapp_client_id = \"Registered App's Client_ID\"\nregapp_directory_id = \"Registered App's Directory ID\"\nregapp_client_secret = \"Registered App's Client Secret Value\"\n\n# Initialize Spark session with Azure Data Lake Storage Gen2 configurations for OAuth Authentication using service princiapl\nspark = SparkSession.builder \\\n    .appName(\"ADLS Access\") \\\n    .config(\"spark.jars\", \n             \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,\"\\\n             \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,\"\\\n             \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar\") \\\n    .config(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\") \\\n    .config(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") \\\n    .config(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", regapp_client_id) \\\n    .config(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", regapp_client_secret) \\\n    .config(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{regapp_directory_id}/oauth2/token\") \\\n    .getOrCreate()\n</code></pre>"},{"location":"DE-Projects/FetchJsonWriteParquet.html#fetch-and-load-json-data","title":"Fetch and Load JSON Data","text":"<p>The next snippet fetches JSON data from a web API using the <code>requests</code> library and loads it into a DataFrame:</p> <pre><code>import requests\n\n# URL of the JSON data source\njson_url = 'https://data.ct.gov/resource/5mzw-sjtu.json'\nresponse = requests.get(json_url)\njson_data = response.json()\n\n# Convert JSON to a DataFrame\ndf = spark.createDataFrame(json_data)\n</code></pre>"},{"location":"DE-Projects/FetchJsonWriteParquet.html#define-paths-and-partition-data","title":"Define Paths and Partition Data","text":"<p>It's crucial to define the storage paths for 'silver' and 'gold' layers. Here's how the script partitions data:</p> <pre><code># Define base paths for silver and gold storage layers\nsilver_path = f\"abfss://{storage_account_name}.dfs.core.windows.net/silver\"\ngold_path = f\"abfss://{storage_account_name}.dfs.core.windows.net/gold\"\n\n# Example column to partition the data by\npartition_column = 'date_column'\n\n# Write the data to the silver layer, partitioned by 'partition_column'\ndf.write.partitionBy(partition_column).mode(\"overwrite\").json(silver_path)\n\n# Read and rename the partition column for clarity\ndf_silver = spark.read.json(silver_path).withColumnRenamed(partition_column, \"date_partition\")\n\n# Write the transformed data to the gold layer as Parquet\ndf_silver.write.partitionBy(\"date_partition\").mode(\"overwrite\").parquet(gold_path)\n</code></pre>"},{"location":"DE-Projects/FetchJsonWriteParquet.html#stop-spark-session","title":"Stop Spark Session","text":"<p>Once all operations are complete, terminate the Spark session:</p> <pre><code>spark.stop()\n</code></pre>"},{"location":"DE-Projects/FetchJsonWriteParquet.html#appendix","title":"Appendix","text":""},{"location":"DE-Projects/FetchJsonWriteParquet.html#reason-for-partitioning","title":"Reason for Partitioning","text":"<p>Partitioning is crucial when dealing with large datasets. It helps in breaking the data into smaller, more manageable pieces, which can be processed faster and more efficiently. In this script, I have partitoned the data by date_column. However, you can partiton it further by other criteria.</p>"},{"location":"DE-Projects/FetchJsonWriteParquet.html#register-app-and-assign-blob-contributor-role","title":"Register App and Assign Blob Contributor Role","text":"<p> Follow these merged steps to register your app in Azure AD and give it access to the silver and gold containers: </p> <ul> <li>Go to Azure Portal \u2192 Azure Active Directory \u2192 App registrations, and create a new registration. Note your Application (client) ID and Directory (tenant) ID.</li> <li>Within the app, navigate to Certificates &amp; secrets to generate a new client secret. Remember to save the client secret value securely.</li> <li>In your Azure Storage account, under Access Control (IAM), add a role assignment. Select \"Storage Blob Data Contributor\" and assign it to your registered app using the Application (client) ID.</li> </ul>"},{"location":"DE-Projects/FetchJsonWriteParquet.html#the-complete-script","title":"The complete script","text":"<p>You can use this script directly in a jupyter notebook or create a function with it. For higher loads the script can be run in spark cluster.</p> <pre><code>from pyspark.sql import SparkSession\nimport requests\n\n# Note: Chek your actual client ID, tenant ID, and client secret value. Your's will be diffferent\nstorage_account_name = \"&lt;input yours here&gt;\"\nregapp_client_id = \"&lt;input yours here&gt;\"\nregapp_directory_id = \"&lt;input yours here&gt;\"\nregapp_client_secret = \"&lt;input yours here&gt;\"\n\n# Initialize Spark session with Azure Data Lake Storage Gen2 configurations for OAuth Authentication using service princiapl\nspark = SparkSession.builder \\\n    .appName(\"ADLS Access\") \\\n    .config(\"spark.jars\", \n             \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,\"\\\n             \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,\"\\\n             \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar\") \\\n    .config(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\") \\\n    .config(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") \\\n    .config(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", regapp_client_id) \\\n    .config(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", regapp_client_secret) \\\n    .config(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{regapp_directory_id}/oauth2/token\") \\\n    .getOrCreate()\n\n# Azure Storage configuration\nsilver_container_name = \"silver\"\ngold_container_name = \"gold\"\n\n# Download JSON data from the web\njson_url = 'https://data.ct.gov/resource/5mzw-sjtu.json'\nresponse = requests.get(json_url)\njson_data = response.json()\n# Assuming json_data is a list of dictionaries (records)\n# Create a DataFrame from the JSON data\ndf = spark.createDataFrame(json_data)\n\n# Define the base path for silver and gold containers\nbase_silver_path = f\"abfss://{silver_container_name}@{storage_account_name}.dfs.core.windows.net/\"\nbase_gold_path = f\"abfss://{gold_container_name}@{storage_account_name}.dfs.core.windows.net/\"\n\n# Add the relative path and partition the data by the 'date_column'\n# Replace 'date_column' with the actual column name you want to partition on\npartition_column = 'daterecorded'  # Example partition column. You can choose another column in your case.\nsilver_path = f\"{base_silver_path}data/\"\ngold_path = f\"{base_gold_path}data/\"\n\n# Write JSON data to the silver container, partitioned by 'date_column'\ndf.write.partitionBy(partition_column).mode(\"overwrite\").json(silver_path)\n\n# Read the JSON file back into a DataFrame, it will recognize partitions automatically\ndf_silver = spark.read.json(f\"{silver_path}\").withColumnRenamed(\"daterecorded\", \"date_partition\")\n\n# Write the DataFrame to the gold container as a Parquet file, also partitioned\ndf_silver.write.partitionBy(\"date_partition\").mode(\"overwrite\").parquet(gold_path)\n\n# Stop the Spark session\nspark.stop()\n</code></pre>"},{"location":"DE-Projects/FetchJsonWriteParquet.html#conclusion","title":"Conclusion","text":"<p> This is a versatile script that can be executed directly in a Jupyter notebook or encapsulated within a function for convenience. With minor configuration tweaks it can be run in a spark cluster to handle larger workloads. Note: with this script you might see warnings like *Warning: Ignoring non-Spark config property: fs.azure.account.oauth.provider.type.strgacweatherapp.dfs.core.windows.net* </p>"},{"location":"DE-Projects/InstallScala.html","title":"Installing Scala on Windows <p>Scala can be installed on Windows using Coursier tool. It's basically a command line tool which opens up when you click on the scala installer exe. Follow these steps to install scala on Windows. </p>","text":""},{"location":"DE-Projects/InstallScala.html#download-and-install-scala","title":"Download and Install Scala <ul> <li>Download the .zip file and open it.</li> <li>Run the <code>cs-x86_64-pc-win32.exe</code> file.</li> <li>A Command Prompt window will open. When prompted, press <code>Y</code> to proceed.</li> </ul>","text":""},{"location":"DE-Projects/InstallScala.html#set-up-environment-variables","title":"Set Up Environment Variables <p> The installation might not automatically set `SCALA_HOME` and add the `\\bin` folder to your PATH. This setup is necessary for Scala to work properly. </p> <p>After the installation, you need to manually add <code>SCALA_HOME</code> and <code>SCALA_HOME\\data\\bin</code> to your system's PATH.</p> <p></p>  <p>Note: Scala's bin should be in the path. Else, you won't be able to run it from command prompt.</p>  <p></p>","text":""},{"location":"DE-Projects/JsonFlatAzureSDK.html","title":"Azure SDK","text":""},{"location":"DE-Projects/JsonFlatAzureSDK.html#table-of-contents","title":"Table of Contents","text":""},{"location":"DE-Projects/JsonFlatAzureSDK.html#flatten-json-files-in-azure-blob-storage-using-azure-sdk-for-python","title":"Flatten JSON Files in Azure Blob Storage using Azure SDK for Python","text":""},{"location":"DE-Projects/JsonFlatAzureSDK.html#background","title":"Background","text":"<p>A ADLS container has many JSON files with nested structure. This article shows how to flatten those Json files for better handling later.</p>"},{"location":"DE-Projects/JsonFlatAzureSDK.html#prerequisites","title":"Prerequisites","text":"<ul> <li>A python-development ready VS Code environment</li> <li>Install the required Azure SDK(libs) to work with Azure storage.</li> </ul> <p><pre><code>pip install azure-storage-blob\n</code></pre> - A source container with the JSON files. - A destination container where the flattened json will be kept. The script can either replace the original files with the processed ones or store them in a separate folder. - Storage account keys. This code uses Account key method to authenticate. You can get the keys using these steps:   - Open the Storage Accounts containing the container. Go to Settings,  Access keys.   - Copy and keep the Connection string from here.</p>"},{"location":"DE-Projects/JsonFlatAzureSDK.html#create-the-script","title":"Create the script","text":"<p>To create the Python code in Visual Studio Code follow these steps:</p> <ul> <li>Create a new file with the <code>.py</code> extension.</li> <li>Import the necessary libraries:</li> </ul> <pre><code>import json\nfrom azure.storage.blob import BlobServiceClient\nimport logging\n</code></pre> <ul> <li>Define the <code>flatten_json()</code> function:</li> </ul> <pre><code>def flatten_json(y, parent_key='', sep='_'):\n  items = []\n  for k, v in y.items():\n    new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n    if isinstance(v, dict):\n      items.extend(flatten_json(v, new_key, sep=sep).items())\n    else:\n      items.append((new_key, v))\n  return dict(items)\n</code></pre> <ul> <li>Define the <code>main()</code> function:</li> </ul> <pre><code>def main():\n  # Initialize Blob Service Client\n  blob_service_client = BlobServiceClient.from_connection_string(\n    \"DefaultEndpointsProtocol=https;AccountName=&lt;Your_Storage_Account_Name&gt;;AccountKey=&lt;The_Storage_Act_Key&gt;;EndpointSuffix=core.windows.net\")\n\n  # Iterate over blobs in the \"source_container\" container\n  container_client = blob_service_client.get_container_client(\"source_container\")\n  for blob in container_client.list_blobs():\n    try:\n      # Download the blob\n      blob_client = blob_service_client.get_blob_client(container=\"dest_container\", blob=blob.name)\n      data_str = blob_client.download_blob().readall().decode('utf-8')\n\n      # Decode the blob data to a string\n      data = json.loads(data_str)\n\n      # Flatten the JSON data\n      flattened_data = flatten_json(data)\n\n      # Move blob to the \"silver\" container\n      target_blob_client = blob_service_client.get_blob_client(container=\"dest_container\", blob=blob.name)\n      target_blob_client.upload_blob(json.dumps(flattened_data), overwrite=True)\n\n      # Delete the original blob (optional)\n      # blob_client.delete_blob()\n    except Exception as e:\n      logging.error(f\"Error processing blob {blob.name}: {e}\")\n\nif __name__ == \"__main__\":\n  main()\n</code></pre> <ul> <li>Save the file.</li> <li>Press <code>F5</code> to run the code.</li> </ul>"},{"location":"DE-Projects/JsonFlatAzureSDK.html#explanation-of-key-elements-in-the-script","title":"Explanation of key elements in the script","text":"<p>Here is what the scirpt does. This will help you understand how Azure SDK for Blob Stroage works:</p> <ol> <li>Initializes a BlobServiceClient object using the <code>from_connection_string()</code> method. This object is used to interact with the Azure Blob Storage service.</li> <li>Gets a container client for the <code>silver</code> container using the <code>get_container_client()</code> method. This object is used to interact with the specified container.</li> <li>Iterates over all blobs in the container using the <code>list_blobs()</code> method.</li> <li>For each blob, the function does the following:<ul> <li>Downloads the blob using the <code>get_blob_client()</code> and <code>download_blob()</code> methods.</li> <li>Decodes the blob data to a string using the <code>decode()</code> method.</li> <li>Parses the JSON data in the string using the <code>json.loads()</code> function.</li> <li>Flattens the JSON data using the <code>flatten_json()</code> function that we provided.</li> <li>Moves the blob to the <code>silver</code> container using the <code>upload_blob()</code> method.</li> <li>Deletes the original blob (optional).</li> </ul> </li> </ol>"},{"location":"DE-Projects/JsonFlatAzureSDK.html#appendix","title":"Appendix","text":""},{"location":"DE-Projects/JsonFlatAzureSDK.html#the-complete-script","title":"The complete script","text":"<p>Here is the complete script in one piece:</p> <pre><code>\"\"\"\nAuthor: Das\nLTS: Very easy to run this code. Just pip install azure-storage-blob. And run the code anywhere with python.\n    Fully working code anywhere. Windows/Ubuntu.\n---\nThis script uses Azure SDK BlobServiceClient class to interact with Azure Blob Storage. It downloads blobs from a container, \nflattens their JSON content, and uploads them back to another container. \n\nThe BlobServiceClient class is part of the Azure SDK for Python and is used to interact with Azure Blob Storage. \n    It provides methods for getting clients for specific containers and blobs, downloading blobs, and uploading blobs.\n    The from_connection_string method is used to create an instance of BlobServiceClient using a connection string.\n    The download_blob method returns a stream of data that can be read by calling readall.\n    The upload_blob method uploads data to a blob, overwriting it if it already exists.\n\"\"\"\n\nimport json\nfrom azure.storage.blob import BlobServiceClient # BlobServiceClient is part of the Azure SDK for Python. It's used to interact with Azure Blob Storage.\nimport logging\n\n# Function to flatten JSON objects\ndef flatten_json(y, parent_key='', sep='_'):\n    items = []\n    for k, v in y.items():\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten_json(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n# Main function\ndef main():\n    # Initialize Blob Service Client using connection string. This is the main entry point for interacting with blobs in Azure Storage.\n    blob_service_client = BlobServiceClient.from_connection_string(\"DefaultEndpointsProtocol=https;AccountName=&lt;your-storage-account-Name&gt;;AccountKey=&lt;Your_Storage_Act_Con_String&gt;;EndpointSuffix=core.windows.net\")\n\n    # Get a client for the \"silver\" container in the Blob Service. This client provides operations to interact with a specific container.\n    container_client = blob_service_client.get_container_client(\"silver\")\n    # Iterate over blobs in the \"weather-http\" container\n    for blob in container_client.list_blobs():\n        try:\n            # Get a client for the current blob. This client provides operations to interact with a specific blob.\n            blob_client = blob_service_client.get_blob_client(container=\"silver\", blob=blob.name)\n            # Download the blob data and decode it from bytes to string. The download_blob method returns a stream of data.\n            data_str = blob_client.download_blob().readall().decode('utf-8')\n\n            try:\n                data = json.loads(data_str)\n            except json.JSONDecodeError:\n                data = json.loads(blob_client.download_blob().readall().decode('utf-8'))\n\n            # Flatten the JSON data\n            flattened_data = flatten_json(data)\n\n            # Get a client for the target blob in the \"silver\" container. This client will be used to upload data to this blob. Move blob to the \"silver\" container\n            target_blob_client = blob_service_client.get_blob_client(container=\"silver\", blob=blob.name)\n            # Upload the flattened JSON data to the target blob, overwriting it if it already exists. The upload_blob method uploads data to a blob.\n            target_blob_client.upload_blob(json.dumps(flattened_data), overwrite=True)\n\n            # Uncomment the following line to delete the original blob after moving it to the \"silver\" container\n            #blob_client.delete_blob()  # Delete the original blob after moving\n\n        except Exception as e:\n            logging.error(f\"Error processing blob {blob.name}: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"DE-Projects/JsonFlatAzureSDK.html#azure-python-sdklibs-ecosystem","title":"Azure Python SDK(libs) ecosystem","text":"Library Explanation \ud83d\udc0dAzure SDK For Python Superset of all python packages (libs) for Azure. Can't be installed with a single pip. \ud83d\udce6 Azure Storage SDKs Subset of Azure SDK. Multiple libraries. Hence, no single pip command. \ud83d\udca6 Azure Blob Storage SDK Subset of Azure Storage SDK. Single Library - <code>pip install azure-storage-blob</code> \ud83d\udee0\ufe0f BlobServiceClient Class Storage Account Level \ud83d\udcc1 Container Client Class Container Level \ud83d\udcc4 Blob Client Class Blob Level"},{"location":"DE-Projects/JsonFlatAzureSDK.html#convert-the-script-into-an-azure-function","title":"Convert the script into an Azure Function","text":"<p>The logic from my script can be easily incoporated into an azure function. You can easily put the entire logic into the functions <code>function_app.py</code>. Refer to my other articles on how to work with Azure Functions.</p> <p>\ud83c\udf1f Conclusion: The Azure SDK for Python is a superset of libraries to work with Azure services. The Azure Blob Storage SDK for Python is a subset of the Azure SDK for working with Azure Blob Storage.</p> <p>The script in this article uses the Azure Blob Storage SDK for Python to flatten JSON files in an Azure Blob Storage container. The script first downloads the blob from the container, then flattens the JSON data, and finally uploads the flattened JSON data back to the container.</p>"},{"location":"DE-Projects/JsonFlatAzureSDK.html#additional-samples","title":"Additional samples","text":""},{"location":"DE-Projects/JsonFlatAzureSDK.html#uploading-data-to-azure-blob-storage","title":"Uploading Data to Azure Blob Storage","text":"<pre><code>from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n\n# Connection string to your Azure Storage account\nconnection_string = \"your_connection_string\"\ncontainer_name = \"your_container_name\"\nblob_name = \"your_blob_name\"\ndata = \"Sample data about your exes\"\n\n# Create a BlobServiceClient\nblob_service_client = BlobServiceClient.from_connection_string(connection_string)\n\n# Create a container if it doesn't exist\ncontainer_client = blob_service_client.get_container_client(container_name)\ncontainer_client.create_container()\n\n# Create a BlobClient\nblob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n\n# Upload data to the blob\nblob_client.upload_blob(data, overwrite=True)\nprint(\"Data uploaded successfully\")\n</code></pre>"},{"location":"DE-Projects/JsonFlatAzureSDK.html#downloading-data-from-azure-blob-storage","title":"Downloading Data from Azure Blob Storage","text":"<pre><code>from azure.storage.blob import BlobServiceClient\n\n# Connection string to your Azure Storage account\nconnection_string = \"your_connection_string\"\ncontainer_name = \"your_container_name\"\nblob_name = \"your_blob_name\"\n\n# Create a BlobServiceClient\nblob_service_client = BlobServiceClient.from_connection_string(connection_string)\n\n# Create a BlobClient\nblob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n\n# Download data from the blob\nblob_data = blob_client.download_blob().readall()\nprint(\"Downloaded data:\", blob_data.decode())\n</code></pre>"},{"location":"DE-Projects/JsonFlatAzureSDK.html#listing-blobs-in-a-container","title":"Listing Blobs in a Container","text":"<pre><code>from azure.storage.blob import BlobServiceClient\n\n# Connection string to your Azure Storage account\nconnection_string = \"your_connection_string\"\ncontainer_name = \"your_container_name\"\n\n# Create a BlobServiceClient\nblob_service_client = BlobServiceClient.from_connection_string(connection_string)\n\n# Get a container client\ncontainer_client = blob_service_client.get_container_client(container_name)\n\n# List blobs in the container\nblobs = container_client.list_blobs()\nfor blob in blobs:\n    print(\"Blob name:\", blob.name)\n</code></pre>"},{"location":"DE-Projects/JsonFlatAzureSDK.html#querying-data-example-with-csv-data","title":"Querying Data (Example with CSV Data)","text":"<p>If your data is in a CSV format and stored in blobs, you can query it using Azure Synapse or Data Lake Analytics for more advanced queries. Here's a simple example using CSV data in blobs:</p> <pre><code>import pandas as pd\nfrom azure.storage.blob import BlobServiceClient\n\n# Connection string to your Azure Storage account\nconnection_string = \"your_connection_string\"\ncontainer_name = \"your_container_name\"\nblob_name = \"your_blob_name.csv\"\n\n# Create a BlobServiceClient\nblob_service_client = BlobServiceClient.from_connection_string(connection_string)\n\n# Create a BlobClient\nblob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n\n# Download blob data to a stream\nstream = blob_client.download_blob().readall()\n\n# Read CSV data into a DataFrame\ndf = pd.read_csv(pd.compat.BytesIO(stream))\nprint(\"Data from CSV blob:\\n\", df)\n\n# Example query: Find whereabouts of a specific ex\nwhereabouts = df[df['Name'] == 'ExName']['Whereabouts'].iloc[0]\nprint(\"Whereabouts of ExName:\", whereabouts)\n</code></pre> <p>\u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com</p>"},{"location":"DE-Projects/LocalPython_AzureBlob.html","title":"Local Python to Blob","text":""},{"location":"DE-Projects/LocalPython_AzureBlob.html#rearranging-files-in-azure-blob-storage-using-python","title":"Rearranging Files in Azure Blob Storage Using Python","text":""},{"location":"DE-Projects/LocalPython_AzureBlob.html#background","title":"Background","text":"<p>During some of my projects, I often had to rearrange files inside Azure blob containers. In this write-up, I'm sharing a simple Python method I frequently use with the Azure SDKs. </p> <p>But before we dive in, it's essential to understand there are many other ways to achieve the same thing, some of which might be better for regular company setups:</p> <ol> <li>You can run Spark externally and use Hadoop Jars with Pyspark. This is quite useful if you're looking for an in-house solution and don't want to spend too much on Databricks or ADFs. I've explained this method in detail in other articles on my site.</li> <li>There are also options like Databricks, Azure Logic Apps, ADF, and the like.</li> </ol> <p>One thing to note: While this Python method is easy, it's a bit slower compared to using Hadoop Jars with Spark. Also, if you're using Python externally, monitoring and maintenance can be a bit challenging.</p> <p>I'm sharing this technique because it shows the capabilities of Azure SDK when paired with Python for Blob containers. It's good to know what can be done, even if there might be faster or more efficient methods out there.</p>"},{"location":"DE-Projects/LocalPython_AzureBlob.html#prerequisites","title":"Prerequisites","text":"<ol> <li>Azure Blob Storage Account and a container with files.</li> <li><code>azure-identity</code> and <code>azure-storage-blob</code> Python packages. Install these with:    <pre><code>pip install azure-identity azure-storage-blob\n</code></pre></li> </ol>"},{"location":"DE-Projects/LocalPython_AzureBlob.html#overview","title":"Overview","text":"<p>Our primary goal is to rearrange files stored in my container named like <code>2023-10-23-00.json</code> into a hierarchical structured format like <code>year=2023/month=10/day=23/hour=00.json</code>.</p>"},{"location":"DE-Projects/LocalPython_AzureBlob.html#kickstart-step-by-step-guide-to-writing-the-code","title":"Kickstart: Step-by-step guide to writing the code","text":"<ol> <li> <p>Set up Azure Blob Storage SDK:    Start by authenticating against Azure with the credentials from your registered Azure app:    <pre><code>from azure.identity import ClientSecretCredential\nfrom azure.storage.blob import BlobServiceClient\n\ncredential = ClientSecretCredential(\n    tenant_id=\"YOUR_TENANT_ID\", \n    client_id=\"YOUR_CLIENT_ID\", \n    client_secret=\"YOUR_CLIENT_SECRET\"\n)\n\naccount_url = \"https://YOUR_STORAGE_ACCOUNT_NAME.blob.core.windows.net/\"  \nblob_service_client = BlobServiceClient(account_url=account_url, credential=credential)\n</code></pre></p> </li> <li> <p>Listing Blobs:    Retrieve all blobs in your container and then filter the desired files:    <pre><code>container_client = blob_service_client.get_container_client(\"YOUR_CONTAINER_NAME\")\nblobs = container_client.list_blobs()\nold_files = [blob.name for blob in blobs if \"-*.json\" in blob.name]\n</code></pre></p> </li> <li> <p>File Rearrangement:    Deconstruct each blob's name to get the year, month, day, and hour. Then, design the new path:    <pre><code>for old_file_path in old_files:\n    filename = old_file_path.split('/')[-1]\n    year, month, day, hour = filename.split('-')[:4]\n    new_directory = f\"year={year}/month={month}/day={day}/\"\n    new_blob_path = new_directory + f\"hour={hour}.json\"\n</code></pre></p> </li> <li> <p>Copying &amp; Deleting Blobs:    Azure Blob Storage requires copying the blob to the new position and deleting the original:    <pre><code>source_blob = container_client.get_blob_client(old_file_path)\ndestination_blob = container_client.get_blob_client(new_blob_path)\ndestination_blob.start_copy_from_url(source_blob.url)\nsource_blob.delete_blob()\n</code></pre></p> </li> <li> <p>Simulating Directory-like Structure:    Azure Blob Storage doesn't have real directories. Instead, using <code>/</code> in blob names can emulate directory structures. By copying a blob to a new path with <code>/</code>, we can recreate a folder-like structure.</p> </li> <li> <p>Error Handling:    It's essential to manage errors. I ran into an <code>InvalidUri</code> error when attempting to create a directory-like structure  using 0-byte blobs. I tackled this by directly copying the blob to the desired path, which handled the pathing effectively.</p> </li> </ol>"},{"location":"DE-Projects/LocalPython_AzureBlob.html#conclusion","title":"Conclusion","text":"<p>Shifting files in Azure Blob Storage can initially appear challenging, mainly due to its flat structure. Nevertheless, with the Azure SDK for Python and the correct strategy, restructuring blobs becomes feasible. Always remember to rigorously test your approach in a non-production environment before applying it to vital data.</p>"},{"location":"DE-Projects/LocalPython_AzureBlob.html#appendix","title":"Appendix","text":""},{"location":"DE-Projects/LocalPython_AzureBlob.html#why-use-a-structure-like-year2023month10day23hour00json","title":"Why use a structure like <code>year=2023/month=10/day=23/hour=00.json</code>?","text":"<p>This hierarchical directory structure aligns very well with common best practices for storing data in columnar storage formats like Parquet and ORC. Here's how:</p> <ol> <li> <p>Partitioning: The structure <code>year=2023/month=10/day=23/hour=00.json</code> inherently sets up partitioning for the data. When converted to Parquet or other storage formats, this structure will make it incredibly efficient to read specific partitions (like all data from a particular month or day) without scanning through the entire dataset. This reduces the amount of data that needs to be read and thus speeds up queries.</p> </li> <li> <p>Column Pruning: If your dataset inside the JSON files includes multiple columns, Parquet (and other columnar formats) allows for column pruning. This means that if a particular analysis requires only a subset of columns, only those specific columns are read from the storage, saving both time and resources.</p> </li> <li> <p>Compression: Both Parquet and ORC are known for efficient compression. When you have data organized hierarchically, and you're storing it in a columnar format, you can achieve significant storage savings. The structure also ensures that similar data types (like timestamps) are stored together, which can result in better compression ratios.</p> </li> <li> <p>Compatibility with Big Data Tools: Tools like Spark and Hive work exceptionally well with columnar storage formats and can directly utilize the hierarchical structure for optimized data reads. If you ever decide to analyze the data using these tools, having them in this structure and format would be advantageous.</p> </li> <li> <p>Schema Evolution: One of the advantages of Parquet (and to some extent, ORC) is the support for schema evolution. If your data structure changes over time (new columns are added, for example), these formats can handle those changes gracefully. Having an organized directory structure makes managing these schema changes over different time periods more straightforward.</p> </li> </ol> <p>Long story short:</p> <p>Given these advantages, if you're considering a transition to Parquet or another efficient storage format in the future, this hierarchical structure will certainly come in handy and help in making the data storage and retrieval processes more efficient.</p>"},{"location":"DE-Projects/LocalPython_AzureBlob.html#complete-tested-code","title":"Complete tested code","text":"<p>If you want to test the complete code together here is it. Remember, place your original ids, secret in the placeholder. Also, its not a good practice to use creds like this they shoudl be stored in vaults. But for focussing on the core functionality and to reduce the number of code I used the ids and passwords right in the code.</p> <pre><code># Import necessary libraries\nfrom azure.identity import ClientSecretCredential\nfrom azure.storage.blob import BlobServiceClient\nimport re\n\n# Replace your actual Azure 'App registrations' credentials here\ntenant_dir_id_regd_app = \"YOUR_TENANT_ID\"\nclient_id_regd_app = \"YOUR_CLIENT_ID\"\nclient_secret_regd_app = \"YOUR_CLIENT_SECRET\"\n\n# Specify the Azure Blob container name and storage account name\ncontainer_name = \"YOUR_CONTAINER_NAME\"\nstorage_act_name = \"YOUR_STORAGE_ACCOUNT_NAME\"\n\n# Set up authentication using Azure ClientSecretCredential\n# This is useful when you're using Azure AD for authentication\ncredential = ClientSecretCredential(\n    tenant_id=tenant_dir_id_regd_app, \n    client_id=client_id_regd_app, \n    client_secret=client_secret_regd_app\n)\n\n# Initialize BlobServiceClient with the given account and credentials\naccount_url = f\"https://{storage_act_name}.blob.core.windows.net/\"\nblob_service_client = BlobServiceClient(account_url=account_url, credential=credential)\ncontainer_client = blob_service_client.get_container_client(container_name)\n\n# List all blobs in the specified ADLS container\n# Then, filter out the files using a regular expression to match the desired format\nblobs = container_client.list_blobs()\nold_files = [blob.name for blob in blobs if re.match(r'^\\d{4}-\\d{2}-\\d{2}-\\d{2}.json$', blob.name)]\n\n# Display the number of files that match the format and will be processed\nprint(f\"Number of files to be processed: {len(old_files)}\")\n\nfor old_file_path in old_files:\n    # Break down the old file path to extract year, month, day, and hour\n    filename = old_file_path.split('/')[-1]\n    year, month, day, hour = filename.split('-')[:4]\n\n    # Create the new hierarchical directory structure based on extracted date and time details\n    new_blob_path = f\"year={year}/month={month}/day={day}/hour={hour}\"\n\n    # Display the file movement details\n    print(f\"Moving {old_file_path} to {new_blob_path}\")\n\n    # Copy content from the old blob to the new blob location\n    source_blob = container_client.get_blob_client(old_file_path)\n    destination_blob = container_client.get_blob_client(new_blob_path)\n    destination_blob.start_copy_from_url(source_blob.url)\n\n    # Once the content is successfully copied, remove the old blob\n    source_blob.delete_blob()\n\nprint(\"Files rearranged successfully!\")\n</code></pre> <p>\u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com</p>"},{"location":"DE-Projects/Microsoft_OpenJDK.html","title":"Microsoft OpenJDK","text":""},{"location":"DE-Projects/Microsoft_OpenJDK.html#installing-microsoft-openjdk","title":"Installing Microsoft OpenJDK","text":"<p>There are many versions of JDK. Refer to the table at the end to see them. Here we will focus on OpenJDK. Many popular applications and platforms use OpenJDK as their Java runtime. For example, Minecraft (Java Edition), Apache Cassandra, Apache Kafka, Jenkins, and ElasticSearch all rely on OpenJDK. Development tools like IntelliJ IDEA and Eclipse also run well on it. Spring Boot applications, Atlassian products like Jira and Confluence, big data tools like Apache Hadoop and Apache Spark, as well as Docker images and web servers like Apache Tomcat and Jetty, commonly use OpenJDK. Even Android Studio can run with OpenJDK. So, OpenJDK is used a lot in  real-world applications.</p> <p>In this article I will give you a brief intro to Microsoft OpenJDK and how to install it.</p>"},{"location":"DE-Projects/Microsoft_OpenJDK.html#when-to-use-microsoft-openjdk","title":"When to Use Microsoft OpenJDK?","text":"<p>If you're using JDK in the Microsoft ecosystem, it's best to go with Microsoft's JDK. If any issues come up, you can easily get support from Microsoft. But if you're in a general setup or prefer tools not tied to one company, OpenJDK might be a better choice.</p> <p>Microsoft also says their OpenJDK can replace any OpenJDK, even in non-Microsoft environments like AWS + Linux. Plus, their version has extra fixes and enhancements, making it work better in the Microsoft ecosystem.</p>"},{"location":"DE-Projects/Microsoft_OpenJDK.html#installation-steps","title":"Installation Steps","text":""},{"location":"DE-Projects/Microsoft_OpenJDK.html#for-windows","title":"For Windows:","text":"<ol> <li>Download and Run the installer:</li> <li>Go to Microsoft Build of OpenJDK website.</li> <li>Choose the Windows <code>.msi</code> installer and download and run it.</li> <li> <p>During installation, choose the option to set <code>JAVA_HOME</code> and update the <code>Path</code>.     </p> </li> <li> <p>Verify Installation and check JAVA_HOME variable</p> </li> <li>Open Command Prompt and type <code>java -version</code>. You should see the installed version of Microsoft OpenJDK     <ul> <li>Enter <code>set</code> in command prompt. It will show you all the environment variable. Look for JAVA_HOME to see if it is set properly. </li> </ul> </li> </ol>"},{"location":"DE-Projects/Microsoft_OpenJDK.html#for-macos","title":"For macOS:","text":"<ol> <li>Download Archive:</li> <li>Visit the Microsoft Build of OpenJDK website.</li> <li> <p>Download the <code>.tar.gz</code> archive for macOS.</p> </li> <li> <p>Extract and Install:</p> </li> <li>Open Terminal and navigate to the downloaded archive.</li> <li> <p>Extract it using:      <pre><code>sudo tar zxvf microsoft-jdk-17-macos-x64.tar.gz -C /Library/Java/JavaVirtualMachines/\n</code></pre></p> </li> <li> <p>Set JAVA_HOME:</p> </li> <li>Add the following to your <code>.bash_profile</code>, <code>.zshrc</code>, or <code>.bashrc</code>:      <pre><code>export JAVA_HOME=/Library/Java/JavaVirtualMachines/microsoft-17.jdk/Contents/Home\nexport PATH=$JAVA_HOME/bin:$PATH\n</code></pre></li> <li> <p>Apply changes with:      <pre><code>source ~/.bash_profile\n</code></pre></p> </li> <li> <p>Verify Installation:</p> </li> <li>Run <code>java -version</code></li> <li>You should see the Microsoft OpenJDK version installed.</li> </ol>"},{"location":"DE-Projects/Microsoft_OpenJDK.html#for-linux-ubuntudebian","title":"For Linux (Ubuntu/Debian):","text":"<ol> <li>Download Archive:</li> <li> <p>Head to the Microsoft Build of OpenJDK website and download the <code>.tar.gz</code> for Linux.</p> </li> <li> <p>Extract and Install:</p> </li> <li>Open Terminal and navigate to the download location.</li> <li> <p>Extract using:      <pre><code>sudo tar zxvf microsoft-jdk-17-linux-x64.tar.gz -C /usr/lib/jvm\n</code></pre></p> </li> <li> <p>Set JAVA_HOME and Update Alternatives:</p> </li> <li>Add to your <code>.bashrc</code> or equivalent:      <pre><code>export JAVA_HOME=/usr/lib/jvm/microsoft-17\nexport PATH=$JAVA_HOME/bin:$PATH\n</code></pre></li> <li> <p>Set as default:      <pre><code>sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/microsoft-17/bin/java 1\nsudo update-alternatives --config java\n</code></pre></p> </li> <li> <p>Verify Installation:</p> </li> <li>Run <code>java -version</code></li> <li>You should see the installed Microsoft OpenJDK version.</li> </ol>"},{"location":"DE-Projects/Microsoft_OpenJDK.html#conclusion","title":"Conclusion","text":"<p>Microsoft Build of OpenJDK is just like the regular OpenJDK when it comes to working with apps. The main difference is that it's made by Microsoft, so it might work better with Microsoft products. Plus, if you're using it on Azure and something goes wrong with Java, you can get help from Microsoft support. In short, it's basically OpenJDK with Microsoft's name on it.</p>"},{"location":"DE-Projects/Microsoft_OpenJDK.html#jdk-brands","title":"JDK Brands","text":"JDK Version Free to Download Advantages Oracle JDK Yes (for personal use, development, and testing)** Official JDK from Oracle, commercial support, and long-term updates available. OpenJDK Yes Open-source, reference implementation of Java SE, widely supported. Amazon Corretto Yes Production-ready, free support, optimized for AWS, multiplatform. Eclipse Temurin (AdoptOpenJDK) Yes Free, well-supported, and widely used in various environments. Microsoft Build of OpenJDK Yes Free, optimized for Azure and Microsoft products, integrated with MS ecosystem. GraalVM Yes (Community Edition) High-performance, supports multiple languages, ahead-of-time compilation. Azul Zulu Yes Certified OpenJDK build, various support options, optimized for enterprise use."},{"location":"DE-Projects/Project_MigrationToAzureBlob.html","title":"Azure Blob Migration","text":""},{"location":"DE-Projects/Project_MigrationToAzureBlob.html#project-overview-migrating-and-enabling-search-for-archived-pdf-data-in-azure-blob-storage","title":"Project Overview: Migrating and Enabling Search for Archived PDF Data in Azure Blob Storage","text":"<p>The client, had several hundred GBs of archived data, mostly in PDF format, stored in another CMS. The CMS incurred heavy licensing fees and was on-prem without any DR or HA.</p> <p>The goal was to move this data to Azure Blob Storage and enable efficient search capabilities to improve accessibility and enable DR and reduce maintance costs from the current CMS system.</p>"},{"location":"DE-Projects/Project_MigrationToAzureBlob.html#solution-implementation","title":"Solution Implementation","text":"<p>We began by assessing the existing data structure, which was organized by deparment. We developed a migration plan to transfer the data in batches, ensuring minimal disruption to the client\u2019s operations.</p> <p>Tools Used: - Azure Blob Storage - PowerShell &amp; AzCopy for scripting and automating the data migration process - Azure Cognitive Search for indexing the content of the PDF files - Azure Storage Explorer for manual verification and management of the files - Azure SDKs for custom development and integration tasks</p> <p>The migration process involved extracting data from the legacy CMS and staging it in a network folder. Using PowerShell scripts and AzCopy, we transferred these files to Azure Blob Storage, preserving the original directory structure by using paths in the blob names.</p> <p>To enable searching within the uploaded PDFs, we set up Azure Cognitive Search. This involved creating a data source connected to the Azure Blob Storage container, defining an index to capture necessary metadata and content fields, and configuring an indexer to handle text extraction from PDFs.</p> <p>The indexer was scheduled to run periodically to ensure that new files were indexed promptly. This setup allowed users to perform efficient searches on the archived PDF data using the Azure Cognitive Search REST API or SDKs, providing quick and reliable access to the information stored in the cloud.</p>"},{"location":"DE-Projects/Project_MigrationToAzureBlob.html#outcome","title":"Outcome","text":"<p>The project successfully moved the client\u2019s archived PDF data to Azure Blob Storage and enabled search capabilities through Azure Cognitive Search. This solution provided the client with improved data accessibility and better search functionality. The use of tools and technologies from 2017 ensured a maintainable solution that met the client\u2019s needs effectively.</p>"},{"location":"DE-Projects/Project_MongoCMS.html","title":"MongoDB CMS","text":""},{"location":"DE-Projects/Project_MongoCMS.html#building-a-simple-cms-with-mongodb-python-and-flask","title":"Building a Simple CMS with MongoDB, Python, and Flask","text":"<p>Usually, we use Content Management Software like IBM Filenet or OpenText Documentum to create content warehouses. These applications store content (PDFs, JPEGs) as objects and save the associated metadata in databases like MSSQL or Oracle. They offer front-end applications like Webtop and Documentum Administrator to access the files.</p> <p>With MongoDB, you can create a similar application almost in no time. This article will show you how easy it is to build such an app.</p> <p>We will use MongoDB as a container running in Docker. Our application for generating PDFs and metadata, as well as the uploading part, will run on a local Windows machine. We'll use Python 3.11 to develop the application.</p> <p>The front end will be created using Flask.</p>"},{"location":"DE-Projects/Project_MongoCMS.html#project-background","title":"Project Background","text":"<p>The project can be divided into the following parts:</p> <ol> <li>Creating the MongoDB container</li> <li>Creating a script to generate PDFs and associated metadata in JSON</li> <li>Uploading the PDFs to MongoDB</li> <li>Creating a front-end UI using Flask to search the content in MongoDB</li> </ol>"},{"location":"DE-Projects/Project_MongoCMS.html#create-a-mongodb-container","title":"Create a MongoDB Container","text":"<p>First, create a Docker network and volume:</p> <pre><code>docker network create spark-network\ndocker volume create mongo_data\n</code></pre> <p>Execute the following command to run the MongoDB container:</p> <pre><code>docker run -d --name mongoDB --network spark-network -p 27017:27017 -v mongo-data:/data/db mongo\n</code></pre> <p>Explanation of the command options: - <code>-d</code>: Run the container in detached mode. - <code>--name mongoDB</code>: Assign the name \"mongoDB\" to the container. - <code>--network spark-network</code>: Connect the container to the network \"spark-network\". - <code>-p 27017:27017</code>: Map port 27017 on the host to port 27017 in the container. - <code>-v mongo-data:/data/db</code>: Use a Docker volume named \"mongo-data\" to persist MongoDB data.</p> <p>You can connect to MongoDB using a MongoDB client or management tool like MongoDB Compass, Robo 3T, or Studio 3T. The connection string is:</p> <pre><code>mongodb://localhost:27017\n</code></pre>"},{"location":"DE-Projects/Project_MongoCMS.html#creating-sample-pdfs-and-metadata-files","title":"Creating Sample PDFs and Metadata Files","text":"<p>Create the folder structure <code>mongoDB_CMS\\content_and_metadata</code> and place the script files inside it. </p> <p> Note: Before running the scripts, install the required Python packages using command prompt/terminal: **pip install fpdf pymongo** </p>"},{"location":"DE-Projects/Project_MongoCMS.html#script-to-create-metadata-and-pdf-files","title":"Script to Create Metadata and PDF Files","text":"<p>Save the following script as <code>1_create_files.py</code>:</p> <pre><code>from fpdf import FPDF\nimport json\nimport os\n\n# Base directory to save content and metadata files inside content_and_metadata folder\n#base_dir = os.path.join(os.getcwd(), 'content_and_metadata')\n#os.makedirs(base_dir, exist_ok=True)\n# Base directory to save content and metadata files inside a particular folder\n# base_dir = r'C:\\Users\\dwaip\\Desktop\\mongoDB_CMS'\n\n# Base directory, to save the content and metadata in the current folder\nbase_dir = os.getcwd()  # Get the current working directory\n\n\n# Generate 10 PDF files and corresponding metadata files\nmetadata_list = [\n    {\"title\": f\"Document {i}\", \"description\": f\"Description for document {i}\", \"file_name\": f\"document{i}.pdf\"}\n    for i in range(1, 11)\n]\n\nfor i, metadata in enumerate(metadata_list, 1):\n    # Create PDF file\n    pdf = FPDF()\n    pdf.add_page()\n    pdf.set_font(\"Arial\", size=12)\n    pdf.cell(200, 10, txt=f\"Document {i}\", ln=True, align='C')\n    pdf.cell(200, 10, txt=\"This is a sample PDF file.\", ln=True, align='C')\n\n    pdf_file_path = os.path.join(base_dir, f\"document{i}.pdf\")\n    pdf.output(pdf_file_path)\n\n    # Create metadata file\n    metadata_file_path = os.path.join(base_dir, f'metadata{i}.json')\n    with open(metadata_file_path, 'w') as f:\n        json.dump(metadata, f, indent=4)\n\nprint(f\"PDF and metadata files created successfully inside: {base_dir}\")\n</code></pre> <p>Navigate to the folder and run the script:</p> <pre><code>python 1_create_files.py\n</code></pre>"},{"location":"DE-Projects/Project_MongoCMS.html#upload-the-content-and-metadata-to-mongodb","title":"Upload the Content and Metadata to MongoDB","text":"<p>Save the following script as <code>2_upload_to_mongodb.py</code>:</p> <pre><code>from pymongo import MongoClient\nimport json\nimport gridfs\nimport os\n\n# Connect to MongoDB\nclient = MongoClient('mongodb://localhost:27017/')\ndb = client['cms_db']\nfs = gridfs.GridFS(db)\n\n# Directory path\nbase_dir = os.getcwd()  # Get the current working directory\n# base_dir = r'C:\\Users\\dwaip\\Desktop\\mongoDB_CMS' #For any custom directory\n\n# Read and upload metadata and content files\nfor file in os.listdir(base_dir):\n    if file.endswith('.json'):\n        # Read metadata file\n        metadata_path = os.path.join(base_dir, file)\n        with open(metadata_path, 'r') as f:\n            metadata = json.load(f)\n            file_name = metadata['file_name']\n            file_path = os.path.join(base_dir, file_name)\n\n            # Check if the content file exists\n            if os.path.exists(file_path):\n                # Upload file to GridFS\n                with open(file_path, 'rb') as content_file:\n                    file_id = fs.put(content_file, filename=file_name)\n\n                # Add file_id to metadata\n                metadata['file_id'] = str(file_id)  # Convert ObjectId to string\n\n                # Insert metadata into MongoDB\n                db.metadata.insert_one(metadata)\n            else:\n                print(f\"Content file {file_name} not found. Skipping...\")\n\nprint(f\"Files and metadata uploaded successfully in: {base_dir}\")\n</code></pre> <p>Using cmd, go to the folder and run the script:</p> <pre><code>python 2_upload_to_mongodb.py\n</code></pre>"},{"location":"DE-Projects/Project_MongoCMS.html#create-and-run-the-flask-application-to-search-cms","title":"Create and Run the Flask Application to Search CMS","text":"<p>Create the folder structure <code>mongoDB_CMS\\FlaskApp\\templates</code> and create two HTML files inside the <code>templates</code> folder.</p>"},{"location":"DE-Projects/Project_MongoCMS.html#indexhtml","title":"<code>index.html</code>","text":"<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;title&gt;PDF CMS&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Welcome to PDF CMS&lt;/h1&gt;\n    &lt;form action=\"/search\" method=\"post\"&gt;\n        &lt;input type=\"text\" name=\"query\" placeholder=\"Search for PDFs...\"&gt;\n        &lt;button type=\"submit\"&gt;Search&lt;/button&gt;\n    &lt;/form&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"DE-Projects/Project_MongoCMS.html#searchhtml","title":"<code>search.html</code>","text":"<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;title&gt;Search Results&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Search Results for \"{{ query }}\"&lt;/h1&gt;\n    &lt;ul&gt;\n        {% for result in results %}\n            &lt;li&gt;\n                &lt;strong&gt;{{ result['title'] }}&lt;/strong&gt;&lt;br&gt;\n                {{ result['description'] }}&lt;br&gt;\n                &lt;a href=\"/download/{{ result['file_id'] }}\"&gt;Download PDF&lt;/a&gt;\n            &lt;/li&gt;\n        {% else %}\n            &lt;li&gt;No results found.&lt;/li&gt;\n        {% endfor %}\n    &lt;/ul&gt;\n    &lt;a href=\"/\"&gt;Go back&lt;/a&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"DE-Projects/Project_MongoCMS.html#apppy","title":"<code>app.py</code>","text":"<p>Save the following script as <code>app.py</code> inside <code>mongoDB_CMS\\FlaskApp\\</code>:</p> <pre><code>from flask import Flask, render_template, request, send_file, jsonify\nfrom pymongo import MongoClient\nfrom gridfs import GridFS\nfrom bson.objectid import ObjectId\nimport io\n\napp = Flask(__name__)\n\n# Connect to MongoDB\nclient = MongoClient('mongodb://localhost:27017/')\ndb = client['cms_db']\nfs = GridFS(db)\n\n# Create text index for searching\ndb.metadata.create_index([(\"$**\", \"text\")])\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.route('/search', methods=['GET', 'POST'])\ndef search():\n    query = request.form.get('query', '')\n    print(f\"Search query: {query}\")\n\n    if query:\n        results = db.metadata.find({\"$text\": {\"$search\": query}})\n        results_list = list(results)\n        print(f\"Search results: {results_list}\")\n    else:\n        results_list = []\n\n    return render_template('search.html', results=results_list, query=query)\n\n@app.route('/download/&lt;file_id&gt;')\ndef download(file_id):\n    file_id = ObjectId(file_id)\n    file_data = fs.get(file_id)\n    return send_file(io.BytesIO(file_data.read()), download_name=file_data.filename, as_attachment=True)\n\n@app.route('/api/search', methods=['POST'])\ndef api_search():\n    query = request.json.get('query', '')\n    results = db.metadata.find({\"$text\": {\"$search\": query}})\n    return jsonify([result for result in results])\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre> <p>Using CMD go the to the <code>mongoDB_CMS\\FlaskApp\\</code> folder and run the Flask application:</p> <pre><code>python app.py\n</code></pre>"},{"location":"DE-Projects/Project_MongoCMS.html#use-the-search-application","title":"Use the search application","text":"<p>Navigate to <code>http://127.0.0.1:5000/</code> to access the application.  </p> <p>You will see a search interface where you can search for PDF documents.  </p> <p>The search results will display the documents with a link to download the PDFs. </p>"},{"location":"DE-Projects/Project_MongoCMS.html#appendix","title":"Appendix","text":""},{"location":"DE-Projects/Project_MongoCMS.html#how-the-pdf-is-uploaded-and-linked-with-the-metadata","title":"How the pdf is uploaded and linked with the metadata","text":"<ul> <li> <p>The script iterates over <code>.json</code> files in the directory, reading each metadata file.</p> <p><pre><code>for file in os.listdir(base_dir):\n    if file.endswith('.json'):\n        with open(os.path.join(base_dir, file), 'r') as f:\n            metadata = json.load(f)\n</code></pre> - It extracts the <code>file_name</code> from the metadata to locate the PDF file.</p> <p><pre><code>file_name = metadata['file_name']\nfile_path = os.path.join(base_dir, file_name)\n</code></pre> - If the PDF file exists, it is uploaded to GridFS, generating a unique <code>file_id</code>.</p> <p><pre><code>if os.path.exists(file_path):\n    with open(file_path, 'rb') as content_file:\n        file_id = fs.put(content_file, filename=file_name)\n</code></pre> - The <code>file_id</code> is added to the metadata and stored in the <code>metadata</code> collection in MongoDB.</p> <pre><code>metadata['file_id'] = str(file_id)\ndb.metadata.insert_one(metadata)\n</code></pre> </li> </ul> <p>Files:</p> <p>Files are stored in MongoDB using GridFS, which breaks each file into 255 KB chunks. These chunks are stored as separate documents in the <code>fs.chunks</code> collection. The <code>fs.files</code> collection stores information about each file, like the filename, upload date, file length, and references to the chunks that make up the file.</p> <p>Metadata:</p> <p>Metadata for each PDF is stored in a separate collection called <code>cms_db.metadata</code>. This includes details like the title, description, and filename of the PDF. Each metadata document has a <code>file_id</code> field, which stores the ObjectId of the corresponding file in GridFS. This <code>file_id</code> links the metadata to the actual file, allowing the application to find and serve the file based on user requests.</p>"},{"location":"DE-Projects/Project_MongoCMS.html#how-mongodb-stores-files-and-metadata","title":"How MongoDB Stores Files and Metadata","text":""},{"location":"DE-Projects/Project_MongoCMS.html#default-metadata-gridfs-metadata","title":"Default Metadata (GridFS Metadata)","text":"<p>GridFS automatically handles the basic information about each file. It stores files in chunks of 255 KB in the <code>fs.chunks</code> collection. The <code>fs.files</code> collection holds metadata like the filename, upload date, file length, chunk size, MD5 hash for checking file integrity, and references to the file chunks. This ensures that files are stored and retrieved correctly.</p>"},{"location":"DE-Projects/Project_MongoCMS.html#custom-metadata","title":"Custom Metadata","text":"<p>Apart from the default metadata, custom metadata is stored in a separate collection (e.g., <code>cms_db.metadata</code>). This includes details specific to the application, such as the PDF title, description, and the user-friendly filename. Each custom metadata document also has a <code>file_id</code> field that links to the corresponding file in GridFS. This makes it easy for the application to find and serve the file when needed.</p>"},{"location":"DE-Projects/Project_MongoCMS.html#conclusion","title":"COnclusion:","text":"<p>Using MongoDB for Enterprise Content Management can be a good choice if you have specific needs, but traditional ECM systems are often better for several reasons. Traditional ECM systems like SharePoint, Alfresco, and Documentum come with built-in features such as document libraries, version control, metadata management, workflow automation, compliance tools, and user access controls. MongoDB, on the other hand, does not offer these features out-of-the-box. Additionally, ECM systems have user-friendly interfaces designed for document management, requiring less customization compared to MongoDB.</p> <p>ECM systems also help meet regulatory requirements for document retention, data privacy, and security by providing built-in tools for audit trails, access controls, and document lifecycle management. They offer strong governance features to manage documents according to company policies. Moreover, ECM solutions integrate smoothly with other enterprise applications like Microsoft 365 and ERP systems, whereas MongoDB requires more development effort for similar integration. ECM systems come with ready-to-use APIs and connectors, reducing the need for custom development.</p> <p>Another advantage of ECM systems is their quick deployment with minimal setup effort. Using MongoDB would require significant custom development to achieve the same functionalities. Finally, ECM solutions come with support and maintenance services to keep the system up-to-date and secure.</p>"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint.html","title":"Setting Up SSRS Reports in SharePoint for a Client with 100+ Field Offices","text":""},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint.html#background","title":"Background","text":"<p>The company has over 100 field offices, from which data is collected daily using different ETL workflows. Each workflow represents a specific lending process.</p> <p>The data includes both documents and metadata. The operational documents are stored in a Document warehouse, while the related metadata is kept in MSSQL tables.</p> <p>The analytical data for reporting is stored in MSSQL tables. At the end of each month, the company generates reports to ensure they include the latest data.</p>"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint.html#goal","title":"Goal","text":"<p>Create an end-to-end workflow using SSRS and SharePoint to automate the report generation process using the latest data from the MSSQL Server.</p>"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint.html#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint.html#mssql-database-design","title":"MSSQL Database Design","text":"<p>First, the tables for storing the analytical data were created. Each table represented one process like: - <code>CustomerApplication</code> - <code>CreditEvaluation</code> - <code>LoanApproval</code> - <code>Disbursement</code> - <code>RepaymentTracking</code></p> <p>There were over 15 such processes.</p>"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint.html#creation-of-etl-processes","title":"Creation of ETL Processes","text":"<p>Field office staff captured data at their office and pushed it to a shared network folder. For each batch, one JSON file was created. This contained the metadata which needed to be flattened and stored in MSSQL. For this, SSIS was used.</p>"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint.html#configuring-sharepoint","title":"Configuring SharePoint","text":"<p>Under the root site for Loan Origination and Processing, a subsite was created for managing the reports: <code>https://&lt;root_site_for_Loan_Accounting&gt;/sites/MonthlyReports</code>.</p>"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint.html#setting-up-ssrs-in-sharepoint","title":"Setting Up SSRS in SharePoint","text":"<p>SQL Server Reporting Services (SSRS) was installed in SharePoint integrated mode on the SharePoint farm. SSRS was configured to run with our SharePoint instance.</p>"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint.html#creation-of-report-libraries-in-sharepoint","title":"Creation of Report Libraries in SharePoint","text":"<p>Report libraries were created in the SharePoint site's Site Contents. Following is an overview of the steps:</p> <ul> <li>Go to your SharePoint site.</li> <li>Click on the Settings symbol (gear icon) and select \"Site Contents.\"</li> <li>Click \"Add an app\" and search for \"Report Library.\"</li> <li>Name the app (e.g., \"MonthlyReport1_LAC\") and click \"Create.\"</li> <li>Click on the three dots (...) next to your Report Library and select \"App Settings.\"</li> <li>Click \"Add from existing site content types\" and add the \"SQL Server Reporting Service\" content type.</li> </ul>"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint.html#setting-up-data-sources-and-datasets-in-ssrs","title":"Setting Up Data Sources and Datasets in SSRS","text":"<p>After creating the report libraries, we created the required data sources. Following is an overview of the steps:</p> <ol> <li>Create Data Source</li> <li>In your Report Library, create a new data source.</li> <li>Enter the connection string to your MSSQL database, including server name, database name, and authentication details.</li> <li> <p>Example: <code>Server=myServerAddress;Database=myDataBase;User Id=myUsername;Password=myPassword;</code></p> </li> <li> <p>Create Datasets</p> </li> <li>Use the created data source to define datasets that will pull the required data from your MSSQL tables.</li> <li>Example query for dataset:      <pre><code>SELECT * FROM Disbursement WHERE Date BETWEEN @StartDate AND @EndDate\n</code></pre></li> <li>Set parameters (<code>@StartDate</code>, <code>@EndDate</code>) to filter data for the month.</li> </ol>"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint.html#designing-and-deploying-reports","title":"Designing and Deploying Reports","text":"<p>Once the setup was done, we collaborated with higher management for the report designing. SSDT was used for designing.</p> <p>Here is an overview of the steps:</p> <ol> <li>Design Reports Using Report Builder or SSDT</li> <li>Open Report Builder or SQL Server Data Tools (SSDT).</li> <li>Create a new report, connect it to the data source and dataset created earlier.</li> <li> <p>Design the layout of the report (tables, charts, etc.).</p> </li> <li> <p>Deploy Reports to SharePoint</p> </li> <li>Save the report as an RDL file.</li> <li>Upload the RDL file to the Report Library in SharePoint.</li> </ol>"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint.html#scheduling-and-automation","title":"Scheduling and Automation","text":"<p>After designing and creating the reports, we scheduled them so that they automatically reach the designated recipients every month.</p> <p>Following is an overview of the steps:</p> <p>Set Up Report Subscriptions    - In SharePoint, go to the Report Library and find the uploaded report.    - Set up a subscription to automatically generate the report at the end of each month.    - Configure the subscription to deliver the report to specific users via email or save it to a document library.</p>"},{"location":"DE-Projects/Project_SSRS_SSIS_SharePoint.html#access-and-permissions","title":"Access and Permissions","text":"<p>We provided necessary permissions to the libraries and subsites so that the reports are not accessible to everyone, only to the people maintaining or upgrading them in the future. Also, some users were given rights to navigate to the SharePoint site and go to the Report Library to view the latest reports directly.</p>"},{"location":"DE-Projects/SPY_ETF_Buy_Recommender.html","title":"SPY ETF Recommender","text":""},{"location":"DE-Projects/SPY_ETF_Buy_Recommender.html#requirement-document-etf-spy-recommendation-app","title":"Requirement Document: ETF SPY Recommendation App","text":""},{"location":"DE-Projects/SPY_ETF_Buy_Recommender.html#objective","title":"Objective:","text":"<p>I want to create an app that provides buy or not recommendations specifically for the ETF SPY, based on real-time data analysis and AI-driven insights.</p>"},{"location":"DE-Projects/SPY_ETF_Buy_Recommender.html#key-requirements","title":"Key Requirements:","text":"<ol> <li>Data Sources:</li> <li>ETF SPY Data: The app should collect historical and real-time data for SPY, including prices, trading volume, dividend history, and expense ratios.</li> <li>Market Sentiment Data: The app should gather sentiment data from financial news, social media platforms like Twitter, and forums like Reddit to gauge market mood towards SPY.</li> <li> <p>Macroeconomic Data: The app should also consider macroeconomic indicators like interest rates, inflation, and GDP growth that could impact SPY\u2019s performance.</p> </li> <li> <p>Data Processing:</p> </li> <li>Feature Engineering: Extract relevant features from the data, such as moving averages, RSI, sentiment scores, and other technical indicators.</li> <li> <p>Data Storage: The app should store collected data in a database, ensuring it is updated in real-time.</p> </li> <li> <p>AI Model Development:</p> </li> <li>Model Selection: The app should use machine learning models, like Random Forests or Neural Networks, to analyze the data and make predictions.</li> <li> <p>Training and Evaluation: The model should be trained on historical data and regularly updated with new data. It should also be evaluated for accuracy using backtesting techniques.</p> </li> <li> <p>Recommendation Logic:</p> </li> <li>Buy or Not Decision: Based on the AI model\u2019s predictions and market sentiment analysis, the app should provide a simple buy or not recommendation.</li> <li> <p>User Personalization: Allow users to input their risk tolerance and investment goals, which will tailor the recommendations.</p> </li> <li> <p>User Interface:</p> </li> <li>Dashboard: The app should have a clean and user-friendly dashboard displaying SPY\u2019s current status, AI-driven recommendations, and key metrics.</li> <li> <p>Alerts: Users should receive notifications or alerts when a significant change in recommendation occurs.</p> </li> <li> <p>Deployment:</p> </li> <li>Cloud Hosting: The app should be hosted on a cloud platform like AWS or Azure to ensure scalability and reliability.</li> <li>Real-Time Updates: The app should integrate with data sources via APIs to provide real-time updates and recommendations.</li> </ol>"},{"location":"DE-Projects/SPY_ETF_Buy_Recommender.html#solutioning-document-etf-spy-recommendation-app","title":"Solutioning Document: ETF SPY Recommendation App","text":""},{"location":"DE-Projects/SPY_ETF_Buy_Recommender.html#1-data-collection","title":"1. Data Collection:","text":"<ul> <li>APIs: I will use Yahoo Finance API for fetching SPY\u2019s financial data, including prices, volume, and dividends. For market sentiment, I\u2019ll use the Twitter API and NewsAPI to gather social media and news sentiment on SPY.</li> <li>Data Pipeline: I\u2019ll set up a data pipeline using Apache Kafka to stream real-time data from these sources into my app\u2019s backend.</li> </ul>"},{"location":"DE-Projects/SPY_ETF_Buy_Recommender.html#2-data-processing","title":"2. Data Processing:","text":"<ul> <li>Feature Engineering: I\u2019ll extract technical indicators like moving averages, RSI, and volatility from SPY\u2019s historical data. Sentiment analysis will be performed on the collected news and social media data using NLP tools.</li> <li>Database: I\u2019ll store all processed data in a PostgreSQL database, ensuring real-time updates with proper indexing for quick access.</li> </ul>"},{"location":"DE-Projects/SPY_ETF_Buy_Recommender.html#3-ai-model-development","title":"3. AI Model Development:","text":"<ul> <li>Model Selection: I\u2019ll start with a Random Forest model for its robustness and ability to handle different types of data. I might also experiment with LSTM networks for better time-series predictions.</li> <li>Training and Evaluation: I\u2019ll use historical SPY data to train the model and backtest it against past market conditions to ensure accuracy.</li> </ul>"},{"location":"DE-Projects/SPY_ETF_Buy_Recommender.html#4-recommendation-logic","title":"4. Recommendation Logic:","text":"<ul> <li>Buy or Not Decision: The model\u2019s output, combined with sentiment scores, will feed into a decision-making algorithm. If the model predicts positive returns and sentiment is high, the app will recommend a buy; otherwise, it will advise holding off.</li> <li>User Personalization: I\u2019ll add options for users to set their risk levels and investment preferences, which will adjust the final recommendation accordingly.</li> </ul>"},{"location":"DE-Projects/SPY_ETF_Buy_Recommender.html#5-user-interface","title":"5. User Interface:","text":"<ul> <li>Dashboard Design: I\u2019ll create a dashboard using React for the frontend, showing SPY\u2019s current data, AI predictions, and a clear buy/not recommendation. I\u2019ll also include charts and graphs for better visualization.</li> <li>Alerts: I\u2019ll implement real-time notifications using Firebase Cloud Messaging to alert users when the recommendation changes.</li> </ul>"},{"location":"DE-Projects/SPY_ETF_Buy_Recommender.html#6-deployment","title":"6. Deployment:","text":"<ul> <li>Cloud Hosting: I\u2019ll deploy the app on AWS, using services like EC2 for hosting the application and RDS for the PostgreSQL database.</li> <li>Real-Time Integration: The app will be connected to all data sources via APIs to ensure it provides up-to-the-minute recommendations.</li> </ul>"},{"location":"DE-Projects/AzureSkyWeather/HomeProjectAzureSkyWeather.html","title":"Overview","text":"<p>Objective: Our goal is to develop an end-to-end data engineering solution using the Microsoft technology stack. We're mainly aiming to create a future-proof and efficient storage solution to store weather data sourced from web APIs for data science and analytics, .</p> <p>Project Components and Workflow:</p> <ol> <li>Data Ingestion Techniques:</li> <li>Step1A: Here, we'll use an HTTP-triggered Azure Function to pull in data from a Web API. The whole activity will be scheduled via Azure Logic Apps.</li> <li> <p>Step1B: We'll also see how a timer-triggered Azure Function can do the same job, making additional services unnecessary.</p> </li> <li> <p>Ways to Sort Data:</p> </li> <li>Step2A: We'll learn how to organise the weather files into directories like <code>year=yy/month=mm/day=dd/hour=h</code> using Local Spark and Hadoop Jars.</li> <li> <p>Step2B: As an alternative, we'll use Python combined with Azure's Python SDK. Plus, I'll show how this can be set up in a timer-based Azure Function for regular execution.</p> </li> <li> <p>Cleaning and Transforming Data:</p> </li> <li>Step3A: Let's explore data cleaning and its conversion to parquet format using standalone Spark and Hadoop jars.</li> <li> <p>Step3B: We'll switch to Azure Databricks to see its advantages and how it simplifies our work.</p> </li> <li> <p>Advanced Processing: The following steps will dive deeper into data science and analytics tasks.</p> </li> </ol> <p>Thought Behind This: Nowadays, many professionals prefer tools like Azure Data Factory, Azure Databricks, and Azure Synapse Analytics because of their all-in-one solutions. But, through this project, my idea is to show alternate methods. By understanding various alternatives, will strengthen your knowledge of data in the microsoft domain. Also knowing many methods will make you more confident in your choce of products.</p>"},{"location":"DE-Projects/AzureSkyWeather/HomeProjectAzureSkyWeather.html#step-1a-data-ingestion-using-azure-http-triggered-function-and-azure-logic-apps","title":"Step 1A: Data ingestion using Azure HTTP-Triggered Function And Azure Logic Apps","text":"<p>This is the first step of our project, focusing on data ingestion. In this segment, I'll show how we can fetch data from an API using an HTTP-triggered Azure Function, developed via VS Code. To schedule our function, we'll make use of Azure Logic Apps. By the end of this section, you'll have a comprehensive understanding of creating Azure functions. Read more...</p>"},{"location":"DE-Projects/AzureSkyWeather/HomeProjectAzureSkyWeather.html#step-1b-data-ingestion-using-just-azure-timer-triggered-function","title":"Step 1B: Data ingestion using Just Azure Timer-Triggered Function","text":"<p>Now, I'll show a different way. We'll use a Timer-Triggered Azure function that already has a built-in timer. This is another option. Choosing the best way can depend on things like cost. Azure Logic Apps can cost more than Timer-Triggered functions. I've kept this article short because many steps for both functions are the same. Read more...</p>"},{"location":"DE-Projects/AzureSkyWeather/HomeProjectAzureSkyWeather.html#step-2a-organize-the-weather-files-by-timestamp-using-plain-python","title":"Step 2A: Organize the weather files by timestamp using Plain Python","text":"<p>Here, I will show you how can can organize the weather files into directories like <code>year=yy/month=mm/day=dd/hour=h</code> using just Plain python. The code we will create can be easily incoporated into a Timer-trigger Azure function. That way we can schedule it to sort our files at a definite time. Read more...</p>"},{"location":"DE-Projects/AzureSkyWeather/HomeProjectAzureSkyWeather.html#step-2b-organize-the-weather-files-by-timestamp-using-spark-and-hadoop","title":"Step 2B: Organize the weather files by timestamp using Spark and Hadoop","text":"<p>Now, I will show you how do the same using Standalone Spark and Hadoop Jars. This will give you a good idea about connecting using Spark with Hadoop Jars to work with Azure blob storageRead more...</p> <p>\u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html","title":"Project AzureSkyWeather. Part 1A: Using Azure HTTP-Triggered Function","text":""},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#overview","title":"Overview","text":"<p>In this project we use an  Azure HTTP Function to get the current weather data from <code>weatherapi.com</code>. Developed in Visual Studio Code using the Python V2 programming model, it stores data as hourly-named JSON files in Azure Data Lake. It's HTTP-triggered, with Azure Logic Apps managing the periodic fetch schedule. The function logs activities and handles errors, and if provided a 'name' in a request, it sends a personalized greeting. For a detailed overview, please see the Comprehensive Project Summary in the appendix.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#choice-of-azure-services-for-data-ingestion","title":"Choice of Azure Services For Data Ingestion","text":"<p>We developed the Azure Function in Visual Studio Code on Windows using the Python V2 programming model. The V2 model simplifies Azure Function app development by allowing scheduling, binding, and the entire logic to be written in a main Python file, contrasting with V1 which requires additional setup in a JSON file. We sourced our weather data from weatherapi.com, a cost-free option that doesn't require credit card and lets you call the api almost indefinately. HTTP-triggered functions need scheduling, unlike Timer-triggered ones. We managed this with Azure Logic Apps. In terms of cost, the Azure function with a Timer-trigger is the most economical, followed by the HTTP-Triggered Azure function paired with Azure Logic App-based scheduling. For a sample cost breakdown of the Logic App, please refer to the section Calculate Azure Logic Apps Cost. Even so, for a simple hourly workflow, the expense remains minimal. It's worth noting that other options, especially Databricks, can be considerably pricier. Similarly, ADF and Synapse can also come at a higher cost. Here's a brief overview of the available options for creating an app like this:</p> <ul> <li> <p>Azure Functions (Python V2): These are ideal for small to moderate workloads and straightforward data retrieval tasks. Most of the logic is self-contained within the main Python file[V2 model], making it a cost-effective, easy-to-build choice.</p> </li> <li> <p>Azure Data Factory or Azure Databricks: For more complex workflows and efficient handling of larger data volumes, Azure Data Factory and Azure Databricks offer powerful options. However, they may incur higher operational costs due to their capabilities and scalability.</p> </li> <li> <p>Azure Logic Apps: Positioned between the other options, Azure Logic Apps are suitable for uncomplicated workflows with moderate data volumes, offering a visual workflow designer for easy setup. Their cost varies based on the number of steps. While here we use Azure Functions for core logic and Logic Apps for scheduling, some opt for a single solution, like a timer-triggered Azure App or fully utilizing an Azure Logic App.</p> </li> </ul>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#lets-ready-the-prerequisites","title":"Let's ready the prerequisites","text":""},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#get-the-weather-api","title":"Get the Weather API","text":"<p>First, create an account at weatherapi.com. Then, navigate to My Account. Copy the API key and keep it handy for later use.</p> <p></p> <p>We chose weatherapi.com because it doesn't require sign-ups, and it offers most of the features we needed.     </p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#set-up-azure-blobadlsg2-storage","title":"Set Up Azure Blob(ADLSG2) Storage","text":"<p>In the Azure portal, create a resource group, a storage account, and a blob container named <code>weather-http</code>. This is where the Azure Function will store the .json weather files. I've omitted the detailed steps here to focus on the main process. Remember: <code>Enable Hierarchical Namespace</code>. This will turn your storage into a Data Lake.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#create-the-azure-function-app","title":"Create the Azure Function App","text":"<p>Azure Function App is a container for Azure Functions. It can be set up before or during the function deployment. I recommend deploying during development and creating a distinct Function App for each function to prevent overwriting. I will share 'run-time' deployment steps later in this tutorial.</p> <p></p> <p>Sneak Peek: Deploying a Function App via VS Code.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#prepare-vs-code-and-local-enviroment-for-azure-function-development-and-testing","title":"Prepare VS Code and local enviroment for Azure Function Development and Testing","text":"<p>Install the following VS studio extensions:</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#azure-functions-extension-for-visual-studio-code","title":"Azure Functions Extension For Visual Studio Code","text":""},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#python-extension-for-visual-studio-code","title":"Python Extension for Visual Studio Code","text":"<p>Note: This is just the extension, not the actual Python interpreter. I assume you already have Python installed. For my work, I use the Conda distribution of Python. </p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#azurite","title":"Azurite","text":""},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#azure-functions-sdk","title":"azure-functions SDK","text":"<p>When testing our Azure Function locally, the system will look for the <code>azure-functions</code> SDK/package/library in the local system. This package must be present in the Python environment you're using; otherwise, you might encounter errors like: </p> <p></p> <p>As I use the Conda version of Python, I executed <code>pip install azure-functions</code> in the Anaconda prompt (with admin privileges). </p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#azure-storage-blob-library-for-local-testing","title":"<code>azure-storage-blob</code> library for local testing","text":"<p>As we are using Azure Blob Functions, we need to ensure that the Azure Blob Storage library/package/sdk is available during local testing . To install it locally using pip <code>install azure-storage-blob</code>.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#development-and-deploymment","title":"Development and Deploymment","text":""},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#create-the-python-based-azure-function","title":"Create the Python-based Azure Function","text":"<p>To create an Azure Function in Visual Studio Code, follow these steps:</p> <ul> <li>Open Visual Studio Code and access the Azure icon in the Activity bar.</li> <li>In the Workspace (local) area, click the thunder button, and select Create New Project.</li> </ul> <p></p> <ul> <li>Choose a local folder location for our project.</li> </ul> <p></p> <ul> <li>Select Python as the programming language.</li> </ul> <p></p> <ul> <li>Opt for Model V2 as the programming model.</li> </ul> <p></p> <ul> <li> <p>Choose the Python environment that you intend to use. Make sure to select the correct environment with all the required dependencies and packages e.g. Azure Functions.</p> </li> <li> <p>Select HTTP trigger</p> </li> </ul> <p></p> <ul> <li>Provide a unique name for our function.</li> </ul> <p></p> <ul> <li>VS Code will generate a complete project structure, including all the necessary components for developing our function.</li> </ul> <p></p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#write-our-custom-logic-in-the-azure-function","title":"Write our custom logic in the Azure Function","text":"<p>Let's dive into our weather code! Replace <code>function_app.py</code> with the following. Remember to fill in <code>&lt;your_connection_string&gt;</code> and <code>&lt;weather_api_endpoint&gt;</code> with the actual details.</p> <pre><code># Import necessary libraries and modules\nimport logging, requests  # logging for log management, requests to make HTTP requests\nfrom datetime import datetime  # datetime for handling date and time operations\nimport azure.functions as func  # Azure functions library\nfrom azure.storage.blob import BlobServiceClient  # Azure blob storage client library\n\n# Set the authentication level for the Azure function\napp = func.FunctionApp(http_auth_level=func.AuthLevel.FUNCTION)\n\n# Define the route for the Azure HTTP function\n@app.route(route=\"FetchWeatherHTTP\")\ndef FetchWeatherHTTP(req: func.HttpRequest) -&gt; func.HttpResponse:\n    # Log that the function is being processed\n    logging.info('Python HTTP trigger function processed a request.')\n\n    # Generate a filename for the weather data based on the current hour. This helps organize and retrieve data efficiently.\n    weatherFileName = datetime.now().strftime(\"%Y-%m-%d-%H.json\")\n\n    # Create a connection to Azure Blob Storage using the connection string\n    # You can obtain the connection string from the Azure Portal: Storage Account -&gt; Access Keys -&gt; key1 -&gt; Connection String\n    connection_string = \"&lt;your_connection_string&gt;\"\n    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n    blob_client = blob_service_client.get_blob_client(container=\"weather-http\", blob=weatherFileName)\n\n    # Fetch weather data using the WeatherAPI endpoint. Replace 'YOUR_API_KEY' with your actual API key and 'LOCATION' with your desired location.\n    response = requests.get(\"&lt;weather_api_endpoint&gt;\")\n    # Check if the data fetch was successful. If not, log an error and return a failure response.\n    if response.status_code != 200:\n        logging.error('Failed to fetch weather data.')\n        return func.HttpResponse(\"Failed to fetch weather data.\", status_code=500)\n\n    # Parse the response content into JSON format\n    data = response.json()\n\n    # Try to store the fetched weather data into Azure Blob Storage.\n    # If there's an error (e.g., network issue, permission issue), it'll log the error and return a failure response.\n    try:\n        blob_client.upload_blob(str(data), overwrite=True)\n    except Exception as e:\n        logging.error(f\"Error uploading data to Blob Storage: {e}\")\n        return func.HttpResponse(\"Error storing weather data.\", status_code=500)\n\n    # Check if there's a 'name' parameter in the request.\n    # This is a feature that was initially scaffolded by VS Code to demonstrate how to handle request parameters.\n    name = req.params.get('name')\n    if not name:\n        # If 'name' isn't in the request's parameters, check the request body.\n        try:\n            req_body = req.get_json()\n        except ValueError:\n            pass\n        else:\n            name = req_body.get('name')\n\n    # If a name is provided, return a personalized greeting.\n    # If not, return a generic success message.\n    if name:\n        return func.HttpResponse(f\"Hello, {name}. Weather data fetched and stored successfully.\")\n    else:\n        return func.HttpResponse(\n             \"Weather data fetched and stored successfully. Pass a name in the query string or in the request body for a personalized response.\",\n             status_code=200\n        )\n</code></pre>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#include-azure-storage-blob-in-requirementstxt","title":"Include  <code>azure-storage-blob</code> in requirements.txt","text":"<p>We've wrapped up the main coding. Now, just add <code>azure-storage-blob</code> to <code>requirements.txt</code> to instruct the deployment to install the library in Azure.</p> <p></p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#test-our-azure-function","title":"Test Our Azure Function","text":"<ul> <li> <p>It's time to test our Azure Function. Follow these steps:</p> </li> <li> <p>Open Visual Studio Code and press <code>Ctrl+Shift+P</code>. Select Azurite: Start.</p> <p></p> <p>This action starts the Azurite Blob Service. You can check the status at the bottom right corner of VS Code, where you should see something like this:</p> <p></p> </li> <li> <p>Start debugging by pressing <code>F5</code>. Then, on the left, click the Azure icon. Navigate to the workspace and locate our function (refresh if needed). Right-click the function and select Execute.</p> <p></p> </li> <li> <p>If the execution is successful, you'll see an output similar to this:</p> <p></p> <p>Additionally, a JSON file will be created in our container:</p> <p></p> </li> </ul>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#create-azure-function-app","title":"Create Azure Function App","text":"<p>For Azure Functions, we need a Function App\u2014a container for them. Instead of setting this up on the Azure Portal, I like using Visual Studio Code for its simplicity during initial deployment. Here's how to set it up with VS Code:</p> <ul> <li>Launch Setup in Visual Studio:</li> <li>Click the Azure icon, then select the thunder icon in the workspace.</li> <li>Choose <code>Create Function app in Azure..(Advanced)</code>.   </li> <li> <p>Comment: The icon might not always be visible; ensure your project is 'loaded' properly.</p> </li> <li> <p>Name Your Function App:</p> </li> <li> <p>Assign a unique name to your function app.   </p> </li> <li> <p>Select the Right Runtime:</p> </li> <li> <p>If you're working on an Azure Function in Python, ensure you set the runtime environment to Python.   </p> </li> <li> <p>Configure the resource group:</p> </li> <li> <p>Decide on using an existing resource group or create a new one. Ensure consistency in the chosen region.   </p> </li> <li> <p>Choose the hosting Plan:</p> </li> <li>Carefully select the hosting plan. If you're budget-conscious, consider the Consumption-based plan.    </li> <li>Comment: Based on my experience, running our current project, 24 executions for around seven days costed me few dollars. But, you case cuold be different. Overall, I found </li> <li>Choose Storage account:</li> <li> <p>Allocate a storage account for the Azure Function App. Using separate storage accounts for each function app simplifies the structure.   </p> </li> <li> <p>Opt for Enhanced Monitoring:</p> </li> <li>Incorporate an Application Insights resource for detailed insights and improved monitoring.</li> </ul> <p>After these steps, your Azure Function App is set up. The next phase involves deploying your Azure Function to this newly created app.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#deploy-the-azure-function-to-the-app","title":"Deploy the Azure Function To The App","text":"<ul> <li>The deployment process is straightforward. In the workspace, click the thunder icon and choose Deploy to Function App.</li> </ul> <ul> <li>Visual Studio Code will display the Function App where you can deploy our Azure Function. Select the Function App.</li> </ul> <ul> <li>Click Deploy</li> </ul> <p>Note: This will overwrite ANY function present in the Azure Func app.</p> <ul> <li>After successful deployment, you will see output like the following in the console:</li> </ul> <p></p> <ul> <li>And you can see the Function inside the Azure Function App:</li> </ul> <p></p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#schedule-the-function-using-azure-logic-apps","title":"Schedule the Function using Azure Logic Apps","text":"<ul> <li>Create a Logic App in the Azure Portal and name it <code>Anything-Meaningful-Haha</code>.</li> <li>Go to the Logic App Designer to design your workflow.</li> <li>Search for \"Recurrence\" and add this step.</li> <li>For the \"Interval\", enter \"1\". For \"Frequency\", select \"Hour\". Choose your time zone. Leave the \"Start time\" empty.</li> <li> <p>Next, add an action. This step is crucial. We have two options: HTTP Action and HTTP Webhook Action. For this scenario, choose the HTTP Action. Here's why: </p> </li> <li> <p>HTTP Action:</p> <ul> <li>Use this when you simply want to call an HTTP endpoint (in your case, the Azure Function) without waiting for a prolonged period or any kind of asynchronous processing.</li> <li>The Logic App will receive the immediate response from the Azure Function and then proceed to the next action or finish the workflow.</li> </ul> </li> <li> <p>HTTP Webhook Action:</p> <ul> <li>Use this when there's a need to wait for an asynchronous process to complete before moving on in the Logic App workflow. The Logic App will pause until it receives a \"callback\" from the endpoint, signaling that the task is done.</li> <li>More complex and often used in scenarios where there might be a long delay or wait time between triggering an action and its completion.</li> </ul> </li> </ul> <p>For our scenario, the HTTP Action is a simpler and more suitable choice. It will trigger the Azure Function, and once the Function responds (which it does after fetching and storing the weather data), the Logic App will consider the action completed. There is no need to wait for the Azure Function to complete its task and send a callback. Here is a real-world example of the output from choosing each scenario:</p> <p></p> <ul> <li> <p>In the HTTP Action, provide the URI and Method as GET. To obtain the URI from our Azure HTTP-Triggered Function, follow these steps:</p> </li> <li> <p>Open the Azure App hosting the Azure Function. </p> </li> <li>Click the Azure Function, navigate to Overview, then at the top, click Get Function URL and copy the URL.</li> </ul> <p>The workflow creation is complete. Save the workflow and wait for the next hour.</p> <p>In the Overview section of the Logic App, under Runs history, check the runs to see if the desired action (in our case, the creation of the weather JSON in the blob storage) is being performed.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#post-deployment-monitoring","title":"Post-deployment Monitoring","text":"<p>The purpose of this http-triggered logic-app scheduled Azure Function is to periodically fetch weather data and store it as a JSON file in an Azure Blob container. After deploying the function, you can monitor its performance and analyze invocations by following these steps:</p> <ul> <li>Open the Azure Function in the Azure portal.</li> </ul> <p></p> <ul> <li> <p>Go to the \"Monitor\" section to access detailed information about function invocations.</p> </li> <li> <p>Check the Azure Blob container to verify if the JSON files are being created as expected.</p> </li> </ul>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#common-errors","title":"Common Errors","text":""},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#azurewebjobsstorage-app-setting-is-not-present","title":"\"AzureWebJobsStorage\" app setting is not present","text":"<p>The \"AzureWebJobsStorage\" app setting error indicates that our Azure Functions app is missing a crucial configuration related to the storage account connection string. This could also be realted to the following deployment failure message</p> <pre><code>12:23:58 PM FetchWeatherAzFunc: Deployment Log file does not exist in /tmp/oryx-build.log\n12:23:58 PM FetchWeatherAzFunc: The logfile at /tmp/oryx-build.log is empty. Unable to fetch the summary of build\n12:23:58 PM FetchWeatherAzFunc: Deployment Failed. deployer = ms-azuretools-vscode deploymentPath = Functions App ZipDeploy. Extract zip. Remote build.\n12:24:00 PM FetchWeatherAzFunc: Deployment failed.\n12:35:57 PM FetchWeatherAzFunc: Starting deployment.\n</code></pre> <p>To resolve this:</p> <ul> <li> <p>Create or Identify a Storage Account: If you don't already have an Azure Storage account, create one in the same region as our function app.</p> </li> <li> <p>Get the Storage Account Connection String: Navigate to the Azure Storage account in the Azure Portal. Under the \"Settings\" section, click on \"Access keys.\" Here, you'll find two keys (key1 and key2) each with a connection string. You can use either of these connection strings for the next step.</p> </li> <li> <p>Update Function App Settings:</p> </li> <li>Navigate to our Azure Functions app in the Azure Portal.</li> <li>Under the \"Settings\" section, click on \"Configuration.\"</li> <li>In the \"Application settings\" tab, locate or create the <code>AzureWebJobsStorage</code> setting.</li> <li>Add the setting:<ul> <li>Name: <code>AzureWebJobsStorage</code></li> <li>Value: [our Azure Storage Account connection string from step 2]</li> </ul> </li> <li> <p>Click \"OK\" or \"Save\" to add the setting and save our changes on the main configuration page.</p> </li> <li> <p>Restart our Function App: After adding the necessary setting, restart our function app for the changes to take effect.</p> </li> </ul> <p>Following these steps will resolve the error related to the \"AzureWebJobsStorage\" app setting.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#logic-apps-while-trying-to-add-azure-function-no-resources-of-this-type-found-under-this-subscription","title":"Logic Apps while trying to add azure function - No resources of this type found under this subscription","text":"<p>Ideally, if you want to add an Azure Function to a Logic App, it should appear like this:</p> <p></p> <p>However, sometimes you may encounter the main Azure App, but not the bespoke function you created. Instead, you might see an error message such as \"No resources of this type found under this subscription.\" One of the reason for this could be realted to the storage account which the Azure Function uses. There could be other reason's too like the different regions. </p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#logic-apps-job-shows-running-status-indefinately","title":"Logic Apps Job shows Running status indefinately","text":"<p>If your Logic App consistently displays a \"Running\" status for a prolonged duration, several factors might be at play. A common cause is when an HTTP Webhook action is used; the Logic App waits for the webhook's response. It's crucial to verify that the webhook returns a response. Otherwise, the Logic App's status remains \"Running\" until a timeout or until a response is received.</p> <p></p> <p>A solution is to use a straightforward <code>HTTP action</code>. In this method, the step won't await any response; it marks itself as successful upon receiving an output.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#appendix","title":"Appendix","text":""},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#detailed-project-summary","title":"Detailed Project Summary","text":"<p>In the 'Overview' and 'Solutions Summary Approach' sections, I've sketched out a basic outline of our project. To dive deeper into the details, refer to the content below</p> <p>Objective</p> <p>The objective of the Azure function is to fetch current weather data periodically and store it efficiently for later use.</p> <p>Deployment and Environment:     - The function is developed using Visual Studio Code on Windows.    - It utilizes the Python V2 programming model for Azure Functions, simplifying the development process.</p> <p>Data Retrieval:     - The function sources weather data from <code>weatherapi.com</code>.    - It fetches current weather information without the need for credit card details, with data availability spanning up to 15 days.</p> <p>Data Storage:     - Weather data is saved as JSON files in an Azure Blob Storage container.    - Filenames are generated based on the current hour, ensuring organized and chronological storage of data. </p> <p>Trigger Mechanism:    - This Azure Function is HTTP-triggered, requiring external scheduling.    - The scheduling is managed through Azure Logic Apps, allowing for regular, automated data retrieval.</p> <p>Error Handling and Logging:     - The function contains robust logging, capturing both successful data retrievals and potential issues.    - It has mechanisms in place to handle errors, especially when fetching data from the API or while uploading to Blob Storage.</p> <p>Response Mechanism:    - Beyond its primary data retrieval and storage task, the function can provide a personalized greeting if a 'name' parameter is passed in the request.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#choice-of-storage-why-azure-data-lake","title":"Choice of storage. Why Azure Data Lake?","text":"<p>We've selected Azure Data Lake Storage Gen2 (ADLS G2) for storing our weather data. This decision was largely driven by its seamless integration with Azure Functions, which streamlines our data collection process. Apart form this ADLS G2 also has many other advantages, like:</p> <ul> <li> <p>Hierarchical Organization: ADLS G2 allows us to neatly structure our data, similar to a directory system. We can store our weather data grouped by year, month day etc.</p> </li> <li> <p>Enhanced Security: The Active Directory permission can be applied to the folders and files. This makes the storage as secure as it can get.</p> </li> <li> <p>Analytics Performance: ADLS G2 is optimized for heavy-duty data analytics, ensuring efficient query operations over vast datasets.</p> </li> </ul> <p>In the next stage, we decided to convert our JSON-formatted data into Parquet. Here's why:</p> <ul> <li> <p>Analytical Efficiency: Parquet, with its columnar storage design, streamlines analytical queries, letting us access specific data without scanning the entire dataset.</p> </li> <li> <p>Storage Efficiency: Parquet compresses data effectively, optimizing storage use and potentially lowering costs.</p> </li> <li> <p>Adaptable Structure: Parquet inherently carries schema information, allowing for alterations in data structure without disrupting existing data.</p> </li> </ul> <p>A noteworthy advantage of Parquet is its compatibility with various storage systems, including NoSQL databases, data warehouses, and lakehouses. This ensures that we can migrate or integrate our data effortlessly if our analytical demands change.</p> <p>To summarize, our storage strategy not only prepares our data for immediate analysis but we can also easily move it to other storage solution in future.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#creating-an-azure-functions-app-using-azure-portal","title":"Creating an Azure Functions App Using Azure Portal","text":"<p>An Azure Functions App acts as a host for one or more Azure Functions. In essence, while an individual Azure Function is a piece of code that runs in response to an event, the Function App serves as the hosting and management unit for that function (or multiple functions).</p> <ul> <li>To create an Azure Function, click \"Create\" under \"Function App\". </li> <li>Fill in the required details, including Subscription, Resource Group, Function App name, etc. For the runtime, choose Python. Click \"Review + create\" and then \"Create\". </li> </ul>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#azure-function-vs-code-project-folder-structure","title":"Azure Function VS-Code Project Folder Structure","text":"<p>Visual Studio Creates the following project structure for Azure Functions Project:</p> <p></p> <p>The main Project Folder  Contents: <ul> <li>.venv/: (Optional) Python virtual environment for local development.</li> <li>.vscode/: (Optional) Visual Studio Code configuration.</li> <li>function_app.py: Main entry point. Default location for functions, triggers, and bindings.</li> <li>additional_functions.py: (Optional) Additional Python files for logical grouping and reference in function_app.py.</li> <li>tests/: (Optional) Test cases for our function app.</li> <li>.funcignore: (Optional) Declares files not to be published to Azure, often including .vscode/ for editor settings, .venv/ for local Python virtual environment, tests/ for test cases, and local.settings.json to prevent local app settings from being published.</li> <li>host.json: Contains configuration options affecting all functions in a function app. Gets published to Azure.</li> <li>local.settings.json: Stores app settings and connection strings for local development, not published to Azure.</li> <li>requirements.txt: Lists Python packages installed when publishing to Azure.</li> <li>Dockerfile: (Optional) Used for custom container publishing.</li> </ul>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#structure-of-the-function-function_apppy","title":"Structure of the function function_app.py.","text":"<p>The <code>function_app.py</code> file is the key entry point. It resides at the root directory and serves as a Python script containing the definitions of the functions within the Function App.</p> <p>Each function defined in <code>function_app.py</code> must specify several crucial elements:</p> <ul> <li>Function Name: It should have a unique and descriptive name.</li> <li>Function Trigger: This defines what activates the function. For example, it could be triggered by an HTTP request or based on a timer schedule.</li> <li>Function Code: This is where you place the actual logic for the function.</li> </ul> <p></p> <p>Note, that the structure of <code>function_app.py</code> varies a little bit based on the type of trigger used. E.g., when creating a Timer-triggered function, a specific template code is generated VS Code. </p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#some-details-of-azure-functions-library","title":"Some details of <code>azure-functions</code> library","text":"<p>The command <code>pip install azure-functions</code> adds the Azure Functions SDK to your Python environment. This SDK facilitates local development, testing, and execution of Azure Functions. It includes tools that allow functions to interact with Azure and other services, such as responding to an HTTP request or storing data in Azure Blob Storage. </p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#azurite-extension","title":"Azurite Extension","text":"<p>Azurite is mainly used for local development and testing of applications that interact with Azure Storage services. It provides an emulation of Azure Storage services, including Blob, Queue, and Table services, in a local environment. </p> <p>(CTRL+SHIFT+P)\"Azurite: Start\" is a command that starts the Azurite server for local debugging etc. After that you typically use F5 to start debugging.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/HTTPTriggered_AzureFunc.html#calculate-azure-logic-apps-cost","title":"Calculate Azure Logic Apps Cost","text":"<p>Azure Logic Apps pricing primarily hinges on the total executions and the combined action and connector executions. To illustrate, for an hourly-triggered Azure Function:</p> <ul> <li>The Logic App runs hourly, translating to 24 daily executions.</li> <li>Every execution typically involves at least two action/connector executions: one for the recurring hourly trigger and another for invoking the Azure Function.</li> </ul> <p>Considering rates of approximately $0.000025 per execution and $0.000125 per action or connector execution (though rates can differ based on the region or Microsoft's adjustments):</p> <p><code>Estimated Monthly Cost = 24 x 30 x (0.000025 + 2 x 0.000125) \u2248 $0.198</code></p> <p>So, running this Azure Logic App with an hourly trigger might cost about 20 cents monthly. Yet, it's crucial to:</p> <ul> <li>I wrote this article in 2022. So, confirm current prices on the Azure Logic Apps pricing page.</li> <li>Use Azure's monitoring tools to observe your actual costs.</li> <li>Remember Azure's Logic Apps free tier, which offers a set number of free runs and actions monthly, potentially lowering costs if you stay within these free tier boundaries.</li> </ul> <p>Overall, for straightforward hourly triggers, Azure Logic Apps offer an affordable solution to schedule tasks without added coding.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html","title":"Azure Functions","text":""},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#azure-functions-quickstart-create-debug-deploy-monitor","title":"Azure Functions Quickstart - Create, Debug, Deploy, Monitor","text":"<p>Here, we'll learn quickly about Azure Functions. It's a cloud service by Azure where you can run pieces of code without worry about server and hosting. We'll start by setting up our workspace in VS Code and adding it with essential extensions. Then, we'll test our function. After debugging, we'll deploy it to Azure. Then, we'll monitor its performance. If you want to know more, I've put some essential info in the appendix section.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#create","title":"Create","text":"<p>Let's start by installing the following three Visual Studio Extensions and One Command Line Tool:</p> Extension Name Why Install? How to Install? Azure Tools for Visual Studio Code This extension holds a collection of extensions, including the Azure Functions extension. While you can install just the Azure Functions extension in this 'pack', having the full collection is never bad <code>Ctrl+Shift+X</code>, \"Azure Tools for Visual Studio Code\", <code>Install</code> Python extension for Visual Studio Code Provides extensive features for Python development, enabling linting, debugging, and IntelliSense for Azure Functions written in Python. <code>Ctrl+Shift+X</code>, \"Python\", <code>Install</code> Azurite An Azure Storage emulator, crucial for local testing and debugging of Azure Functions. <code>Ctrl+Shift+X</code>, \"Azurite\", <code>Install</code>.  Azure Functions Core Tools Command-line tools essential for local development and testing of Azure Functions. These tools enable local function runtime, debugging, and deployment capabilities. Install via npm with the command: <code>npm install -g azure-functions-core-tools@3 --unsafe-perm true</code> (for version 3.x) Or using GUI"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#create-the-azure-function-project","title":"Create the Azure Function Project","text":"<ul> <li>Open Visual Studio and Click the Azure Icon on the Left</li> <li>In the Workspace (local) area, click the thunder button, and select Create New Project. </li> <li>Choose a folder location for the project </li> <li>Select Python as the programming language. </li> <li>Opt for Model V2 as the programming model. </li> <li>Choose the Python environment. Refer to the Appendix section for more details.</li> <li>Select HTTP trigger. Refer to the appendix section below for more details. </li> <li>Provide a unique name for our function. </li> <li>VS Code will generate a complete project structure like shown below </li> <li>Write your custom code, say you want to perform some blob operations, in <code>function_app.py</code>. This is the main/entry point function to the Fnction app.</li> </ul>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#add-python-packages-to-requirementstxt","title":"Add Python packages to <code>requirements.txt</code>","text":"<p>Add library names of Python packages you imported in your script, like <code>numpy</code> , in <code>requirements.txt</code>.</p> <ul> <li>When you start you local debugging VS Code will install the <code>requirements.txt</code> packages to your local python virtual enviornment's <code>.venv</code>.</li> </ul> <p> - During actual deployment, VS Code will install the packages to Azure cloud.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#debug","title":"Debug","text":"<p>Now, I will show you how to debug the azure function:</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#testdebug-the-azure-function","title":"Test/Debug the Azure Function","text":"<ul> <li> <p>With your function_app.py open press <code>Ctrl+Shift+P</code>. Select Azurite: Start.</p> <p></p> <p>This action starts the Azurite storage Emulator. You can check the status at the bottom right corner of VS Code, where you should see something like this:</p> <p></p> </li> <li> <p>Press <code>F5</code>. Then, under Workspace Right-click the function and select Execute.   </p> </li> <li> <p>If the execution is successful, the output will be similar to this:   </p> </li> </ul>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#deploy","title":"Deploy","text":"<p>Here I will show you how to deploy the function to Azure.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#create-an-azure-function-app","title":"Create an Azure Function App","text":"<p>Now, our function is ready and we need to deploy it to Azure. To deploy an azure function we need Azure Function App. This is like a container for the function. You can create the Azure Function app from the portal. But, here I will show you how to do it right from VS code.</p> <ul> <li> <p>Click the Azure icon, then select the thunder icon in the workspace.</p> </li> <li> <p>Choose <code>Create Function app in Azure..(Advanced)</code>.</p> </li> </ul> <p></p> <ul> <li>Assign a unique name to your function app.</li> </ul> <p></p> <ul> <li>If you're working on an Azure Function in Python, ensure you set the runtime environment to Python.</li> </ul> <p></p> <ul> <li>Decide on using an existing resource group or create a new one. Ensure consistency in the chosen region.</li> </ul> <p></p> <ul> <li>Carefully select the hosting plan. If you're budget-conscious, consider the Consumption-based plan. </li> </ul> <p></p> <ul> <li>Allocate a storage account for the Azure Function App. Using separate storage accounts for each function app simplifies the structure.</li> </ul> <p></p> <ul> <li>Incorporate an Application Insights resource for detailed insights and improved monitoring.</li> </ul> <p>After these steps, your Azure Function App is set up. The next phase involves deploying your Azure Function to this newly created app.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#deploy-the-azure-function-to-the-azure-function-app","title":"Deploy the Azure Function To The Azure Function App","text":"<ul> <li> <p>The deployment process is straightforward. In the workspace, click the thunder icon and choose Deploy to Function App.   </p> </li> <li> <p>Visual Studio Code will display the Function App where you can deploy our Azure Function. Select the Function App.</p> </li> </ul> <p></p> <ul> <li>Click Deploy</li> </ul> <p></p> <p>Note: This will overwrite ANY function present in the Azure Func app. - After successful deployment, you will see output like the following in the console:</p> <p> - And you can see the Function inside the Azure Function App:</p> <p></p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#monitor","title":"Monitor","text":"<p>Afer deploying the function, I will show you how to monitor it in the portal.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#monitor-the-function-post-deployment","title":"Monitor the function post deployment","text":"<ul> <li>Open the Azure Function in the Azure portal.</li> </ul> <ul> <li>Go to the \"Monitor\" section to access detailed information about function invocations.</li> </ul>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#appendix","title":"Appendix","text":""},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#key-takeways","title":"Key takeways","text":"<ul> <li>Azure function is different from Azure Function App. Azure Function app is the container which holds Azure Functions.</li> <li>Azure functions can be developed using Python V2 Programming model, which uses decorators, lesser files, less-complex folder structure and a function_app.py</li> <li>HTTP-triggered functions and Timer-triggered functions are common in Function apps. Timer-triggered function have in-built trigger mechanism.</li> </ul>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#when-to-choose-azure-functions","title":"When to choose Azure Functions","text":"<p>Imagine you're thinking of using Azure Functions to convert JSON files to Parquet. Should you just use simple Python code in Azure Functions or go for Databricks? Here are some advantages and challenges of Azure Functions to help you decide:</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#advantages","title":"Advantages:","text":"<ul> <li>Auto-scaling: The function can scale up and down. This means you don't have to worry about resources if the workload increases, and you don't have to be concerned about costs if it decreases.</li> <li>Pay-as-long-as-you-use: You only pay for the actual time your code runs, making it very cost-efficient.</li> <li>Triggers: It offers numerous event triggers and has a built-in timer for automatic scheduling.</li> <li>Serverless:  There's no need to fret about server infrastructure. Just focus on writing the correct code.</li> <li>The V2 programming model makes it easier and to create Azure functions. See section below.</li> </ul>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#challenges","title":"Challenges:","text":"<ul> <li> <p>Time Limit: There's a limit to how long Azure Functions can run. If you have a big file or slow processing, it might not finish in time.</p> </li> <li> <p>Not for Heavy Work: Azure Functions is good for small tasks. If you're doing a lot of heavy calculations or have very big files, it might not be the best choice.</p> </li> <li> <p>Slow Start: If your function is not used for a while and then suddenly starts, it might take a bit more time to begin, which can delay your processing.</p> </li> </ul> <p>For tasks that involve heavy data manipulation, transformation, and analysis, Databricks often becomes a preferred choice due to its scale-out architecture, optimized data processing capabilities, advanced dataframe support, built-in data cleansing tools, integrated machine learning libraries, and robust resource management. In contrast, for simpler tasks like just converting files, and when the data volume isn't immense, Azure Functions can offer a speedy and cost-effective approach.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#azure-functions-v2-python-programming-model-vs-v1","title":"Azure Functions: V2 Python Programming Model Vs V1","text":"<p>The V2 programming model for Python, gives more Python-centric development experience for Azure Functions. Here are some key points about the V2 model:</p> <ul> <li>Need fewer files for a function app, so you can have many functions in one file.</li> <li>Decorators are used instead of the <code>function.json</code> file for triggers and things.</li> <li>Blueprints are a new thing in V2. They help group functions in an app.</li> <li>With blueprints, functions aren't directly indexed. They need to be registered first.</li> <li>All functions go in one <code>function_app.py</code> file, no need for many folders.</li> <li>No need for the <code>function.json</code> file now. Decorators in the <code>function_app.py</code> file do the job.</li> </ul>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#the-folder-structure-of-azure-functionsv2","title":"The folder structure of Azure Functions(V2)","text":"<p>This is how the project folder structure looks like:</p> <p></p> <p>Here is what each item means:</p> &lt;project_root&gt;/ \u2502 \u251c\u2500\u2500 \ud83d\udcc1 .venv/ - (Optional)  Python virtual environment. \u2502 \u251c\u2500\u2500 \ud83d\udee0 .venv/pyvenv.cfg - Local python, version, command. \u2502 \u251c\u2500\u2500 \ud83d\udcc1 .vscode/ - (Optional) VS Code config. \u2502 \u251c\u2500\u2500 \ud83d\udc0d function_app.py - Default location for functions. \u2502 \u251c\u2500\u2500 \ud83d\udc0d additional_functions.py - (Optional) Additional functions. \u2502 \u251c\u2500\u2500 \ud83d\udcc1 tests/ - (Optional) Test cases. \u2502 \u251c\u2500\u2500 \ud83d\udee0 .funcignore - (Optional) Declares ignored files. \u2502 \u251c\u2500\u2500 \ud83c\udf10 host.json -  Configuration options. \u2502 \u251c\u2500\u2500 \ud83c\udfe0 local.settings.json -  Local app settings. \u2502 \u251c\u2500\u2500 \ud83d\udcc4 requirements.txt - Python packages for Azure. \u2502 \u2514\u2500\u2500 \ud83d\udc33 Dockerfile - (Optional)  Custom container. <p>I will try to modify the section later to give you a better understanding of the project structure.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#what-is-this-azurite","title":"What is this Azurite","text":"<p>When you click F5 you will see a message which looks like the one below.  </p> <p></p> <p>This is where Azurite comes into play. Azurite is a free tool to mimic Azure Storage on your computer. It helps in testing Azure storage without actually using the real Azure services. It saves money,can work offline, its safe, and quick.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#what-happens-during-debugging","title":"What happens during debugging","text":"<p>Here are the events that take place when you debug an Azure Function using VS Code:</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#azure-function-core-tools-warms-up","title":"Azure Function Core Tools Warms Up","text":"<ul> <li>The Azure Functions Core Tools will set up the libraries mentioned in the <code>requirements.txt</code> file to the virtual environment's .venv\\lib\\site-packages using the command:     <pre><code>Executing task: .venv\\Scripts\\python -m pip install -r requirements.txt\n</code></pre> </li> <li>Then the virtual environment is activated with <code>.venv\\Scripts\\activate</code>.</li> <li>It starts the debugger using <code>func host start</code> </li> <li>Then it attaches to the Azure Function runtime, loads the Azure Function app and set a breakpoint at the first line of the code.</li> <li>You'll notice two main things in the output:<ul> <li>Functions: This lists down all functions in your Azure Function app.</li> <li>A line like <code>[2023-10-25T04:16:47.402Z] Host lock lease acquired by instance ID '0000000000000000000000002B26484C'</code>, tells that the debugger has locked  the Azure Function host. This lock prevents the Azure Function host from being restarted by another process while the debugger is attached. </li> </ul> </li> </ul>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#user-clicks-execute-function-now","title":"User Clicks <code>Execute Function Now..</code>","text":"<ul> <li>Now the user right-clicks on the function and clicks <code>Execute function Now..</code>. This executes the function.   </li> <li>The rest is stepping through the function and checking if all is working fine. And, finally the debugging completes.</li> </ul>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#common-errors","title":"Common Errors","text":""},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/HttpTriggered/nav2_AzureFunctions.html#azurewebjobsstorage-app-setting-is-not-present","title":"\"AzureWebJobsStorage\" app setting is not present","text":"<p>The \"AzureWebJobsStorage\" app setting error indicates that our Azure Functions app is missing a crucial configuration related to the storage account connection string. This could also be realted to the following deployment failure message</p> <pre><code>12:23:58 PM FetchWeatherAzFunc: Deployment Log file does not exist in /tmp/oryx-build.log\n12:23:58 PM FetchWeatherAzFunc: The logfile at /tmp/oryx-build.log is empty. Unable to fetch the summary of build\n12:23:58 PM FetchWeatherAzFunc: Deployment Failed. deployer = ms-azuretools-vscode deploymentPath = Functions App ZipDeploy. Extract zip. Remote build.\n12:24:00 PM FetchWeatherAzFunc: Deployment failed.\n12:35:57 PM FetchWeatherAzFunc: Starting deployment.\n</code></pre> <p>To resolve this:</p> <ul> <li> <p>Create or Identify a Storage Account: If you don't already have an Azure Storage account, create one in the same region as our function app.</p> </li> <li> <p>Get the Storage Account Connection String: Navigate to the Azure Storage account in the Azure Portal. Under the \"Settings\" section, click on \"Access keys.\" Here, you'll find two keys (key1 and key2) each with a connection string. You can use either of these connection strings for the next step.</p> </li> <li> <p>Update Function App Settings:</p> </li> <li>Navigate to our Azure Functions app in the Azure Portal.</li> <li>Under the \"Settings\" section, click on \"Configuration.\"</li> <li>In the \"Application settings\" tab, locate or create the <code>AzureWebJobsStorage</code> setting.</li> <li>Add the setting:<ul> <li>Name: <code>AzureWebJobsStorage</code></li> <li>Value: [our Azure Storage Account connection string from step 2]</li> </ul> </li> <li> <p>Click \"OK\" or \"Save\" to add the setting and save our changes on the main configuration page.</p> </li> <li> <p>Restart our Function App: After adding the necessary setting, restart our function app for the changes to take effect. Following these steps will resolve the error related to the \"AzureWebJobsStorage\" app setting.</p> </li> </ul> <p>\u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc.html","title":"Project AzureSkyWeather. Part 1B: Using Azure Timer-Triggered Function","text":""},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc.html#overview","title":"Overview","text":"<p>In Part 1A, we used Azure HTTP Function to get the weather from <code>weatherapi.com</code>. We also used Azure Logic Apps to set when to fetch this weather data. Now, I'll show a different way using just Azure Functions with a timer. This method is simpler because everything is in one place, and it might save some money compared to using both Azure Functions and Logic Apps.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc.html#setting-the-pre-requisites","title":"Setting the pre-requisites","text":"<p>In Part 1A, we explained how to set up the VS environment and other starting tasks. We'll skip those since they're the same. Now, we'll focus on coding. Both HTTP-Triggered and Timer-Triggered functions have the same project structure, but the content in the <code>function_app.py</code> file is different.</p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc.html#create-the-python-based-azure-function","title":"Create the Python-based Azure Function","text":"<p>To create an Timer-Triggered Azure Function in Visual Studio Code, follow these steps:</p> <ul> <li>Open Visual Studio Code and access the Azure icon in the Activity bar.</li> <li>In the Workspace (local) area, click the thunder button, and select Create New Project.</li> </ul> <p></p> <ul> <li>Choose a local folder location for our project.</li> </ul> <p></p> <ul> <li>Select Python as the programming language.</li> </ul> <p></p> <ul> <li>Opt for Model V2 as the programming model.</li> </ul> <p></p> <ul> <li> <p>Choose the Python environment that you intend to use. Make sure to select the correct environment with all the required dependencies and packages e.g. Azure Functions.</p> </li> <li> <p>Select Timer trigger</p> </li> </ul> <p></p> <ul> <li>Provide a unique name for our function.</li> </ul> <p></p> <ul> <li>VS Code will generate a complete project structure, including all the necessary components for developing our function.</li> </ul> <p></p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc.html#write-our-custom-logic-in-the-azure-function","title":"Write our custom logic in the Azure Function","text":"<p>Now, update <code>function_app.py</code> with the code below. Ensure you input <code>&lt;your_connection_string_to_Azure_storage&gt;</code> and <code>&lt;weather_api_endpoint&gt;</code> details.</p> <pre><code>\"\"\"\n-------------------------------------------------------------------------------\nAuthor: Das\nDate: Oct 2023\nDescription: This script contains an Azure Function designed to fetch the current \nweather data for London from the Weather API. The acquired data is subsequently stored \nin Azure Blob Storage, with each hour generating a distinct file based on the current \nweather conditions.\n\nNOTES: \n- Do not adjust the import structure to:\n    - Avoid \"module 'azure.storage.blob' has no attribute 'from_connection_string'.\"\n    - Ensure 'from datetime import datetime' remains, otherwise AttributeError: module 'datetime' has no attribute 'now' \n-------------------------------------------------------------------------------\n\"\"\"\n\nimport logging, requests\nfrom datetime import datetime\nimport azure.functions as func\nfrom azure.storage.blob import BlobServiceClient\n\napp = func.FunctionApp()\n\n# schedule=\"0 */5 * * * *\" triggers every five minutes. Tested and working.\n# schedule=\"0 0 * * * *\" triggers every hour.\n# schedule=\"0 3 * * * *\" triggers every day at 3 AM.\n\n@app.schedule(schedule=\"0 0 * * * *\", arg_name=\"myTimer\", run_on_startup=True, use_monitor=False) \ndef FetchWeatherTimerTriggered(myTimer: func.TimerRequest) -&gt; None:\n    if myTimer.past_due:\n        logging.info('The timer is past due!')\n\n    logging.info('Python timer trigger function executed.')\n\n    # Custom code - Start\n\n    # Generate a filename based on the current hour to separate hourly weather data.\n    # The file will be overwritten, overwrite=True below, if the function is triggered multiple times within the same hour.\n    weatherFileName = datetime.now().strftime(\"%Y-%m-%d-%H.json\")\n\n    # Initialize BlobServiceClient using the given connection string.\n    # Storage Act, Access Keys, key1 -&gt; Connection string(Copy)\n    # Note, here we are using a different container, \"weather-timer\". Just to separate it from HTTP-Triggered code.\n\n    connection_string = \"&lt;your_connection_string_to_Azure_storage&gt;\"\n    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n    blob_client = blob_service_client.get_blob_client(container=\"weather-timer\", blob=weatherFileName)\n\n    \"\"\"\n        As per the website. Their API Endpoint URL structure is like: http://api.weatherapi.com/v1/current.json?key=YOUR_API_KEY&amp;q=LOCATION\n    \"\"\"\n    # Use the requests library in Python to fetch data from the constructed endpoint:\n    response = requests.get(\"&lt;weather_api_endpoint&gt;\")\n    if response.status_code != 200:\n        logging.error('Failed to fetch weather data.')\n        return func.HttpResponse(\"Failed to fetch weather data.\", status_code=500)\n\n    data = response.json()\n\n    # Attempt to store the fetched weather data in Azure Blob Storage.\n    try:\n        blob_client.upload_blob(str(data), overwrite=True)\n    except Exception as e:\n        logging.error(f\"Error uploading data to Blob Storage: {e}\")\n        return func.HttpResponse(\"Error storing weather data.\", status_code=500)\n\n    # Custom code - End\n</code></pre>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc.html#include-azure-storage-blob-in-requirementstxt","title":"Include  <code>azure-storage-blob</code> in requirements.txt","text":"<p>We've wrapped up the main coding. Now, just add <code>azure-storage-blob</code> to <code>requirements.txt</code> to instruct the deployment to install the library in Azure like we did in Part 1A.</p> <p></p>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc.html#test-our-azure-function","title":"Test Our Azure Function","text":"<ul> <li> <p>It's time to test our Azure Function. Follow these steps:</p> </li> <li> <p>Open Visual Studio Code and press <code>Ctrl+Shift+P</code>. Select Azurite: Start.</p> <p></p> <p>This action starts the Azurite Blob Service. You can check the status at the bottom right corner of VS Code, where you should see something like this:</p> <p></p> </li> <li> <p>Start debugging by pressing <code>F5</code>. Then, on the left, click the Azure icon. Navigate to the workspace and locate our function (refresh if needed). Right-click the function and select Execute.</p> <p></p> </li> <li> <p>If the execution is successful, you'll see an output similar to this:</p> <p></p> <p>Additionally, a JSON file will be created in our container:</p> <p></p> </li> </ul>"},{"location":"DE-Projects/AzureSkyWeather/1_Ingestion/TimerTriggered/TimerTriggered_AzureFunc.html#deployment-and-rest-of-the-steps","title":"Deployment and rest of the steps","text":"<p>The next steps are like Part 1A. Check there for more details.</p> <p>\u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com</p>"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details.html","title":"Transformation","text":""},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details.html#perform-data-validation-and-quality-checks","title":"Perform Data Validation and Quality Checks","text":"<p>Before you organize your data into a specific model or format, it's crucial to ensure the data is correct and of high quality. Validation includes checking for correct data types, missing or null values, adherence to a predefined schema, and other business rule validations. This step is essential to avoid the \"garbage in, garbage out\" problem.</p>"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details.html#data-validation-and-quality-check-strategy","title":"Data Validation and Quality Check Strategy","text":""},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details.html#validation-points","title":"Validation Points:","text":"<ol> <li>Schema Validation:</li> <li> <p>Ensure fields like <code>temp_c</code>, <code>lat</code>, <code>lon</code>, etc., are of correct data types (e.g., float, integer).</p> </li> <li> <p>Range Checks:</p> </li> <li> <p>Validate values within expected ranges, e.g., latitude between -90 and 90.</p> </li> <li> <p>Consistency Checks:</p> </li> <li> <p>Cross-check related data points (e.g., <code>temp_c</code> and <code>temp_f</code> should match in Celsius/Fahrenheit conversions).</p> </li> <li> <p>Temporal Consistency:</p> </li> <li> <p>Check for correct time zone conversions and alignment of <code>localtime</code>.</p> </li> <li> <p>Missing Data:</p> </li> <li> <p>Look for nulls or missing values where not expected.</p> </li> <li> <p>Duplication Checks:</p> </li> <li>Ensure there are no duplicate records.</li> </ol>"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details.html#script-overview","title":"Script Overview","text":"<p>This script will use a combination of <code>great-expectations</code>, <code>Pandera</code>, and <code>Pydantic</code> for data validation, and is designed to be run in a Spark environment. The objective is to use of these popular validation libraries.</p> <ol> <li>Data Reading: Leverage Spark to read JSON files from ADLS.</li> <li>Validation: </li> <li>Use <code>Pydantic</code> for schema validation.</li> <li>Implement <code>Pandera</code> for range and consistency checks.</li> <li>Utilize <code>great-expectations</code> for more complex validations like temporal consistency, missing data, and duplication checks.</li> <li>Data Modeling: Define a data model, perhaps a star schema for data warehousing.</li> <li>Parquet Conversion: Prepare for converting the validated and modeled data into Parquet (to be detailed later).</li> </ol>"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details.html#python-script-skeleton","title":"Python Script Skeleton","text":"<pre><code>from pyspark.sql import SparkSession\nfrom pydantic import BaseModel, validator\nfrom pandera import DataFrameSchema, Column, Check\nimport great_expectations as ge\nimport pandas as pd\n\n# Part 1: Spark Session Initialization\nspark = SparkSession.builder.appName(\"WeatherDataProcessor\").getOrCreate()\n\n# Part 2: Read JSON files from ADLS (based on your ADLSSorter script)\n# [ ... Existing ADLSSorter code to fetch files ... ]\n\n# Pydantic model for schema validation\nclass WeatherData(BaseModel):\n    lat: float\n    lon: float\n    temp_c: float\n    temp_f: float\n    localtime_epoch: int\n\n    @validator('temp_c')\n    def temp_celsius_fahrenheit_match(cls, v, values, **kwargs):\n        # Validate temp_c and temp_f conversion\n        temp_f = values.get('temp_f')\n        if temp_f and abs(temp_f - (v * 9/5 + 32)) &gt; 0.1:\n            raise ValueError('temp_c and temp_f do not match')\n        return v\n\n# Pandera schema for range and consistency checks\nweather_schema = DataFrameSchema({\n    \"lat\": Column(float, Check(lambda x: -90 &lt;= x &lt;= 90)),\n    \"lon\": Column(float, Check(lambda x: -180 &lt;= x &lt;= 180)),\n    # ... additional range checks ...\n})\n\n# Great Expectations for more advanced checks\ndef validate_with_great_expectations(df):\n    # Define expectations (temporal consistency, missing data, etc.)\n    # ge_df = ge.from_pandas(df)\n    # ge_df.expect_column_values_to_be_between(...)\n    # ... define other expectations ...\n    pass\n\n# Main processing loop\nfor file_path in old_files:\n    # Read data\n    df = spark.read.json(file_path)\n    pd_df = df.toPandas()\n\n    # Validate data\n    # Pydantic\n    try:\n        WeatherData(**pd_df.to_dict(orient=\"list\"))\n    except ValidationError as e:\n        print(f\"Validation error: {e}\")\n\n    # Pandera\n    try:\n        weather_schema.validate(pd_df)\n    except SchemaError as e:\n        print(f\"Schema validation error: {e}\")\n\n    # Great Expectations\n    validate_with_great_expectations(pd_df)\n\n    # Data Modeling and Transformation (to be defined)\n    # ...\n\n# Note: Conversion to Parquet will be handled in the next phase.\n</code></pre>"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details.html#create-a-data-model","title":"Create a Data Model","text":"<p>Once you're confident in your data quality, the next step is to structure or model the data. This could mean transforming raw data into a more meaningful and usable format, aligning it with a dimensional model (like a star schema), or preparing it for specific analytical needs. This stage is where you'd typically perform operations like filtering, grouping, aggregating, or joining different data sets.</p>"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details.html#benefits-of-a-data-model","title":"Benefits of a data model","text":"<p>Storing files as Parquet without a specific schema or structure is okay if you're merely archiving data or doing simple, infrequent analytics. However, for more complex analytics and reporting, creating a proper data model can have several benefits, like:</p> <p>a) Faster Query Performance: - By organizing data into a structured model (like star or snowflake schema), you can optimize query performance. The data model reduces the amount of data scanned during queries.</p> <p>b) Understandability and Consistency: - Having a defined schema makes it easier for data scientists, analysts, and other stakeholders to understand the data. It ensures everyone is working with data in a consistent manner.</p> <p>c) Joining Multiple Data Sources: - If you have (or plan to have) multiple data sources, a structured data model simplifies joining them. For instance, weather data can be enriched with location data, demographic data, etc.</p> <p>d) Data Integrity: - A structured data model, especially when coupled with a database or data warehouse, can ensure data integrity with primary and foreign key constraints.</p> <p>e) Improved Data Quality: - Data models can have defined constraints, ensuring that incoming data meets specific quality standards.</p> <p>Star Schema: - A commonly used schema in data warehousing. It includes a central fact table (e.g., hourly weather measurements) and surrounding dimension tables (e.g., location, date, time). It's simple and often results in fast query performances.</p> <p>Snowflake Schema: - A normalized version of the star schema. It can save storage but might result in more complex queries.</p>"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details.html#consideration","title":"Consideration","text":"<p>When deciding on whether to structure your Parquet files according to a data model, consider: - The types of queries you'll be running. - The expected volume of data. - The frequency of data access. - Whether you plan to integrate with other data sources in the future.</p>"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details.html#conversion-to-parquet","title":"Conversion to Parquet:","text":"<p>After the data is validated and properly modeled, converting it into an efficient storage format like Parquet is the final step. Parquet is a columnar storage format, offering efficient data compression and encoding schemes. This format is optimal for analytic querying performance and works well with big data technologies. Converting to Parquet after validation and modeling ensures that you're storing high-quality, well-structured data, making your analytics processes more efficient.</p>"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details.html#conversion-to-parquet-strategy","title":"Conversion to parquet Strategy:","text":"<p>When considering a robust data storage strategy, especially for data analytics and long-term storage, the structure and features of Parquet come into play. Parquet is a columnar storage file format, which is optimized for use with big data processing tools like Apache Spark, Apache Hive, and many others. Here's what you should consider for the best Parquet structure:</p> <ol> <li> <p>Columnar Storage: Take advantage of Parquet's columnar storage format. This means when querying specific columns, only those columns' data will be read, resulting in improved performance and reduced I/O.</p> </li> <li> <p>Schema Evolution: One of the significant advantages of Parquet is its ability to handle schema evolution. Make sure your solution can accommodate changes to the schema over time without affecting the existing data.</p> </li> <li> <p>Compression: Parquet supports various compression techniques like SNAPPY, GZIP, and more. Depending on your analytics use-case, select the compression method that offers a good trade-off between storage cost and query performance.</p> </li> <li> <p>Partitioning: For your use-case, since you are already organizing by year, month, day, and hour, you should partition the Parquet files this way. This will speed up query times since only the relevant partitions need to be read.</p> </li> <li> <p>Example: <code>/year=2023/month=10/day=17/hour=13/data.parquet</code></p> </li> <li> <p>Row Group Size: Parquet organizes data into smaller row groups, allowing for more efficient column pruning. Adjusting the size of row groups can have a performance impact. The default is typically 128MB, but you might want to adjust based on your typical query patterns.</p> </li> <li> <p>Metadata: Parquet files store metadata about the data they contain, which helps in understanding the schema and optimizing queries. Ensure this metadata is kept up-to-date.</p> </li> <li> <p>Consistent Schema: Ensure that the schema for your Parquet files remains consistent, especially if you're ingesting data regularly. Any changes in the incoming JSON schema should be handled gracefully.</p> </li> <li> <p>Data Lake Integration: Since you're using Azure Data Lake Storage, ensure that the tools you're using for analytics are well-integrated with ADLS and can take full advantage of the features both ADLS and Parquet provide.</p> </li> <li> <p>Regular Compaction: Over time, as data gets updated or deleted, you might end up with many small Parquet files. This is suboptimal for query performance. Implement a regular compaction process to combine these smaller files into larger, more efficient Parquet files.</p> </li> <li> <p>Avoiding Too Many Small Files: If your ingestion process creates too many small files, it can degrade performance. Consider batching incoming data to create larger Parquet files.</p> </li> </ol> <p>Given the JSON structure you provided, you might want to flatten it out a bit for more effective columnar storage, unless you're often querying multiple subfields of <code>location</code> or <code>current</code> together. The columnar nature of Parquet means that nesting can sometimes reduce performance benefits, especially if the data is queried column-by-column.</p>"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details.html#appendix","title":"Appendix","text":""},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details.html#data-processing-libraries-overview","title":"Data Processing Libraries Overview","text":""},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details.html#data-validation-libraries","title":"Data Validation Libraries","text":"Library Why Use Cases Great Expectations Provides a robust suite for JSON data testing, documentation, and validation, ideal for complex structures with clear, definable rules and expectations, ensuring type, range, structure, and content accuracy. Works with both spark and pandas dataframes. Extensive data quality checks, complex validation rules, data documentation. Pandera Provides a flexible and expressive API for pandas DataFrame validation, allowing for easy statistical checks, type validation, and more. When working with Pandas for data manipulation and needing validation tightly coupled with these operations. Pydantic Used primarily for data parsing and validation with a strong emphasis on strict type validations via Python type annotations. Best for scenarios where you are dealing with JSON-like data structures, needing strong type checks and simple data validation."},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details.html#data-modeling-libraries","title":"Data Modeling Libraries","text":"Library Why Use Cases Pandas Extremely popular for data manipulation and analysis in Python, with a very straightforward, user-friendly API. Pandas is a Python library for data manipulation and analysis. It is well-suited for working with small to medium-sized datasets on a single machine. PySpark Best for large-scale data processing. It can handle very large datasets that don't fit into a single machine's memory. Large datasets, needing distributed computing, or integrating with other components in a big data ecosystem."},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details.html#data-format-conversion-libraries","title":"Data Format Conversion Libraries","text":"Library Why Use Cases PyArrow PyArrow provides a bridge between the columnar storage format Parquet and Python data analysis libraries. It's fast and efficient. High-speed data serialization/deserialization, working with Parquet files, large datasets. Pandas Directly supports reading and writing Parquet files (though it uses PyArrow or fastparquet under the hood). If you're already using Pandas for data manipulation, converting to/from Parquet is very straightforward."},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details.html#conclusion","title":"Conclusion","text":"<ul> <li>Great Expectations was chosen for its comprehensive data validation and documentation capabilities, key for ensuring data quality in analytics and reporting pipelines.</li> <li>PyArrow is recommended for handling large data volumes and efficient Parquet format conversion due to its performance and direct support for the format.</li> <li>Pandas can be a streamlined choice for moderate-sized data, offering both data manipulation and validation (with Pandera for added validation support), and easy Parquet conversion.</li> </ul>"},{"location":"DE-Projects/AzureSkyWeather/2_Transformation/Solution_Details.html#choice-of-platform","title":"Choice of Platform","text":"Stage Objective Azure Product Data Validation Ensure JSON consistency, completeness, and data types. Azure Databricks (Use PySpark for large-scale data processing and validation) Conversion to Parquet Convert JSON to Parquet for efficient storage/querying. Azure Databricks (Native support for JSON &amp; Parquet via PySpark) Organize Data into a Data Model Structure Parquet files for optimal analytics/reporting. Azure Data Factory (For complex transformation logic)  Azure Databricks (PySpark DataFrame API for reshaping data) Storage Store transformed Parquet files securely and efficiently. Azure Data Lake Storage Gen2 (Optimized for Azure analytics platforms) Analytics &amp; Querying Run analytics and queries on data. Azure Synapse Analytics (For massive parallel processing)  Azure Databricks (For deeper analytics/ML) Monitoring and Maintenance Monitor pipeline health and performance. Azure Monitor and Azure Log Analytics (Full-stack monitoring, advanced analytics) Data Security Secure data at rest and in transit. Azure Data Lake Storage Gen2 (Encryption at rest)  Azure Key Vault (Manage cryptographic keys/secrets) Automation and Scheduling Automate the pipeline processes. Azure Data Factory (Define and orchestrate data-driven workflows) <p>\u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com</p>"},{"location":"DE-Projects/JsonValidator/AzureFunction-ValidateJSOns.html","title":"Clean and Validate JSON Using Azure Functions","text":""},{"location":"DE-Projects/JsonValidator/AzureFunction-ValidateJSOns.html#introduction","title":"Introduction","text":"<p>In a project, I encountered numerous ASCII-formatted JSON files with single quotes stored in a ADLS Container, rendering the format invalid. My task involved converting these files to UTF-8, replacing the quotes, validating them against a JSON schema, and relocating them to different containers. For this, I used Azure Functions, which is efficient for smaller datasets, but for larger ones, I recommend using Spark or Databricks.</p>"},{"location":"DE-Projects/JsonValidator/AzureFunction-ValidateJSOns.html#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Create the JSON Schema: For well-strucuctured JSONs we can create the schema easily. You can take help from sites liks transform.tools.</li> <li>Azure Function Setup: To create an auto-scheduled function you will need to create a timer-triggered Azure function. You can refer to my article for details.</li> <li>Data Lake Storage Configuration: My example works with ADLS Gen2. The code should also work with simple azure blob storage.</li> </ul>"},{"location":"DE-Projects/JsonValidator/AzureFunction-ValidateJSOns.html#the-code","title":"The Code","text":"<p>Here's the code for <code>function_app.py</code>. Please enter your details as required in the placeholders.</p> <pre><code>import requests, logging, json\nfrom jsonschema import validate\nfrom jsonschema.exceptions import ValidationError\nfrom datetime import datetime\nimport azure.functions as func\nfrom azure.storage.blob import BlobServiceClient\n\ndef is_valid_json(data, schema):\n    try:\n        validate(instance=data, schema=schema)\n        return True\n    except ValidationError as e:\n        logging.error(f\"Validation error: {e}\")\n        return False\n\napp = func.FunctionApp()\n\n@app.schedule(schedule=\"&lt;your CRON, e.g. 0 0 * * * *&gt;\", arg_name=\"myTimer\", run_on_startup=True,\n              use_monitor=False) \ndef AzFuncCheckNMoveJson(myTimer: func.TimerRequest) -&gt; None:\n    if myTimer.past_due:\n        logging.info('The timer is past due!')\n\n    logging.info('Python timer trigger function executed.')\n\n    \"\"\"\n    Initialize BlobServiceClient using the given connection string.\n    Storage Act, Access Keys, key1 -&gt; Connection string(Copy)\n    \"\"\"\n    blob_service_client = BlobServiceClient.from_connection_string(\"DefaultEndpointsProtocol=https;AccountName=&lt;your_storage_act_name&gt;;AccountKey=&lt;your_account_key&gt;;EndpointSuffix=core.windows.net\")\n\n    # Fetch the schema\n    schema_blob_client = blob_service_client.get_blob_client(container=\"schema\", blob=\"JSON_schema.json\")\n    try:\n        schema_json = json.loads(schema_blob_client.download_blob().readall())\n    except Exception as e:\n        logging.error(f\"Error fetching schema: {e}\")\n\n\n   # Iterate over blobs in the \"weather-http\" container\n    container_client = blob_service_client.get_container_client(\"&lt;raw_container&gt;\")\n    for blob in container_client.list_blobs():\n        try:\n            blob_client = blob_service_client.get_blob_client(container=\"weather-http\", blob=blob.name)\n            data_str = blob_client.download_blob().readall().decode('utf-8')\n\n            data_str = data_str.replace(\"'\", '\"')\n\n            try:\n                data = json.loads(data_str)\n            except json.JSONDecodeError:\n                data = json.loads(blob_client.download_blob().readall().decode('utf-8'))\n\n            # Validate the JSON data\n            if is_valid_json(data, schema_json):\n                target_container = \"silver\"\n            else:\n                target_container = \"error\"\n\n            # Move blob to the target container\n            target_blob_client = blob_service_client.get_blob_client(container=target_container, blob=blob.name)\n            target_blob_client.upload_blob(json.dumps(data), overwrite=True)\n            blob_client.delete_blob()  # Delete the original blob after moving\n\n        except Exception as e:\n            logging.error(f\"Error processing blob {blob.name}: {e}\")\n\n    logging.info(\"Processing complete.\")\n</code></pre>"},{"location":"DE-Projects/JsonValidator/AzureFunction-ValidateJSOns.html#requirementstxt","title":"requirements.txt","text":"<p>The following entries should be there in <code>requirements.txt</code> file</p> <p>azure-functions  requests  azure-storage-blob jsonschema</p>"},{"location":"DE-Projects/JsonValidator/AzureFunction-ValidateJSOns.html#conclusion","title":"Conclusion","text":"<p>The use of jsonschema in this project proved invaluable for efficient JSON validation, eliminating the need for iterative item-by-item examination. This method enhanced both speed and accuracy. While Azure Functions were a good choice for our dataset size, they're best for smaller datasets. For larger volumes, solutions like Spark with Azure Data Lake Storage (ADLS) are recommended.</p>"},{"location":"DE-Projects/JsonValidator/Python-ValidateJSONs.html","title":"Validating JSON Data with Python and JSON Schema","text":""},{"location":"DE-Projects/JsonValidator/Python-ValidateJSONs.html#introduction","title":"Introduction","text":"<p>This is a short tutorial on how to use Python to validate JSON file data. Rather than iterating through all the fields in the JSON, usage of libraries like JSONschema etc. are considered more efficient. Hence, I thought of sharing my experience.</p>"},{"location":"DE-Projects/JsonValidator/Python-ValidateJSONs.html#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Python Installation:     Ensure Python is installed on your system. If not, download and install it from the official Python website.</p> </li> <li> <p>JSON Files:</p> </li> <li>Weather Data File (<code>weather.json</code>): A JSON file containing weather data, typically obtained from weather APIs e.g. weatherapi.</li> <li> <p>JSON Schema File (<code>weather_schema.json</code>): A JSON schema file that defines the expected structure, required fields, and data types of the weather data JSON. Tools like Quicktype or JSON Schema Tool can help generate a schema from a JSON example.</p> </li> <li> <p><code>jsonschema</code> Library:     This Python library is used for validating JSON data against a schema. </p> </li> </ol> <p>Install it using pip:    <code>pip install jsonschema</code></p>"},{"location":"DE-Projects/JsonValidator/Python-ValidateJSONs.html#how-to-run-the-script","title":"How to Run the Script","text":"<ol> <li> <p>Prepare Your JSON Files:     Ensure your <code>weather.json</code> and <code>weather_schema.json</code> are placed in the same directory as your script.</p> </li> <li> <p>Run the Script:     Open Visual Studio Code, create a Python file or a Jupyter notebook</p> <p></p> </li> <li> <p>And copy-paste the code below. Adjust as needed. <pre><code>import json\nfrom jsonschema import validate, ValidationError\n\n# Load the JSON data and schema\nwith open('weather.json', 'r') as file:\n    weather_data = json.load(file)\n\nwith open('weather_schema.json', 'r') as file:\n    schema = json.load(file)\n\n# Function to validate JSON\ndef is_valid_json(data, schema):\n    try:\n        validate(instance=data, schema=schema)\n        return True\n    except ValidationError as ve:\n        print(f\"Validation error: {ve}\")\n        return False\n\n# Validate the weather data\nif is_valid_json(weather_data, schema):\n    print(\"Validation Successful!\")\nelse:\n    print(\"Validation Failed!\")\n</code></pre>    Run and debug using the editor.</p> </li> </ol>"},{"location":"DE-Projects/JsonValidator/Python-ValidateJSONs.html#code-overview","title":"Code Overview","text":"<p>The script consists of the following components:</p> <ol> <li> <p>Loading JSON Data and Schema: We use Python's built-in <code>json</code> module to load the weather data and the schema from their respective files.</p> </li> <li> <p>Validation Logic: Utilizing the <code>jsonschema.validate()</code> function, we check if the JSON data adheres to the schema, capturing any validation errors that might indicate discrepancies.</p> </li> <li> <p>Error Handling: The script identifies and prints specific validation errors, making it easier to pinpoint issues in the data or the schema.</p> </li> </ol>"},{"location":"DE-Projects/JsonValidator/Python-ValidateJSONs.html#conclusion","title":"Conclusion","text":"<p> This code can be further enhanced to be included in an Azure Function. For example, if you are fetching data as JSON format from a web API, this code can be added there to perform data validation. </p> <p>\u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com</p>"},{"location":"DE-Projects/Sparkzure/HomeProjectSparkzure.html","title":"Overview","text":"<p>In this article I will show you how you can connect to Azure Storage and perform blob operations from a Standlone Pyspark Setup. We will make use of Hadoop jars to perform the same.</p>"},{"location":"DE-Projects/Sparkzure/HomeProjectSparkzure.html#how-to-connect-an-localon-premise-pyspark-setup-with-azure-data-lake","title":"How to Connect an local/on-premise Pyspark Setup with Azure Data Lake","text":"<p>We all know that Azure Databricks easily connects with ADLS because they share the same Azure setup. But if you're using a basic Spark setup, it gets a bit tricky because you have to carefully manage some JAR files. In this guide, we'll start our journey by making a simple connection to Azure from a standalone Spark and fetching some basic info. I'll also show you how to set up this connection inside a Docker container. We'll also learn how to connect Visual Studio Code to this Docker, so you can run PySpark easily! Read more...</p>"},{"location":"DE-Projects/Sparkzure/HomeProjectSparkzure.html#method-1-how-to-rearrange-items-in-blob-storage-using-an-localon-premise-pyspark-hadoop-jars","title":"Method 1: How to rearrange items in Blob storage using an local/on-premise Pyspark &amp; Hadoop Jars","text":"<p>In this article, we'll see how to sort files in an Azure Data Lake Container by using a Standalone Spark application. You could use Azure Data Factory, Databricks, or Azure Logic Apps, but this method stands out. It's an alternative and often much cheaper than the other mentioned Azure services. This is a real-world requirement; having a structure like this can make partition pruning more efficient during query time, especially if you're using a system like Apache Hive or Delta Lake. Read more...</p>"},{"location":"DE-Projects/Sparkzure/HomeProjectSparkzure.html#method-2-how-to-rearrange-items-in-blob-storage-using-an-localon-premise-python-and-azure-python-librariessdk","title":"Method 2: How to rearrange items in Blob storage using an local/on-premise Python and Azure Python libraries(SDK)","text":"<p>I the previous article I showed you how to sort files using Spark and Hadoop Jars. Here I will show you how using just straightforward python method in a local setup we can achieve teh same output, i.e, sort the files inside our Azure blob container. This is a real-world-scenario and such well-partioned data structures are required for analysis and Migration. Read more...</p>"},{"location":"DE-Projects/Sparkzure/HomeProjectSparkzure.html#how-to-schedule-our-blob-file-organizer-python-script-usign-azure-timer-trigger-function","title":"How to schedule our Blob file organizer Python script usign Azure Timer-Trigger Function","text":"<p>In the last two articles I showed you and artisinal-appreaoch to organize content inside Azure blob storage. But, we may require to schedule such function. Here I will show you how I converted my Python script[Method2] to a timer-triggered Azure function which runs everyday at 11.30 PM. Read more...</p> <p>\u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com</p>"},{"location":"DE-Projects/StreamKraft/HomeProjectStreamKraft.html","title":"Overview","text":"<ul> <li>StreamKraft: Real-Time Music Data Pipeline. Kafka. SparkStreaming. MongoDB.</li> <li>Project Setup</li> <li>Step 1 - Dataset prepartion for Fake-streaming</li> <li>Step 2 - Fake-streaming to Kafka Topic</li> <li>Step 3 - Reading from Kafka with Spark Streaming and saving to MongoDB</li> <li>Appendix</li> </ul>"},{"location":"DE-Projects/StreamKraft/HomeProjectStreamKraft.html#streamkraft-real-time-music-data-pipeline-kafka-sparkstreaming-mongodb","title":"StreamKraft: Real-Time Music Data Pipeline. Kafka. SparkStreaming. MongoDB.","text":"<p>In this project, we'll be building a real-time data processing pipeline using the million songs dataset. We'll use a Python script along with the dataset to create a fake streaming that will be sent to a Kafka topic. Then, we'll use Spark streaming to receive data from the topic and store it in MongoDB. The entire project will be run in a containerized environment.</p> <p>Along the way, we'll learn how to connect the Spark cluster to the Kafka environment, how to stream data to and from a Kafka topic, and how to use Spark streaming to save data to MongoDB.</p>"},{"location":"DE-Projects/StreamKraft/HomeProjectStreamKraft.html#project-setup","title":"Project Setup","text":"<p>Here is a list of some key components of the project</p> Category Software/Configuration Hardware Mac M1 RAM 16 GB Operating System macOS 14.2 Containerization Docker Spark Cluster Bitnami Spark 3.5 Cluster - 1 Master - 2 Workers Images 1. Bitnami Spark Cluster  2. Kafka Setup  Message Broker Apache Kafka KRaft(Kafka Without Zookeper) (Docker Image: confluentinc/cp-kafka 7.5.1) Programming Language Python 3.11 Integrated Development Environment Visual Studio Code"},{"location":"DE-Projects/StreamKraft/HomeProjectStreamKraft.html#step-1-dataset-prepartion-for-fake-streaming","title":"Step 1 - Dataset prepartion for Fake-streaming","text":"<ul> <li>Download the million dollar summary dataset here </li> <li>Place it in a folder accessible to the code snippet below and run the following script</li> </ul> <pre><code>import json\nimport h5py\nimport numpy as np\n\ndef h5_to_json(h5_file_path, json_file_path):\n    with h5py.File(h5_file_path, 'r') as h5_file:\n        # Access the metadata group\n        metadata = h5_file['metadata']\n        # Access the songs dataset within metadata\n        songs = metadata['songs']\n\n        # Create a list to hold song data\n        data_to_export = []\n\n        # Iterate over each entry in the songs dataset\n        for song in songs:\n            # Convert fields to the correct Python type, e.g., decode bytes to string\n            song_data = {\n                'analyzer_version': song['analyzer_version'].decode('UTF-8') if song['analyzer_version'] else None,\n                'artist_7digitalid': song['artist_7digitalid'].item(),\n                'artist_familiarity': song['artist_familiarity'].item(),\n                'artist_hotttnesss': song['artist_hotttnesss'].item(),\n                'artist_id': song['artist_id'].decode('UTF-8'),\n                'artist_latitude': song['artist_latitude'].item() if song['artist_latitude'] else None,\n                'artist_location': song['artist_location'].decode('UTF-8') if song['artist_location'] else None,\n                'artist_longitude': song['artist_longitude'].item() if song['artist_longitude'] else None,\n                'artist_mbid': song['artist_mbid'].decode('UTF-8'),\n                'artist_name': song['artist_name'].decode('UTF-8'),\n                'artist_playmeid': song['artist_playmeid'].item(),\n                'idx_artist_terms': song['idx_artist_terms'].item(),\n                'idx_similar_artists': song['idx_similar_artists'].item(),\n                'release': song['release'].decode('UTF-8'),\n                'release_7digitalid': song['release_7digitalid'].item(),\n                'song_hotttnesss': song['song_hotttnesss'].item(),\n                'song_id': song['song_id'].decode('UTF-8'),\n                'title': song['title'].decode('UTF-8'),\n                'track_7digitalid': song['track_7digitalid'].item()\n            }\n            data_to_export.append(song_data)\n\n        # Write the data to a JSON file\n        with open(json_file_path, 'w') as json_file:\n            json.dump(data_to_export, json_file, indent=4)\n\n# Replace with your actual file paths\nh5_to_json(r'C:\\Users\\dwaip\\OneDrive\\Work\\Projects\\AzureTuneStream\\dataset\\msd_summary_file.h5', r'C:\\Users\\dwaip\\OneDrive\\Work\\Projects\\AzureTuneStream\\dataset\\msd_summary_file.json')\n</code></pre>"},{"location":"DE-Projects/StreamKraft/HomeProjectStreamKraft.html#step-2-fake-streaming-to-kafka-topic","title":"Step 2 - Fake-streaming to Kafka Topic","text":"<p>Execute the script provided to simulate real-world live streaming to a Kafka topic using data from the million dollar dataset.</p> <p>Note: - Ensure all non-Kafka containers are part of the <code>confluent-kafka_default</code> network, automatically assigned to Kafka Clusters upon creation with docker-compose. - To add non-Kafka containers to the <code>confluent-kafka_default</code> network, use the following command:</p> <pre><code>docker network connect confluent-kafka_default [container_name]\n</code></pre> <p>If the other containres are not added, they will not be able to connect to the broker.</p> <pre><code>\"\"\"\nThis code streams the 700 MB streamify content to kafka.\n1. The container running this code should be part of \"confluent-kafka_default\" network. To add, run this command:\n    connect confluent-kafka_default [external-container-name-or-id]\n2. Only this broker address works \"broker:29092\".\n3. Topics were populated in Kafka and checked at http://localhost:9021\n4. Takes 36 seconds to execute\n\"\"\"\n\"\"\"\nFrequently encountered error: \n    AnalysisException: Failed to find data source: kafka.\n    This error happens and then goes away. Sometimes happens all of a sudden. Repeteadly doing it resolves the error.\n    Solution: Sometimes just restart solves it. Probably a connection problem.\n\n\"\"\"\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, to_json, struct\nimport json\n\n# Initialize a Spark session with enhanced memory settings\nspark = SparkSession \\\n    .builder \\\n    .appName(\"FakeEventStreamerSparkCluster\") \\\n    .master(\"spark://spark-master:7077\") \\\n    .config(\"spark.driver.memory\", \"15g\") \\\n    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0')\\\n    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n    .getOrCreate()\n\n# # AnalysisException: Failed to find data source: kafka.\n\n# Set log level to monitor execution\nspark.sparkContext.setLogLevel(\"INFO\")\nprint(\"Spark session initialized.\")\n\n# Read the JSON file into a Spark DataFrame\nspark_df = spark.read.json('/opt/shared-data/dataset.json', multiLine=True)\n\n# Serialize the DataFrame into a JSON string\nserialized_df = spark_df.select(to_json(struct([col(c) for c in spark_df.columns])).alias('value'))\n\n# Kafka producer settings\nkafka_servers = \"broker:29092\"\nkafka_topic = \"tunestream\"\n\n# Write the DataFrame to Kafka\nserialized_df.write \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_servers) \\\n    .option(\"topic\", kafka_topic) \\\n    .save()\n\nprint(\"Data sent to Kafka.\")\n\nspark.stop()\n</code></pre>"},{"location":"DE-Projects/StreamKraft/HomeProjectStreamKraft.html#step-3-reading-from-kafka-with-spark-streaming-and-saving-to-mongodb","title":"Step 3 - Reading from Kafka with Spark Streaming and saving to MongoDB","text":"<p>Once the data is successfully streaming through the Kafka topic, we can use Spark Streaming to analyze and process the incoming information. This shows a real-time data analysis.</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, to_json, struct\nimport json\n\n# Common errors: Caused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n# Solution: No proper solution. Goes away by itself\n\n# https://mvnrepository.com/artifact/org.mongodb.spark/mongo-spark-connector\n# Initialize a Spark session with enhanced memory settings\nspark = SparkSession \\\n    .builder \\\n    .appName(\"FakeEventStreamerSparkCluster\") \\\n    .master(\"spark://spark-master:7077\") \\\n    .config(\"spark.driver.memory\", \"15g\") \\\n    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n    .getOrCreate()\n\nspark.sparkContext.setLogLevel(\"INFO\") # for verbose comments\n\n# Read from Kafka\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n    .option(\"subscribe\", \"tunestream\") \\\n    .load()\ndf.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n\n# Common error: UnsupportedOperationException: Data source mongo does not support streamed writing.\n# Write to MongoDB\nmongodb_uri = \"mongodb://my-mongodb:27017\" # \"mongodb://&lt;mongoDBContainerNameOriPAddress&gt;:27017\"\ndatabase_name = \"tuneStreamDB\" # Spark will create, if not present\ncollection_name = \"tuneStreamData\" # Spark will create, if not present\n\n# The snippet below gives rise to : UnsupportedOperationException: Data source mongo does not support streamed writing. Follow the workaround shown later.\n# df.writeStream \\\n#     .format(\"mongo\") \\\n#     .option(\"uri\", mongodb_uri) \\\n#     .option(\"database\", database_name) \\\n#     .option(\"collection\", collection_name) \\\n#     .trigger(processingTime=\"10 seconds\") \\\n#     .start() \\\n#     .awaitTermination()\n\n'''\nMongoDB Spark Connector does not currently support streamed writing. As a workaround, we write data to a batch DataFrame and then to MongoDB.\n'''\n# Use foreachBatch to write batch data to MongoDB\nquery = df.writeStream \\\n    .trigger(processingTime=\"10 seconds\") \\\n    .foreachBatch(lambda df, epochId: df.write.format(\"mongo\") \\\n    .option(\"uri\", mongodb_uri) \\\n    .option(\"database\", database_name) \\\n    .option(\"collection\", collection_name) \\\n    .mode(\"append\") \\\n    .save()) \\\n    .start() \\\n\n# Log the query execution plan\nquery.explain()\n# Wait for the termination of the streaming query\nquery.awaitTermination()\nspark.stop()\n</code></pre>"},{"location":"DE-Projects/StreamKraft/HomeProjectStreamKraft.html#appendix","title":"Appendix","text":"<p>Pyspark Structured Streaming.</p> <p>Streaming using a file-based dataset.</p>"},{"location":"DevOps/2.1_Self-Hosted_Agent_Windows.html","title":"Installing a Self-Hosted Agent on Local Windows(No Container)","text":""},{"location":"DevOps/2.1_Self-Hosted_Agent_Windows.html#download-and-configure-the-agent","title":"Download and Configure the Agent","text":"<p>Log on to the machine using the account for which you've prepared permissions.</p>"},{"location":"DevOps/2.1_Self-Hosted_Agent_Windows.html#sign-in-to-azure-pipelines","title":"Sign In to Azure Pipelines","text":"<ol> <li>Open your web browser and sign in to Azure DevOps.</li> <li>Navigate to your organization settings: <code>https://dev.azure.com/{your-org}</code>.</li> </ol>"},{"location":"DevOps/2.1_Self-Hosted_Agent_Windows.html#navigate-to-agent-pools","title":"Navigate to Agent Pools","text":"<ol> <li>Choose Azure DevOps &gt; Organization settings.</li> <li>Select Agent pools.</li> </ol> <ol> <li>Choose the Agent pools tab.</li> </ol> <ol> <li>Select the Default pool, go to the Agents tab, and choose New agent.</li> </ol>"},{"location":"DevOps/2.1_Self-Hosted_Agent_Windows.html#download-the-agent","title":"Download the Agent","text":"<ol> <li>In the Get the agent dialog box, choose Windows.</li> <li>On the left pane, select the processor architecture of your Windows OS (x64 for 64-bit Windows, x86 for 32-bit Windows).</li> <li>On the right pane, click the Download button.</li> </ol>"},{"location":"DevOps/2.1_Self-Hosted_Agent_Windows.html#unpack-the-agent","title":"Unpack the Agent","text":"<ol> <li>Follow the instructions on the page to download the agent.</li> <li>Unpack the agent into a directory without spaces in its path (e.g., <code>C:\\agents</code>).</li> </ol> <p>Put the extracted contents directly inside the <code>C:\\agents</code> folder.</p> <p></p>"},{"location":"DevOps/2.1_Self-Hosted_Agent_Windows.html#configure-the-agent","title":"Configure the Agent","text":"<ol> <li>Open a Command Prompt in the <code>C:\\agents</code> directory.</li> <li>Run the <code>config.cmd</code> script.</li> <li>When setup asks for your server URL, enter <code>https://dev.azure.com/{org-Name}</code> (e.g., <code>https://dev.azure.com/MOUMITA001/</code>).</li> <li>Choose PAT (Personal Access Token) for authentication and enter your PAT.</li> </ol>"},{"location":"DevOps/2.1_Self-Hosted_Agent_Windows.html#starting-an-agent","title":"Starting an Agent","text":"<p>To start the agent, run the <code>run.cmd</code> script in the <code>C:\\agents</code> directory.</p> <p></p>"},{"location":"DevOps/2.1_Self-Hosted_Agent_Windows.html#removing-an-agent","title":"Removing an Agent","text":"<p>To remove the agent, follow these steps:</p> <ol> <li>Stop the agent by closing the Command Prompt window running the agent.</li> <li>Open a new Command Prompt in the <code>C:\\agents</code> directory.</li> <li>Run the <code>config.cmd remove</code> command.</li> </ol> <p></p> <p></p> <p></p>"},{"location":"DevOps/2.2_Self-Hosted_Agent_Windows_Container.html","title":"Create a Self-Hosted Agent in Docker Windows Container","text":"<p>You can run your Azure Pipeline agent in Windows, Windows Container, and Ubuntu Container.</p> <p> This guide will show you how to set up your Azure Pipeline agent in a Windows Container. </p>"},{"location":"DevOps/2.2_Self-Hosted_Agent_Windows_Container.html#setup","title":"Setup","text":"<p>We'll use a Windows laptop/desktop with Docker Desktop installed. First, we'll switch Docker Desktop to Windows Mode (it runs in Linux Mode by default) because the container will be a Windows OS.</p> <p>Then we'll create the image, and the rest will be easy.</p>"},{"location":"DevOps/2.2_Self-Hosted_Agent_Windows_Container.html#install-docker-for-windows","title":"Install Docker for Windows","text":"<p>First, make sure Docker Desktop is installed on your Windows machine.</p>"},{"location":"DevOps/2.2_Self-Hosted_Agent_Windows_Container.html#switch-docker-to-use-windows-containers","title":"Switch Docker to Use Windows Containers","text":"<p>Note: Docker can run both Linux and Windows containers. By default, Docker runs in Linux mode. For more details, refer to this guide.</p> <p>In PowerShell, run this command:</p> <p>Note: If you have existing Linux containers, they will disappear when you switch to Windows mode but will return when you switch back to Linux mode. You will get a new Docker desktop!</p> <pre><code>&amp; $Env:ProgramFiles\\Docker\\Docker\\DockerCli.exe -SwitchDaemon\n</code></pre> <p></p> <p>To switch back to Linux mode later, refer to the image below:</p> <p></p>"},{"location":"DevOps/2.2_Self-Hosted_Agent_Windows_Container.html#create-and-build-the-dockerfile","title":"Create and Build the Dockerfile","text":"<p>Next, create the Dockerfile.</p> <p>Open a command prompt and create a new directory:</p> <pre><code>mkdir \"C:\\azp-agent-in-docker\\\"\ncd \"C:\\azp-agent-in-docker\\\"\n</code></pre> <p>Save the following content to a file called <code>C:\\azp-agent-in-docker\\azp-agent-windows.dockerfile</code>:</p> <pre><code>FROM mcr.microsoft.com/windows/servercore:ltsc2022\n\nWORKDIR /azp/\n\nCOPY ./start.ps1 ./\n\nCMD powershell .\\start.ps1\n</code></pre> <p>Save the following content to <code>C:\\azp-agent-in-docker\\start.ps1</code>:</p> <p>Note, this script is taken as-is from this location.</p> <pre><code>function Print-Header ($header) {\n  Write-Host \"`n${header}`n\" -ForegroundColor Cyan\n}\n\nif (-not (Test-Path Env:AZP_URL)) {\n  Write-Error \"error: missing AZP_URL environment variable\"\n  exit 1\n}\n\nif (-not (Test-Path Env:AZP_TOKEN_FILE)) {\n  if (-not (Test-Path Env:AZP_TOKEN)) {\n    Write-Error \"error: missing AZP_TOKEN environment variable\"\n    exit 1\n  }\n\n  $Env:AZP_TOKEN_FILE = \"\\azp\\.token\"\n  $Env:AZP_TOKEN | Out-File -FilePath $Env:AZP_TOKEN_FILE\n}\n\nRemove-Item Env:AZP_TOKEN\n\nif ((Test-Path Env:AZP_WORK) -and -not (Test-Path $Env:AZP_WORK)) {\n  New-Item $Env:AZP_WORK -ItemType directory | Out-Null\n}\n\nNew-Item \"\\azp\\agent\" -ItemType directory | Out-Null\n\n# Let the agent ignore the token env variables\n$Env:VSO_AGENT_IGNORE = \"AZP_TOKEN,AZP_TOKEN_FILE\"\n\nSet-Location agent\n\nPrint-Header \"1. Determining matching Azure Pipelines agent...\"\n\n$base64AuthInfo = [Convert]::ToBase64String([Text.Encoding]::ASCII.GetBytes(\":$(Get-Content ${Env:AZP_TOKEN_FILE})\"))\n$package = Invoke-RestMethod -Headers @{Authorization=(\"Basic $base64AuthInfo\")} \"$(${Env:AZP_URL})/_apis/distributedtask/packages/agent?platform=win-x64&amp;`$top=1\"\n$packageUrl = $package[0].Value.downloadUrl\n\nWrite-Host $packageUrl\n\nPrint-Header \"2. Downloading and installing Azure Pipelines agent...\"\n\n$wc = New-Object System.Net.WebClient\n$wc.DownloadFile($packageUrl, \"$(Get-Location)\\agent.zip\")\n\nExpand-Archive -Path \"agent.zip\" -DestinationPath \"\\azp\\agent\"\n\ntry {\n  Print-Header \"3. Configuring Azure Pipelines agent...\"\n\n  .\\config.cmd --unattended `\n    --agent \"$(if (Test-Path Env:AZP_AGENT_NAME) { ${Env:AZP_AGENT_NAME} } else { hostname })\" `\n    --url \"$(${Env:AZP_URL})\" `\n    --auth PAT `\n    --token \"$(Get-Content ${Env:AZP_TOKEN_FILE})\" `\n    --pool \"$(if (Test-Path Env:AZP_POOL) { ${Env:AZP_POOL} } else { 'Default' })\" `\n    --work \"$(if (Test-Path Env:AZP_WORK) { ${Env:AZP_WORK} } else { '_work' })\" `\n    --replace\n\n  Print-Header \"4. Running Azure Pipelines agent...\"\n\n  .\\run.cmd\n} finally {\n  Print-Header \"Cleanup. Removing Azure Pipelines agent...\"\n\n  .\\config.cmd remove --unattended `\n    --auth PAT `\n    --token \"$(Get-Content ${Env:AZP_TOKEN_FILE})\"\n}\n</code></pre> <p>Build the Docker image:</p> <pre><code>docker build --tag \"azp-agent:windows\" --file \"./azp-agent-windows.dockerfile\" .\n</code></pre> <p>The final image will be tagged as <code>azp-agent:windows</code>.</p>"},{"location":"DevOps/2.2_Self-Hosted_Agent_Windows_Container.html#start-the-image","title":"Start the Image","text":"<p>Now that you have created an image, you can run a container. This will install the latest version of the agent, configure it, and run the agent. It targets the specified agent pool (the Default agent pool by default) of a specified Azure DevOps or Azure DevOps Server instance of your choice:</p> <pre><code>docker run -e AZP_URL=\"&lt;Azure DevOps instance&gt;\" -e AZP_TOKEN=\"&lt;Personal Access Token&gt;\" -e AZP_POOL=\"&lt;Agent Pool Name&gt;\" -e AZP_AGENT_NAME=\"Docker Agent - Windows\" --name \"azp-agent-windows\" azp-agent:windows\n</code></pre>"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container.html","title":"Setting Up a Self-Hosted Agent in Docker Windows Container","text":"<p>In Azure DevOps, an agent is a software that runs on a machine and executes build and deployment tasks. These tasks are defined in your pipelines and can be executed on various environments, including Windows, Linux, and macOS. Agents are essential for continuous integration and continuous deployment (CI/CD) as they handle the actual operations required to build, test, and deploy your applications.</p> <p>There are two main types of agents in Azure DevOps:</p> <ol> <li>Microsoft-Hosted Agents: These agents are managed by Microsoft and automatically provisioned for you. They are ephemeral, meaning a new agent is created for each run and discarded afterward.</li> <li>Self-Hosted Agents: These agents are managed by you. You have full control over the machine where the agent runs. This type of agent is useful when you need more control over the environment, want to install specific software, or need to connect to resources within your private network.</li> </ol> <p>This guide focuses on setting up a self-hosted agent in a Ubuntu Container.</p>"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container.html#common-error-with-startsh","title":"Common Error with start.sh","text":"<p>The main error you will get is that the <code>start.sh</code> file is not findable. This happens if you create a file for Linux from Windows. The only resolution that works confidently is to convert the file using the link:</p> <p>https://toolslick.com/conversion/text/dos-to-unix</p> <p>Just upload the <code>start.sh</code> file, convert, and download. Then use as is.</p> <p>Note: The Notepad++ method of changing the format won't work. Neither will PowerShell. These methods are not reliable.</p>"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container.html#steps-to-create-docker-container","title":"Steps to Create Docker Container","text":""},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container.html#create-and-navigate-to-folder","title":"Create and Navigate to Folder","text":"<p>Create a folder <code>azp-agent-in-docker</code> and navigate to it.</p>"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container.html#save-dockerfile","title":"Save Dockerfile","text":"<p>Save the following content in <code>azp-agent-in-docker/azp-agent-linux.dockerfile</code>:</p> <pre><code>FROM ubuntu:22.04\nENV TARGETARCH=\"linux-x64\"\n# Also can be \"linux-arm\", \"linux-arm64\".\n\nRUN apt update\nRUN apt upgrade -y\nRUN apt install -y curl git jq libicu70\n\nWORKDIR /azp/\n\nCOPY ./start.sh ./\nRUN chmod +x ./start.sh\n\n# Create agent user and set up home directory\nRUN useradd -m -d /home/agent agent\nRUN chown -R agent:agent /azp /home/agent\n\nUSER agent\n# Another option is to run the agent as root.\n# ENV AGENT_ALLOW_RUNASROOT=\"true\"\n\nENTRYPOINT [ \"./start.sh\" ]\n</code></pre>"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container.html#save-startsh","title":"Save start.sh","text":"<p>Save this content to <code>azp-agent-in-docker/start.sh</code>:</p> <pre><code>#!/bin/bash\nset -e\n\nif [ -z \"${AZP_URL}\" ]; then\n  echo 1&gt;&amp;2 \"error: missing AZP_URL environment variable\"\n  exit 1\nfi\n\nif [ -z \"${AZP_TOKEN_FILE}\" ]; then\n  if [ -z \"${AZP_TOKEN}\" ]; then\n    echo 1&gt;&amp;2 \"error: missing AZP_TOKEN environment variable\"\n    exit 1\n  fi\n\n  AZP_TOKEN_FILE=\"/azp/.token\"\n  echo -n \"${AZP_TOKEN}\" &gt; \"${AZP_TOKEN_FILE}\"\nfi\n\nunset AZP_TOKEN\n\nif [ -n \"${AZP_WORK}\" ]; then\n  mkdir -p \"${AZP_WORK}\"\nfi\n\ncleanup() {\n  trap \"\" EXIT\n\n  if [ -e ./config.sh ]; then\n    print_header \"Cleanup. Removing Azure Pipelines agent...\"\n\n    # If the agent has some running jobs, the configuration removal process will fail.\n    # So, give it some time to finish the job.\n    while true; do\n      ./config.sh remove --unattended --auth \"PAT\" --token $(cat \"${AZP_TOKEN_FILE}\") &amp;&amp; break\n\n      echo \"Retrying in 30 seconds...\"\n      sleep 30\n    done\n  fi\n}\n\nprint_header() {\n  lightcyan=\"\\033[1;36m\"\n  nocolor=\"\\033[0m\"\n  echo -e \"\\n${lightcyan}$1${nocolor}\\n\"\n}\n\n# Let the agent ignore the token env variables\nexport VSO_AGENT_IGNORE=\"AZP_TOKEN,AZP_TOKEN_FILE\"\n\nprint_header \"1. Determining matching Azure Pipelines agent...\"\n\nAZP_AGENT_PACKAGES=$(curl -LsS \\\n    -u user:$(cat \"${AZP_TOKEN_FILE}\") \\\n    -H \"Accept:application/json;\" \\\n    \"${AZP_URL}/_apis/distributedtask/packages/agent?platform=${TARGETARCH}&amp;top=1\")\n\nAZP_AGENT_PACKAGE_LATEST_URL=$(echo \"${AZP_AGENT_PACKAGES}\" | jq -r \".value[0].downloadUrl\")\n\nif [ -z \"${AZP_AGENT_PACKAGE_LATEST_URL}\" -o \"${AZP_AGENT_PACKAGE_LATEST_URL}\" == \"null\" ]; then\n  echo 1&gt;&amp;2 \"error: could not determine a matching Azure Pipelines agent\"\n  echo 1&gt;&amp;2 \"check that account \"${AZP_URL}\" is correct and the token is valid for that account\"\n  exit 1\nfi\n\nprint_header \"2. Downloading and extracting Azure Pipelines agent...\"\n\ncurl -LsS \"${AZP_AGENT_PACKAGE_LATEST_URL}\" | tar -xz &amp; wait $!\n\nsource ./env.sh\n\ntrap \"cleanup; exit 0\" EXIT\ntrap \"cleanup; exit 130\" INT\ntrap \"cleanup; exit 143\" TERM\n\nprint_header \"3. Configuring Azure Pipelines agent...\"\n\n./config.sh --unattended \\\n  --agent \"${AZP_AGENT_NAME:-$(hostname)}\" \\\n  --url \"${AZP_URL}\" \\\n  --auth \"PAT\" \\\n  --token $(cat \"${AZP_TOKEN_FILE}\") \\\n  --pool \"${AZP_POOL:-Default}\" \\\n  --work \"${AZP_WORK:-_work}\" \\\n  --replace \\\n  --acceptTeeEula &amp; wait $!\n\nprint_header \"4. Running Azure Pipelines agent...\"\n\nchmod +x ./run.sh\n\n# To be aware of TERM and INT signals call ./run.sh\n# Running it with the --once flag at the end will shut down the agent after the build is executed\n./run.sh \"$@\" &amp; wait $!\n</code></pre>"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container.html#build-docker-image","title":"Build Docker Image","text":"<p>Run the following command within that directory:</p> <pre><code>docker build --tag \"azp-agent:linux\" --file \"./azp-agent-linux.dockerfile\" .\n</code></pre> <p>The final image is tagged <code>azp-agent:linux</code>.</p>"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container.html#start-the-image","title":"Start the Image","text":"<p>Now that you have created an image, you can run a container. This installs the latest version of the agent, configures it, and runs the agent. It targets the specified agent pool (the Default agent pool by default) of a specified Azure DevOps or Azure DevOps Server instance of your choice:</p> <pre><code>docker run -e AZP_URL=\"&lt;Azure DevOps instance&gt;\" -e AZP_TOKEN=\"&lt;Personal Access Token&gt;\" -e AZP_POOL=\"&lt;Agent Pool Name&gt;\" -e AZP_AGENT_NAME=\"Docker Agent - Linux\" --name \"azp-agent-linux\" azp-agent:linux\n</code></pre> <p>You might need to specify <code>--interactive</code> and <code>--tty</code> flags (or simply <code>-it</code>) if you want to be able to stop the container and remove the agent with Ctrl + C.</p> <pre><code>docker run --interactive --tty &lt; . . . &gt;\n</code></pre> <p>If you want a fresh agent container for every pipeline job, pass the <code>--once</code> flag to the run command.</p> <pre><code>docker run &lt; . . . &gt; --once\n</code></pre>"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container.html#what-all-types-of-agents-can-we-have","title":"What all types of Agents can we have?","text":""},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container.html#microsoft-hosted-agents","title":"Microsoft-Hosted Agents","text":"<p>Microsoft-hosted agents are managed by Azure DevOps. They are easy to use because they come pre-configured with common tools and are always up to date. You don\u2019t need to maintain them. However, they might not be the best choice if you need custom software or specific configurations.</p>"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container.html#self-hosted-agents","title":"Self-Hosted Agents","text":""},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container.html#on-premises-self-hosted-agents","title":"On-Premises Self-Hosted Agents","text":"<p>These agents run on your own servers. They are great for accessing internal resources like databases and file shares. You have full control over the environment, which is good for running specialized software or meeting security requirements.</p>"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container.html#cloud-self-hosted-agents","title":"Cloud Self-Hosted Agents","text":"<p>These agents run on virtual machines in the cloud, like Azure VMs. They are perfect if you need to scale up or down based on demand. You get the benefits of the cloud without having to manage physical hardware.</p>"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container.html#why-use-different-containers","title":"Why Use Different Containers","text":"<ul> <li>Windows Containers: Best for running Windows applications like .NET Framework and ASP.NET. They ensure compatibility and make deployment easier.</li> <li>Ubuntu Containers: Ideal for open-source software, Python apps, Node.js, and other tools. They are lightweight and flexible.</li> </ul>"},{"location":"DevOps/2.3_Self-Hosted_Agent_Linux_Container.html#when-to-use-on-premises-windows-containers","title":"When to Use On-Premises Windows Containers","text":"<p>Use these when you need to run Windows apps that need access to your internal resources. They give you the benefits of containers while allowing access to your internal systems and data.</p>"},{"location":"DevOps/2_Azure-Pipelines.html","title":"Azure Pipelines","text":"<p>In Azure DevOps, Azure Pipelines are used to automatically build, test, and deploy any project to Dev, UAT, or PROD environments. This is how CI/CD is implemented using Azure Pipelines.</p> <p>You can use Azure Pipelines for any type of project, be it C#, Python, iOS, Android, etc. Azure Pipeline is written in a YAML file, called <code>azure-pipelines.yml</code>, in your VS project folder.</p> <p>Create your first pipeline</p>"},{"location":"DevOps/2_Azure-Pipelines.html#azure-pipelines-agents","title":"Azure Pipelines Agents","text":"<p>When a pipeline runs, it creates one or more jobs. These jobs need 'a place' to run. This place is the agent. The agent is a machine, VM, or cloud environment with the agent software installed, where the jobs can execute.</p> <p>There are three types of agents:</p> <ul> <li>Microsoft-hosted agent</li> <li>Self-hosted agent</li> <li>Azure VM agents</li> </ul> <p>The agent is where the jobs for the pipelines run.</p>"},{"location":"DevOps/2_Azure-Pipelines.html#create-a-pipeline-in-azure-devops","title":"Create a Pipeline in Azure DevOps","text":""},{"location":"DevOps/2_Azure-Pipelines.html#what-you-need","title":"What You Need","text":"<ul> <li>GitHub account: Create a free repository on GitHub.</li> <li>Azure DevOps organization: Create one for free. If your team already has one, ensure you're an administrator of the Azure DevOps project you want to use.</li> <li>Ability to run pipelines on Microsoft-hosted agents: Your Azure DevOps organization must have access to Microsoft-hosted parallel jobs. You can either purchase a parallel job or request a free grant.</li> </ul>"},{"location":"DevOps/2_Azure-Pipelines.html#steps-to-create-a-pipeline","title":"Steps to Create a Pipeline","text":"<ol> <li>Fork the Repository</li> <li>Open this link and click on Fork.    </li> <li> <p>Click on Create fork.    </p> </li> <li> <p>Sign in to Azure DevOps</p> </li> <li> <p>Sign in to your Azure DevOps organization and go to your project.</p> </li> <li> <p>Create a New Pipeline</p> </li> <li> <p>Go to Pipelines, and then select New pipeline or Create pipeline if it's your first pipeline.    </p> </li> <li> <p>Connect to GitHub</p> </li> <li>Follow the steps in the wizard by first selecting GitHub as the location of your source code.     </li> <li> <p>You might be redirected to GitHub to install the Azure Pipelines app. If so, select Approve &amp; install.</p> </li> <li> <p>Configure the Pipeline</p> </li> <li>Azure Pipelines will analyze your repository and recommend the Python package pipeline template.</li> <li>When your new pipeline appears, check the YAML to see what it does. When you're ready, select Save and run.</li> <li> <p>You'll be prompted to commit a new <code>azure-pipelines.yml</code> file to your repository. After you're happy with the message, select Save and run again.     </p> </li> <li> <p>Watch Your Pipeline Run</p> </li> <li>If you want to watch your pipeline in action, select the build job.</li> </ol> <p>You have now created and ran a pipeline that Azure automatically created for you, as your code was a good match for the Python package template. You now have a working YAML pipeline (<code>azure-pipelines.yml</code>) in your repository that's ready for you to customize!</p>"},{"location":"DevOps/2_Azure-Pipelines.html#editing-your-pipeline","title":"Editing Your Pipeline","text":"<ul> <li>When you're ready to make changes to your pipeline, select it in the Pipelines page, and then edit the <code>azure-pipelines.yml</code> file.    </li> </ul>"},{"location":"DevOps/2_Azure-Pipelines.html#no-hosted-parallelism-available","title":"No Hosted Parallelism Available","text":"<p>This means you haven't purchased Parallelism. You can do this:</p> <ol> <li>Request Free Parallelism: Request Parallelism.</li> <li>Check Usage: Go to your Azure DevOps settings and check if all your parallel jobs are in use.</li> <li>Buy More Parallelism: If needed, buy more parallelism from the billing section in Azure DevOps.</li> <li>Set Up Self-hosted Agents: If you have your own resources, set up self-hosted agents to run your pipelines.</li> </ol> <p>You can run the Azure Pipeline agent on your own computer and run the pipeline on a self-hosted Azure Pipeline Agent: Learn more.</p>"},{"location":"DevOps/3.1_Hello_GitHub_Actions_Workflow.html","title":"Background","text":"<p> GitHub Actions Workflows are the core of GitHub CI/CD. You can set them up easily. VS Code has many extensions for GitHub. In this article, I will show how to create a simple GitHub Actions Workflow. It will help you understand the flow. Later using similar steps you can create complex Action Workflows. Let's get started. </p>"},{"location":"DevOps/3.1_Hello_GitHub_Actions_Workflow.html#create-an-end-to-end-github-actions-workflow-using-vs-code","title":"Create an End-to-End GitHub Actions Workflow Using VS Code","text":""},{"location":"DevOps/3.1_Hello_GitHub_Actions_Workflow.html#create-a-new-repository-using-github-desktop","title":"Create a New Repository Using GitHub Desktop","text":"<ul> <li>Open GitHub Desktop. Click on New repository. Provide a name for your repository, check the local path, and click on Create repository.</li> </ul> <ul> <li>Click on Publish repository. Then Click on Open in Visual Studio Code.</li> </ul> <ul> <li>In the VS Code you will be able to see your folder .gitattributes file</li> </ul>"},{"location":"DevOps/3.1_Hello_GitHub_Actions_Workflow.html#set-up-the-github-actions-workflow-in-vs-code","title":"Set Up the GitHub Actions Workflow in VS Code","text":"<ul> <li>Create Folder Structure:</li> <li>In VS Code, click on the create folder icon and create a <code>.github</code> folder.</li> <li>Inside the <code>.github</code> folder, create another folder named <code>workflows</code>.</li> </ul> <ul> <li>Create Workflow File:</li> <li>Inside the <code>workflows</code> folder, create a file named <code>hello-github-actions.yml</code>.</li> <li>Add the following code to <code>hello-github-actions.yml</code>:     <pre><code>name: Hello GitHub Actions\non: [push]\njobs:\n  say-hello:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n      - name: Run hello.py\n        run: python hello.py\n</code></pre></li> </ul>"},{"location":"DevOps/3.1_Hello_GitHub_Actions_Workflow.html#create-a-python-script","title":"Create a Python Script","text":"<ul> <li>In the root of your repository, create a new file named <code>hello.py</code>.</li> <li>Add the following code to <code>hello.py</code>:     <pre><code>print(\"Hello, GitHub Actions!\")\n</code></pre> </li> </ul>"},{"location":"DevOps/3.1_Hello_GitHub_Actions_Workflow.html#commit-and-push-the-changes","title":"Commit and Push the Changes","text":"<ul> <li>Click on the source control icon in VS Code.</li> <li>Uncomment or Write a commit message (e.g., \"This is a message bla bla bla\").</li> <li>Click on Commit to commit the changes.</li> <li>Click on Push to push the changes to GitHub. This will trigger the workflow.</li> </ul>"},{"location":"DevOps/3.1_Hello_GitHub_Actions_Workflow.html#check-the-workflow-execution","title":"Check the Workflow Execution","text":"<ul> <li>In VS Code, click on the GitHub Actions Extension in the left pane. This will show you the GitHub Actions Workflow that you created.</li> <li>You can click on the globe icon to see the workflow directly on GitHub.</li> </ul>"},{"location":"DevOps/3.1_Hello_GitHub_Actions_Workflow.html#summary","title":"Summary","text":"<p>Well, this was a hand-holding article. You can do a lot more with GitHub Actions workflows.</p>"},{"location":"DevOps/3.2_Sample_Workflows.html","title":"Sample Workflows","text":""},{"location":"DevOps/3.2_Sample_Workflows.html#example-1-basic-ci-pipeline","title":"Example 1: Basic CI Pipeline","text":"<p>This example shows a simple CI pipeline that runs tests on every push to the main branch.</p> <p>Workflow YAML:</p> <pre><code>name: CI Pipeline\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  build-and-test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v2\n\n      - name: Set up Node.js\n        uses: actions/setup-node@v2\n        with:\n          node-version: '14'\n\n      - name: Install dependencies\n        run: npm install\n\n      - name: Run tests\n        run: npm test\n</code></pre> <p>Explanation: - on: push: Triggers the workflow on pushes to the main branch. - jobs: Defines the job that will run. - steps: Lists the steps to be executed, such as checking out the code, setting up Node.js, installing dependencies, and running tests.</p>"},{"location":"DevOps/3.2_Sample_Workflows.html#example-2-cicd-pipeline-with-deployment","title":"Example 2: CI/CD Pipeline with Deployment","text":"<p>This example shows a CI/CD pipeline that builds, tests, and deploys a Node.js application to an Azure App Service.</p> <p>Workflow YAML:</p> <pre><code>name: CI/CD Pipeline\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  build-test-deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v2\n\n      - name: Set up Node.js\n        uses: actions/setup-node@v2\n        with:\n          node-version: '14'\n\n      - name: Install dependencies\n        run: npm install\n\n      - name: Run tests\n        run: npm test\n\n      - name: Build project\n        run: npm run build\n\n      - name: Deploy to Azure Web App\n        uses: azure/webapps-deploy@v2\n        with:\n          app-name: 'my-web-app'\n          publish-profile: ${{ secrets.AZURE_WEBAPP_PUBLISH_PROFILE }}\n          package: .\n</code></pre> <p>Explanation: - on: push: Triggers the workflow on pushes to the main branch. - jobs: Defines the job that will run. - steps: Lists the steps to be executed, including checking out the code, setting up Node.js, installing dependencies, running tests, building the project, and deploying to Azure Web App. - secrets: Securely stores sensitive information like the Azure publish profile.</p>"},{"location":"DevOps/3.2_Sample_Workflows.html#example-3-scheduled-workflow","title":"Example 3: Scheduled Workflow","text":"<p>This example demonstrates a workflow that runs tests every day at midnight.</p> <p>Workflow YAML:</p> <pre><code>name: Scheduled Test Run\n\non:\n  schedule:\n    - cron: '0 0 * * *'\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v2\n\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.8'\n\n      - name: Install dependencies\n        run: pip install -r requirements.txt\n\n      - name: Run tests\n        run: pytest\n</code></pre> <p>Explanation: - on: schedule: Triggers the workflow based on a cron schedule (every day at midnight). - jobs: Defines the job that will run. - steps: Lists the steps to be executed, such as checking out the code, setting up Python, installing dependencies, and running tests.</p>"},{"location":"DevOps/3.2_Sample_Workflows.html#example-4-multi-environment-deployment","title":"Example 4: Multi-Environment Deployment","text":"<p>This example shows how to deploy to different environments (staging and production) based on the branch.</p> <p>Workflow YAML:</p> <pre><code>name: Multi-Environment Deployment\n\non:\n  push:\n    branches:\n      - main\n      - staging\n\njobs:\n  build-and-deploy:\n    runs-on: ubuntu-latest\n\n    strategy:\n      matrix:\n        environment: [staging, production]\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v2\n\n      - name: Set up Node.js\n        uses: actions/setup-node@v2\n        with:\n          node-version: '14'\n\n      - name: Install dependencies\n        run: npm install\n\n      - name: Run tests\n        run: npm test\n\n      - name: Build project\n        run: npm run build\n\n      - name: Deploy to Azure Web App\n        uses: azure/webapps-deploy@v2\n        with:\n          app-name: ${{ matrix.environment == 'production' &amp;&amp; 'my-web-app-prod' || 'my-web-app-staging' }}\n          publish-profile: ${{ secrets.AZURE_WEBAPP_PUBLISH_PROFILE_${{ matrix.environment | upper }} }}\n          package: .\n</code></pre> <p>Explanation: - on: push: Triggers the workflow on pushes to the main and staging branches. - strategy: matrix: Defines a matrix strategy to run the job for both staging and production environments. - steps: Lists the steps to be executed, including checking out the code, setting up Node.js, installing dependencies, running tests, building the project, and deploying to Azure Web App. - conditional logic: Uses conditional logic to deploy to the appropriate environment based on the branch.</p>"},{"location":"DevOps/3_GitHub-Actions.html","title":"Getting started with GitHub","text":""},{"location":"DevOps/3_GitHub-Actions.html#repositories-branches-push-pull-commit","title":"Repositories, Branches, Push, Pull, Commit","text":"<ul> <li>Repositories</li> <li>Branches</li> <li>Push</li> <li>Pull</li> <li>Commits</li> </ul>"},{"location":"DevOps/3_GitHub-Actions.html#understanding-github-actions","title":"Understanding GitHub Actions","text":"<p>In this article, I'll explain the core concepts of GitHub Actions.</p> <p>Further reading.</p>"},{"location":"DevOps/3_GitHub-Actions.html#for-the-busy-readers","title":"For the Busy Readers","text":"<p>GitHub Actions is a tool to automate code building and deployment tasks. With GitHub Actions, you create workflows. These workflows are YAML files inside the <code>.github/workflows</code> folder in your project. Workflows contain jobs, and jobs contain steps. Steps can be simple commands, scripts, or pre-built actions from the GitHub community.</p> <p>Another explanation:</p> <p>GitHub Actions is a GitHub feature that lets you run tasks when certain events happen in your code repository. You can use it to trigger any part of a CI/CD pipeline using webhooks on GitHub.</p> <p>GitHub Actions offers over 13,000 pre-written and tested CI/CD workflows. You can also write your own workflows or customize existing ones using YAML files.</p> <p>GitHub Actions is not just for CI/CD pipelines. Here are some other uses:</p> <ul> <li>Running nightly builds in the master branch for testing.</li> <li>Cleaning up old issues or bug reports.</li> <li>Creating bots that respond to comments or commands on pull requests or issues.</li> </ul> <p>In short, GitHub Actions is the GitHub feature that automates CI/CD and more.</p> <p>    GitHub Action is a tool. GitHub Actions Workflow is the output of the tool. </p>"},{"location":"DevOps/3_GitHub-Actions.html#key-concepts-in-github-actions","title":"Key Concepts in GitHub Actions","text":"<p>Here are the key terms you will hear most of the time when dealing with GitHub Actions:</p> <ol> <li>Workflows: YAML files inside the <code>.github/workflows</code> folder.</li> <li>Jobs: Workflows contain jobs, which are sets of steps.</li> <li>Steps: Jobs are made up of steps that run commands, scripts, or actions.</li> <li>Runners: Servers that execute the jobs (can be GitHub-provided or self-hosted).</li> </ol> <p>How It Works: 1. Trigger: An event, like pushing code or creating a pull request, triggers the workflow. 2. Workflow Activation: The specified workflow for that trigger starts running. 3. Jobs Execution: Jobs within the workflow run, either independently or in sequence. 4. Runners: Jobs use virtual machines provided by GitHub or self-hosted machines to run.</p>"},{"location":"DevOps/3_GitHub-Actions.html#example-workflow","title":"Example Workflow","text":"<p>You commit code to your repository. The workflow is triggered. Your code is built, tested, and deployed automatically.</p> <p>GitHub provides virtual machines for Linux, Windows, and macOS.</p>"},{"location":"DevOps/3_GitHub-Actions.html#anatomy-of-a-github-workflow","title":"Anatomy of a GitHub Workflow","text":"<p>A GitHub Actions workflow is written as a YAML file inside the <code>.github/workflows</code> directory of a project/repository. Each workflow is stored as a separate YAML file in your code repository.</p> <p>This is how a typical workflow YAML file looks:</p> <pre><code>name: CI\n\non:\n  push:\n    branches: [ main ]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n    - name: Build\n      run: make build\n</code></pre> <ol> <li>name: This workflow is named \"CI\" (Continuous Integration). You can name it anything you like.</li> <li>on: This workflow runs when there is a push to the <code>main</code> branch.</li> <li>jobs: This workflow has one job named <code>build</code>.</li> <li>runs-on: This job runs on an <code>ubuntu-latest</code> virtual machine.</li> <li>steps: <ul> <li>Checkout code: Uses <code>actions/checkout@v3</code> to pull the code.</li> <li>Build: Runs the <code>make build</code> command to build the project.</li> </ul> </li> </ol>"},{"location":"DevOps/ADF_CICD.html","title":"ADF -  Dev to PROD using GitHub workflows and Azure Pipelines","text":"<p>Moving your Azure Data Factory from Dev to PROD can be CI/CDed using GitHub Actions Workflows or using Azure DevOps Pipelines.</p> <p>Main Article</p> <p>ADF uses ARM templates to store the configuration of pipelines, datasets, data flows, etc.</p> <p>There are two methods to deploy a data factory to another environment:</p> <ul> <li>Automated deployment using Data Factory's integration with Azure Pipelines or GitHub workflows.</li> <li>Manual deployment by uploading a Resource Manager template using Data Factory UX integration with Azure Resource Manager.</li> </ul>"},{"location":"DevOps/ADF_CICD.html#using-github","title":"Using GitHub","text":"<p>Set Up Version Control - Connect ADF with GitHub: Open your ADF in the development environment. Go to the \"Manage\" tab and connect your ADF to a GitHub repository. - Continuous Integration (CI): Develop and test your ADF pipelines, datasets, and linked services. Commit and push your changes to the GitHub repository.</p> <p>Create a GitHub Actions Workflow Use this YAML code for your workflow:</p> <pre><code>name: ADF CI\n\non:\n  push:\n    branches:\n      - main  # or your development branch\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v2\n\n    - name: Set up .NET\n      uses: actions/setup-dotnet@v1\n      with:\n        dotnet-version: '3.x'\n\n    - name: Restore dependencies\n      run: dotnet restore\n\n    - name: Build\n      run: dotnet build --no-restore\n\n    - name: Publish artifacts\n      uses: actions/upload-artifact@v2\n      with:\n        name: drop\n        path: |\n          ARMTemplateForFactory.json\n          ARMTemplateParametersForFactory.json\n</code></pre> <p>Continuous Deployment (CD) Create a deployment workflow in <code>.github/workflows</code>:</p> <pre><code>name: ADF CD\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v2\n\n    - name: Azure Login\n      uses: azure/login@v1\n      with:\n        creds: ${{ secrets.AZURE_CREDENTIALS }}\n\n    - name: Deploy ARM Template\n      uses: azure/arm-deploy@v1\n      with:\n        resource-group: 'YOUR_RESOURCE_GROUP_NAME'\n        template: './ARMTemplateForFactory.json'\n        parameters: './ARMTemplateParametersForFactory.json'\n        deployment-name: 'adf-deployment'\n</code></pre> <p>Manage Parameters Set different values for production in your parameters file, like connections and dataset paths.</p> <p>Run the Workflow Push changes to the GitHub repository to trigger the CI workflow. After a successful build, the CD workflow will automatically deploy changes to the production ADF.</p>"},{"location":"DevOps/ADF_CICD.html#using-azure-devops","title":"Using Azure DevOps","text":"<p>Set Up Version Control - Connect ADF with Azure Repos: Open your ADF in the development environment. Go to the \"Manage\" tab and connect your ADF to an Azure Repos Git repository. - Continuous Integration (CI): Develop and test your ADF pipelines, datasets, and linked services. Commit and push your changes to the Azure Repos Git repository.</p> <p>Create a Build Pipeline Go to Azure DevOps &gt; Pipelines &gt; Builds &gt; New Pipeline. Select your Azure Repos Git repository. Use this YAML code:</p> <pre><code>trigger:\n  branches:\n    include:\n      - main  # or your development branch\n\npool:\n  vmImage: 'ubuntu-latest'\n\nsteps:\n- task: UseDotNet@2\n  inputs:\n    packageType: 'sdk'\n    version: '3.x'\n    installationPath: $(Agent.ToolsDirectory)/dotnet\n\n- task: DotNetCoreCLI@2\n  inputs:\n    command: 'restore'\n    projects: '**/*.csproj'\n\n- task: DotNetCoreCLI@2\n  inputs:\n    command: 'build'\n    projects: '**/*.csproj'\n\n- task: PublishPipelineArtifact@1\n  inputs:\n    targetPath: '$(Build.ArtifactStagingDirectory)'\n    artifact: 'drop'\n</code></pre> <p>Continuous Deployment (CD) Create a release pipeline in Azure DevOps:</p> <p>Create a Release Pipeline 1. Go to Azure DevOps &gt; Pipelines &gt; Releases &gt; New pipeline. 2. Select an empty job, add an artifact, and choose the build pipeline you created. 3. Add a new stage for deployment.</p> <p>Add a task to deploy the ARM template of your ADF in the deployment stage:</p> <pre><code>- task: AzureResourceManagerTemplateDeployment@3\n  inputs:\n    deploymentScope: 'Resource Group'\n    azureResourceManagerConnection: 'AzureRMConnection'\n    subscriptionId: 'YOUR_SUBSCRIPTION_ID'\n    action: 'Create Or Update Resource Group'\n    resourceGroupName: 'YOUR_RESOURCE_GROUP_NAME'\n    location: 'YOUR_RESOURCE_LOCATION'\n    templateLocation: 'Linked artifact'\n    csmFile: '$(System.DefaultWorkingDirectory)/_your-build-pipeline/drop/ARMTemplateForFactory.json'\n    csmParametersFile: '$(System.DefaultWorkingDirectory)/_your-build-pipeline/drop/ARMTemplateParametersForFactory.json'\n</code></pre> <p>Manage Parameters Set different values for production in your parameters file, like connections and dataset paths.</p> <p>Run the Pipeline Push changes to the Azure Repos Git repository to trigger the CI pipeline. After a successful build, the CD pipeline will automatically deploy changes to the production ADF.</p>"},{"location":"DevOps/Biceps.html","title":"What is Bicep?","text":"<p>Bicep is a new Language for developing ARM templates. Bicep eventually compiles to JSON files but, its less complicated than JSON.</p> <p>There are two types of ARM template files: JSON and Bicep</p> <p>You can deploy azure resources using Bicep using Azure pipelines or Github Actions.</p> <p>We will see Bicep and Github actions here.</p> <p>Deploy Azure resources by using Bicep and GitHub Actions</p> <p>To deploy on Azure here are the options we have:</p> <p>Azure portal Azure CLI - This is an IAC option Azure PowerShell - This is an IAC option Azure Resource Manager templates (JSON and Bicep)</p> <p>This is an Infrasructre as code:</p> <pre><code>az group create --name storage-resource-group --location eastus\n</code></pre> <p>If you use it, it will make things faster also more accurate. Manually creating might be typo error by user.</p> <p>Benefits of Infrasturctre as code than going-to-portal-and-doing-it-manually:</p> <p>Create new enviornments quickly Deploy a feature to dev and than using parameters deploy it to prod. This was it will be identical and less error. DR. If you have everything as IAC. you can quicly create the DR enviornment.</p> <p>Imperative and declarative code You can write an instruction manual for new toy assembly in different ways. When you automate the deployment of services and infrastructure, you can take two approaches: imperative and declarative.</p> <p>With imperative code, you execute a sequence of commands, in a specific order, to reach an end configuration. This process defines what the code should accomplish, and it defines how to accomplish the task. The imperative approach is like a step-by-step instruction manual.</p> <p>With declarative code, you specify only the end configuration. The code doesn't define how to accomplish the task. The declarative approach is like the exploded view instruction manual.</p> <p>When you choose between using an imperative approach and a declarative approach to resource provisioning, consider the tools that might already be in use in your organization. Also consider which approach might match your own skills.</p> <p>Imperative code In Azure, an imperative code approach is accomplished programmatically by using a scripting language like Bash or Azure PowerShell. The scripts execute a series of steps to create, modify, and even remove your resources.</p> <p>This example shows two Azure CLI commands that create a resource group and a storage account.</p> <p>Azure CLI</p> <p>Copy</p>"},{"location":"DevOps/Biceps.html#usrbinenv-bash","title":"!/usr/bin/env bash","text":"<p>az group create \\   --name storage-resource-group \\   --location eastus</p> <p>az storage account create \\   --name mystorageaccount \\   --resource-group storage-resource-group \\   --location eastus \\   --sku Standard_LRS \\   --kind StorageV2 \\   --access-tier Hot \\   --https-only true</p> <p>Declarative code In Azure, a declarative code approach is accomplished by using templates. Many types of templates are available to use, including:</p> <p>JSON Bicep Ansible, by RedHat Terraform, by HashiCorp  Note</p> <p>This module focuses on using Bicep templates.</p> <p>Take a look at the following example of a Bicep template that configures a storage account. The configuration of the storage account matches the Azure CLI example.</p> <p><pre><code>Copy\nresource storageAccount 'Microsoft.Storage/storageAccounts@2022-05-01' = {\n  name: 'mystorageaccount'\n  location: 'eastus'\n  sku: {\n    name: 'Standard_LRS'\n  }\n  kind: 'StorageV2'\n  properties: {\n    accessTier: 'Hot'\n    supportsHttpsTrafficOnly: true\n  }\n}\n</code></pre> The resources section defines the storage account configuration. This section contains the name, location, and properties of the storage account, including its SKU and the kind of account.</p> <p>You might notice that the Bicep template doesn't specify how to deploy the storage account. It specifies only what the storage account needs to look like. The actual steps that are executed behind the scenes to create this storage account or to update it to match the specification are left for Azure to decide.</p> <p>Learn  Training  Browse  Fundamentals of Bicep  Introduction to infrastructure as code using Bicep  What is Azure Resource Manager? Completed 100 XP 7 minutes You've spent some time with your team learning the benefits of infrastructure as code and the different approaches that are available. Your company is growing at a rapid pace and your team knows it will be deploying a significant number of resources to Azure. As a team, you've decided that declarative infrastructure as code is the right approach to resource provisioning. The team doesn't want to maintain scripts that list every deployment step. Before beginning the process of building your first template, you need to understand how Azure Resource Manager works. Investigating the types of templates that are available to use with Azure will help you determine the next steps in your infrastructure-as-code strategy.</p> <p>In this unit, you'll learn about Resource Manager and the two types of Resource Manager templates.</p> <p>Azure Resource Manager concepts Azure Resource Manager is the service that's used to deploy and manage resources in Azure. You can use Resource Manager to create, update, and delete resources in your Azure subscription. You can interact with Resource Manager by using many tools, including the Azure portal. Resource Manager also provides a series of other features, like access control, auditing, and tagging, to help manage your resources after deployment.</p> <p>Terminology As you begin your cloud journey with Resource Manager, it's important to understand some terms and concepts:</p> <p>Resource: A manageable item that's available on the Azure platform. Virtual networks, virtual machines, storage accounts, web apps, and databases are examples of resources.</p> <p>Resource group: A logical container that holds related resources for an Azure solution. The resource group includes resources you want to manage as a group. Most Azure resources are contained in a resource group. You decide which resources belong in a resource group based on what makes the most sense for your solution.</p> <p>Note</p> <p>A small number of resources aren't contained in resource groups. These resource types are for specific purposes like managing access control and enforcing policies. You'll learn more about these resources in a later module.</p> <p>Subscription: A logical container and billing boundary for your resources and resource groups. Each Azure resource and resource group is associated with only one subscription.</p> <p>Management group: A logical container that you use to manage more than one subscription. You can define a hierarchy of management groups, subscriptions, resource groups, and resources to efficiently manage access, policies, and compliance through inheritance.</p> <p>Azure Resource Manager template (ARM template): A template file that defines one or more resources to deploy to a resource group, subscription, management group, or tenant. You can use the template to deploy the resources in a consistent and repeatable way. There are two types of ARM template files: JSON and Bicep. This module focuses on Bicep.</p> <p>Benefits Resource Manager provides many benefits and capabilities related to infrastructure-as-code resource provisioning:</p> <p>You can deploy, manage, and monitor the resources in your solution as a group instead of individually. You can redeploy your solution throughout the development lifecycle and have confidence that your resources are deployed in a consistent state. You can manage your infrastructure through declarative templates instead of by using scripts. You can specify resource dependencies to ensure that resources are deployed in the correct order. Operations: Control plane and data plane You can execute two types of operations in Azure: control plane operations and data plane operations. Use the control plane to manage the resources in your subscription. Use the data plane to access features that are exposed by a resource.</p> <p>For example, you use a control plane operation to create a virtual machine, but you use a data plane operation to connect to the virtual machine by using Remote Desktop Protocol (RDP).</p> <p>Control plane When you send a request from any of the Azure tools, APIs, or SDKs, Resource Manager receives, authenticates, and authorizes the request. Then, it sends the request to the Azure resource provider, which takes the requested action. Because all requests are handled through the same API, you see consistent results and capabilities in all the different tools that are available in Azure.</p> <p>The following image shows the role that Resource Manager plays in handling Azure requests:</p> <p>Diagram that shows how Azure Resource Manager accepts requests from all Azure clients and libraries.</p> <p>All control plane operation requests are sent to a Resource Manager URL. For example, the create or update operation for virtual machines is a control plane operation. Here's the request URL for this operation:</p> <p>HTTP</p> <p>Copy PUT https://management.azure.com/subscriptions//resourceGroups//providers/Microsoft.Compute/virtualMachines/{virtualMachineName}?api-version=2022-08-01 The control plane understands which resources need to be created and which resources already exist. Resource Manager understands the difference between these requests and won't create identical resources or delete existing resources, although there are ways to override this behavior. <p>Data plane When a data plane operation starts, the requests are sent to a specific endpoint in your Azure subscription. For example, the Detect Language operation in Azure AI services is a data plane operation because the request URL is:</p> <p>HTTP</p> <p>Copy POST https://eastus.api.cognitive.microsoft.com/text/analytics/v2.0/languages Resource Manager features like access control and locks don't always apply to data plane operations. For example, a user might not have permissions to manage a virtual machine by using the control plane, but the user can sign in to the operating system.</p> <p>What are ARM templates? Azure Resource Manager templates are files that define the infrastructure and configuration for your deployment. When you write an ARM template, you take a declarative approach to your resource provisioning. These templates describe each resource in the deployment, but they don't describe how to deploy the resources. When you submit a template to Resource Manager for deployment, the control plane can deploy the defined resources in an organized and consistent manner. In the preceding unit, you learned about the differences between imperative code and declarative code.</p> <p>Why use ARM templates? There are many benefits to using ARM templates, either JSON or Bicep, for your resource provisioning.</p> <p>Repeatable results: ARM templates are idempotent, which means that you can repeatedly deploy the same template and get the same result. The template doesn't duplicate resources.</p> <p>Orchestration: When a template deployment is submitted to Resource Manager, the resources in the template are deployed in parallel. This process allows deployments to finish faster. Resource Manager orchestrates these deployments in the correct order if one resource depends on another.</p> <p>Preview: The what-if tool, available in Azure PowerShell and Azure CLI, allows you to preview changes to your environment before template deployment. This tool details any creations, modification, and deletions that will be made by your template.</p> <p>Testing and Validation: You can use tools like the Bicep linter to check the quality of your templates before deployment. ARM templates submitted to Resource Manager are validated before the deployment process. This validation alerts you to any errors in your template before resource provisioning.</p> <p>Modularity: You can break up your templates into smaller components and link them together at deployment.</p> <p>CI/CD integration: Your ARM templates can be integrated into multiple CI/CD tools, like Azure DevOps and GitHub Actions. You can use these tools to version templates through source control and build release pipelines.</p> <p>Extensibility: With deployment scripts, you can run Bash or PowerShell scripts from within your ARM templates. These scripts perform tasks, like data plane operations, at deployment. Through extensibility, you can use a single ARM template to deploy a complete solution.</p> <p>JSON and Bicep templates Two types of ARM templates are available for use today: JSON templates and Bicep templates. JavaScript Object Notation (JSON) is an open-standard file format that multiple languages can use. Bicep is a new domain-specific language that was recently developed for authoring ARM templates by using an easier syntax. You can use either template format for your ARM templates and resource deployments.</p>"},{"location":"DevOps/CI_CD_On_This_Site.html","title":"Understanding a full CI/CD process with a practical example","text":""},{"location":"DevOps/CI_CD_On_This_Site.html#introduction","title":"Introduction","text":"<p>This website has a full fledged CI/CD process that automatically builds and deploys the website whenever I push changes to the <code>main</code> or <code>master</code> branch. The CI/CD process is powered by GitHub Actions, which automates the entire workflow from markdown-to-html conversion to deployment. In this document I try to explain how the entire setup works, including the folder structure, the CI/CD process, and the key concepts involved.</p>"},{"location":"DevOps/CI_CD_On_This_Site.html#key-concepts-covered","title":"Key Concepts Covered","text":"<ul> <li>Continuous Integration (CI): How the markdown files are automatically built into HTML</li> <li>Continuous Deployment (CD): How the built HTML files are automatically deployed to GitHub Pages</li> <li>GitHub Actions: The automation platform that powers our workflow</li> <li>Runners: The virtual machines where the CI/CD jobs are executed</li> </ul>"},{"location":"DevOps/CI_CD_On_This_Site.html#main-folders-in-ths-project","title":"Main folders in ths project","text":"<p>All the source code of this website is present in a VSCode project. The main folders in this project are:</p> <pre><code>\u251c\u2500\u2500 docs/                # Contains all the markdown files that I write\n\u251c\u2500\u2500 site/                # Contains the generated HTML files (the built website)\n\u251c\u2500\u2500 .github/             # Contains the CI/CD configuration and workflow files\n\u2502   \u2514\u2500\u2500 workflows/       # Contains the CI/CD workflow files\n\u2502       \u2514\u2500\u2500 ci.yml      # The main and only CI/CD workflow file\n\u251c\u2500\u2500 mkdocs.yml           # Configuration file for MkDocs (the documentation generator)\n\u2514\u2500\u2500 requirements.txt     # Python dependencies for the project\n</code></pre>"},{"location":"DevOps/CI_CD_On_This_Site.html#the-github-and-workflows-folder","title":"The <code>.github</code> and <code>workflows</code> folder","text":"<p>The <code>.github</code> folder is a constant system folder that GitHub uses to manage repository settings and automation. You will need to create this folder if you want to use CI/CD. Remember these points:</p> <ul> <li>It's a special folder that GitHub recognizes for repository configuration</li> <li>The <code>.github/workflows/</code> subfolder contains workflow definition files (YAML)</li> <li>These workflow files define the automated CI/CD processes</li> <li>Additional configuration files for issues, pull requests, etc. can also be stored here</li> </ul> <p>When you put a workflow YAML file (e.g., .github/workflows/ci.yml) to your repository, GitHub specifically indexes only the YAML files (with extensions <code>.yml</code> or <code>.yaml</code>) inside the <code>.github/workflows/</code> directory. GitHub Actions does not index other file types placed in this directory.</p> <p>To remember:</p> <ul> <li>GitHub only indexes files with <code>.yml</code> or <code>.yaml</code> extensions inside the <code>.github/workflows/</code> directory</li> <li>Other file types (like <code>.md</code>, <code>.txt</code>, or <code>.json</code>) placed in the <code>.github/workflows/</code> directory will be stored but not processed as workflows</li> <li>Files outside the <code>.github/workflows/</code> directory, even if they are YAML files with similar structure, will not be processed as GitHub Actions workflows</li> <li>Subdirectories within <code>.github/workflows/</code> are supported - YAML files in these subdirectories will also be indexed as workflows</li> </ul>"},{"location":"DevOps/CI_CD_On_This_Site.html#how-are-the-workflow-folders-ymlyaml-files-indexed","title":"How are the workflow folder's .yml/yaml files indexed?","text":"<p>GitHub Actions Service is a fully automatic, dedicated, backend GitHub service that handles the indexing process. Indexing is event-driven, meaning, it occurs automatically when a new file is added to the .github/workflows/ directory or when an existing file is modified. The indexing process involves several steps:</p> <ol> <li>File Detection: GitHub's backend continuously monitors the <code>.github/workflows/</code> directory for any changes. When a new file is added or an existing file is modified, the indexing process is triggered.</li> <li>File Parsing: The service reads the contents of the YAML file to understand its structure and content. It checks for syntax errors and validates the file against the GitHub Actions schema.</li> <li>Event Registration: GitHub specifically extracts and indexes the <code>on:</code> section to register which events (like <code>push</code>, <code>pull_request</code>, <code>schedule</code>, etc.) should trigger this workflow. It registers these events in GitHub's event system, allowing the workflow to be triggered when those events occur.</li> <li>Branch/Path Filters: For event triggers that support filters (like <code>branches:</code>, <code>paths:</code>, <code>tags:</code>), GitHub indexes these patterns to quickly determine if a particular event should trigger the workflow.</li> <li>Workflow Metadata: Information like the workflow <code>name:</code>, <code>permissions:</code>, and other top-level attributes are indexed.</li> <li>Job Definitions: The structure of jobs, their dependencies, and conditions are indexed to determine execution order.</li> <li>References to Reusable Workflows: Any references to reusable workflows (using <code>uses:</code> with another workflow) are indexed to establish relationships between workflows.</li> <li>Storage: The indexed information is stored in GitHub's database, making it available for the workflow execution engine. Each repository's workflow data is stored separately for security and performance reasons. This indexed data is not directly accessible to users through the GitHub UI or API</li> </ol>"},{"location":"DevOps/CI_CD_On_This_Site.html#the-ciyml-file","title":"The <code>ci.yml</code> file","text":"<p>For this website, the <code>ci.yml</code> file is the main and ONLY file which contains all our CI/CD logic. There are no other files for CI/CD. It must be present inside <code>.github/workflows/</code> folder. Remember, the <code>on</code> section of the <code>ci.yml</code> file is the most important part. It tells GitHub Actions when to run the workflow. In our case, we have set it to run on every push to the <code>main</code> or <code>master</code> branch.</p> <p>Let's analyze the source code we have put in our <code>ci.yml</code> :</p> <pre><code>name: ci                 # Name shown in GitHub Actions UI\non:\n  push:                  # Trigger this workflow when code is pushed\n    branches:            # Only on these specific branches\n      - master           \n      - main\npermissions:\n  contents: write        # Grant permission to modify repository contents\njobs:\n  deploy:                # Define a job named \"deploy\"\n    runs-on: ubuntu-latest  # Run on Ubuntu virtual machine\n    steps:               # Define sequence of steps to execute\n      # CI Phase: Setup and Build\n      - uses: actions/checkout@v4  # Get repository code\n      - name: Configure Git Credentials\n        run: |\n          git config user.name github-actions[bot]\n          git config user.email 41898282+github-actions[bot]@users.noreply.github.com\n      - uses: actions/setup-python@v5  # Install Python\n        with:\n          python-version: 3.x\n      - run: echo \"cache_id=$(date --utc '+%V')\" &gt;&gt; $GITHUB_ENV  # Setup caching\n      - uses: actions/cache@v4\n        with:\n          key: mkdocs-material-${{ env.cache_id }}\n          path: .cache\n          restore-keys: |\n            mkdocs-material-\n      - run: pip install -r requirements.txt  # Install dependencies\n\n      # CD Phase: Deploy\n      - run: mkdocs gh-deploy --force  # Build site and deploy to GitHub Pages\n</code></pre>"},{"location":"DevOps/CI_CD_On_This_Site.html#general-workflow-file-structure","title":"General workflow File Structure:","text":"<pre><code>name: Workflow Name       # Display name in GitHub UI\non: [push, pull_request]  # Events that trigger the workflow\njobs:                     # Groups of steps to run\n  job_name:               # A single job definition\n    runs-on: ubuntu-latest # Environment to run on\n    steps:                 # Ordered list of actions to take\n      - uses: actions/checkout@v4  # Use a predefined action\n      - name: Custom Step          # Run a custom command\n        run: echo \"Hello World\"\n</code></pre>"},{"location":"DevOps/CI_CD_On_This_Site.html#the-cicd-process-step-by-step","title":"The CI/CD Process Step-by-Step","text":"<p>The CI/CD process starts the momenent the user(I) creates any markdown file and push it to the <code>main</code> or <code>master</code> branch. Here\u2019s a step-by-step breakdown of how it works:</p>"},{"location":"DevOps/CI_CD_On_This_Site.html#step-by-step-breakdown-of-the-cicd-process","title":"Step-by-Step Breakdown of the CI/CD Process","text":"<ol> <li> <p>Triggering the Workflow - The Sequence:</p> <ul> <li>Step 1.1: I push changes to the <code>main</code> or <code>master</code> branch using Git on my local machine</li> <li>Step 1.2: My git client sends a POST request to GitHub's servers containing my commits</li> <li>Step 1.3: GitHub receives and stores these commits in its database</li> <li>Step 1.4: This storage action automatically triggers GitHub's event system</li> <li>Step 1.5: The event system identifies this as a \"push\" event and activates the webhook system</li> <li>Step 1.6: The webhook processor begins scanning my repository structure, specifically looking in the <code>.github/workflows/</code> directory</li> <li>Step 1.7: When it finds my <code>ci.yml</code> file, it reads the configuration to check if this workflow should run</li> <li>Step 1.8: The processor confirms the push was to <code>main</code> or <code>master</code> branch (as specified in the workflow trigger)</li> <li>Step 1.9: After validation succeeds, GitHub's workflow scheduler receives a signal to execute this workflow</li> <li>Step 1.10: The scheduler places this job in a queue for the next available runner   </li> </ul> </li> <li> <p>Setting Up the Environment - What Happens Next:</p> <ul> <li>Step 2.1: GitHub allocates an Ubuntu virtual machine from its runner pool</li> <li>Step 2.2: The runner VM boots up and initializes the GitHub Actions runner software</li> <li>Step 2.3: The runner software downloads my workflow configuration and begins executing it step by step</li> <li>Step 2.4: First, the <code>actions/checkout@v4</code> action runs, which clones my repository into the VM</li> <li>Step 2.5: After cloning, Git credentials are automatically configured for the deployment process</li> <li>Step 2.6: Next, <code>actions/setup-python@v5</code> runs to install Python on the VM</li> <li>Step 2.7: The runner then sets up dependency caching based on my configuration</li> <li>Step 2.8: Finally, it installs all required dependencies by running <code>pip install -r requirements.txt</code> </li> </ul> </li> <li> <p>Building and Deploying - The Build Process:</p> <ul> <li>Step 3.1: With the environment ready, the runner executes <code>mkdocs gh-deploy --force</code></li> <li>Step 3.2: MkDocs reads my configuration and processes all my Markdown files</li> <li>Step 3.3: It converts everything to HTML, CSS, and JavaScript in a temporary <code>site/</code> directory</li> <li>Step 3.4: MkDocs then initializes a Git repository within this directory</li> <li>Step 3.5: It stages all the generated files and creates a commit</li> <li>Step 3.6: Using the previously configured Git credentials, it pushes to the <code>gh-pages</code> branch</li> <li>Step 3.7: The <code>--force</code> flag ensures the push succeeds even if there are conflicts   </li> </ul> </li> <li> <p>Publication to GitHub Pages - After the Push:</p> <ul> <li>Step 4.1: GitHub receives the pushed content to the <code>gh-pages</code> branch(https://github.com/dwdas9/home/tree/gh-pages)</li> <li>Step 4.2: This push automatically triggers GitHub Pages' deployment system</li> <li>Step 4.3: GitHub Pages processes the content and prepares it for publication</li> <li>Step 4.4: The system uploads the files to GitHub's CDN for global distribution</li> <li>Step 4.5: DNS is configured to serve this content at my custom URL</li> <li>Step 4.6: The website becomes publicly available at <code>https://dwdas9.github.io/home</code> </li> </ul> </li> </ol>"},{"location":"DevOps/CI_CD_On_This_Site.html#breaking-down-ci-vs-cd-in-our-workflow","title":"Breaking Down CI vs. CD in Our Workflow","text":"<p>The ci.yml is all-in-one file that contains both CI and CD processes. This is a common practice for smaller projects, but larger projects may separate these concerns into different workflow files.</p>"},{"location":"DevOps/CI_CD_On_This_Site.html#continuous-integration-ci-components","title":"Continuous Integration (CI) Components","text":"<ul> <li>Code Checkout: Fetching the latest code from the repository</li> <li>Environment Setup: Installing Python and configuring Git credentials</li> <li>Dependency Management: Installing required packages from requirements.txt</li> <li>Caching: Optimizing build speed with smart caching</li> </ul>"},{"location":"DevOps/CI_CD_On_This_Site.html#continuous-deployment-cd-components","title":"Continuous Deployment (CD) Components","text":"<ul> <li>Build Process: Converting markdown to HTML (part of the mkdocs command)</li> <li>Deployment: Pushing built files to the gh-pages branch</li> <li>Publication: Making the site available through GitHub Pages</li> </ul>"},{"location":"DevOps/CI_CD_On_This_Site.html#appendix","title":"Appendix","text":""},{"location":"DevOps/CI_CD_On_This_Site.html#github-actions","title":"GitHub Actions","text":"<p>The CI/CD platform on GitHub is called GitHub Actions. It allows you to automate workflows directly from your GitHub repository. Here are some key features:</p> <ul> <li>Integrated Automation: Run workflows directly from your repository</li> <li>Event-Driven: Trigger workflows based on repository events (pushes, pull requests, etc.)</li> <li>Customizable: Build complex automation with reusable actions and custom scripts</li> <li>Scalable: Use GitHub-hosted runners or add your own self-hosted runners</li> </ul>"},{"location":"DevOps/CI_CD_On_This_Site.html#how-github-actions-works-behind-the-scenes","title":"How GitHub Actions Works Behind the Scenes","text":"<p>GitHub Actions operates through a sophisticated event system:</p> <ol> <li> <p>Event Detection</p> <ul> <li>GitHub continuously monitors repository events (pushes, pull requests, issues, etc.)</li> <li>When an event occurs, GitHub's webhook system generates a notification</li> </ul> </li> <li> <p>Workflow Discovery</p> <ul> <li>GitHub examines all <code>.github/workflows/*.yml</code> files in your repository</li> <li>It identifies workflows configured to respond to the triggered event</li> </ul> </li> <li> <p>Configuration Parsing</p> <ul> <li>For matching workflows, GitHub parses the <code>on</code> field to confirm the event match</li> <li>It also checks any filters (like branch names or file paths)</li> </ul> </li> <li> <p>Execution Environment</p> <ul> <li>GitHub allocates a runner (virtual machine) for the workflow</li> <li>The runner creates a clean environment for each workflow run</li> </ul> </li> <li> <p>Step Execution</p> <ul> <li>The runner processes each step in sequence</li> <li>It reports progress and collects logs for monitoring</li> </ul> </li> </ol>"},{"location":"DevOps/CI_CD_On_This_Site.html#workflows","title":"Workflows","text":"<p>A workflow is an automated process(e.g. a CI/CD process/pipline) which is written in YAML and stored in the <code>.github/workflows/</code> folder inside the root folder of the repository. It consists of one or more jobs that run sequentially or in parallel. Each job can have multiple steps, which are individual tasks executed in order.</p>"},{"location":"DevOps/CI_CD_On_This_Site.html#events","title":"Events","text":"<p>Events are triggers that start workflows. Common events include: - push: Triggered when code is pushed to a branch - pull_request: Triggered when a pull request is created or updated - schedule: Triggered at specific times (like a cron job) - repository_dispatch: Triggered by external webhooks - workflow_dispatch: Manually triggered workflows</p>"},{"location":"DevOps/CI_CD_On_This_Site.html#jobs","title":"Jobs","text":"<p>Jobs are units of work within a workflow. Each job runs in its own environment and can have dependencies on other jobs. Jobs can run in parallel or sequentially, depending on how they are defined.</p>"},{"location":"DevOps/CI_CD_On_This_Site.html#steps","title":"Steps","text":"<p>Steps are individual tasks within a job. Each step can run a command, use an action, or execute a script. Steps are executed in the order they are defined in the workflow file.</p>"},{"location":"DevOps/CI_CD_On_This_Site.html#actions","title":"Actions","text":"<p>Actions are reusable units of work that can be shared across workflows. They can be created by GitHub or by the community. Actions can be used to perform tasks like checking out code, setting up environments, or deploying applications.</p>"},{"location":"DevOps/CI_CD_On_This_Site.html#runners","title":"Runners","text":"<p>Runners are the virtual machines that execute the jobs defined in your workflows. GitHub provides hosted runners, or you can set up self-hosted runners on your own infrastructure. Runners can be configured with different operating systems and software environments.</p>"},{"location":"DevOps/CI_CD_On_This_Site.html#what-are-the-components-called-in-azure-devops","title":"What are the components called in Azure DevOps?","text":"GitHub Component Azure DevOps Equivalent Description GitHub Actions Azure Pipelines The overall CI/CD service .github/workflows/ci.yml azure-pipelines.yml The pipeline definition file GitHub Runner Azure Pipeline Agent The execution environment that runs the CI/CD jobs Self-hosted Runner Self-hosted Agent Custom machines that you maintain to run jobs Workflow Pipeline A configurable automated process with one or more jobs Job Job A set of steps executed on the same runner/agent Step Task An individual unit of work within a job Actions Tasks Reusable units of work GitHub Secrets Azure DevOps Variable Groups/Library Secure storage for sensitive information Triggers (on: push) Triggers (trigger: branches) Event conditions that start a workflow/pipeline matrix strategy matrix strategy Running the same job with different configurations repository_dispatch Service hooks Custom webhook triggers GITHUB_ENV Pipeline variables Environment variables checkout action checkout task Fetching repository code workflow_dispatch Manual triggers Manually triggered runs"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps.html","title":"Tools Comparison","text":""},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps.html#jenkins-vs-github-vs-azure-devops","title":"Jenkins Vs GitHub Vs Azure DevOps","text":"<p> CI/CD makes your code building and deployment automatic so that you can build and deploy yoru code to PROD quickly . CI/CD is implemented using workflows. In GitHub they are called GitHub Actions workflows, In Azure DevOps they are called Azure Pipelines.  In this article, I will give you a quick summary of the popular CI/CD tools. </p> <p> For busy people, GitHub is the best. It's very well integrated with VS Code and has everything in one place \u2013 repositories, version control, CI/CD, many VS Code extensions, GitHub Desktop, and much more. Learning GitHub is the easiest among all the options. </p>"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps.html#jenkins","title":"Jenkins","text":""},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps.html#what-is-jenkins","title":"What is Jenkins?","text":"<p>Jenkins is an open-source CI/CD software mainly based on Java. It helps with CI/CD, but for version control, you need to use something else like GitHub.</p>"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps.html#hosting-and-pricing","title":"Hosting and Pricing","text":"<p>Jenkins is self-hosted, meaning you need to install and manage it on your infrastructure, whether on-premise or on cloud services like Azure, AWS, or Google Cloud. Jenkins is free software, but you will have to pay for the infrastructure, servers, maintenance, and everything else.</p>"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps.html#best-for","title":"Best For","text":"<p>If your company prefers open-source tools and needs a lot of customization, Jenkins is a good choice. You can control everything and customize it heavily. But remember, you have to handle all the infrastructure, installation, maintenance, and troubleshooting yourself. Companies like Netflix, Etsy, and Yahoo use Jenkins. Note, Jenkins doesn't come with version control like Git, which you'll need for CI/CD projects.</p>"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps.html#github","title":"GitHub","text":"<p>GitHub is the most popular, the easiest, and the most all-in-one choice for CI/CD. It provides version control and CI/CD all as a service. You can have your own runners too (servers) for hybrid setups and to reduce costs. Personally, this is my first choice.</p>"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps.html#what-is-github-actions","title":"What is GitHub Actions?","text":"<p>The CI/CD part of GitHub is done using GitHub Actions. These are workflows for building, deploying, etc., written in a .yml file.</p>"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps.html#hosting-and-pricing_1","title":"Hosting and Pricing","text":"<p>GitHub Actions is cloud-based. It uses GitHub\u2019s infrastructure, so you don\u2019t need to manage any servers. But you can have your own runners (hybrid setup) if you want more customization and don't want to pay a lot for services. GitHub is free for public repositories. For private repositories, a number of hosted runner minutes are included with each pricing tier. Additional minutes cost $0.008 per minute for Linux.</p>"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps.html#best-for_1","title":"Best For","text":"<ul> <li>The easiest, most all-in-one solution with version control (GitHub repositories) and CI/CD (GitHub Actions) together.</li> <li>Examples: Facebook, Airbnb, many open-source projects.</li> </ul>"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps.html#azure-devops","title":"Azure DevOps","text":""},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps.html#what-is-azure-devops","title":"What is Azure DevOps?","text":"<p>Azure DevOps is a collection of development tools by Microsoft that supports the entire software development lifecycle, including Git repositories, CI/CD pipelines, and more. It was earlier known as TFS.</p> <p>So, like GitHub, it also has its own repositories and version control. It can connect easily with Git repositories as well.</p>"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps.html#hosting-and-pricing_2","title":"Hosting and Pricing","text":"<p>Azure DevOps offers both cloud and self-hosted options. You can use Microsoft\u2019s cloud infrastructure or set up your own servers. Azure DevOps is:</p> <ul> <li>Free for open-source projects with up to 10 parallel jobs.</li> <li>Basic plan: Free for the first 5 users, then $6 per user/month with one free pipeline.</li> <li>Additional pipelines start at $40/month for cloud-hosted or $15/month for self-hosted.</li> </ul>"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps.html#best-for_2","title":"Best For","text":"<ul> <li>Teams heavily invested in the Microsoft ecosystem.</li> <li>Organizations needing integration with Azure services.</li> <li>Examples: Microsoft, Adobe, Accenture.</li> </ul>"},{"location":"DevOps/JenkinsVsGitHubVsAzureDevOps.html#lets-put-the-comparison-in-a-table","title":"Let's put the comparison in a table","text":"CI Tool Open Source Hosting Free Version Build Agent Pricing Supported Platforms GitHub Actions No Cloud Yes Additional minutes start at $0.008 per minute Linux, Windows, macOS Jenkins Yes Self-hosted Yes Free Linux, Windows, macOS GitLab CI No Cloud/Self-hosted Yes Additional units start at $10 for 1,000 minutes Linux, Windows, macOS, Docker Azure DevOps No Cloud/Self-hosted Yes Additional pipelines start at $15/month (self-hosted) Linux, Windows, macOS"},{"location":"DevOps/Docker/ContainerStacks/Airflow/1_AirflowDocker.html","title":"Airflow on Docker","text":""},{"location":"DevOps/Docker/ContainerStacks/Airflow/1_AirflowDocker.html#apache-airflow-on-windows-docker","title":"Apache Airflow on Windows Docker","text":"<p>Here, I will show you how to install Airflow on a Docker container using two methods:</p>"},{"location":"DevOps/Docker/ContainerStacks/Airflow/1_AirflowDocker.html#method-1-simple-development-setup","title":"Method 1: Simple Development Setup","text":"<ul> <li>Single Container: This method runs everything inside one container. It includes all Airflow services like the scheduler, webserver, worker, etc., bundled together.</li> <li>Database: Uses SQLite as the database</li> <li>Docker Volume: A Docker volume is used to store data.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/Airflow/1_AirflowDocker.html#method-2-full-production-setup","title":"Method 2: Full Production Setup","text":"<ul> <li>Number of Containers: This method sets up seven containers. These include:</li> <li>Scheduler: Manages task scheduling.</li> <li>Webserver: Provides the Airflow user interface.</li> <li>Worker: Executes the tasks.</li> <li>PostgreSQL: Used as the database to store metadata.</li> <li>Redis: Acts as a message broker between the components.</li> <li>Triggerer: Manages task triggering.</li> <li>Flower: For monitoring the Celery workers.</li> <li>Local Folders: Data is stored in local(laptop) folders.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/Airflow/1_AirflowDocker.html#method-1-simple-development-setup_1","title":"Method 1 - Simple Development Setup","text":""},{"location":"DevOps/Docker/ContainerStacks/Airflow/1_AirflowDocker.html#download-airflow-docker-image","title":"Download Airflow Docker Image","text":"<p>Run the following command in your command prompt or power shell to pull the latest Airflow Docker image: <code>docker pull apache/airflow:latest</code></p> <p></p>"},{"location":"DevOps/Docker/ContainerStacks/Airflow/1_AirflowDocker.html#create-a-docker-volume","title":"Create a Docker Volume","text":"<p>Execute this command to create a Docker volume named <code>airflow-volume</code> for data persistence: <code>docker volume create airflow-volume</code></p>"},{"location":"DevOps/Docker/ContainerStacks/Airflow/1_AirflowDocker.html#initialize-airflow-database","title":"Initialize Airflow Database","text":"<p>Initialize the Airflow database using the following two commands: <pre><code>docker run --rm --network dasnet -v airflow-volume:/opt/airflow apache/airflow:latest db init\n</code></pre> <pre><code>docker run --rm --network dasnet -v airflow-volume:/opt/airflow apache/airflow:latest users create  --username airflow  --firstname FIRST_NAME  --lastname LAST_NAME   --role Admin   --email admin@example.com   --password airflow\n</code></pre></p> <p>Note: I use a network dasnet. Hence --network part. You can totally remove the --network.</p>"},{"location":"DevOps/Docker/ContainerStacks/Airflow/1_AirflowDocker.html#start-the-airflow-webserver","title":"Start the Airflow Webserver","text":"<p>To start the Airflow webserver, use this command:</p> <p><pre><code>docker run -d --name airflow --network dasnet -p 8080:8080 -e AIRFLOW_UID=50000 -v airflow-volume:/opt/airflow apache/airflow:latest webserver\n</code></pre> </p> <p>Note: I use a network dasnet. Hence --network part. You can totally remove the --network.</p>"},{"location":"DevOps/Docker/ContainerStacks/Airflow/1_AirflowDocker.html#login-into-airflow-ui","title":"Login into Airflow UI","text":"<p>To login open http://localhost:8080 and enter credential: airflow/airflow</p>"},{"location":"DevOps/Docker/ContainerStacks/Airflow/1_AirflowDocker.html#method-2-full-production-setup_1","title":"Method 2 - Full Production Setup","text":""},{"location":"DevOps/Docker/ContainerStacks/Airflow/1_AirflowDocker.html#create-required-folders","title":"Create Required Folders","text":"<ul> <li>Create a base directory, anywhere, for Airflow, e.g., <code>C:\\Airflow_Das</code>.</li> <li>Within this directory, create three subdirectories: <code>dags</code>, <code>plugins</code>, and <code>logs</code>.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/Airflow/1_AirflowDocker.html#download-the-docker-compose-file","title":"Download the Docker Compose File","text":"<ul> <li>Save the  <code>docker-compose.yaml</code> from link to <code>Airflow_Das</code> folder. </li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/Airflow/1_AirflowDocker.html#note-on-docker-composeyaml","title":"Note on <code>docker-compose.yaml</code>","text":"<ul> <li>When this article was written, the Airflow image used was <code>apache/airflow:2.7.2</code>. You can find the relevant <code>docker-compose.yaml</code> file here.</li> <li>If the link doesn\u2019t work, visit the Apache Airflow site and search for the latest <code>docker-compose.yaml</code>.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/Airflow/1_AirflowDocker.html#initialize-and-run-airflow","title":"Initialize and Run Airflow","text":"<ul> <li>Open PowerShell(with admin priv) and cd to <code>Airflow_Das</code></li> <li>Run command: <code>docker-compose up airflow-init</code> </li> <li>Then run command: <code>docker-compose up</code>     You can see the logs cascading down your PowerShell window. Wait a few seconds and then you can safely close the window.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/Airflow/1_AirflowDocker.html#verify-the-installation","title":"Verify the Installation","text":"<ul> <li> <p>On Docker Desktop, look for a container named <code>Airflow_Das</code>, containing seven subcontainers.</p> <p></p> </li> </ul> <p>Note: airflow-init-1, init container will exit after initialization. This is the expected, normal, beheviour. Don't panic.</p> <ul> <li>Open <code>localhost:8080</code> in a web browser.</li> <li>Log in with the username and password: <code>airflow</code>.     </li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/Airflow/1_AirflowDocker.html#components-installed","title":"Components Installed","text":"<p>The table shows some important components of our Airflow setup.</p> Component What It Does Environment Variables Folders Ports Command Locations Inside Container Webserver The main part of Airflow where you can see and manage your workflows, logs, etc. <code>AIRFLOW__CORE__EXECUTOR</code> (sets the type of executor), <code>AIRFLOW__WEBSERVER__WORKERS</code> (number of workers) <code>./dags:/opt/airflow/dags</code>, <code>./logs:/opt/airflow/logs</code>, <code>./plugins:/opt/airflow/plugins</code> <code>8080:8080</code> <code>airflow webserver</code> <code>/opt/airflow</code> (inside container) Scheduler Handles the scheduling of workflows, making sure tasks run on time. <code>AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL</code> (how often to check the DAG folder) <code>./dags:/opt/airflow/dags</code>, <code>./logs:/opt/airflow/logs</code>, <code>./plugins:/opt/airflow/plugins</code> N/A <code>airflow scheduler</code> <code>/opt/airflow</code> Worker Runs the tasks in the workflows. Needed when using CeleryExecutor. <code>AIRFLOW__CORE__EXECUTOR</code> (CeleryExecutor), <code>AIRFLOW__CELERY__BROKER_URL</code> (URL for Celery broker) <code>./dags:/opt/airflow/dags</code>, <code>./logs:/opt/airflow/logs</code>, <code>./plugins:/opt/airflow/plugins</code> N/A <code>airflow celery worker</code> <code>/opt/airflow</code> Postgres The database that stores all the Airflow information like DAGs, task statuses, etc. <code>POSTGRES_USER=airflow</code>, <code>POSTGRES_PASSWORD=airflow</code>, <code>POSTGRES_DB=airflow</code> <code>postgres_data:/var/lib/postgresql/data</code> N/A <code>postgres</code> <code>/var/lib/postgresql/data</code> Redis A messaging service that helps workers communicate with each other when using CeleryExecutor. <code>REDIS_PASSWORD=redis_password</code> (if you want to secure it) <code>redis_data:/data</code> N/A <code>redis-server</code> <code>/data</code> Flower A tool to monitor and manage Celery workers and tasks. <code>FLOWER_BASIC_AUTH=admin:password</code> (to secure it) N/A <code>5555:5555</code> <code>flower</code> N/A"},{"location":"DevOps/Docker/ContainerStacks/Airflow/1_AirflowDocker.html#reference","title":"Reference","text":"<p>Airflow official Docker Link</p> <p>\u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html","title":"Ubuntu, Python, OpenJDK &amp; PySpark","text":"<p>In this article I will show you how to create Docker containers with Pyspark and Spark components.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#for-busy-people","title":"For Busy People","text":"<ol> <li>Save the Dockerfile content as <code>Dockerfile</code> (no extension).</li> <li><code>cd</code> to the folder containtng  the Dockerfile</li> <li>Run Commands:    <pre><code>docker build -t ubuntu-pyspark .\ndocker run -it --name Ubuntu-PySpark --network dasnet ubuntu-pyspark\n</code></pre></li> </ol> <p>That\u2019s it!</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#steps-to-create-the-image-and-container","title":"Steps to Create the Image and Container","text":"<p>In this article I will show you how to create a single container with Ubuntu OS, Python and PySpark. We will use just a dockerfile to create it.</p> <p>Follow the steps below to create the container.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#create-the-dockerfile","title":"Create the Dockerfile","text":"In a folder create a file Dockerfile (No extension) with the content below. <pre><code># Use Ubuntu 20.04 as the base image to avoid \"externally-managed-environment\" restrictions\nFROM ubuntu:20.04\n\n# Set environment variable to avoid interactive prompts during package installation\nENV DEBIAN_FRONTEND=noninteractive\n\n# Update the package list to ensure we have the latest information about available packages\nRUN apt-get update\n\n# Install necessary packages including curl, sudo, and nano\nRUN apt-get install -y curl sudo nano software-properties-common\n\n# Add the 'deadsnakes' PPA (Personal Package Archive) to access newer Python versions\nRUN add-apt-repository ppa:deadsnakes/ppa\n\n# Add the OpenJDK PPA to get the latest JDK versions\nRUN add-apt-repository ppa:openjdk-r/ppa\n\n# Update the package list again to include the new PPAs\nRUN apt-get update\n\n# Install Python 3.12, pip, and OpenJDK 17\nRUN apt-get install -y python3.12 python3-pip openjdk-17-jdk-headless\n\n# Install the PySpark library using pip\nRUN pip3 install pyspark\n\n# Clean up the package lists to reduce the image size\nRUN apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Create a root user and set its password\nRUN echo 'root:Passw0rd' | chpasswd\n\n# Create a new user 'dwdas', set a password, and add this user to the sudo group\nRUN useradd -ms /bin/bash dwdas &amp;&amp; echo 'dwdas:Passw0rd' | chpasswd &amp;&amp; adduser dwdas sudo\n\n# Allow the 'dwdas' user to run sudo commands without a password\nRUN echo 'dwdas ALL=(ALL) NOPASSWD:ALL' &gt;&gt; /etc/sudoers\n\n# Set the working directory to the home directory of the new user\nWORKDIR /home/dwdas\n\n# Switch to the new user 'dwdas'\nUSER dwdas\n\n# Expose port 8888, commonly used for Jupyter Notebook, if needed\nEXPOSE 8888\n\n# Set the default command to start a bash shell\nCMD [\"bash\"]\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#build-the-image","title":"Build the Image","text":"<p>Open CMD, navigate to the folder with the Dockerfile, and run:</p> <pre><code>docker build -t ubuntu-pyspark-img .\n</code></pre> <p>After successfully running the command, you will see an image in your Docker Desktop app:</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#run-the-docker-container","title":"Run the Docker Container","text":"<p>In command prompt, run:</p> <pre><code>docker run -it --name Debian-PySpark --network dasnet debian-pyspark\n</code></pre> <p>This will create a container with the image we created earlier and start it. You can see it from the Container section of your Docker window.</p> <p></p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#details-of-the-container","title":"Details of the container","text":"<p>Here are the details of the installed components. The table will be a handy reference to know which components are installed and important locations, variables etc.</p> Component Details Base Image <code>ubuntu:20.04</code> Python Version Python 3.12, installed via the <code>deadsnakes</code> PPA Java Version OpenJDK 17 (Headless), installed via the <code>openjdk-r</code> PPA PySpark Version Latest version of PySpark installed via pip Home Directory for User <code>/home/dwdas</code> Spark Home <code>/opt/bitnami/spark</code> Java Home <code>/opt/bitnami/java</code> Python Path <code>/opt/bitnami/spark/python/</code> (for PySpark integration) Spark Configuration Directory <code>/opt/bitnami/spark/conf</code> Spark Worker Directory <code>/opt/bitnami/spark/work</code> Environment Variables <code>DEBIAN_FRONTEND=noninteractive</code> to avoid interactive prompts during installation User Created <code>dwdas</code> with sudo privileges and passwordless sudo access Exposed Port Port <code>8888</code>, commonly used for Jupyter Notebooks Default Command <code>bash</code> shell set as the default command Network Configuration Connected to the <code>dasnet</code> network Spark Ports Spark Master: <code>7077</code> (mapped to host port <code>17077</code>), Spark Master UI: <code>8080</code> (mapped to host port <code>16080</code>), Spark Worker UI: <code>8081</code> (mapped to host port <code>16002</code>), <code>8082</code> (mapped to host port <code>16004</code>)"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#error-package-not-found-404-not-found","title":"Error: Package Not Found (404 Not Found)","text":"<p>When building the Docker image, I got a <code>404 Not Found</code> error because some packages like <code>python3.12</code> and <code>openjdk-17-jdk-headless</code> couldn't be found. This usually happens if the package lists are outdated or there's an issue with the repositories. Here's how to fix it:</p> <ol> <li> <p>Update Package Lists: Run <code>apt-get update</code> first to make sure your package lists are current.</p> </li> <li> <p>Add Correct PPAs: Update the Dockerfile to include these PPAs:</p> </li> <li><code>deadsnakes</code> for Python.</li> <li> <p><code>openjdk-r</code> for OpenJDK.</p> </li> <li> <p>Use <code>--fix-missing</code> Option: If the problem continues, try <code>apt-get install --fix-missing</code> to fix missing packages.</p> </li> <li> <p>Install Specific Versions: If the latest version isn't available, try installing a slightly older but stable version.</p> </li> </ol>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#debian-downloaded-python-pyspark-no-venv","title":"Debian, Downloaded Python, Pyspark - no venv.","text":"<p>This Section shows you how to create a Docker container with the latest Debian, Python 3.11, OpenJDK 17, and PySpark. We\u2019ll set up a root user and a named user with essential environment variables.</p> <p>Note: If you install python using  apt-get install in new Debain it will ask you to install in venv mode. We want to avoid this. Hence we download it(weget) then intstall it manually.</p> <p>We\u2019ll use a Dockerfile and docker-compose.yml for the setup.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#steps-to-create-the-image-and-container_1","title":"Steps to Create the Image and Container","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#create-a-dockerfile","title":"Create a Dockerfile:","text":"Create a Dockerfile.txt with the contents below and remove the .txt extension <pre><code>  # Use Debian as the base image\n  FROM debian:latest\n\n  # Set environment variable to avoid interactive prompts during package installation\n  ENV DEBIAN_FRONTEND=noninteractive\n\n  # Update the package lists and install essential packages\n  RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\n    curl \\\n    wget \\\n    tar \\\n    bash \\\n    ca-certificates \\\n    sudo \\\n    build-essential \\\n    libssl-dev \\\n    zlib1g-dev \\\n    libbz2-dev \\\n    libreadline-dev \\\n    libsqlite3-dev \\\n    libffi-dev\n\n  # Copy the Python source tarball into the image\n  COPY Python-3.11.9.tgz /tmp/\n\n  # Extract, build, and install Python 3.11.9\n  RUN cd /tmp &amp;&amp; \\\n      tar -xvf Python-3.11.9.tgz &amp;&amp; \\\n      cd Python-3.11.9 &amp;&amp; \\\n      ./configure --enable-optimizations &amp;&amp; \\\n      make -j 8 &amp;&amp; \\\n      make altinstall &amp;&amp; \\\n      cd .. &amp;&amp; \\\n      rm -rf Python-3.11.9 Python-3.11.9.tgz\n\n  # Create symbolic links for python, python3, pip, and pip3\n  RUN ln -s /usr/local/bin/python3.11 /usr/bin/python &amp;&amp; \\\n      ln -s /usr/local/bin/python3.11 /usr/bin/python3 &amp;&amp; \\\n      ln -s /usr/local/bin/pip3.11 /usr/bin/pip &amp;&amp; \\\n      ln -s /usr/local/bin/pip3.11 /usr/bin/pip3\n\n  # Install OpenJDK 17\n  RUN apt-get install -y openjdk-17-jdk-headless\n\n  # Install the PySpark library using pip\n  RUN python3.11 -m pip install pyspark\n\n  # Set environment variables\n  ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\n  ENV PYTHONPATH=/usr/local/lib/python3.11/dist-packages\n  ENV PYSPARK_PYTHON=/usr/local/bin/python3.11\n  ENV PATH=$PATH:$JAVA_HOME/bin\n\n  # Clean up the package lists to reduce the image size\n  RUN apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n  # Create a root user and set its password\n  RUN echo 'root:Passw0rd' | chpasswd\n\n  # Create a new user 'dwdas', set a password, and add this user to the sudo group\n  RUN useradd -ms /bin/bash dwdas &amp;&amp; echo 'dwdas:Passw0rd' | chpasswd &amp;&amp; adduser dwdas sudo\n\n  # Allow the 'dwdas' user to run sudo commands without a password\n  RUN echo 'dwdas ALL=(ALL) NOPASSWD:ALL' &gt;&gt; /etc/sudoers\n\n  # Set the working directory to the home directory of the new user\n  WORKDIR /home/dwdas\n\n  # Switch to the new user 'dwdas'\n  USER dwdas\n\n  # Expose port 8888, commonly used for Jupyter Notebook, if needed\n  EXPOSE 8888\n\n  # Set the default command to start a bash shell\n  CMD [\"bash\"]\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#download-python-and-place-in-the-same-folder","title":"Download  Python and place in the same folder","text":"<p>Download Python 3.11.9 from this site and place it in the same directory.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#build-the-docker-image","title":"Build the Docker Image:","text":"<ul> <li>Open a terminal and navigate to the directory containing the Dockerfile.</li> <li> <p>Run the following command to build the Docker image:</p> <pre><code>docker build -t my-debian-pyspark .\n</code></pre> </li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#run-the-docker-container_1","title":"Run the Docker Container:","text":"<ul> <li> <p>Once the image is built, run the container using the command:</p> <pre><code>docker run -it --name my-debian-pyspark-container my-debian-pyspark\n</code></pre> </li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#details-of-the-container_1","title":"Details of the Container","text":"Category Details Base Image Debian (latest) Python Version Python 3.11.9 Java Version OpenJDK 17 PySpark Version Latest via pip Environment Variables <code>JAVA_HOME</code>: <code>/usr/lib/jvm/java-17-openjdk-amd64</code>, <code>PYTHONPATH</code>: <code>/usr/local/lib/python3.11/dist-packages</code>, <code>PYSPARK_PYTHON</code>: <code>/usr/local/bin/python3.11</code>, <code>PATH</code>: <code>$PATH:$JAVA_HOME/bin</code> Installed Packages Build tools (curl, wget, tar, etc.), Python 3.11.9 (source), OpenJDK 17, PySpark (pip) User Configuration Root user &amp; <code>dwdas</code> (password: <code>Passw0rd</code>, sudo access) Exposed Port 8888 (for Jupyter) Default Command Bash shell start"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#debian-pip-python-pip-pyspark-venv","title":"Debian, Pip Python, Pip Pyspark - venv.","text":"<p>This section shows you how to create a Docker container with the latest Debian, Python 3.11, OpenJDK 17, and PySpark using the recommended venv approach. We\u2019ll set up a root user and a named user with essential environment variables.</p> <p>Note: Newer Debian versions enforce using venv for pip install. We will install Python using apt-get and set up venv from the command line.</p> <p>We\u2019ll use a Dockerfile and docker-compose.yml for the setup.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#steps-to-create-the-container","title":"Steps to Create the  Container","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#create-a-dockerfile_1","title":"Create a Dockerfile:","text":"Create a Dockerfile with the following content: <pre><code>  # Use the latest version of Debian as the base image\n  FROM debian:latest\n\n  # Set environment variable to avoid interactive prompts during package installation\n  ENV DEBIAN_FRONTEND=noninteractive\n\n  # Update the package lists and install essential packages\n  RUN apt-get update &amp;&amp; \\\n      apt-get install -y curl wget tar bash ca-certificates sudo gnupg\n\n  # Install Python 3.11, venv, pip, and OpenJDK 17\n  RUN apt-get install -y python3.11 python3.11-venv python3.11-dev python3-pip openjdk-17-jdk-headless\n\n  # Create a virtual environment\n  RUN python3.11 -m venv /opt/venv\n\n  # Activate the virtual environment and install PySpark\n  RUN /opt/venv/bin/python -m pip install pyspark\n\n  # Set environment variables\n  ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\n  ENV PYTHONPATH=/opt/venv/lib/python3.11/site-packages\n  ENV PYSPARK_PYTHON=/opt/venv/bin/python\n  ENV PATH=$PATH:$JAVA_HOME/bin:/opt/venv/bin\n\n  # Clean up the package lists to reduce the image size\n  RUN apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n  # Create a root user and set its password\n  RUN echo 'root:Passw0rd' | chpasswd\n\n  # Create a new user 'dwdas', set a password, and add this user to the sudo group\n  RUN useradd -ms /bin/bash dwdas &amp;&amp; echo 'dwdas:Passw0rd' | chpasswd &amp;&amp; adduser dwdas sudo\n\n  # Allow the 'dwdas' user to run sudo commands without a password\n  RUN echo 'dwdas ALL=(ALL) NOPASSWD:ALL' &gt;&gt; /etc/sudoers\n\n  # Set the working directory to the home directory of the new user\n  WORKDIR /home/dwdas\n\n  # Switch to the new user 'dwdas'\n  USER dwdas\n\n  # Expose port 8888, commonly used for Jupyter Notebook, if needed\n  EXPOSE 8888\n\n  # Set the default command to start a bash shell\n  CMD [\"bash\"]\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#build-the-docker-image_1","title":"Build the Docker Image:","text":"<ul> <li>Open a terminal and navigate to the directory containing the Dockerfile.</li> <li> <p>Run the following command to build the Docker image:</p> <pre><code>docker build -t my-debian-pyspark-venv .\n</code></pre> </li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#run-the-docker-container_2","title":"Run the Docker Container:","text":"<ul> <li> <p>Once the image is built, run the container using the command:</p> <pre><code>docker run -it --name my-debian-pyspark-venv-container my-debian-pyspark-venv\n</code></pre> </li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#details-of-the-container_2","title":"Details of the Container","text":"Category Details Base Image Debian (latest) Python Version Python 3.11.9 Java Version OpenJDK 17 PySpark Version Installed via pip in a virtual environment Virtual Environment Created with <code>python3.11 -m venv /opt/venv</code> Environment Variables <code>JAVA_HOME</code>: <code>/usr/lib/jvm/java-17-openjdk-amd64</code>, <code>PYTHONPATH</code>: <code>/opt/venv/lib/python3.11/site-packages</code>, <code>PYSPARK_PYTHON</code>: <code>/opt/venv/bin/python</code>, <code>PATH</code>: <code>$PATH:$JAVA_HOME/bin:/opt/venv/bin</code> Installed Packages Essential tools (curl, wget, tar, bash, etc.), Python 3.11, venv, pip, OpenJDK 17, PySpark (in venv) User Configuration Root user &amp; <code>dwdas</code> (password: <code>Passw0rd</code>, sudo access) Exposed Port 8888 (for Jupyter) Default Command Bash shell start <p>To modify the setup so that the <code>conf</code> directory in the Spark container is mapped to a local folder on your machine (and the folder is auto-created), we need to update the <code>docker run</code> command to include a volume mapping. </p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#single-node-bitnami-spark-with-master-and-worker","title":"Single-Node Bitnami Spark With Master and Worker","text":"<p>Here, we will use the official Bitnami Spark Docker image to set up a single-node Spark environment. This setup will include both the Master and Worker.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#steps-to-create-the-container_1","title":"Steps to create the container","text":"<p>You can either download, unzip, and run the <code>.bat</code> file from this link to create the entire container. Or, you can follow the steps manually. Both methods will give the same result.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#create-a-custom-dockerfile","title":"Create a Custom Dockerfile","text":"Create a Dockerfile with the following content <pre><code>  # Use the official Bitnami Spark image as the base. I always pull a constant image and not :latest.\n  FROM bitnami/spark:3.5.2-debian-12-r2\n\n  # Step 1: Switch to root user to install software\n  # We need to be root to install utilities and set up sudo permissions.\n  USER root\n\n  # Step 2: Update the package list and install utilities. py4j and ipykernel is for VS studio connection.\n  # Install common utilities like sudo, ping, and nano.\n  # Update the base system\n  RUN apt-get update &amp;&amp; \\\n      apt-get install -y sudo nano iputils-ping grep curl wget vim net-tools procps lsof telnet &amp;&amp; \\\n      apt-get clean\n\n  # Install pip (if not already installed)\n  RUN apt-get install -y python3-pip\n\n  # Install py4j and ipykernel using pip. Required VS Code connection.\n  RUN pip3 install py4j ipykernel\n\n  # Step 3: Set the root user password to 'Passw0rd'\n  # This sets the root password to 'Passw0rd' for future access.\n  RUN echo \"root:Passw0rd\" | chpasswd\n\n  # Step 4: Give sudo privileges to the 'spark' user\n  # Here, we are allowing the 'spark' user to run commands as sudo without a password.\n  RUN echo \"spark ALL=(ALL) NOPASSWD: ALL\" &gt;&gt; /etc/sudoers\n\n  # After finishing the setup, we dont switch back to any user. The bitnami original Dockerfile switches to user 1001 and the directory is /opt/bitnami/spark\n\n  # Step 6: Expose necessary ports for Spark Web UI and communication\n  # 4040: Spark Worker Web UI\n  # 7077: Spark Master communication\n  # 8080: Spark Master Web UI\n  EXPOSE 4040 7077 8080\n\n  # End of the Dockerfile\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#create-a-local-conf-directory","title":"Create a Local <code>conf</code> Directory","text":"<p>Before running the container, create a folder named <code>local-spark-conf</code> in the same folder where your Dockerfiles are. This folder will store the configuration files and will be mapped to the <code>conf</code> directory inside the container.</p> <pre><code>mkdir local-spark-conf\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#build-the-docker-image_2","title":"Build the Docker Image","text":"<p>Once the Dockerfile is ready, you can build the Docker image with the following command:</p> <pre><code>docker build -t bitnami-spark-single-node .\n</code></pre> <p>This command will create a Docker image called <code>bitnami-spark-single-node</code> using the Dockerfile you just created.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#run-the-container-and-map-the-conf-directory","title":"Run the Container and Map the <code>conf</code> Directory","text":"<p>Now, we run the Spark container with the <code>conf</code> directory mapped to the local folder you created earlier. If the folder doesn\u2019t exist, Docker will create it.</p> <pre><code>docker run -d --network dasnet --name bitnami-spark-single-node -p 4040:4040 -p 8080:8080 -p 7077:7077 -v ./local-spark-conf:/opt/spark/conf bitnami-spark-single-node\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#configuration-reference","title":"Configuration Reference","text":"<p>Here is a list of some components in this environment. The list is compiled from the official bitnami spark github page.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#environment-details","title":"Environment Details","text":"Component Value OS debian 12: bitnami/minideb:bookworm Python python-3.12.5-1: /opt/bitnami/python/bin/python PYTHONPATH /opt/bitnami/spark/python/ Java java-17.0.12-10-1: JAVA_HOME = /opt/bitnami/java JAVA_HOME /opt/bitnami/java SPARK_HOME /opt/bitnami/spark SPARK_USER spark SPARK JARS Location for Installing External Jars /opt/bitnami/spark/jars Workdir /opt/bitnami/spark User 1001 Entrypoint /opt/bitnami/scripts/spark/entrypoint.sh Command /opt/bitnami/scripts/spark/run.sh Certificates /opt/bitnami/spark/conf/certs SPARK_SSL_KEYSTORE_FILE /opt/bitnami/spark/conf/certs/spark-keystore.jks SPARK_MODE master SPARK_MASTER_URL spark://spark-master:7077 SPARK_SSL_ENABLED no Docker Logs Command docker logs bitnami-spark-single-node <p>Note: The Dockerfile also install py4j and ipykernel. These are reuqired for VS code to container using remote containers extension.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.1_PySpark.html#read-only-environment-variables-official-bitnami-github","title":"Read-only Environment Variables (Official Bitnami github)","text":"Name Description Value SPARK_BASE_DIR Spark installation directory. ${BITNAMI_ROOT_DIR}/spark SPARK_CONF_DIR Spark configuration directory. ${SPARK_BASE_DIR}/conf SPARK_DEFAULT_CONF_DIR Spark default configuration directory. ${SPARK_BASE_DIR}/conf.default SPARK_WORK_DIR Spark workspace directory. ${SPARK_BASE_DIR}/work SPARK_CONF_FILE Spark configuration file path. ${SPARK_CONF_DIR}/spark-defaults.conf SPARK_LOG_DIR Spark logs directory. ${SPARK_BASE_DIR}/logs SPARK_TMP_DIR Spark tmp directory. ${SPARK_BASE_DIR}/tmp SPARK_JARS_DIR Spark jar directory. ${SPARK_BASE_DIR}/jars SPARK_INITSCRIPTS_DIR Spark init scripts directory. /docker-entrypoint-initdb.d SPARK_USER Spark user. spark SPARK_DAEMON_USER Spark system user. spark SPARK_DAEMON_GROUP Spark system group. spark"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.2_Bitnami_Spark_Cluster.html","title":"Create a Spark cluster using Bitnami Spark Image <p>Here I will show you how you can setup a Spark Cluster, 1 master, 2 workers node. We will use the Bitnami Docker image for this. This cluster works wonderfully and the easiest of all setups. </p>  <p>Make sure Docker is installed and running on your machine. You can Download Docker Desktop for windows from here.</p>","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.2_Bitnami_Spark_Cluster.html#follow-just-three-steps-below-to-create-the-setup","title":"Follow just three steps below to create the setup","text":"<ul> <li>Create a file docker-compose.yml with the content from the docker-compose section.</li> <li>Create a file Dockerfile, with the content from Dockerfile. Note Dockerfile has NO extension.</li> <li> <p>Now open Command prompt and run the following commands   <pre><code>docker network create dasnet\ndocker-compose -p bitnami-spark-cluster build\ndocker-compose -p bitnami-spark-cluster up -d\n</code></pre> </p> </li> <li> <p>Open the Docker app and navigate to the container section. The containers should be up and running.   </p> </li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.2_Bitnami_Spark_Cluster.html#dockerfile-and-docker-compose","title":"Dockerfile and docker-compose","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.2_Bitnami_Spark_Cluster.html#dockerfile","title":"Dockerfile","text":"<p>Save the content below in file, Dockerfile(no extension)</p> <pre><code># Use the official Bitnami Spark image as the base image\n# To use the latest version just replace the line below with FROM bitnami/spark:latest\nFROM bitnami/spark:3.5\n\n# Switch to root user to install necessary packages and set permissions\nUSER root\n\n# Install sudo package\nRUN apt-get update &amp;&amp; apt-get install -y sudo\n\n# Add a non-root user named dwdas with a home directory and bash shell\nRUN useradd -ms /bin/bash dwdas\n\n# Set the password for dwdas as Passw0rd\nRUN echo \"dwdas:Passw0rd\" | chpasswd\n\n# Add the user to the sudo group and configure sudoers file to allow passwordless sudo\nRUN adduser dwdas sudo\nRUN echo \"dwdas ALL=(ALL) NOPASSWD:ALL\" &gt;&gt; /etc/sudoers\n\n# Ensure dwdas has write permissions to necessary directories and files\nRUN mkdir -p /opt/bitnami/spark/tmp &amp;&amp; chown -R dwdas:dwdas /opt/bitnami/spark/tmp\nRUN chown -R dwdas:dwdas /opt/bitnami/spark/conf\nRUN chown -R dwdas:dwdas /opt/bitnami/spark/work\nRUN chown -R dwdas:dwdas /opt/bitnami/spark/logs\n\n# Switch back to the non-root user\nUSER dwdas\n\n# Set the working directory\nWORKDIR /home/dwdas\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.2_Bitnami_Spark_Cluster.html#docker-composeyml","title":"docker-compose.yml","text":"<p>Here is the content for the docker-compose.yml file. Simply copy the contents into a file named <code>docker-compose.yml</code> in your folder.</p> <pre><code>version: '3.9'  # Specify the version of Docker Compose syntax. Latest is 3.9 on Aug 2024\n\nservices:  # Define the services (containers) that make up your application\n\n  master:\n    build:\n      context: .  # Build context is the current directory (where the Dockerfile is located)\n      dockerfile: Dockerfile  # Dockerfile to use for building the image\n    image: bitnami-spark-master  # Name the image for the master node\n    container_name: master  # Set a custom name for the master container\n    environment:\n      - SPARK_MODE=master  # Environment variable to set the Spark mode to master\n      - SPARK_RPC_AUTHENTICATION_ENABLED=no\n      - SPARK_RPC_ENCRYPTION_ENABLED=no\n      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n      - SPARK_SSL_ENABLED=no\n      - SPARK_USER=spark\n    ports:\n      # Mapping ports: &lt;Host_Port&gt;:&lt;Container_Port&gt;\n      # Use rare ports on the host to avoid conflicts, while using standard ports inside the container\n      - \"16080:8080\"  # Map a rare port on the host (16080) to the standard port 8080 on the container for Spark Master web UI\n      - \"17077:7077\"  # Map a rare port on the host (17077) to the standard port 7077 on the container for Spark Master communication\n    volumes:\n      - spark-warehouse:/user/hive/warehouse  # Mount the shared volume for Hive warehouse\n    networks:\n      - dasnet  # Connect the master container to the defined network\n\n  worker1:\n    build:\n      context: .  # Build context is the current directory (where the Dockerfile is located)\n      dockerfile: Dockerfile  # Dockerfile to use for building the image\n    image: bitnami-spark-worker  # Name the image for the worker node\n    container_name: worker1  # Set a custom name for the first worker container\n    environment:\n      - SPARK_MODE=worker  # Environment variable to set the Spark mode to worker\n      - SPARK_MASTER_URL=spark://master:7077  # URL for the worker to connect to the master, using the standard port 7077\n      - SPARK_WORKER_MEMORY=2G  # Set the memory allocated for the worker\n      - SPARK_WORKER_CORES=2  # Set the number of CPU cores allocated for the worker\n      - SPARK_RPC_AUTHENTICATION_ENABLED=no\n      - SPARK_RPC_ENCRYPTION_ENABLED=no\n      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n      - SPARK_SSL_ENABLED=no\n      - SPARK_USER=spark\n    depends_on:\n      - master  # Ensure that the master service is started before this worker\n    ports:\n      # Mapping ports: &lt;Host_Port&gt;:&lt;Container_Port&gt;\n      - \"16002:8081\"  # Map a rare port on the host (16002) to the standard port 8081 on the container for Spark Worker 1 web UI\n    volumes:\n      - spark-warehouse:/user/hive/warehouse  # Mount the shared volume for Hive warehouse\n    networks:\n      - dasnet  # Connect the worker container to the defined network\n\n  worker2:\n    build:\n      context: .  # Build context is the current directory (where the Dockerfile is located)\n      dockerfile: Dockerfile  # Dockerfile to use for building the image\n    image: bitnami-spark-worker  # Name the image for the worker node\n    container_name: worker2  # Set a custom name for the second worker container\n    environment:\n      - SPARK_MODE=worker  # Environment variable to set the Spark mode to worker\n      - SPARK_MASTER_URL=spark://master:7077  # URL for the worker to connect to the master, using the standard port 7077\n      - SPARK_WORKER_MEMORY=2G  # Set the memory allocated for the worker\n      - SPARK_WORKER_CORES=2  # Set the number of CPU cores allocated for the worker\n      - SPARK_RPC_AUTHENTICATION_ENABLED=no\n      - SPARK_RPC_ENCRYPTION_ENABLED=no\n      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n      - SPARK_SSL_ENABLED=no\n      - SPARK_USER=spark\n    depends_on:\n      - master  # Ensure that the master service is started before this worker\n    ports:\n      # Mapping ports: &lt;Host_Port&gt;:&lt;Container_Port&gt;\n      - \"16004:8082\"  # Map a rare port on the host (16004) to a different standard port 8082 on the container for Spark Worker 2 web UI\n    volumes:\n      - spark-warehouse:/user/hive/warehouse  # Mount the shared volume for Hive warehouse\n    networks:\n      - dasnet  # Connect the worker container to the defined network\n\nvolumes:\n  spark-warehouse:\n    driver: local  # Use the local driver to create a shared volume for the warehouse\n\nnetworks:\n  dasnet:\n    external: true  # Use the existing 'dasnet' network, created externally via 'docker network create dasnet'\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.2_Bitnami_Spark_Cluster.html#how-to-add-an-extra-node-to-the-cluster","title":"How to add an extra node to the cluster <p> To add an extra node just copy paste the contents worker2 and replace the values like `container_name: worker3` `ports: 16005:8083` etc. </p>","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.2_Bitnami_Spark_Cluster.html#configuration-reference","title":"Configuration Reference    Configuration Item Value     Base Image <code>bitnami/spark:3.5</code>   Spark Version <code>3.5.2</code>   Python Version <code>3.12</code>   Java Version <code>OpenJDK 17.0.12</code>   Environment Variables <code>SPARK_MODE</code>, <code>SPARK_USER</code>, <code>SPARK_MASTER_URL</code>, <code>SPARK_WORKER_MEMORY</code>, <code>SPARK_WORKER_CORES</code>, <code>SPARK_RPC_AUTHENTICATION_ENABLED=no</code>, <code>SPARK_RPC_ENCRYPTION_ENABLED=no</code>, <code>SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no</code>, <code>SPARK_SSL_ENABLED=no</code>   Java Home <code>/opt/bitnami/java</code>   Spark Home <code>/opt/bitnami/spark</code>   Python Path <code>/opt/bitnami/spark/python/</code>   Pyspark Installation Location <code>/opt/bitnami/spark/python/pyspark</code>   Users <code>dwdas</code> with sudo privileges, <code>spark</code> (default user)   User Directory <code>/home/dwdas</code>   Ports Master: <code>16080:8080</code> (Web UI), <code>17077:7077</code> (RPC); Workers: <code>16002:8081</code> (Web UI worker1), <code>16004:8082</code> (Web UI worker2) Master http://localhost:16080/, Worker1 http://localhost:16002/, Worker2 http://localhost:16004/   Volumes <code>/opt/bitnami/spark/tmp</code>, <code>/opt/bitnami/spark/conf</code>, <code>/opt/bitnami/spark/work</code>, <code>/opt/bitnami/spark/logs</code>, <code>spark-warehouse:/user/hive/warehouse</code>   Network Configuration Custom Docker network <code>dasnet</code>   Entry Point <code>/opt/bitnami/scripts/spark/entrypoint.sh</code>   CMD <code>/opt/bitnami/scripts/spark/run.sh</code>   Spark Web UI Ports Master: <code>8080</code>, Worker 1: <code>8081</code>, Worker 2: <code>8082</code>   Spark RPC Port <code>7077</code> (mapped to <code>17077</code> on host)   Spark SSL Configuration <code>SPARK_SSL_ENABLED=no</code>   Spark Authentication <code>SPARK_RPC_AUTHENTICATION_ENABLED=no</code>   Spark Configuration Files <code>/opt/bitnami/spark/conf</code>   spark-sql CLI Just use <code>spark-sql</code>. Location: <code>/opt/bitnami/spark/bin/spark-sql</code>   spark-shell CLI Just use <code>spark-shell</code>. Location: <code>/opt/bitnami/spark/bin/spark-shell</code>","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.2_Bitnami_Spark_Cluster.html#bitnami-spark-reference","title":"Bitnami Spark Reference <p>You can check out the home directory for the Bitnami Spark Container here. It has all the documentation, configuration files, and version details you might need. If you want to see how the base image is built, you can look at their Dockerfile.</p> <p>Bitnami makes sure to keep up with any changes from the source and quickly rolls out new versions using their automated systems. This means you get the latest fixes and features without any delay. Whether you're using containers, virtual machines, or cloud images, Bitnami keeps everything consistent, making it easy to switch formats based on what your project needs.</p> <p>All Bitnami images are based on minideb, which is a lightweight Debian-based container image, or scratch, which is an empty image\u2014so you get a small base that\u2019s easy to work with.</p> <p> These containers are set up as non-root by default, but I\u2019ve added myself as a root user. </p>","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.2_Bitnami_Spark_Cluster.html#bitnami-spark-cluster-with-shared-volume","title":"Bitnami Spark Cluster with Shared Volume <p>Here, I will show you how to set up a Spark Cluster with the following Details:</p>    Configuration Detail Value     Image <code>bitnami/spark:latest</code> (Spark 3.5.1), OS_FLAVOUR=debian-12   Spark Mode 1 Master + 2 Workers   Spark Submission URL <code>spark://spark-master:7077</code>   Spark URLs Local Master http://localhost:8080/, Worker 1 http://localhost:8081/, Worker 2 http://localhost:8082/   Spark Worker Specs Memory 2G, Cores 2   Common Mounted Volumes <code>spark-warehouse:/user/hive/warehouse</code> to all nodes   Mounted Folder Details Temp Dir: <code>/opt/bitnami/spark/tmp</code>, Spark Conf: <code>/opt/bitnami/spark/conf</code>, <code>/opt/bitnami/spark/work</code>, Logs Dir: <code>/opt/bitnami/spark/logs</code>   Networks <code>spark-network</code> (bridge network)   User <code>dwdas</code> (Password: Passw0rd, Sudo Privileges: <code>NOPASSWD:ALL</code>), Working dir: <code>/home/dwdas</code>   Java Version OpenJDK 17.0.11 (LTS)   Python Version Python 3.11.9   JAVA_HOME <code>/opt/bitnami/java</code>   SPARK_HOME <code>/opt/bitnami/spark</code>   PYTHONPATH <code>/opt/bitnami/spark/python/</code>   spark-sql CLI Just use <code>spark-sql</code>. Location: <code>/opt/bitnami/spark/bin/spark-sql</code>   spark-shell CLI Just use <code>spark-shell</code>. Location: <code>/opt/bitnami/spark/bin/spark-shell</code>","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.2_Bitnami_Spark_Cluster.html#steps-to-create-the-environment","title":"Steps to create the environment","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.2_Bitnami_Spark_Cluster.html#create-the-dockerfile","title":"Create the Dockerfile","text":"<p>Create a  Dockerfile.txt with the contents below(remove .txt later).</p> <pre><code># Use the official Bitnami Spark image as the base image\nFROM bitnami/spark:latest\n\n# Switch to root user to install necessary packages and set permissions\nUSER root\n\n# Install sudo package\nRUN apt-get update &amp;&amp; apt-get install -y sudo\n\n# Add a non-root user named dwdas with a home directory and bash shell\nRUN useradd -ms /bin/bash dwdas\n\n# Set the password for dwdas as Passw0rd\nRUN echo \"dwdas:Passw0rd\" | chpasswd\n\n# Add the user to the sudo group and configure sudoers file to allow passwordless sudo\nRUN adduser dwdas sudo\nRUN echo \"dwdas ALL=(ALL) NOPASSWD:ALL\" &gt;&gt; /etc/sudoers\n\n# Ensure dwdas has write permissions to necessary directories and files\nRUN mkdir -p /opt/bitnami/spark/tmp &amp;&amp; chown -R dwdas:dwdas /opt/bitnami/spark/tmp\nRUN chown -R dwdas:dwdas /opt/bitnami/spark/conf\nRUN chown -R dwdas:dwdas /opt/bitnami/spark/work\nRUN chown -R dwdas:dwdas /opt/bitnami/spark/logs\n\n# Switch back to the non-root user\nUSER dwdas\n\n# Set the working directory\nWORKDIR /home/dwdas\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.2_Bitnami_Spark_Cluster.html#create-the-docker-composeyml-file","title":"Create the docker-compose.yml file","text":"<p>In the same folder, create a <code>docker-compose.yml</code> with the content below.</p> <pre><code>version: '3'  # Specify the version of Docker Compose syntax\n\nservices:  # Define the services (containers) that make up your application\n\n  spark-master:\n    build:\n      context: .  # Build context is the current directory (where the Dockerfile is located)\n      dockerfile: Dockerfile  # Dockerfile to use for building the image\n    image: bitnami-spark-master  # Name the image for the master node\n    container_name: spark-master  # Set a custom name for the master container\n    environment:\n      - SPARK_MODE=master  # Environment variable to set the Spark mode to master\n    ports:\n      - \"8080:8080\"  # Map port 8080 on the host to port 8080 on the container for Spark Master web UI\n      - \"7077:7077\"  # Map port 7077 on the host to port 7077 on the container for Spark Master\n    volumes:\n      - spark-warehouse:/user/hive/warehouse  # Mount the shared volume for Hive warehouse\n    networks:\n      - spark-network  # Connect the master container to the defined network\n\n  spark-worker-1:\n    build:\n      context: .  # Build context is the current directory (where the Dockerfile is located)\n      dockerfile: Dockerfile  # Dockerfile to use for building the image\n    image: bitnami-spark-worker  # Name the image for the worker node\n    container_name: spark-worker-1  # Set a custom name for the first worker container\n    environment:\n      - SPARK_MODE=worker  # Environment variable to set the Spark mode to worker\n      - SPARK_MASTER_URL=spark://spark-master:7077  # URL for the worker to connect to the master\n      - SPARK_WORKER_MEMORY=2G  # Set the memory allocated for the worker\n      - SPARK_WORKER_CORES=2  # Set the number of CPU cores allocated for the worker\n    depends_on:\n      - spark-master  # Ensure that the master service is started before this worker\n    ports:\n      - \"8081:8081\"  # Map port 8081 on the host to port 8081 on the container for Spark Worker 1 web UI\n    volumes:\n      - spark-warehouse:/user/hive/warehouse  # Mount the shared volume for Hive warehouse\n    networks:\n      - spark-network  # Connect the worker container to the defined network\n\n  spark-worker-2:\n    build:\n      context: .  # Build context is the current directory (where the Dockerfile is located)\n      dockerfile: Dockerfile  # Dockerfile to use for building the image\n    image: bitnami-spark-worker  # Name the image for the worker node\n    container_name: spark-worker-2  # Set a custom name for the second worker container\n    environment:\n      - SPARK_MODE=worker  # Environment variable to set the Spark mode to worker\n      - SPARK_MASTER_URL=spark://spark-master:7077  # URL for the worker to connect to the master\n      - SPARK_WORKER_MEMORY=2G  # Set the memory allocated for the worker\n      - SPARK_WORKER_CORES=2  # Set the number of CPU cores allocated for the worker\n    depends_on:\n      - spark-master  # Ensure that the master service is started before this worker\n    ports:\n      - \"8082:8081\"  # Map port 8082 on the host to port 8081 on the container for Spark Worker 2 web UI\n    volumes:\n      - spark-warehouse:/user/hive/warehouse  # Mount the shared volume for Hive warehouse\n    networks:\n      - spark-network  # Connect the worker container to the defined network\n\nvolumes:\n  spark-warehouse:\n    driver: local  # Use the local driver to create a shared volume for the warehouse\n\nnetworks:\n  spark-network:\n    driver: bridge  # Use the bridge driver to create an isolated network for the Spark cluster\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.2_Bitnami_Spark_Cluster.html#build-your-docker-images","title":"Build Your Docker Images","text":"<p>In command prompt run the following command to build your Docker images</p> <pre><code>docker-compose -p bitnami-spark-cluster build\n</code></pre> <p></p> <p>There will be two images: One for the Master and the other for the worker nodes. The images section will also show one dangling image. Ignore it.</p> <p></p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.2_Bitnami_Spark_Cluster.html#run-the-containers-using-the-images","title":"Run the containers using the images","text":"<p>Now run the following:</p> <pre><code>docker-compose -p bitnami-spark-cluster up -d\n</code></pre> <p>You should see three containers running inside a compose stack.</p> <p></p> <p>And a volume will be created and shown in teh Volumes section:</p> <p></p> <p>The volume will be mounted to all the three containers:</p> <p></p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.2_Bitnami_Spark_Cluster.html#check-permissions-for-userhivewarehouse","title":"Check Permissions for /user/hive/warehouse","text":"<p>The <code>/user/hive/warehouse</code> folder is the mounted directory we created. The user <code>dwdas</code> should have the correct permissions for this folder in all three containers. To check this, connect to each container through the terminal using either:</p> <pre><code>docker exec -it spark-master bash\n</code></pre> <p>Or through Docker Desktop's EXEC tab.</p> <p>Then run:</p> <pre><code>ls -ld /user/hive/warehouse\n# The permission should be rwx for dwdas\n</code></pre> <p></p> <p>Note: If you don't have <code>rwx</code> or owner permissions for the warehouse folder and see an output like this:</p> <p></p> <p>You will need to either provide <code>rwx</code> permission for <code>dwdas</code> or make <code>dwdas</code> the owner of the folder:</p> <pre><code>chown dwdas:dwdas /user/hive/warehouse\n</code></pre> <p>If that doesn't work, you can use:</p> <pre><code>chmod 777 /user/hive/warehouse\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.2_Bitnami_Spark_Cluster.html#connect-to-sparksql-using-spark-sql-cli","title":"Connect to SparkSQL using spark-sql CLI","text":"<p>Open any container's terminal(Master/worker) and input spark-sql.</p> <p></p> <p>Let's create a simple table and see query it. paste the following in the terminal:</p> <pre><code>CREATE TABLE Hollywood (name STRING);\nINSERT INTO Hollywood VALUES ('Inception'), ('Titanic');\nSELECT * FROM Hollywood;\n</code></pre> <p>A table will be created in SPARK as extended tabble and you will be able to see the select result: </p> <p><pre><code>DESCRIBE EXTENDED HOLLYWOOD\n</code></pre> Will give this result:</p> <p></p> <p>Note: its a manged table in location /home/dwdas/spark-warehouse/</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.2_Bitnami_Spark_Cluster.html#connect-to-a-container-and-create-a-spark-session","title":"Connect to a container and create a spark session","text":"<p>Open VS Code and use Dev Containers to attach to the running Master container. Then, open a Jupyter notebook and run the following commands.</p> <p>To know how to connect VS code to a container, refer to my article here.</p> <p><pre><code>from pyspark.sql import SparkSession\n\n# Initialize the SparkSession\n## Note: file:///user/hive/warehouse is the way and not /user/hive/warehouse. Add file: Else, wont be able to find the directory\nspark = SparkSession.builder \\\n    .appName(\"HiveExample\") \\\n    .config(\"spark.sql.warehouse.dir\", \"file:///user/hive/warehouse\") \\\n    .config(\"spark.sql.legacy.createHiveTableByDefault\", \"false\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n\n# Explanation:\n# - `spark.sql.warehouse.dir`: Specifies the default location for managed databases and tables.\n# - `spark.sql.legacy.createHiveTableByDefault`: Ensures that Hive tables are not created by default unless explicitly specified.\n# - `enableHiveSupport()`: Enables Hive support, allowing Spark to leverage Hive Metastore, run HiveQL queries, and use Hive functions.\n\n# Sample data\ndata = [(\"Kim Jong Obama\", 28), (\"Vladimir Trump\", 35)]\ncolumns = [\"Name\", \"Age\"]\n\n# Create a DataFrame\ndf = spark.createDataFrame(data, columns)\n\n# Save the DataFrame as a managed table\ndf.write.mode(\"overwrite\").saveAsTable(\"people\")\n\n# Verify the table creation\nspark.sql(\"SELECT * FROM people\").show()\n\n# Additional Information:\n# - Hive Metastore Integration: Spark can directly access the Hive Metastore, providing a unified metadata layer for Spark and Hive.\n# - HiveQL Queries: You can run HiveQL queries using Spark SQL, using the familiar Hive syntax.\n# - Hive Functions: Spark supports Hive's built-in functions in Spark SQL queries.\n# - Table Management: Spark can read from and write to Hive-managed tables, including creating, dropping, and altering tables.\n# - Compatibility with Hive Data Formats: Spark can read from and write to Hive's data formats like ORC and Parquet.\n# - Access to Hive UDFs: User-defined functions created in Hive can be used within Spark SQL queries.\n</code></pre> If everything works fine, we will be able to see the spark internal table created!</p> <p></p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.2_Bitnami_Spark_Cluster.html#common-errors","title":"Common Errors","text":"<p>I will update this section when I get time.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.4_Spark_Hive_MSSQL.html","title":"Setup a complete data warehouse with Spark cluster &amp; external Hive(MSSQL Metastore)","text":"<p>Here, I will show you how to create a complete warehouse setup with a Spark Cluster and a standalone Hive with an external metastore database (MSSQL). The setup will use 5 containers:</p> Service URL/Connection Image Spark Master http://localhost:16789/ <code>spark:3.5.1-debian-12-r7</code> Spark Worker 1 http://localhost:16791/ <code>spark:3.5.1-debian-12-r7</code> Spark Worker 2 http://localhost:16792/ <code>spark:3.5.1-debian-12-r7</code> SQL Server (SSMS) <code>localhost,1433</code>, SQL Server Auth: <code>dwdas/Passw0rd</code> <code>mcr.microsoft.com/mssql/server:2019-latest</code> Hive Metastore <code>thrift://hive-metastore:9083</code> <code>apache/hive:4.0.0</code>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.4_Spark_Hive_MSSQL.html#for-the-busy-people","title":"For the busy people","text":"<p>Follow these steps to get your setup ready:</p> <ul> <li>Download the zip file on your laptop.</li> <li>Go to <code>Step1xx</code>, <code>Step2xx</code>, and <code>Step3xx</code> folders and run <code>run.bat</code> in each.</li> </ul> <p>This will create a setup like the one shown below:</p> <p></p> <p>A shared volume should be present in the volumes with the mapping shown below:</p> <p></p> <p>Here are some important details about the setup:</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.4_Spark_Hive_MSSQL.html#how-was-the-setup-created","title":"How was the setup created","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.4_Spark_Hive_MSSQL.html#bit-background-about-bitnami-spark-container","title":"Bit Background About Bitnami Spark Container","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.4_Spark_Hive_MSSQL.html#their-dockerfile","title":"Their Dockerfile","text":"<p>The official Dockerfile used by Bitnami to create their Spark container can be found on GitHub. Some key parts of the file are:</p> <pre><code>RUN chown -R 1001:1001 /opt/bitnami/spark\nWORKDIR /opt/bitnami/spark\nUSER 1001\nENTRYPOINT [ \"/opt/bitnami/scripts/spark/entrypoint.sh\" ]\nCMD [ \"/opt/bitnami/scripts/spark/run.sh\" ]\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.4_Spark_Hive_MSSQL.html#non-root-containers","title":"Non-Root Containers","text":"<p>By default, Bitnami containers run as non-root users. This means when you log into the container, you are not the root user. The container itself runs as a non-root user, but you can change this by specifying <code>User: root</code> in your Dockerfile or Docker-compose.</p> <p>More details can be found here.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.4_Spark_Hive_MSSQL.html#the-user-1001","title":"The User 1001","text":"<p>Bitnami Spark containers use the non-root user ID 1001. When you start the container and log in, you will be user 1001. It's important that this user has access to all required directories and volumes. Ensure necessary permissions are set when building the container.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.4_Spark_Hive_MSSQL.html#folder-structure","title":"Folder Structure","text":"<p>Bitnami containers store application files in <code>/opt/bitnami/APPNAME/</code>. For example, Spark files are located in <code>/opt/bitnami/spark</code>. More details can be found in the Bitnami directory structure documentation.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.4_Spark_Hive_MSSQL.html#spark-container-setup-details","title":"Spark container setup details","text":"<p>First, I created the Spark cluster using the Bitnami Spark image <code>bitnami/spark:3.5.1-debian-12-r7</code>. I chose Bitnami because it is popular and offers a constant version for stability. However, you can also use <code>bitnami/spark:latest</code>.</p> <ul> <li>I used a Dockerfile and docker-compose approach.</li> <li>Created a <code>spark-defaults.conf</code> with just one setting:    <pre><code>spark.sql.warehouse.dir = /data/spark-warehouse\n</code></pre></li> <li>Created a <code>Dockerfile</code> with the following important activities:</li> <li>Created a <code>/data</code> folder.</li> <li>Copied the custom <code>spark-defaults.conf</code> from laptop to <code>/conf</code>.</li> <li>Set the root password and provided user 1001 permissions to /data folder.</li> <li> <p>Created a <code>docker-compose.yml</code> file that included these changes(rest are usual:    <pre><code>volumes:\n  - shared-data:/data\n  - ./spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf\nenvironment:\n  - SPARK_MODE=master  # Environment variable to set the Spark mode to master\n  - SPARK_LOCAL_DIRS=/data/tmp  # Local directories for Spark\n</code></pre></p> </li> <li> <p>Finally, created a <code>run.bat</code> to do the following:    <pre><code>REM Create the Docker volume\ndocker volume create shared-data\n\nREM Build and run the Docker Compose services\ndocker-compose -p bitnami-spark-cluster build\ndocker-compose -p bitnami-spark-cluster up -d\n</code></pre></p> </li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.4_Spark_Hive_MSSQL.html#mssql-2019-container-setup-details","title":"MSSQL 2019 container setup details","text":"<p>Setting up the MSSQL container was more straightforward and required less customization than the Spark cluster. I used a Dockerfile and docker-compose approach.</p> <ul> <li>Created a <code>Dockerfile</code> with the following important activities:</li> <li>Created a user <code>dwdas</code> with the password <code>Passw0rd</code>.</li> <li> <p>Set the root user password to <code>Passw0rd</code>.</p> </li> <li> <p>Created a <code>docker-compose.yml</code> file that included these changes:    <pre><code>ports:\n  - \"1433:1433\"  # Map port 1433 of the host to port 1433 of the container for MSSQL communication.\nenvironment:\n  - SA_PASSWORD=Passw0rd  # Set the system administrator password for MSSQL.\n\nnetworks:\n  - spark-network\n</code></pre></p> </li> <li> <p>Finally, created a <code>run.bat</code> to do the following:    <pre><code>REM Create the Docker volumes. This is mapped to all Spark containers and the Hive container.\ndocker volume create shared-data\n\nREM Build and run the Docker Compose services\ndocker-compose -p bitnami-spark-cluster build\ndocker-compose -p bitnami-spark-cluster up -d\n\nREM Post-Setup: Create MSSQL user and database\ndocker exec -it mssql-container-name /opt/mssql-tools/bin/sqlcmd -S localhost -U SA -P \"Passw0rd\" -Q \"CREATE LOGIN dwdas WITH PASSWORD='Passw0rd'; CREATE DATABASE hive_metastore;\"\n</code></pre></p> <p>run.bat: After the container started, I created an MSSQL user <code>dwdas</code> with the password <code>Passw0rd</code> and an empty database <code>hive_metastore</code>. This was done inside the <code>run.bat</code> file.</p> </li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.4_Spark_Hive_MSSQL.html#hive-server-container-setup-details","title":"Hive Server container setup details","text":"<p>Setting up the Hive server can be very tricky. I used the official Apache Hive image with a Dockerfile and docker-compose approach.</p> <ul> <li>Download JDBC Driver:</li> <li> <p>Downloaded <code>sqljdbc_7.2.2.0_enu.tar.gz</code> from Microsoft Download Center.</p> <p></p> </li> <li> <p>Extracted the <code>mssql-jdbc-7.2.2.jre8.jar</code> driver to the current folder.</p> </li> <li> <p>Create a custom <code>hive-site.xml</code>: Created a custom <code>hive-site.xml</code> with MSSQL connection information and directory configurations. Refer to the Step3-Hive/hive-site.xml in the setup folder for more details.</p> </li> <li> <p>Create a Dockerfile: Refer to the dockerfile in Step3-Hive folder for more details. Apart from usual activity these two steps are most immortant. I.e. placing the driver to the /lib and hive-site.xml to conf.    <pre><code>COPY ./mssql-jdbc-7.2.2.jre8.jar /opt/hive/lib/\nCOPY ./hive-site.xml /opt/hive/conf/\n</code></pre></p> </li> <li> <p>Docker-compose Configuration: Apart from usual stuff the most important was to  the environment variable <code>DB_DRIVER=mssql</code>.</p> </li> <li> <p>Run.bat:</p> </li> <li>The <code>run.bat</code> script builds the images, runs the container.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.4_Spark_Hive_MSSQL.html#hive-mssql-connection","title":"Hive-MSSQL Connection","text":"<p>Apache Hive containers usually use a startup script called '/entrypoint.sh'. This script is set to use Derby(${DB_DRIVER:=derby}) database by default. To use MSSQL instead:</p> <ol> <li>Change the DB_DRIVER environment variable to 'mssql'.</li> <li>Create an empty database called 'hive_metastore' on your MSSQL server.</li> <li>Put your MSSQL connection and db  details in the 'hive-site.xml' file.</li> <li>Add the MSSQL driver file (mssql-jdbc-7.2.2.jre8.jar) to the '/lib' folder.</li> </ol> <p>/entrypoint.sh <pre><code>#!/bin/bash\nset -x\n\n: ${DB_DRIVER:=derby}\n\nSKIP_SCHEMA_INIT=\"${IS_RESUME:-false}\"\n\nfunction initialize_hive {\n  COMMAND=\"-initOrUpgradeSchema\"\n  if [ \"$(echo \"$HIVE_VER\" | cut -d '.' -f1)\" -lt \"4\" ]; then\n     COMMAND=\"-${SCHEMA_COMMAND:-initSchema}\"\n  fi\n  $HIVE_HOME/bin/schematool -dbType $DB_DRIVER $COMMAND\n  if [ $? -eq 0 ]; then\n    echo \"Initialized schema successfully..\"\n  else\n    echo \"Schema initialization failed!\"\n    exit 1\n  fi\n}\n# Additional script content...\n</code></pre> So, how I did this:</p> <ol> <li>During mssql creation I already creatd the hive_metastore empty database and the user</li> <li>The dockerfile managed the copying of the driver and the custom hive-site.xml    <pre><code>COPY ./mssql-jdbc-7.2.2.jre8.jar /opt/hive/lib/\nCOPY ./hive-site.xml /opt/hive/conf/\n</code></pre></li> <li>docker-compose managed set the  <code>DB_DRIVER=mssql</code></li> </ol> <p>Note: An alternative approach to achieve the same feature would be to:</p> <ol> <li>Let the container start with the default settings.</li> <li>Open <code>/entrypoint.sh</code> and change <code>${DB_DRIVER:=mssql}</code>.</li> <li>Run <code>su -c \"chmod 777 /entrypoint.sh\"</code> with the password <code>Passw0rd</code> as the permission will change.</li> <li>Copy <code>hive-site.xml</code> containing MSSQL connection info to <code>/opt/hive/conf/</code>.</li> <li>Restart the container.</li> </ol>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.4_Spark_Hive_MSSQL.html#spark-defaultsconf-hive-sitexml","title":"spark-defaults.conf &amp; hive-site.xml","text":"<p>Only two configureation files were touched. Spark-defaults.conf  in spark server and hive-site.xml in hive server.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.4_Spark_Hive_MSSQL.html#spark-defaultsconf-hive-sitexml_1","title":"spark-defaults.conf &amp; hive-site.xml","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.4_Spark_Hive_MSSQL.html#spark-defaultsconf-important-details","title":"spark-defaults.conf important details","text":"<p>Here, only one value was added. The same file is present in all the spark containres. spark.sql.warehouse.dir=/data/spark-warehouse</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.4_Spark_Hive_MSSQL.html#hive-site-important-details","title":"hive-site important details","text":"<pre><code> &lt;property&gt;\n    &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;\n    &lt;value&gt;/data/hive-warehouse&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;\n    &lt;value&gt;com.microsoft.sqlserver.jdbc.SQLServerDriver&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;\n    &lt;value&gt;jdbc:sqlserver://mssql:1433;DatabaseName=hive_metastore;&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.uris&lt;/name&gt;\n    &lt;value&gt;thrift://hive-metastore:9083&lt;/value&gt;\n  &lt;/property&gt;\n   &lt;property&gt;\n    &lt;name&gt;hive.metastore.db.type&lt;/name&gt;\n    &lt;value&gt;mssql&lt;/value&gt;\n    &lt;description&gt;\n      Expects one of [derby, oracle, mysql, mssql, postgres].\n    &lt;/description&gt;\n  &lt;/property&gt;\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.4_Spark_Hive_MSSQL.html#repository-and-metastore-folder-creation","title":"Repository and Metastore Folder Creation","text":"<p>We have two options to decide on the folder structure:</p> <p>Our choice: Separate warehouses for Spark and Hive 1. Keep Hive warehouse as is:    - In <code>hive-site.xml</code>:      <pre><code>&lt;property&gt;\n  &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;\n  &lt;value&gt;/data/hive/warehouse&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> 2. Set a different warehouse for Spark:    - In Spark configuration:     <code>spark.sql.warehouse.dir=/data/spark/warehouse</code> ``` /data \u251c\u2500\u2500 hive-warehouse/ \u2502   \u2514\u2500\u2500 (Hive managed tables) \u251c\u2500\u2500 spark-warehouse/ \u2502   \u2514\u2500\u2500 (Spark-managed tables)</p> <p><pre><code>**Option 2:** Use Hive Warehouse for both Spark and Hive\n1. Configure Spark to use the Hive metastore:\n   - Set `spark.sql.hive.metastore.version` and `spark.sql.hive.metastore.jars` in Spark configuration.\n2. Use the same warehouse directory for both:\n   - In `hive-site.xml`:\n     ```xml\n     &lt;property&gt;\n       &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;\n       &lt;value&gt;/data/spark-warehouse&lt;/value&gt;\n     &lt;/property&gt;\n     ```\n   - In Spark configuration. Using(`spark-defaults.conf`) or setting during session etc.\n     ```\n     spark.sql.warehouse.dir=/data/spark-warehouse\n     ```\n# &lt;span style=\"color: Blue;\"&gt;Testing&lt;/span&gt;\n\n**Testing Hive**\nGo the the hive container and type **hive**. You should see output lilke this:\n![](images/custom-image-2024-06-24-21-30-25.png)\n\n**Testing spark-shell**\nGo to any of hte spark containers's terminal and key in **spark-shell**, you should see output liek this:\n\n![](images/custom-image-2024-06-25-23-52-53.png)\n\n**Testing spark-sql**\nGo to any of hte spark containers's terminal and key in **spark-sql**, you should see output liek this:\n\n![](images/custom-image-2024-06-25-23-51-47.png)\n\n# When will be managed or external tables created?\n\nWhen configuring Spark to work with Hive, the type of table created (managed vs. external) depends on specific settings in your Spark configuration.\n\nIf `spark-defaults.xml` includes:\n```xml\nspark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083\n</code></pre> Then, spark will create all tables as <code>EXTERNAL</code>. Regardless of whatever settigngs you try.</p> <p></p> <p>If <code>spark-defaults.xml</code> does not include: <pre><code>spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083\n</code></pre> Then tables are created as <code>MANAGED</code> by default and stored in the directory specified by <code>spark.sql.warehouse.dir</code>.</p> <p></p> <p>You can still create Hive tables stored in the Hive warehouse. To do this, include: <pre><code>.config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n.enableHiveSupport() \\\n</code></pre> In this case, the tables will be <code>EXTERNAL</code> and stored in the directory specified by hive.<code>metastore.warehouse.dir</code> in <code>hive-site.xml</code></p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.4_Spark_Hive_MSSQL.html#errors","title":"Errors","text":"<ul> <li> <p>If DB_DRIVER=derby in env var but hive-site.xml has no mssql connection: </p> </li> <li> <p>If you are unable to create folders or perform any operation. It could be the user 1001 is not having enough permissions. This is a typical user Bitnami spark configures to run the container.</p> </li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.4_Spark_Hive_MSSQL.html#server-details","title":"Server Details","text":"Configuration Details Hive Metastore URI <code>spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083</code> User and Database User <code>dwdas</code> (PASSWORD=<code>Passw0rd</code>), Database <code>hive_metastore</code>. hive-site.xml Details <code>hive.metastore.warehouse.dir</code> <code>/user/hive/warehouse</code> <code>javax.jdo.option.ConnectionURL</code> <code>jdbc:sqlserver://mssql:1433;databaseName=hive_metastore</code> spark-defaults.conf Details <code>spark.sql.warehouse.dir</code> <code>/data/spark-warehouse</code> Service Configuration Details Spark Server Configuration Image <code>spark:3.5.1-debian-12-r7</code> User running container <code>1001:1001</code> Environment Variables <code>JAVA_HOME=/opt/bitnami/java</code> <code>PYTHONPATH=/opt/bitnami/spark/python/</code> <code>SPARK_HOME=/opt/bitnami/spark</code> <code>SPARK_USER=spark</code> <code>root: Passw0rd</code> Mounted Volume <code>shared-data:/data</code> SQL Server Configuration Environment Variables <code>SA_PASSWORD=Passw0rd</code> <code>Server name: mssql</code> <code>/opt/mssql/bin/sqlservr</code> Hive Configuration Environment Variables <code>HIVE_HOME=/opt/hive</code> <code>SERVICE_NAME=metastore</code> <code>DB_DRIVER=mssql</code> <code>TEZ_HOME=/opt/tez</code> <code>HIVE_VER=4.0.0</code> <code>JAVA_HOME=/usr/local/openjdk-8</code> <code>PWD=/home/dwdas</code> <code>HADOOP_HOME=/opt/hadoop</code> Thrift Server URL <code>thrift://hive-metastore:9083</code>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html","title":"Create a Seven-Node Hadoop Container on Docker","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#introduction","title":"Introduction","text":"<p>In this guide, I'll walk through the process of setting up an Apache Hadoop cluster using Docker containers. This setup is ideal for development and testing purposes on your local machine. I'll cover the configuration files, container setup, and how to verify that the cluster is functioning correctly.</p> <p>For busy people:</p> <ul> <li>Download and unzip the file to a folder</li> <li>CD and run the following commands     <pre><code>    docker-compose build\n    docker-compose up -d\n</code></pre></li> <li>You will have a full-fledged Hadoop setup</li> </ul> <p></p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed on your machine.</li> <li>Basic knowledge of Docker and Hadoop.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#files-used","title":"Files Used","text":"<ol> <li>Dockerfile: Defines the environment and how the Hadoop services will be run inside the Docker containers.</li> <li>docker-compose.yml: Manages the multi-container application, ensuring all necessary Hadoop services are launched and networked correctly.</li> <li>entrypoint.sh: A script to start the appropriate Hadoop service based on the container's role (e.g., NameNode, DataNode).</li> </ol> <p>Important: If you create the <code>entrypoint.sh</code> file on Windows, you must convert it to Unix format using a tool like Toolslick DOS to Unix Converter before using it in your Docker environment.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#step-1-prepare-the-docker-environment","title":"Step 1: Prepare the Docker Environment","text":"<ol> <li>Dockerfile:    The <code>Dockerfile</code> sets up the Java runtime and Hadoop environment. Here's the Dockerfile used:</li> </ol> <pre><code># Use Java 8 runtime as base image\nFROM openjdk:8-jdk\n\n# Set environment variables\nENV HADOOP_VERSION=2.7.7\nENV HADOOP_HOME=/usr/local/hadoop\nENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\nENV PATH=$PATH:$HADOOP_HOME/bin\n\n# Install Hadoop\nRUN wget https://archive.apache.org/dist/hadoop/core/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz &amp;&amp; \\\n    tar -xzvf hadoop-$HADOOP_VERSION.tar.gz &amp;&amp; \\\n    mv hadoop-$HADOOP_VERSION $HADOOP_HOME &amp;&amp; \\\n    rm hadoop-$HADOOP_VERSION.tar.gz\n\n# Copy and set entrypoint script\n# Note: If entrypoint.sh is created on Windows, convert it to Unix format using dos2unix website.\nCOPY entrypoint.sh /entrypoint.sh\nRUN chmod +x /entrypoint.sh\n\n# Expose Hadoop ports\nEXPOSE 50070 8088 9000 9864 9870 9866 9867\n\n# Set entrypoint\nENTRYPOINT [\"/entrypoint.sh\"]\n</code></pre> <ol> <li>docker-compose.yml:    The <code>docker-compose.yml</code> file orchestrates the Hadoop cluster by defining services like NameNode, DataNode, and ResourceManager.</li> </ol> <pre><code>version: '3.8'\n\nservices:\n  namenode:\n    build: .\n    container_name: namenode\n    hostname: namenode\n    environment:\n      - CLUSTER_NAME=my-hadoop-cluster\n    volumes:\n      - namenode_data:/hadoop/dfs/name\n    ports:\n      - \"29070:50070\"  # HDFS NameNode Web UI on port 29070\n      - \"29870:9870\"   # NameNode Web UI port on port 29870\n      - \"29000:9000\"   # HDFS port on port 29000\n    command: namenode\n    networks:\n      - dasnet\n\n  secondarynamenode:\n    build: .\n    container_name: secondarynamenode\n    hostname: secondarynamenode\n    volumes:\n      - secondarynamenode_data:/hadoop/dfs/secondary\n    command: secondarynamenode\n    networks:\n      - dasnet\n\n  datanode1:\n    build: .\n    container_name: datanode1\n    hostname: datanode1\n    volumes:\n      - datanode1_data:/hadoop/dfs/data\n    command: datanode\n    networks:\n      - dasnet\n\n  datanode2:\n    build: .\n    container_name: datanode2\n    hostname: datanode2\n    volumes:\n      - datanode2_data:/hadoop/dfs/data\n    command: datanode\n    networks:\n      - dasnet\n\n  resourcemanager:\n    build: .\n    container_name: resourcemanager\n    hostname: resourcemanager\n    ports:\n      - \"28088:8088\"  # ResourceManager Web UI on port 28088\n    command: resourcemanager\n    networks:\n      - dasnet\n\n  nodemanager1:\n    build: .\n    container_name: nodemanager1\n    hostname: nodemanager1\n    ports:\n      - \"29864:9864\"  # NodeManager Web UI on port 29864\n    command: nodemanager\n    networks:\n      - dasnet\n\n  historyserver:\n    build: .\n    container_name: historyserver\n    hostname: historyserver\n    ports:\n      - \"29866:9866\"  # HistoryServer Web UI on port 29866\n      - \"29867:9867\"  # Additional service on port 29867\n    command: historyserver\n    networks:\n      - dasnet\n\nvolumes:\n  namenode_data:\n  secondarynamenode_data:\n  datanode1_data:\n  datanode2_data:\n\nnetworks:\n  dasnet:\n    external: true\n</code></pre> <ol> <li>entrypoint.sh:    This script starts the appropriate Hadoop service based on the container\u2019s role. Below is the script:</li> </ol> <pre><code>#!/bin/bash\n# Format namenode if necessary\nif [ \"$1\" == \"namenode\" ]; then\n  $HADOOP_HOME/bin/hdfs namenode -format -force -nonInteractive\nfi\n# Start SSH service\nservice ssh start\n# Start Hadoop service based on the role\nif [ \"$1\" == \"namenode\" ]; then\n  $HADOOP_HOME/sbin/hadoop-daemon.sh start namenode\nelif [ \"$1\" == \"datanode\" ]; then\n  $HADOOP_HOME/sbin/hadoop-daemon.sh start datanode\nelif [ \"$1\" == \"secondarynamenode\" ]; then\n  $HADOOP_HOME/sbin/hadoop-daemon.sh start secondarynamenode\nelif [ \"$1\" == \"resourcemanager\" ]; then\n  $HADOOP_HOME/sbin/yarn-daemon.sh start resourcemanager\nelif [ \"$1\" == \"nodemanager\" ]; then\n  $HADOOP_HOME/sbin/yarn-daemon.sh start nodemanager\nelif [ \"$1\" == \"historyserver\" ]; then\n  $HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver\nfi\n# Keep the container running\ntail -f /dev/null\n</code></pre> <p>Note: Convert <code>entrypoint.sh</code> to Unix format if it's created on Windows using Toolslick DOS to Unix Converter.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#step-2-build-and-start-the-cluster","title":"Step 2: Build and Start the Cluster","text":"<ol> <li>Build the Docker Images:</li> <li>Navigate to the directory containing the <code>Dockerfile</code>, <code>docker-compose.yml</code>, and <code>entrypoint.sh</code> files.</li> <li> <p>Run the following command to build the Docker images:      <pre><code>docker-compose build\n</code></pre></p> </li> <li> <p>Start the Cluster:</p> </li> <li>Start the cluster using the following command:      <pre><code>docker-compose up -d\n</code></pre></li> </ol>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#step-3-verify-the-setup","title":"Step 3: Verify the Setup","text":"<ol> <li>Access Hadoop Web UIs:</li> <li>NameNode Web UI: <code>http://localhost:29870</code></li> <li>HDFS NameNode Web UI: <code>http://localhost:29070</code></li> <li>ResourceManager Web UI: <code>http://localhost:28088</code></li> <li>NodeManager Web UI: <code>http://localhost:29864</code></li> <li>HistoryServer Web UI: <code>http://localhost:29866</code></li> </ol> <p>These interfaces will allow you to monitor the status of your Hadoop cluster and the jobs running on it.</p> <ol> <li>Run a Test Job:</li> <li> <p>Create Input Directory in HDFS:      <pre><code>docker exec -it namenode /bin/bash\nhdfs dfs -mkdir -p /input\necho \"Hello Hadoop\" &gt; /tmp/sample.txt\nhdfs dfs -put /tmp/sample.txt /input/\n</code></pre></p> </li> <li> <p>Run the WordCount Job:      <pre><code>hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar wordcount /input /output\n</code></pre></p> </li> <li> <p>Check the Output:      <pre><code>hdfs dfs -cat /output/part-r-00000\n</code></pre></p> <p>Expected output:  <pre><code>Hadoop  1\nHello   1\n</code></pre></p> </li> </ol>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#conclusion","title":"Conclusion","text":"<p>Remember to convert any scripts created on Windows to Unix format before using them in your Docker containers to avoid potential issues. Happy coding!</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#config-reference","title":"Config reference","text":"Element Location/Value Description Hadoop Installation Dir <code>/usr/local/hadoop</code> The directory where Hadoop is installed inside the Docker containers (<code>HADOOP_HOME</code>). Hadoop Config Dir <code>/usr/local/hadoop/etc/hadoop</code> Directory containing Hadoop configuration files (<code>HADOOP_CONF_DIR</code>). HDFS Data Directory <code>/hadoop/dfs/name</code> (NameNode), <code>/hadoop/dfs/data</code> (DataNode) Directories used to store HDFS data, mapped to Docker volumes for persistence. Mapped Ports See <code>docker-compose.yml</code> Ports mapped between host and container for accessing Hadoop Web UIs. NameNode Web UI <code>http://localhost:29870</code> Access URL for NameNode Web UI from the host machine. HDFS NameNode UI <code>http://localhost:29070</code> Access URL for HDFS NameNode Web UI from the host machine. ResourceManager Web UI <code>http://localhost:28088</code> Access URL for YARN ResourceManager Web UI from the host machine. NodeManager Web UI <code>http://localhost:29864</code> Access URL for YARN NodeManager Web UI from the host machine. HistoryServer Web UI <code>http://localhost:29866</code> Access URL for MapReduce Job HistoryServer Web UI from the host machine. HDFS Input Directory <code>/input</code> in HDFS Directory where input files for MapReduce jobs are stored in HDFS. HDFS Output Directory <code>/output</code> in HDFS Directory where output files from MapReduce jobs are stored in HDFS."},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#reference","title":"Reference","text":"<p>Official Hadoop Link</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#create-a-single-node-hadoop-container-on-docker","title":"Create a Single-Node Hadoop Container on Docker","text":"<p>Setting up Hadoop can be quite confusing, especially if you're new to it. The best ready-to-use setups used to be provided by Cloudera, their current docker image(as on Aug 2024) has many issues. In this guide, I\u2019ll help you create a pure Hadoop container with just one node. This guide is divided into two sections: one for quick steps and another for a detailed setup. The container setup has been thoroughly tested and can be created easily.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#quick-steps-for-the-busy-people","title":"Quick Steps for the Busy People","text":"<p>If you\u2019re short on time and don\u2019t want to go through the entire process of setting everything up manually, this section is for you. Just follow the instructions here, use the ready-to-use files, and your container will be up and running in a few minutes!</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#steps-to-build-the-container-using-ready-to-use-files","title":"Steps to build the container using ready-to-use files","text":"<p> Note: The `hadoop-3.4.0.tar.gz` file is not included in the zip because of its large size. I\u2019ve set up the Dockerfile to download this file automatically from the Apache website if it\u2019s not found in the folder. However, sometimes these download links change. If you encounter any issues with the automatic download, you can manually download the `hadoop-3.4.0.tar.gz` file from this link. If this link doesn\u2019t work, simply search online for the file, download it, and place it in the folder. You don\u2019t need to change anything in the Dockerfile; it will work as long as the file is named `hadoop-3.4.0.tar.gz`. </p> <p> If you create a .sh file or any file on Windows and plan to use it in Linux, make sure you convert it to a Linux-friendly format using this site. Other methods may not work as well. It's important to convert the file, or else it might cause many unexpected errors. </p> <ul> <li>Download the hadoop-singlenode.zip file.</li> <li>Unzip it to a folder on your laptop.</li> <li> <p>Open command prompt/terminal and cd to the folder where you unziped the files</p> <p><pre><code>cd path_to_unzipped_folder\n</code></pre> - Build the doccker image from the Dockerfile. There is a dot <pre><code>docker build -t hadoop-singlenode .\n</code></pre> - Run the container from the built image</p> <p><pre><code>docker run -it --name hadoop-singlenode --network dasnet -p 9870:9870 -p 8088:8088 -v namenode-data:/hadoop/dfs/namenode  -v datanode-data:/hadoop/dfs/datanode -v secondarynamenode-data:/hadoop/dfs/secondarynamenode  hadoop-singlenode\n</code></pre> - From inside the running container, start the Hadoop services:</p> <pre><code>sudo service ssh start\n$HADOOP_HOME/sbin/start-dfs.sh\n$HADOOP_HOME/sbin/start-yarn.sh\n</code></pre> </li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#check-the-setup","title":"Check the setup","text":"<ul> <li>Once the services are up you can access the Hadoop links:</li> <li> <p>HDFS NameNode Web UI: http://localhost:9870     </p> </li> <li> <p>YARN ResourceManager Web UI: http://localhost:8088     </p> </li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#detailed-steps-building-the-setup-file-by-file","title":"Detailed steps - building the setup, file by file","text":"<p>If you want to understand how to build the Docker container from scratch or make custom modifications, follow these steps.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#details-of-the-files-used","title":"Details of the files used","text":"<p>The setup uses 6 files, <code>Dockerfile</code>, <code>core-site.xml</code>, <code>hdfs-site.xml</code>, <code>mapred-site.xml</code>, <code>yarn-site.xml</code> and <code>hadoop-env.sh</code></p> <ul> <li> <p>Dockerfile: This is the main file. We only use Dockerfile for our setup and no docker-compose.yml. But, you can add a docker-compose if you need. The filename is <code>Dockerfile</code>(no extension) and has the following content:</p> <p><pre><code># Use a base image with Java 8\nFROM openjdk:8-jdk\n\n# Set environment variables for Hadoop\nENV HADOOP_VERSION=3.4.0\nENV HADOOP_HOME=/usr/local/hadoop\nENV JAVA_HOME=/usr/local/openjdk-8\nENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\nENV HDFS_NAMENODE_USER=dwdas\nENV HDFS_DATANODE_USER=dwdas\nENV HDFS_SECONDARYNAMENODE_USER=dwdas\nENV YARN_RESOURCEMANAGER_USER=dwdas\nENV YARN_NODEMANAGER_USER=dwdas\n\n# Install necessary packages\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y ssh rsync sudo wget &amp;&amp; \\\n    apt-get clean\n\n# Set root password\nRUN echo \"root:Passw0rd\" | chpasswd\n\n# Create a user 'dwdas' with sudo privileges\nRUN useradd -m -s /bin/bash dwdas &amp;&amp; \\\n    echo \"dwdas:Passw0rd\" | chpasswd &amp;&amp; \\\n    usermod -aG sudo dwdas &amp;&amp; \\\n    echo \"dwdas ALL=(ALL) NOPASSWD: ALL\" &gt;&gt; /etc/sudoers\n\n# Optional: Download and extract Hadoop tarball if not already provided\nCOPY hadoop-${HADOOP_VERSION}.tar.gz /tmp/\nRUN if [ ! -f /tmp/hadoop-${HADOOP_VERSION}.tar.gz ]; then \\\n        wget https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz -O /tmp/hadoop-${HADOOP_VERSION}.tar.gz; \\\n    fi &amp;&amp; \\\n    tar -xzf /tmp/hadoop-${HADOOP_VERSION}.tar.gz -C /usr/local &amp;&amp; \\\n    mv /usr/local/hadoop-${HADOOP_VERSION} $HADOOP_HOME &amp;&amp; \\\n    rm /tmp/hadoop-${HADOOP_VERSION}.tar.gz\n\n# Configure Hadoop - Create necessary directories on Docker volumes\nRUN mkdir -p /hadoop/dfs/namenode &amp;&amp; \\\n    mkdir -p /hadoop/dfs/datanode &amp;&amp; \\\n    mkdir -p /hadoop/dfs/secondarynamenode\n\n# Copy configuration files\nCOPY core-site.xml $HADOOP_HOME/etc/hadoop/\nCOPY hdfs-site.xml $HADOOP_HOME/etc/hadoop/\nCOPY mapred-site.xml $HADOOP_HOME/etc/hadoop/\nCOPY yarn-site.xml $HADOOP_HOME/etc/hadoop/\n# Copy and configure hadoop-env.sh. JAVA_HOME MUST be set here also.\nCOPY hadoop-env.sh $HADOOP_HOME/etc/hadoop/hadoop-env.sh\n\n# Set ownership for Hadoop directories and volumes to 'dwdas'\nRUN chown -R dwdas:dwdas $HADOOP_HOME /hadoop/dfs/namenode /hadoop/dfs/datanode /hadoop/dfs/secondarynamenode\n\n# Switch to the dwdas user for all subsequent operations\nUSER dwdas\n\n# Create the .ssh directory and set permissions\nRUN mkdir -p /home/dwdas/.ssh &amp;&amp; \\\n    chmod 700 /home/dwdas/.ssh\n\n# Generate SSH keys for passwordless SSH login and configure SSH\nRUN ssh-keygen -t rsa -P '' -f /home/dwdas/.ssh/id_rsa &amp;&amp; \\\n    cat /home/dwdas/.ssh/id_rsa.pub &gt;&gt; /home/dwdas/.ssh/authorized_keys &amp;&amp; \\\n    chmod 600 /home/dwdas/.ssh/authorized_keys &amp;&amp; \\\n    echo \"Host localhost\" &gt;&gt; /home/dwdas/.ssh/config &amp;&amp; \\\n    echo \"   StrictHostKeyChecking no\" &gt;&gt; /home/dwdas/.ssh/config &amp;&amp; \\\n    chmod 600 /home/dwdas/.ssh/config\n\n# Format HDFS as 'dwdas' user\nRUN $HADOOP_HOME/bin/hdfs namenode -format\n\n# Expose the necessary ports for Hadoop services\nEXPOSE 9870 8088 19888\n\n# Set the container to start in the dwdas user's home directory\nWORKDIR /home/dwdas\n\n# Set the container to start with a bash shell\nCMD [\"bash\"]\n</code></pre> - core-site.xml: Configures the default filesystem and permissions. <pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;/name&gt;\n        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.permissions&lt;/name&gt;\n        &lt;value&gt;false&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre></p> </li> <li> <p>hdfs-site.xml: Configures the NameNode and DataNode directories.     <pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n        &lt;value&gt;file:///usr/local/hadoop/tmp/hdfs/namenode&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n        &lt;value&gt;file:///usr/local/hadoop/tmp/hdfs/datanode&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.replication&lt;/name&gt;\n        &lt;value&gt;1&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.permissions.superusergroup&lt;/name&gt;\n        &lt;value&gt;dwdas&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.cluster.administrators&lt;/name&gt;\n        &lt;value&gt;dwdas&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre></p> </li> <li> <p>mapred-site.xml: Configures the MapReduce framework to use YARN.     <pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n        &lt;value&gt;yarn&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;mapred.job.tracker&lt;/name&gt;\n        &lt;value&gt;hadoop-master:9001&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre></p> </li> <li> <p>yarn-site.xml: Configures YARN services.     <pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;\n        &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre></p> </li> <li> <p>hadoop-env.sh: I haven\u2019t included this entire file because it\u2019s large, and there\u2019s only one change you need to make. Refer to the <code>hadoop-env.sh</code> file in the zip folder. The only modification required is shown below. Note: This step is crucial. Without this, Hadoop will give an error saying it can\u2019t find <code>JAVA_HOME</code>, even if you\u2019ve already set it as an environment variable. This change follows Apache's instructions.</p> <pre><code>export JAVA_HOME=/usr/local/openjdk-8\n</code></pre> </li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#building-running-and-testing-the-setup","title":"Building, Running, and Testing the Setup","text":"<p>The process for building, running, and testing the setup is the same as described in the Quick Steps section. Simply navigate to the folder, build the container, and then run it as before.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#how-to-test-the-setup","title":"How to test the setup?","text":"<p>The table below shows how various components and functionalities could be tested:</p> Category Action Command/URL What to Look For Verify HDFS Check HDFS status <code>hdfs dfsadmin -report</code> A detailed report on the HDFS cluster, showing live nodes, configured capacity, used space, etc. Browse HDFS NameNode Web UI <code>http://localhost:9870</code> The HDFS NameNode web interface should load, showing the health of the file system. Create a directory <code>hdfs dfs -mkdir /test</code> No errors, and the directory <code>/test</code> should be created successfully in HDFS. List directory contents <code>hdfs dfs -ls /</code> The newly created <code>/test</code> directory should be listed. Verify YARN Check YARN NodeManager status <code>yarn node -list</code> A list of nodes managed by YARN, showing their status (e.g., healthy, active). Browse YARN ResourceManager Web UI <code>http://localhost:8088</code> The YARN ResourceManager web interface should load, showing job and node statuses. Verify Hadoop Services Check running services <code>jps</code> List of Java processes such as <code>NameNode</code>, <code>DataNode</code>, <code>ResourceManager</code>, and <code>NodeManager</code>. Test MapReduce Run a test MapReduce job <code>hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar wordcount /test /output</code> The MapReduce job should complete successfully, creating an output directory in HDFS. Verify MapReduce output <code>hdfs dfs -ls /output</code> The <code>/output</code> directory should contain the results of the MapReduce job."},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#setup-details","title":"Setup details","text":"Component Item Location/Value Description Hadoop Installation Directory <code>/usr/local/hadoop</code> Hadoop installation directory. Version <code>3.4.0</code> Hadoop version. Core Config <code>$HADOOP_HOME/etc/hadoop/core-site.xml</code> Core configuration file. HDFS Config <code>$HADOOP_HOME/etc/hadoop/hdfs-site.xml</code> HDFS configuration file. MapReduce Config <code>$HADOOP_HOME/etc/hadoop/mapred-site.xml</code> MapReduce configuration file. YARN Config <code>$HADOOP_HOME/etc/hadoop/yarn-site.xml</code> YARN configuration file. Env Variables <code>$HADOOP_HOME/etc/hadoop/hadoop-env.sh</code> Hadoop environment variables. HDFS Directories NameNode <code>/hadoop/dfs/namenode</code> HDFS NameNode data directory (Docker volume). DataNode <code>/hadoop/dfs/datanode</code> HDFS DataNode data directory (Docker volume). Secondary NameNode <code>/hadoop/dfs/secondarynamenode</code> Secondary NameNode directory (Docker volume). Environment Variables Hadoop Home <code>/usr/local/hadoop</code> Path to Hadoop installation. Java Home <code>/usr/local/openjdk-8</code> Path to Java installation. System Path <code>$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</code> Updated path including Hadoop binaries. HDFS_NAMENODE_USER <code>dwdas</code> User for NameNode service. HDFS_DATANODE_USER <code>dwdas</code> User for DataNode service. HDFS_SECONDARYNAMENODE_USER <code>dwdas</code> User for Secondary NameNode service. YARN_RESOURCEMANAGER_USER <code>dwdas</code> User for ResourceManager service. YARN_NODEMANAGER_USER <code>dwdas</code> User for NodeManager service. Networking Docker Network <code>dasnet</code> Docker network for the Hadoop container. Ports Mapped HDFS NameNode UI <code>9870:9870</code> Port mapping for HDFS NameNode web interface. YARN ResourceManager UI <code>8088:8088</code> Port mapping for YARN ResourceManager web interface."},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#appendix","title":"Appendix","text":"<ul> <li>Alternate command - Start the Namenode <pre><code>$HADOOP_HOME/sbin/hadoop-daemon.sh start namenode\n</code></pre></li> <li> <p>Alternate command - Start the Datanode <pre><code>$HADOOP_HOME/sbin/hadoop-daemon.sh start datanode\n</code></pre></p> </li> <li> <p>Enter <code>jps</code> to see all the hadoop services</p> <p></p> </li> <li> <p>Command to get a report on the Hadoop setup</p> <p><pre><code>hdfs dfsadmin -report\n</code></pre> </p> </li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.5_Hadoop_Cluster_Single_N_MultiNode.html#common-errors-and-their-solutions","title":"Common Errors and Their Solutions","text":"<ul> <li>Port Binding Error:</li> <li>Error: <code>Ports are not available: exposing port TCP 0.0.0.0:50070 -&gt; 0.0.0.0:0: listen tcp 0.0.0.0:50070: bind: An attempt was made to access a socket in a way forbidden by its access permissions.</code></li> <li> <p>Solution: Make sure that the ports you\u2019re using aren\u2019t already in use by other services. You can check and stop any process using these ports with these commands:      <pre><code>netstat -aon | findstr :50070\ntaskkill /PID &lt;PID&gt; /F\n</code></pre>      Or you can change the port numbers in the Dockerfile and <code>docker run</code> command.</p> </li> <li> <p>Permission Denied Error:</p> </li> <li>Error: <code>ERROR: Attempting to operate on hdfs namenode as root but there is no HDFS_NAMENODE_USER defined.</code></li> <li> <p>Solution: Hadoop services should not run as the root user. The Dockerfile sets up a non-root user (<code>dwdas</code>) to run Hadoop and Hive services:      <pre><code>ENV HDFS_NAMENODE_USER=dwdas\nENV HDFS_DATANODE_USER=dwdas\nENV HDFS_SECONDARYNAMENODE_USER=dwdas\nENV YARN_RESOURCEMANAGER_USER=dwdas\nENV YARN_NODEMANAGER_USER=dwdas\n</code></pre></p> </li> <li> <p>Multiple SLF4J Bindings Error:</p> </li> <li>Error: <code>SLF4J: Class path contains multiple SLF4J bindings.</code></li> <li>Solution: This is usually just a warning, not an error. It means there are multiple SLF4J bindings in the classpath. It can generally be ignored, but if it causes issues, you might need to clean up the classpath by removing conflicting SLF4J jars.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.6_Hive_Hadooop_Postgres_Presto.html","title":"Hive, Hadoop, Postgres & Presto","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.6_Hive_Hadooop_Postgres_Presto.html#setting-up-a-hadoop-hive-and-presto-cluster-using-docker-compose","title":"Setting Up a Hadoop, Hive, and Presto Cluster Using Docker Compose","text":"<p>This guide will walk you through setting up a Hadoop ecosystem including HDFS, YARN, Hive, and Presto using Docker Compose. This setup will allow you to perform distributed data processing and SQL-based querying on large datasets. The services are containerized, making it easy to deploy and manage.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.6_Hive_Hadooop_Postgres_Presto.html#overview-of-the-setup","title":"Overview of the Setup","text":"<p>This setup involves several Docker containers, each providing a different service in the Hadoop ecosystem:</p> <ul> <li>HDFS: Provides a distributed file system that stores data across multiple nodes.</li> <li>YARN: Manages resources and job scheduling across the cluster.</li> <li>Hive: A data warehouse that allows SQL-like querying on data stored in HDFS.</li> <li>Presto: A distributed SQL query engine that allows querying large datasets from multiple sources.</li> </ul> <p>The entire setup can be managed using Docker Compose, which simplifies the orchestration of multiple containers.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.6_Hive_Hadooop_Postgres_Presto.html#using-the-setup","title":"Using the Setup","text":"<p>To use this setup, follow these steps:</p> <ol> <li>Download the Zip File: </li> <li> <p>Download the provided zip file that contains all the necessary configuration files and scripts.</p> </li> <li> <p>Unzip the File: </p> </li> <li> <p>Extract the contents of the zip file to a directory of your choice:      <pre><code>unzip hadoop-hive-presto-setup.zip\ncd hadoop-hive-presto-setup\n</code></pre></p> </li> <li> <p>Run the Setup: </p> </li> <li>Start the entire setup by running the following command:      <pre><code>docker-compose up -d\n</code></pre></li> <li> <p>This command will spin up all the necessary containers and services.</p> </li> <li> <p>Access the Services:</p> </li> <li>You can access the various services via their respective ports as described below.</li> </ol>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.6_Hive_Hadooop_Postgres_Presto.html#description-of-services-and-ports","title":"Description of Services and Ports","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.6_Hive_Hadooop_Postgres_Presto.html#description-of-services-ports-and-folder-paths","title":"Description of Services, Ports, and Folder Paths","text":"<p>Below is a detailed description of the services included in this setup, along with the ports used, relevant folder paths, and user access information. This will help both users and administrators to understand and manage the setup effectively.</p> Service Port Description Folder Path User Access Details Namenode (HDFS) 50070 Web UI for monitoring the HDFS Namenode. <code>/hadoop/dfs/name</code> <code>root</code> Used for monitoring the HDFS cluster, filesystem namespace management. Datanode (HDFS) 50075 Web UI for monitoring Datanode data transfer. <code>/hadoop/dfs/data</code> <code>root</code> Stores actual data blocks, communicates with the Namenode. ResourceManager (YARN) 8088 Web UI for monitoring resource allocation and job scheduling. <code>/rmstate</code> <code>root</code> Manages resources across the cluster, schedules and monitors jobs. NodeManager (YARN) 8042 Web UI for monitoring resource usage on individual nodes. <code>/app-logs</code> <code>root</code> Manages containers on each node, monitors resource usage. HistoryServer (YARN) 19888 Web UI for accessing logs and statistics of completed MapReduce jobs. <code>/hadoop/yarn/timeline</code> <code>root</code> Provides access to history logs and statistics of completed jobs. HiveServer2 (Hive) 10000 Service for executing SQL-based queries via JDBC/ODBC. <code>/user/hive/warehouse</code> <code>hive</code> Used for running SQL queries on data stored in HDFS via Hive. Hive Metastore 9083 Service for managing metadata for Hive tables. <code>/user/hive/warehouse</code> (metadata) <code>hive</code> Stores metadata for Hive tables, accessed by HiveServer2 and clients. Hive Metastore PostgreSQL N/A Internal PostgreSQL database for Hive Metastore metadata storage. <code>/var/lib/postgresql/data</code> (inside PostgreSQL container) <code>postgres</code> Stores metadata about Hive tables, partitions, etc., used by the Metastore. Presto Coordinator 8081 Web UI for monitoring queries and worker nodes in Presto. N/A <code>root</code> Provides a distributed SQL query engine to run queries across data sources."},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.6_Hive_Hadooop_Postgres_Presto.html#hive-table-and-data-location","title":"Hive Table and Data Location","text":"<ul> <li>Hive Tables: The tables created in Hive are stored in HDFS under the directory <code>/user/hive/warehouse/</code>.</li> <li>HDFS Data: All data stored in HDFS, including Hive table data, is managed by the Namenode and Datanode services. The default directory for Hive tables is <code>/user/hive/warehouse/</code>.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.6_Hive_Hadooop_Postgres_Presto.html#users-and-access","title":"Users and Access","text":"<ul> <li>root: The <code>root</code> user has access to all the Hadoop services and directories.</li> <li>hive: The <code>hive</code> user has access to Hive-specific directories and can execute SQL queries via HiveServer2.</li> <li>postgres: The <code>postgres</code> user is the database administrator for the PostgreSQL database used by the Hive Metastore.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.6_Hive_Hadooop_Postgres_Presto.html#testing-the-setup","title":"Testing the Setup","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.6_Hive_Hadooop_Postgres_Presto.html#1-connecting-to-hive","title":"1. Connecting to Hive","text":"<p>To test the Hive setup, you can connect to HiveServer2 using Beeline:</p> <pre><code>docker exec -it hive-server2 beeline -u jdbc:hive2://localhost:10000 -n hive -p hive\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.6_Hive_Hadooop_Postgres_Presto.html#2-creating-and-managing-hive-tables","title":"2. Creating and Managing Hive Tables","text":"<p>Once connected to Hive, you can create databases, tables, and insert data:</p> <pre><code>CREATE DATABASE test_db;\nUSE test_db;\nCREATE TABLE test_table (id INT, name STRING);\nINSERT INTO test_table VALUES (1, 'Hadoop'), (2, 'Hive');\nSELECT * FROM test_table;\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.6_Hive_Hadooop_Postgres_Presto.html#3-checking-hdfs","title":"3. Checking HDFS","text":"<p>To verify that the data is stored correctly in HDFS, you can use the following command:</p> <pre><code>docker exec -it namenode hadoop fs -ls /user/hive/warehouse/test_db.db/test_table\n</code></pre> <p>This command will list the files in the specified Hive table directory in HDFS.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.6_Hive_Hadooop_Postgres_Presto.html#4-monitoring-services","title":"4. Monitoring Services","text":"<ul> <li>HDFS Namenode Web UI: http://localhost:50070</li> <li>YARN ResourceManager Web UI: http://localhost:8088</li> <li>YARN NodeManager Web UI: http://localhost:8042</li> <li>YARN HistoryServer Web UI: http://localhost:19888</li> <li>Presto Web UI: http://localhost:8081</li> </ul> <p>These interfaces provide detailed information about the health and status of the services running in your cluster.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.6_Hive_Hadooop_Postgres_Presto.html#testing-the-setup_1","title":"Testing the Setup","text":"<p>To ensure that your setup is working correctly, you can perform the following tests:</p> <ol> <li>Connect to Hive:</li> <li>Use the Beeline CLI to connect to HiveServer2:      <pre><code>docker exec -it hive-server2 beeline -u jdbc:hive2://localhost:10000 -n hive -p hive\n</code></pre></li> <li>Once connected, create a database and a table:      <pre><code>CREATE DATABASE test_db;\nUSE test_db;\nCREATE TABLE test_table (id INT, name STRING);\n</code></pre></li> <li>Insert some data into the table:      <pre><code>INSERT INTO test_table VALUES (1, 'Hadoop'), (2, 'Hive');\n</code></pre></li> <li> <p>Query the data to verify the setup:      <pre><code>SELECT * FROM test_table;\n</code></pre></p> </li> <li> <p>Check HDFS:</p> </li> <li>Ensure that the data was correctly stored in HDFS:      <pre><code>docker exec -it namenode hadoop fs -ls /user/hive/warehouse/test_db.db/test_table\n</code></pre></li> <li> <p>You should see the files corresponding to the inserted data.</p> </li> <li> <p>Monitor YARN Jobs:</p> </li> <li> <p>Visit the ResourceManager Web UI at <code>http://localhost:8088</code> to monitor job execution.</p> </li> <li> <p>Access the Presto UI:</p> </li> <li>Visit the Presto Web UI at <code>http://localhost:8081</code> to monitor queries and worker nodes.</li> </ol>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.6_Hive_Hadooop_Postgres_Presto.html#conclusion","title":"Conclusion","text":"<p>This setup provides a robust environment for distributed data processing and querying using Hadoop, Hive, and Presto. By containerizing these services, you can easily manage and scale your data processing infrastructure. Testing the setup with sample data ensures that all components are correctly configured and working together seamlessly.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.6_Hive_Hadooop_Postgres_Presto.html#connecting-with-spark-cluster","title":"Connecting with Spark Cluster","text":"<p>As we are using bitnami spark cluster the conf direcotory is in . Put the following hive-conf.xml</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n&lt;!-- \n    hive-site.xml for Spark to connect with Hive\n    This configuration enables Spark to interact with Hive Metastore and HDFS.\n--&gt;\n&lt;configuration&gt;\n    &lt;!-- \n        Configures the JDBC connection URL for the Hive Metastore.\n        The hostname and port must match your Docker Compose setup.\n    --&gt;\n    &lt;property&gt;\n        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;\n        &lt;value&gt;jdbc:postgresql://hive-metastore-postgresql/metastore&lt;/value&gt;\n        &lt;description&gt;JDBC connection URL for the Hive Metastore database.&lt;/description&gt;\n    &lt;/property&gt;\n    &lt;!-- \n        Specifies the JDBC driver class for PostgreSQL.\n    --&gt;\n    &lt;property&gt;\n        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;\n        &lt;value&gt;org.postgresql.Driver&lt;/value&gt;\n        &lt;description&gt;JDBC driver class for connecting to the PostgreSQL database.&lt;/description&gt;\n    &lt;/property&gt;\n    &lt;!-- \n        The username and password for connecting to the Hive Metastore database.\n        These credentials should match those used in your PostgreSQL setup.\n    --&gt;\n    &lt;property&gt;\n        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;\n        &lt;value&gt;hive&lt;/value&gt;\n        &lt;description&gt;Username for connecting to the PostgreSQL database.&lt;/description&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;\n        &lt;value&gt;hive&lt;/value&gt;\n        &lt;description&gt;Password for connecting to the PostgreSQL database.&lt;/description&gt;\n    &lt;/property&gt;\n    &lt;!-- \n        Configures the URI for the Hive Metastore service.\n        This URI must match the service in your Hive setup.\n    --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hive.metastore.uris&lt;/name&gt;\n        &lt;value&gt;thrift://hive-metastore:9083&lt;/value&gt;\n        &lt;description&gt;Thrift URI for the Hive Metastore service.&lt;/description&gt;\n    &lt;/property&gt;\n    &lt;!-- \n        The location of the Hive warehouse directory in HDFS.\n        It should match the configuration used in your Hive setup.\n    --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;\n        &lt;value&gt;/user/hive/warehouse&lt;/value&gt;\n        &lt;description&gt;Location of the Hive warehouse directory in HDFS.&lt;/description&gt;\n    &lt;/property&gt;\n    &lt;!-- \n        Enabling Hive support in Spark.\n        This setting allows Spark to interact with Hives Metastore.\n    --&gt;\n    &lt;property&gt;\n        &lt;name&gt;spark.sql.catalogImplementation&lt;/name&gt;\n        &lt;value&gt;hive&lt;/value&gt;\n        &lt;description&gt;Enables Hive support in Spark.&lt;/description&gt;\n    &lt;/property&gt;\n    &lt;!-- \n        Configuring dynamic partitioning in Spark when writing data to Hive tables.\n    --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hive.exec.dynamic.partition&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n        &lt;description&gt;Enables dynamic partitioning in Spark for Hive tables.&lt;/description&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;hive.exec.dynamic.partition.mode&lt;/name&gt;\n        &lt;value&gt;nonstrict&lt;/value&gt;\n        &lt;description&gt;Allows dynamic partitioning without requiring explicit partition columns.&lt;/description&gt;\n    &lt;/property&gt;\n    &lt;!-- \n        HDFS Configuration: Telling Spark where to find the HDFS Namenode.\n        The Namenode is typically accessed at port 8020 in Hadoop setups.\n    --&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;/name&gt;\n        &lt;value&gt;hdfs://namenode:8020&lt;/value&gt;\n        &lt;description&gt;URI of the HDFS Namenode.&lt;/description&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"Spark-Hive Test\") \\\n    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:8020/user/hive/warehouse\") \\\n    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\nspark.sql(\"SHOW DATABASES\").show()\n</code></pre> <p></p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.6_Hive_Hadooop_Postgres_Presto.html#further-reading","title":"Further reading","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.6_Hive_Hadooop_Postgres_Presto.html#create-the-first-project","title":"Create the first project","text":"<p>Download the keggel dataset to a folder retailkegraw Copy the folder to the container /home/dwdas</p> <p>Frst create a virtual enviornment. It resolves many permission issues and makes work very smooth. It's not just an improvement it helps work with pyspark easier.</p> <p>mv C:\\Users\\dwaip\\Desktop\\retailkegraw master:/home/dwdas</p> <p>dwdas@1478d9ddd5d8:<code>/home/dwdas$ python -m venv keglrtl-venv</code></p> <p>source keglrtl-venv/bin/activate</p> <p>https://medium.com/@madtopcoder/putting-hadoop-hive-and-spark-together-for-the-first-time-bf44262575bd</p> Error Description Possible Reason How to Troubleshoot Resolution <code>SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta</code> Spark could not find the Delta Lake source. Missing Delta Lake libraries or incorrect configuration. Ensure Delta Lake is installed and correctly configured in the Spark session. Installed <code>delta-spark</code> and configured Spark with the appropriate Delta Lake JARs. <code>java.io.FileNotFoundException: /opt/bitnami/spark/.ivy2/cache/resolved-org.apache.spark-spark-submit-parent...</code> Spark was unable to write to the Ivy cache directory. Permission issues in the directory. Check and adjust directory permissions or set a custom Ivy cache directory. Adjusted permissions or set a custom Ivy cache directory with write permissions. <code>Permission denied: '/.local/lib'</code> Insufficient permissions to install Python packages globally. Trying to install packages globally without sufficient privileges. Use <code>pip install --user</code> or run with <code>sudo -H</code>. Installed with <code>--user</code> flag or <code>sudo -H</code>. <code>java.lang.NoClassDefFoundError: scala/collection/SeqOps</code> A Scala version mismatch where a dependency required Scala 2.13. Using libraries compiled with Scala 2.13 with Spark compiled for Scala 2.12. Ensure all libraries match Scala 2.12 and clear any Scala 2.13 JARs. Used Scala 2.12-compatible Delta Lake JARs and cleared cached Scala 2.13 JARs. <code>java.lang.NoClassDefFoundError: scala/collection/IterableOnce</code> Another Scala version mismatch due to Scala 2.13 libraries. Similar to the SeqOps error; mixing Scala 2.13 libraries with a Scala 2.12 Spark environment. Check library versions, ensure consistency, and clear any Scala 2.13 dependencies. Aligned all dependencies with Scala 2.12 and cleared conflicting JARs. <code>Py4JJavaError: An error occurred while calling o37.applyModifiableSettings.</code> Spark encountered issues applying settings due to a version mismatch. Likely caused by using incompatible versions of libraries or JARs. Review and match versions of all libraries with Spark\u2019s Scala version. Ensured correct Delta Lake version for Scala 2.12 and cleared cache. <code>Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.</code> Spark was unable to use Delta Lake extensions. Incorrect or missing dependencies, likely due to Scala version mismatch. Ensure the Delta Lake JAR matches the Scala version used by Spark. Used the correct Delta Lake JARs for Scala 2.12 and Spark 3.5.2. <code>rm -rf ~/.ivy2/cache/io.delta</code> <code>rm -rf ~/.ivy2/jars/io.delta</code> <code>rm -rf ~/.m2/repository/io/delta</code> Commands used to clear cached JAR files. Old or conflicting cached dependencies causing version conflicts. Clear Ivy or Maven cache to remove outdated or conflicting JARs. Removed cached JARs to prevent conflicts and ensure fresh downloads. <pre><code>from pyspark.sql import SparkSession\nfrom delta import configure_spark_with_delta_pip\n\n# Build the Spark session with Delta Lake support\n# - config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\"): Adds Delta Lake SQL extensions to Spark, allowing it to recognize and work with Delta Lake features.\n# - config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\"): Configures Spark to use Delta Lake as the default catalog for managing tables.\n# - config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\"): Specifies the Maven coordinates for the Delta Lake library compatible with Scala 2.12 and Spark 3.5.2. This ensures the necessary Delta Lake JARs are included in the Spark session.\nbuilder = SparkSession.builder.appName(\"DeltaTutorial\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\")\n\n# Set up Spark to work with Delta Lake\n# - configure_spark_with_delta_pip(builder): This function from Delta Lake makes sure Spark is ready to use Delta features.\nspark = configure_spark_with_delta_pip(builder).getOrCreate()\n\nspark.sparkContext.setLogLevel(\"ERROR\")\ndf = spark.createDataFrame(data, [\"id\", \"value\"])\n\n# - save(\"/tmp/delta-table\"): Specifies the location where the Delta table will be saved. In this case, it's saved in the `/tmp/delta-table` directory.\ndf.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")\ndf_read = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\ndf_read.show()\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.7_Hadoop_Hive_SingleNode_MySQL.html","title":"Hadoop & Hive with MySQL","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.7_Hadoop_Hive_SingleNode_MySQL.html#step-1-updated-entrypointsh","title":"Step 1: Updated <code>entrypoint.sh</code>","text":"<pre><code>#!/bin/bash\n\n# Exit immediately if a command exits with a non-zero status\nset -e\n\n# Function to start SSH service\nstart_ssh() {\n    echo \"Starting SSH service...\"\n    service ssh start\n}\n\n# Function to start MySQL service and initialize metastore\nstart_mysql() {\n    echo \"Starting MySQL service...\"\n    service mysql start\n\n    echo \"Configuring MySQL for Hive Metastore...\"\n    mysql -uroot -pPassw0rd &lt;&lt;EOF\nCREATE DATABASE IF NOT EXISTS metastore;\nCREATE USER IF NOT EXISTS 'hive'@'localhost' IDENTIFIED BY 'Passw0rd';\nGRANT ALL PRIVILEGES ON metastore.* TO 'hive'@'localhost';\nFLUSH PRIVILEGES;\nEOF\n}\n\n# Function to format HDFS namenode\nformat_namenode() {\n    if [ ! -d \"/usr/local/hadoop/tmp/hdfs/namenode/current\" ]; then\n        echo \"Formatting HDFS namenode...\"\n        hdfs namenode -format -force -nonInteractive\n    else\n        echo \"HDFS namenode already formatted.\"\n    fi\n}\n\n# Function to start HDFS services\nstart_hdfs() {\n    echo \"Starting HDFS services...\"\n    start-dfs.sh\n}\n\n# Function to start YARN services\nstart_yarn() {\n    echo \"Starting YARN services...\"\n    start-yarn.sh\n}\n\n# Function to start MapReduce JobHistory Server\nstart_historyserver() {\n    echo \"Starting MapReduce JobHistory Server...\"\n    mr-jobhistory-daemon.sh start historyserver\n}\n\n# Function to start Hive services\nstart_hive() {\n    echo \"Starting Hive Metastore...\"\n    nohup hive --service metastore &gt; /tmp/hive-metastore.log 2&gt;&amp;1 &amp;\n\n    echo \"Starting HiveServer2...\"\n    nohup hive --service hiveserver2 &gt; /tmp/hive-server2.log 2&gt;&amp;1 &amp;\n}\n\n# Function to keep the container running\nkeep_container_alive() {\n    echo \"All services started. Tail logs to keep the container running...\"\n    tail -f /dev/null\n}\n\n# Execute functions in order\nstart_ssh\nstart_mysql\nformat_namenode\nstart_hdfs\nstart_yarn\nstart_historyserver\nstart_hive\nkeep_container_alive\n</code></pre> <p>Explanation of the Script:</p> <ul> <li><code>set -e</code>: Ensures that the script exits immediately if any command fails.</li> <li><code>start_ssh</code>: Starts the SSH service required by Hadoop.</li> <li><code>start_mysql</code>: Starts MySQL service and configures the Hive metastore database with user <code>hive</code> and password <code>Passw0rd</code>.</li> <li><code>format_namenode</code>: Checks if the namenode is already formatted; if not, formats it.</li> <li><code>start_hdfs</code>: Starts HDFS services including namenode and datanode.</li> <li><code>start_yarn</code>: Starts YARN services including resourcemanager and nodemanager.</li> <li><code>start_historyserver</code>: Starts the MapReduce JobHistory Server.</li> <li><code>start_hive</code>: Starts Hive Metastore and HiveServer2 services in the background and redirects logs to <code>/tmp/</code>.</li> <li><code>keep_container_alive</code>: Keeps the container running by tailing <code>/dev/null</code>.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.7_Hadoop_Hive_SingleNode_MySQL.html#step-2-update-dockerfile","title":"Step 2: Update <code>Dockerfile</code>","text":"<p>Ensure your <code>Dockerfile</code> correctly copies the <code>entrypoint.sh</code> script and sets appropriate permissions.</p> <pre><code># Use a base image with Java\nFROM openjdk:8-jdk\n\n# Set environment variables\nENV HADOOP_VERSION=3.4.0\nENV HIVE_VERSION=4.0.0\nENV MYSQL_ROOT_PASSWORD=Passw0rd\nENV HADOOP_HOME=/usr/local/hadoop\nENV HIVE_HOME=/usr/local/hive\nENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin\n\n# Install necessary packages\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y ssh rsync mysql-server wget &amp;&amp; \\\n    apt-get clean\n\n# Create a user\nRUN useradd -ms /bin/bash dwdas\n\n# Switch to user\nUSER dwdas\nWORKDIR /home/dwdas\n\n# Download and extract Hadoop\nRUN wget https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz &amp;&amp; \\\n    tar -xzf hadoop-${HADOOP_VERSION}.tar.gz &amp;&amp; \\\n    mv hadoop-${HADOOP_VERSION} $HADOOP_HOME &amp;&amp; \\\n    rm hadoop-${HADOOP_VERSION}.tar.gz\n\n# Download and extract Hive\nRUN wget https://downloads.apache.org/hive/hive-${HIVE_VERSION}/apache-hive-${HIVE_VERSION}-bin.tar.gz &amp;&amp; \\\n    tar -xzf apache-hive-${HIVE_VERSION}-bin.tar.gz &amp;&amp; \\\n    mv apache-hive-${HIVE_VERSION}-bin $HIVE_HOME &amp;&amp; \\\n    rm apache-hive-${HIVE_VERSION}-bin.tar.gz\n\n# Copy configuration files\nCOPY --chown=dwdas:dwdas core-site.xml $HADOOP_HOME/etc/hadoop/\nCOPY --chown=dwdas:dwdas hdfs-site.xml $HADOOP_HOME/etc/hadoop/\nCOPY --chown=dwdas:dwdas mapred-site.xml $HADOOP_HOME/etc/hadoop/\nCOPY --chown=dwdas:dwdas yarn-site.xml $HADOOP_HOME/etc/hadoop/\nCOPY --chown=dwdas:dwdas hive-site.xml $HIVE_HOME/conf/\n\n# Copy MySQL JDBC driver\nRUN wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.31.tar.gz &amp;&amp; \\\n    tar -xzf mysql-connector-java-8.0.31.tar.gz &amp;&amp; \\\n    cp mysql-connector-java-8.0.31/mysql-connector-java-8.0.31.jar $HIVE_HOME/lib/ &amp;&amp; \\\n    cp mysql-connector-java-8.0.31/mysql-connector-java-8.0.31.jar $HADOOP_HOME/share/hadoop/common/lib/ &amp;&amp; \\\n    rm -rf mysql-connector-java-8.0.31* \n\n# Set permissions\nRUN chmod +x $HADOOP_HOME/bin/* &amp;&amp; \\\n    chmod +x $HADOOP_HOME/sbin/* &amp;&amp; \\\n    chmod +x $HIVE_HOME/bin/*\n\n# Switch back to root to start services\nUSER root\n\n# Copy and set permissions for entrypoint.sh\nCOPY entrypoint.sh /entrypoint.sh\nRUN chmod +x /entrypoint.sh\n\n# Expose necessary ports\nEXPOSE 50070 8088 10000 10002 19888 3306\n\n# Start services\nCMD [\"/entrypoint.sh\"]\n</code></pre> <p>Explanation of the Dockerfile:</p> <ul> <li>Base Image: Uses <code>openjdk:8-jdk</code> as the base image.</li> <li>Environment Variables: Sets necessary environment variables for Hadoop and Hive.</li> <li>Package Installation: Installs SSH, rsync, MySQL server, and wget.</li> <li>User Creation: Creates a user <code>dwdas</code> and sets the working directory.</li> <li>Downloading Hadoop and Hive: Downloads and extracts Hadoop and Hive binaries.</li> <li>Configuration Files: Copies necessary configuration files into their respective directories.</li> <li>MySQL JDBC Driver: Downloads and places the MySQL JDBC driver in Hive and Hadoop lib directories.</li> <li>Permissions: Ensures all binaries and scripts have the correct execution permissions.</li> <li>Entrypoint Script: Copies the updated <code>entrypoint.sh</code> into the image and sets execution permission.</li> <li>Exposed Ports: Exposes all necessary ports for accessing services.</li> <li>CMD: Sets the default command to execute <code>entrypoint.sh</code> when the container starts.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.7_Hadoop_Hive_SingleNode_MySQL.html#step-3-configuration-files","title":"Step 3: Configuration Files","text":"<p>Ensure that your configuration files (<code>core-site.xml</code>, <code>hdfs-site.xml</code>, <code>mapred-site.xml</code>, <code>yarn-site.xml</code>, and <code>hive-site.xml</code>) are correctly set up.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.7_Hadoop_Hive_SingleNode_MySQL.html#example-hive-sitexml","title":"Example <code>hive-site.xml</code>","text":"<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;\n        &lt;value&gt;jdbc:mysql://localhost:3306/metastore?useSSL=false&amp;amp;createDatabaseIfNotExist=true&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;\n        &lt;value&gt;com.mysql.cj.jdbc.Driver&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;\n        &lt;value&gt;hive&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;\n        &lt;value&gt;Passw0rd&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;\n        &lt;value&gt;/user/hive/warehouse&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;datanucleus.autoCreateSchema&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;datanucleus.autoCreateTables&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;datanucleus.fixedDatastore&lt;/name&gt;\n        &lt;value&gt;false&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;\n        &lt;value&gt;10000&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Note: Ensure all other Hadoop configuration files are correctly set according to your environment and requirements.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.7_Hadoop_Hive_SingleNode_MySQL.html#step-4-update-docker-composeyml","title":"Step 4: Update <code>docker-compose.yml</code>","text":"<pre><code>version: '3.8'\n\nservices:\n  hadoop-hive-mysql:\n    build: .\n    container_name: hadoop-hive-mysql\n    hostname: hadoop-hive-mysql\n    environment:\n      - MYSQL_ROOT_PASSWORD=Passw0rd\n    volumes:\n      - namenode_data:/usr/local/hadoop/tmp/hdfs/namenode\n      - datanode_data:/usr/local/hadoop/tmp/hdfs/datanode\n      - hive_warehouse:/user/hive/warehouse\n      - mysql_data:/var/lib/mysql\n    ports:\n      - \"29070:50070\"    # HDFS NameNode Web UI\n      - \"28088:8088\"     # YARN ResourceManager Web UI\n      - \"21000:10000\"    # HiveServer2\n      - \"21002:10002\"    # HiveServer2 Thrift HTTP\n      - \"21988:19888\"    # MapReduce JobHistory\n      - \"23306:3306\"     # MySQL\n    networks:\n      - dasnet\n\nvolumes:\n  namenode_data:\n  datanode_data:\n  hive_warehouse:\n  mysql_data:\n\nnetworks:\n  dasnet:\n    external: true\n</code></pre> <p>Explanation of <code>docker-compose.yml</code>:</p> <ul> <li>Services: Defines a single service <code>hadoop-hive-mysql</code>.</li> <li>Volumes: Uses Docker volumes for persistent storage of HDFS namenode, datanode, Hive warehouse, and MySQL data.</li> <li>Ports: Maps container ports to host ports using rare ports to avoid conflicts.</li> <li>Networks: Connects to an external network <code>dasnet</code>. Ensure this network exists or remove <code>external: true</code> to let Docker create it.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.7_Hadoop_Hive_SingleNode_MySQL.html#step-5-building-and-running-the-container","title":"Step 5: Building and Running the Container","text":"<ol> <li>Build the Docker Image:</li> </ol> <pre><code>docker-compose build\n</code></pre> <ol> <li>Start the Container:</li> </ol> <pre><code>docker-compose up -d\n</code></pre> <ol> <li> <p>Check Running Services:</p> </li> <li> <p>HDFS NameNode Web UI: http://localhost:29070</p> </li> <li>YARN ResourceManager Web UI: http://localhost:28088</li> <li> <p>MapReduce JobHistory Web UI: http://localhost:21988</p> </li> <li> <p>Connecting to Hive:</p> </li> <li> <p>Using Beeline:</p> <pre><code>beeline -u jdbc:hive2://localhost:21000 -n hive -p Passw0rd\n</code></pre> </li> <li> <p>Using JDBC:</p> <pre><code>jdbc:hive2://localhost:21000/default\n</code></pre> </li> <li> <p>Connecting to MySQL:</p> </li> <li> <p>From Host Machine:</p> <pre><code>mysql -h 127.0.0.1 -P 23306 -u root -pPassw0rd\n</code></pre> </li> <li> <p>Inspect Logs:</p> </li> <li> <p>Hive Logs:</p> <pre><code>docker exec -it hadoop-hive-mysql bash\ntail -f /tmp/hive-metastore.log\ntail -f /tmp/hive-server2.log\n</code></pre> </li> <li> <p>Hadoop Logs:</p> <pre><code>docker exec -it hadoop-hive-mysql bash\nhdfs dfsadmin -report\nyarn node -list\n</code></pre> </li> </ol>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.7_Hadoop_Hive_SingleNode_MySQL.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Ports Already in Use: If any of the mapped ports are already in use on your host machine, modify them in the <code>docker-compose.yml</code> file.</li> <li>Service Failures: Check respective logs inside the container for detailed error messages.</li> <li>Network Issues: Ensure that the Docker network <code>dasnet</code> exists or remove the <code>external</code> specification to let Docker manage it.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.8_Hive-ApacheOfficial.html","title":"Apache Official - Hive Metastore, Postgress and HiveServer2 Setup On Docker","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.8_Hive-ApacheOfficial.html#overview","title":"Overview","text":"<p>This steup is based on Official Hive docker site. There is almost no customization, except the addtion of network dasnet. The setup consists of few docker commands run using CMD/Terminal. That's it.</p> <p>The only issue with this setup is when you restart HiveServer2, it gives the error: \"HiveServer2 running as process 7. Stop it first.\" Other than that, the setup is great. Just rerun the Docker command for HiveServer2 to create a new container. The metastore and PostgreSQL work fine without needing a restart.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.8_Hive-ApacheOfficial.html#steps-to-create-the-setup","title":"Steps to create the setup","text":"<p>Just download thezip file. Unzip it and run the run.bat It will create three conatiners like this:</p> <p></p> <p>Or, if you want to run it manually here is the CMD script(just save it as <code>anyname.bat</code> and run it. That's all.):</p> <pre><code>REM Das Sep-04-2024: This CDs to the current folder\ncd /d %~dp0\n\nREM Step 1: Create a Docker volume\ndocker volume create warehouse\n\nREM Das: We create a postgres container\ndocker run -d --network dasnet --name postgres -e POSTGRES_DB=metastore_db -e POSTGRES_USER=hive -e POSTGRES_PASSWORD=password postgres:13\n\nREM Das: Create metastore-standalone container, connecting to Postgres and initializing the database\ndocker run -d --network dasnet -p 9083:9083 --env SERVICE_NAME=metastore --env DB_DRIVER=postgres --env SERVICE_OPTS=\"-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore_db -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=password\" --mount source=warehouse,target=/opt/hive/data/warehouse --name metastore apache/hive:4.0.0-alpha-2\n\nREM Das: Create HiveServer2 container which will connect to the metastore\ndocker run -d --network dasnet -p 10000:10000 -p 10002:10002 --env SERVICE_NAME=hiveserver2 --env SERVICE_OPTS=\"-Dhive.metastore.uris=thrift://metastore:9083\" --mount source=warehouse,target=/opt/hive/data/warehouse --env IS_RESUME=\"true\" --name hiveserver2 apache/hive:4.0.0-alpha-2\n\nREM Keep the window open so that the user can review the output\npause\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.8_Hive-ApacheOfficial.html#steps-to-verify-the-setup","title":"Steps to verify the Setup","text":"<p>To check the connectivity between all components:</p> <pre><code>docker ps\n</code></pre> <p>You should see the <code>postgres</code>, <code>metastore</code>, and <code>hiveserver2</code> containers running.</p> <p>Connect to Beeline on HiveServer2:</p> <pre><code>docker exec -it hiveserver2 /bin/bash\nbeeline -u 'jdbc:hive2://hiveserver2:10000/'\n</code></pre> <p>Create a sample table:</p> <pre><code>CREATE TABLE movies (id INT, name STRING);\nINSERT INTO movies VALUES (1, 'Rambo'), (2, 'Mogambo');\nSELECT * FROM movies;\n</code></pre> <p>Exit Beeline:</p> <pre><code>!quit;\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.8_Hive-ApacheOfficial.html#how-data-is-stored-in-this-setup-by-default","title":"How data is stored in this setup by default","text":"<p> This setup is fully self-contained and enough for a Hive warehouse. The data is saved in a Docker volume, and the metadata is stored in a PostgreSQL server. You can later modify this setup to add a Hadoop Namenode (like in real-world scenarios) or even use Amazon S3 or ADLS for data storage.  </p> <p>In this setup, Hive data will be stored in the following locations:</p> <ol> <li>Postgres Database: </li> <li>The Postgres container is used to store the metadata (table schemas, databases, etc.) for Hive. This is where the Hive Metastore connects to manage and query metadata.</li> <li> <p>Database: <code>metastore_db</code>, User: <code>hive</code>, Password: <code>password</code>.</p> </li> <li> <p>HDFS for Table Data: </p> </li> <li> <p>We are using the volume <code>warehouse</code> mounted to <code>/opt/hive/data/warehouse</code> in both the Metastore and HiveServer2 containers. This is where Hive stores its table data files. The directory <code>/opt/hive/data/warehouse</code> in the containers corresponds to the <code>warehouse</code> volume on our host system.</p> </li> <li> <p>Metastore Initialization: </p> </li> <li> <p>The Metastore container connects to Postgres and uses the JDBC driver (<code>postgresql-42.5.1.jar</code>) to initialize the Hive Metastore database. This ensures that Hive's metadata is managed via Postgres.</p> </li> <li> <p>HiveServer2:</p> </li> <li>The HiveServer2 container connects to the Metastore (via <code>thrift://metastore:9083</code>) to retrieve metadata and execute Hive queries.</li> <li>The table data is stored in the same warehouse directory mounted as a volume, allowing HiveServer2 to access the data files in HDFS.</li> </ol>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.8_Hive-ApacheOfficial.html#thrift-metastore-uris-for-this-setup","title":"thrift metastore uris for this setup","text":"<ol> <li> <p><code>thrift://metastore:9083</code>: This is the Metastore Thrift URI that other services (like HiveServer2 or Spark) use to connect to the Hive Metastore and retrieve metadata about tables and databases. 9083 is a default port.</p> </li> <li> <p><code>thrift://hiveserver2:10000</code>: This URI is for the HiveServer2 service, which is used to execute Hive queries. Applications (like Beeline or JDBC/ODBC clients) connect to this URI to run SQL queries in Hive. 10000 is a default port.</p> </li> </ol>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.8_Hive-ApacheOfficial.html#how-to-connect-a-spark-session-to-this-setup","title":"How to connect a Spark Session to this setup","text":"<p>In this setup, even without explicitly configuring <code>hive-site.xml</code> or adding any special configuration files, Spark can still interact with Hive by creating a Spark session that points to Hive and using Hive's metastore through the Thrift interface. Here's how it works:</p> <ol> <li>Create a Spark Session:</li> <li> <p>When you create a Spark session, make sure to specify that it should use Hive. Spark will use the Hive Thrift Server (HiveServer2) to access Hive tables and metadata. Here's how you can create the session:</p> <pre><code># Create a Spark session to connect with Hive Metastore\n# 'thrift://metastore:9083' is the URI of the Hive Metastore service in the container.\n# 9083 is the default port for the Hive Metastore using the Thrift protocol.\n# Spark will use this to fetch table metadata and manage Hive tables.\n\n# Enable Hive support so Spark can read/write Hive tables using Hive's metadata and storage.\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n   .appName(\"Spark Hive Example\") \\\n   .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n   .config(\"hive.metastore.uris\", \"thrift://metastore:9083\") \\\n   .enableHiveSupport() \\\n   .getOrCreate()\n</code></pre> </li> <li> <p>Create a Table and Store Data:</p> </li> <li>Once the session is created, you can create tables and store data in Hive without needing <code>hive-site.xml</code>. For example, you can create a table and insert data like this:</li> </ol> <pre><code># Create a DataFrame with some sample data\ndata = [(\"Jango\", 30), (\"Jamila\", 25)]\ndf = spark.createDataFrame(data, [\"name\", \"age\"])\n\n# Create a table in Hive and write data to it\ndf.write.mode(\"overwrite\").saveAsTable(\"default.person\")\n\n# Verify that the table was created and data was stored\nspark.sql(\"SELECT * FROM default.person\").show()\n</code></pre> <ol> <li>Where is the Data Stored?:</li> <li> <p>The data you insert will be saved in the <code>warehouse</code> directory mounted to <code>/opt/hive/data/warehouse</code> in the HiveServer2 container. This happens automatically since you have the Docker volume set up to persist the data.</p> </li> <li> <p>For example, after creating the <code>person</code> table, the actual data will be stored in a directory like <code>/opt/hive/data/warehouse/default/person</code> inside the <code>hiveserver2</code> container, and it will persist even if the container restarts.</p> </li> </ol>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.8_Hive-ApacheOfficial.html#configuration-reference","title":"Configuration Reference","text":"Component Details PostgreSQL Hive Metastore - Database Name: <code>metastore_db</code>  - Username: <code>hive</code>  - Password: <code>password</code>  - Connection URL: <code>jdbc:postgresql://postgres:5432/metastore_db</code> Schema Tool for Hive Tables - Location: <code>/opt/hive/bin/schematool</code>  - Command: <code>/opt/hive/bin/schematool -dbType postgres -initOrUpgradeSchema</code> Hive Configuration Directory <code>/opt/hive/conf</code> Hive Version <code>4.0.0-alpha-2</code> HiveServer2 Startup Script <code>/opt/hive/bin/ext/hiveserver2.sh</code> PostgreSQL Container - Network: <code>dasnet</code>  - Image: <code>postgres:13</code>  - Command: <code>docker run -d --network dasnet --name postgres -e POSTGRES_DB=metastore_db -e POSTGRES_USER=hive -e POSTGRES_PASSWORD=password postgres:13</code> Metastore Container - Network: <code>dasnet</code>  - Port: <code>9083</code>  - Warehouse Data Directory: <code>/opt/hive/data/warehouse</code>  - JDBC Driver: <code>/opt/hive/lib/postgres.jar</code>  - Image: <code>apache/hive:4.0.0-alpha-2</code> HiveServer2 Container - Network: <code>dasnet</code>  - Ports: <code>10000</code> (Thrift service), <code>10002</code> (additional)  - Warehouse Data Directory: <code>/opt/hive/data/warehouse</code>  - Metastore URI: <code>thrift://metastore:9083</code>  - Image: <code>apache/hive:4.0.0-alpha-2</code> <p> Note: This setup works smoothly for the metastore and PostgreSQL combination, but restarting HiveServer2 doesn't work, and you need to recreate the HiveServer2 container alone. </p> <p>Official Hive Docker Hive Files Github</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.8_Hive-ApacheOfficial.html#add-hue-to-the-system","title":"Add hue to the system","text":"<p>To install Apache Hue and connect it to your existing environment, we'll need to configure Hue to work with Hive and PostgreSQL. Here's a step-by-step guide on how to do this:</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.8_Hive-ApacheOfficial.html#step-by-step-guide-to-install-and-connect-hue-to-hive","title":"Step-by-Step Guide to Install and Connect Hue to Hive","text":"<ol> <li> <p>Create a Docker Network (should be already created):    The <code>dasnet</code> network shoudld be already created. If not, create it using command:    <pre><code>docker network create dasnet\n</code></pre></p> </li> <li> <p>Run Hue Docker Container:    You can run Hue in a Docker container and connect it to your existing Hive and PostgreSQL setup.</p> </li> </ol> <p>Here's the command to run Hue:</p> <pre><code>docker run -d --network dasnet -p 8888:8888 --name hue gethue/hue:latest\n</code></pre> <ol> <li>Configure Hue to Connect to Hive</li> </ol> <p>Note: You can either edit the <code>hue.ini</code> file directly inside the container or modify it locally and then copy it to the <code>/usr/share/hue/desktop/conf/</code> directory in the container. After making the changes you can copy the file to container using the followig command(cd to the folder containing the hue.ini file) <code>docker cp ./hue.ini hue:/usr/share/hue/desktop/conf/</code></p> <p></p> <p>To connect Hue to HiveServer2, you need to update the <code>hue.ini</code> configuration file in the Hue container.</p> <ul> <li> <p>First, open a terminal in the Hue container:   <pre><code>docker exec -it hue /bin/bash\n</code></pre></p> </li> <li> <p>Navigate to the directory where the <code>hue.ini</code> file is located:   <pre><code>cd /usr/share/hue/desktop/conf/\n</code></pre></p> </li> <li> <p>Open the <code>hue.ini</code> file using a text editor like <code>nano</code> or <code>vim</code>:   <pre><code>nano hue.ini\n</code></pre></p> </li> <li> <p>Look for the <code>[beeswax]</code> section in the file and add the following lines to point Hue to your HiveServer2 instance:</p> </li> </ul> <pre><code>[[beeswax]]\n# HiveServer2 connection settings\nhive_server_host=localhost\nhive_server_port=10000\n# Optional authentication settings for HiveServer2\nauth_username=hive\nauth_password=password\n</code></pre> <ol> <li>Configure PostgreSQL as Hue's Backend Database</li> </ol> <p>To make Hue use your PostgreSQL database (the one used by Hive Metastore), update the database configuration in the <code>hue.ini</code> file.</p> <ul> <li>In the same <code>hue.ini</code> file, find the <code>[desktop]</code> section and add the following lines to configure PostgreSQL as the backend database:</li> </ul> <pre><code>[desktop]\n[[database]]\nengine=postgresql_psycopg2\nhost=postgres\nport=5432\nuser=hive\npassword=password\nname=metastore_db\n</code></pre> <p>       There should only be one `[[database]]` section and one `[[beeswax]]` section in the configuration file, and these sections should always use double brackets (`[[ ]]`). Common errors that occurs if these are not followed are:    </p> <pre><code>ERROR: Error in configuration file '/usr/share/hue/desktop/conf/hue.ini': Parsing failed with several errors.\nError in configuration file '/usr/share/hue/desktop/conf/hue.ini': Duplicate section name at line 524.\n</code></pre> <ol> <li>Restart the Hue Container, then access it:    After making changes to the <code>hue.ini</code> file, exit the editor and container, and restart the Hue container to apply the changes:</li> </ol> <p><pre><code>docker restart hue\n</code></pre>    Then access hue using the following command:</p> <p><code>http://localhost:8888</code></p> <p>       The credential you choose will  be your root credential. For me it is `dwdas/Passw0rd`    </p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.8_Hive-ApacheOfficial.html#draft-do-not-useconnecting-to-spark-using-hive-sitexml","title":"[Draft DO NOT USE]Connecting To Spark using <code>hive-site.xml</code>","text":"<p>Note: 1. SparkConf: If you programmatically set configurations using the <code>SparkConf</code> object in your Spark application, these settings have the highest priority.  2. Flags passed to <code>spark-submit</code>: If you pass configuration options directly when running the Spark job using <code>spark-submit</code> (e.g., <code>--conf spark.executor.memory=4g</code>), these come next. 3. spark-defaults.conf: This is the default configuration file for Spark, and properties here are considered last. They will only apply if the settings haven't been specified by <code>SparkConf</code> or <code>spark-submit</code>.</p> <p>In this section, we will show how to create a Spark container from scratch and connect it to the existing Hive environment that we already set up using Docker. You will learn step by step how to configure Spark so it can talk to Hive, create tables in Hive, and use PySpark to query and manage those tables.</p> <ol> <li>Create and Run the Spark Container:</li> </ol> <p>First, we need to create a new Spark container. This Spark container will be on the same network as your Hive and PostgreSQL containers so that they can communicate with each other.</p> <p>Run this command to create the Spark container and connect it to the Docker network (<code>dasnet</code>):</p> <pre><code>docker run -d --network dasnet --name spark-container -p 4040:4040 -p 8080:8080 -v /path/to/spark-conf:/opt/spark/conf bitnami/spark:latest\n</code></pre> <ol> <li>Configure Hive for Spark:</li> </ol> <p>Now, we need to tell Spark where to find Hive. We do this by setting up the <code>hive-site.xml</code> file with the correct information about the Hive metastore.</p> <p>Here\u2019s what the file should look like:</p> <pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.uris&lt;/name&gt;\n    &lt;value&gt;thrift://metastore:9083&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;\n    &lt;value&gt;org.postgresql.Driver&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;\n    &lt;value&gt;jdbc:postgresql://postgres:5432/metastore_db&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;\n    &lt;value&gt;hive&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;\n    &lt;value&gt;password&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Copy this file to your Spark container using:</p> <pre><code>docker cp /path/to/hive-site.xml spark-container:/opt/spark/conf/hive-site.xml\n</code></pre> <ol> <li>Add PostgreSQL Driver to Spark:</li> </ol> <p>Since Hive uses PostgreSQL, you need to add the PostgreSQL driver to the Spark container so that Spark can connect to the Hive metastore.</p> <p>Copy the PostgreSQL driver (<code>postgresql-42.5.1.jar</code>) into the Spark container:</p> <pre><code>docker cp /path/to/postgresql-42.5.1.jar spark-container:/opt/spark/jars/\n</code></pre> <ol> <li>Start PySpark with Hive Support:</li> </ol> <p>Now, start the PySpark session from the Spark container with Hive support enabled:</p> <pre><code>docker exec -it spark-container /bin/bash\npyspark --conf spark.sql.catalogImplementation=hive\n</code></pre> <ol> <li>Create and Query Tables Using PySpark:</li> </ol> <p>Inside the PySpark shell, you can create tables and query them. Here\u2019s how:</p> <ul> <li> <p>Create a table:</p> <pre><code>spark.sql(\"CREATE TABLE IF NOT EXISTS sample_table (id INT, name STRING)\")\n</code></pre> </li> <li> <p>Insert data into the table:</p> <pre><code>spark.sql(\"INSERT INTO sample_table VALUES (1, 'Alice'), (2, 'Bob')\")\n</code></pre> </li> <li> <p>Query the table:</p> <pre><code>result = spark.sql(\"SELECT * FROM sample_table\")\nresult.show()\n</code></pre> </li> <li> <p>Verify the Table in Hive:</p> </li> </ul> <p>After creating the table using PySpark, you can check if the table exists in Hive using Beeline:</p> <pre><code>docker exec -it hiveserver2 /bin/bash\nbeeline -u 'jdbc:hive2://hiveserver2:10000/'\n</code></pre> <p>Run this query in Beeline to verify:</p> <pre><code>SHOW TABLES;\nSELECT * FROM sample_table;\n</code></pre> <p>\u00a9 2024 Das. All Rights Reserved.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.9.1_Hive_Concepts.html","title":"Where to Store Hive Warehouse Data in a Distributed System","text":"<p>When we are working with Hive in a distributed system, one important thing is where the data should be stored. By \"data,\" we mean the actual table data that you create using Hive. This is different from the metadata, which is stored in the Hive Metastore.</p> <p>Let\u2019s discuss the different options for storing Hive's warehouse data in a distributed system and how it works when you have multiple services like Spark, Hive, and Hadoop running in separate containers or machines.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.9.1_Hive_Concepts.html#understanding-the-problem","title":"Understanding the Problem","text":"<p>In a multi-container or multi-node system (like when you are running Spark, Hive, Hadoop, etc., on separate machines or containers), you need to make sure that all nodes can access the same data. If each node stores its data locally (e.g., <code>/opt/hive/data/warehouse</code> on each machine), the data won\u2019t be shared across all the nodes, which creates problems. Each machine will have its own local data, and other machines won't be able to see or access it.</p> <p>To avoid this, we need to store the data in a shared location that all nodes can access.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.9.1_Hive_Concepts.html#1-use-hdfs","title":"1. Use HDFS","text":"<p>The most common approach is to use HDFS. HDFS is designed to store data in a distributed way, so all nodes in the cluster can read and write data to the same HDFS location.</p> <p>For example, you would set up Hive\u2019s warehouse directory like this in the <code>hive-site.xml</code> file:</p> <pre><code>&lt;property&gt;\n  &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;\n  &lt;value&gt;hdfs://namenode:8020/user/hive/warehouse&lt;/value&gt;\n  &lt;description&gt;This is the location where Hive stores table data in HDFS.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre> <p>Here, <code>hdfs://namenode:8020</code> is the HDFS path where the actual data is stored. This way, all nodes (Spark, Hive, Hadoop, etc.) can access the same data because they are all connected to HDFS.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.9.1_Hive_Concepts.html#how-it-works-in-a-distributed-system","title":"How it works in a distributed system:","text":"<ul> <li>All your containers or nodes (e.g., Spark, Hive, Hadoop) are configured to use HDFS.</li> <li>When you create a table in Hive, the data is stored in the <code>hdfs://namenode:8020/user/hive/warehouse</code> folder.</li> <li>Whether you're using Spark to query the data or Hive to manage it, everyone accesses the same location in HDFS, which avoids data inconsistency.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.9.1_Hive_Concepts.html#2-use-cloud-storage-amazon-s3-adls-etc","title":"2. Use Cloud Storage (Amazon S3, ADLS etc)","text":"<p>If you\u2019re running Hive in a cloud environment, you can also use cloud storage services like Amazon S3 or ADLS. These services act like distributed storage systems where all nodes can access the same data, just like HDFS.</p> <p>For example, if you are using Amazon S3, you can set the warehouse directory in <code>hive-site.xml</code> like this:</p> <pre><code>&lt;property&gt;\n  &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;\n  &lt;value&gt;s3a://my-bucket/hive/warehouse&lt;/value&gt;\n  &lt;description&gt;This is the location where Hive stores table data in Amazon S3.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.9.1_Hive_Concepts.html#how-it-works-in-a-distributed-system_1","title":"How it works in a distributed system:","text":"<ul> <li>All your containers (Spark, Hive, Hadoop) are configured to access S3.</li> <li>When you create a table in Hive, the actual data is stored in the <code>s3a://my-bucket/hive/warehouse</code> directory.</li> <li>Whether you are using Spark, Hive, or any other service, everyone accesses the same location in S3.</li> </ul> <p>This works well if you are running your system in the cloud, where distributed storage solutions like S3 or Azure Blob Storage are available.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.9.1_Hive_Concepts.html#3-use-nfs-network-file-system-or-shared-filesystems","title":"3. Use NFS (Network File System) or Shared Filesystems","text":"<p>If you don\u2019t want to use HDFS or cloud storage, you can set up a Network File System (NFS) or another type of shared filesystem that all nodes can access.</p> <p>For example, you might set up an NFS mount like <code>/mnt/nfs/hive/warehouse</code>, which is shared across all your servers or containers. In this case, the <code>hive-site.xml</code> configuration would look like this:</p> <pre><code>&lt;property&gt;\n  &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;\n  &lt;value&gt;/mnt/nfs/hive/warehouse&lt;/value&gt;\n  &lt;description&gt;This is the location where Hive stores table data in the shared NFS mount.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.9.1_Hive_Concepts.html#how-it-works-in-a-distributed-system_2","title":"How it works in a distributed system:","text":"<ul> <li>All your containers or servers have the same NFS mount point (e.g., <code>/mnt/nfs/hive/warehouse</code>).</li> <li>When you create a table in Hive, the actual data is stored in the shared directory.</li> <li>All nodes (Spark, Hive, Hadoop, etc.) can read and write data to the same location.</li> </ul> <p>This works for on-premises setups where a shared network drive is used to store data.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.9.1_Hive_Concepts.html#4-avoid-using-local-file-system","title":"4. Avoid Using Local File System","text":"<p>In a distributed system, don\u2019t use the local file system (like <code>/opt/hive/data/warehouse</code>). If you store data on the local file system, each node will have its own copy of the data, and this can lead to problems because:</p> <ul> <li>Data will be spread across different machines, and there will be no consistency.</li> <li>One machine won\u2019t know about the data on another machine, making it impossible to run distributed queries properly.</li> </ul> <p>This option is only fine for single-node setups or local testing, where everything is running on one machine.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.9.2_Hadoop_Concepts.html","title":"Hadoop Modes of Operation","text":"<p>Hadoop can work in three different modes, each suited for different environments and use cases. Let's look at them briefly:</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.9.2_Hadoop_Concepts.html#1-standalone-mode","title":"1. Standalone Mode","text":"<p>This is the simplest mode of Hadoop. In Standalone Mode, Hadoop runs on a single machine and doesn't use HDFS (Hadoop Distributed File System). It's mostly used for testing and learning because there's no real distribution of data or processing.</p> <ul> <li>No HDFS: Files are stored locally.</li> <li>No distributed processing: Everything happens on one machine.</li> <li>Use Case: Good for testing code or doing small experiments.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.9.2_Hadoop_Concepts.html#2-pseudo-distributed-mode","title":"2. Pseudo-distributed Mode","text":"<p>In Pseudo-distributed Mode, Hadoop runs on a single machine, but it behaves as if it\u2019s a distributed system. Each Hadoop service like NameNode, DataNode, and ResourceManager runs as a separate process on the same machine.</p> <ul> <li>HDFS is used: Data is distributed across simulated nodes (which are really just separate processes on the same machine).</li> <li>Single machine but acts distributed.</li> <li>Use Case: For developers who want to test on a single machine but still mimic how Hadoop behaves in a real environment.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.9.2_Hadoop_Concepts.html#3-fully-distributed-mode","title":"3. Fully-Distributed Mode","text":"<p>In Fully-Distributed Mode, Hadoop runs across multiple machines. Each service (like NameNode, DataNode, etc.) runs on different machines, and data is truly distributed.</p> <ul> <li>Real distribution: Data is spread across many machines.</li> <li>Use Case: This is used in production environments where Hadoop processes large datasets across a cluster of machines.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.9.2_Hadoop_Concepts.html#key-takeaway","title":"Key Takeaway","text":"<ul> <li>Standalone: All on one machine, no HDFS.</li> <li>Pseudo-distributed: Simulates distribution on one machine.</li> <li>Fully-distributed: Runs across multiple machines, real-world usage.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/4.9.2_Hadoop_Concepts.html#every-hadoop-architecture-soul-is-fsimage","title":"Every Hadoop architecture soul is fsimage","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/5_Jupyter_AllSparkNotebook.html","title":"Jupyter All-Spark Notebook: Complete Guide","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/5_Jupyter_AllSparkNotebook.html#what-is-jupyter-all-spark-notebook","title":"What is Jupyter All-Spark Notebook?","text":"<p>Jupyter All-Spark Notebook is a pre-configured Docker image maintained by the Jupyter Project. It bundles Jupyter Lab, Python, and Apache Spark into a single, ready-to-run container\u2014eliminating manual setup and dependency conflicts.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/5_Jupyter_AllSparkNotebook.html#who-offers-it-where-to-find-it","title":"Who Offers It &amp; Where to Find It","text":"<ul> <li>Maintained by: Jupyter Project (open source)</li> <li>Available on: Docker Hub and Quay.io</li> <li>Official Images: <code>jupyter/all-spark-notebook</code> (Docker Hub)</li> <li>Version History: Available on Docker Hub with tags like <code>latest</code>, <code>ubuntu-22.04</code>, <code>x86_64-ubuntu-22.04</code>, etc.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/5_Jupyter_AllSparkNotebook.html#whats-inside-key-features","title":"What's Inside &amp; Key Features","text":"Included ComponentsWhat You GetLimitations <ul> <li>Jupyter Lab: Modern web-based notebook interface</li> <li>Python 3: With pandas, numpy, matplotlib, scikit-learn pre-installed</li> <li>Apache Spark: Full Spark distribution (Scala, PySpark, Spark SQL)</li> <li>R &amp; Julia (optional): If using extended images</li> <li>Development Tools: Git, curl, wget, and other utilities</li> </ul> <p>\u2713 Interactive notebook environment for immediate coding \u2713 Single-node Spark processing (ideal for prototyping) \u2713 Works locally on Windows, Mac, or Linux \u2713 Persistent volume for saving notebooks \u2713 No manual Spark or Java configuration needed</p> <p>\u2717 Single-node processing only (not distributed) \u2717 Limited to your machine's RAM and CPU \u2717 No fault tolerance or high availability \u2717 Not suitable for production workloads</p> <p>This image is perfect for learning Spark, prototyping data pipelines, and local development without complex setup.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/5_Jupyter_AllSparkNotebook.html#alternatives","title":"Alternatives","text":"Bitnami SparkApache Spark (Official)Databricks Community EditionJupyter All-Spark (This Guide) <p>Focus: Lightweight, minimal Spark environment Pros: Smaller image size, basic Spark setup Cons: No Jupyter integration, requires separate notebook setup Use case: If you only need Spark without notebook interface</p> <p>Focus: Bare Spark distribution Pros: Official releases, full control Cons: Manual setup required, no notebook interface Use case: Production deployments, cluster setups</p> <p>Focus: Cloud-based Spark with notebooks Pros: Full notebook experience, collaborative, cloud-native Cons: Requires internet, limited free tier Use case: Team collaboration, cloud workflows</p> <p>Focus: Local, all-in-one notebook + Spark Pros: Zero setup, works offline, great for learning Cons: Single-node only Use case: Learning, prototyping, local development</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/5_Jupyter_AllSparkNotebook.html#quick-deploy-latest-version","title":"Quick Deploy: Latest Version","text":"<p>Use these scripts to get Jupyter All-Spark running with the latest image in seconds.</p> WindowsmacOS <p>Save this as <code>start-jupyter.bat</code>. Double-click or run from Command Prompt. Browser opens to <code>http://localhost:8888</code>:</p> start-jupyter.bat <pre><code>@echo off\nsetlocal enabledelayedexpansion\n\nset CONTAINER_NAME=jupyter-allspark\nset IMAGE_NAME=jupyter/all-spark-notebook\nset PORT=8888\nset NOTEBOOK_DIR=%USERPROFILE%\\jupyter-notebooks\n\nif not exist \"!NOTEBOOK_DIR!\" mkdir \"!NOTEBOOK_DIR!\"\n\ndocker ps | find \"!CONTAINER_NAME!\" &gt;nul\nif !errorlevel! equ 0 (\n    echo Container already running at http://localhost:!PORT!\n    start http://localhost:!PORT!\n    exit /b 0\n)\n\ndocker ps -a | find \"!CONTAINER_NAME!\" &gt;nul\nif !errorlevel! equ 0 (\n    docker start !CONTAINER_NAME!\n) else (\n    echo Pulling latest image and starting container...\n    docker run -d ^\n        --name !CONTAINER_NAME! ^\n        -p !PORT!:8888 ^\n        -v \"!NOTEBOOK_DIR!\":/home/jovyan/work ^\n        -e JUPYTER_ENABLE_LAB=yes ^\n        !IMAGE_NAME!\n)\n\ntimeout /t 5 /nobreak\nstart http://localhost:!PORT!\necho Get your token: docker logs !CONTAINER_NAME!\n</code></pre> <p>Save this as <code>start-jupyter.sh</code>. Run: <code>chmod +x start-jupyter.sh &amp;&amp; ./start-jupyter.sh</code>. Browser opens to <code>http://localhost:8888</code>:</p> start-jupyter.sh <pre><code>#!/bin/bash\n\nCONTAINER_NAME=\"jupyter-allspark\"\nIMAGE_NAME=\"jupyter/all-spark-notebook\"\nPORT=\"8888\"\nNOTEBOOK_DIR=\"$HOME/jupyter-notebooks\"\n\nmkdir -p \"$NOTEBOOK_DIR\"\n\nif docker ps | grep -q \"$CONTAINER_NAME\"; then\n    echo \"Container already running at http://localhost:$PORT\"\n    open \"http://localhost:$PORT\"\n    exit 0\nfi\n\nif docker ps -a | grep -q \"$CONTAINER_NAME\"; then\n    echo \"Restarting existing container...\"\n    docker start \"$CONTAINER_NAME\"\nelse\n    echo \"Pulling latest image and starting container...\"\n    docker run -d \\\n        --name \"$CONTAINER_NAME\" \\\n        -p \"$PORT:8888\" \\\n        -v \"$NOTEBOOK_DIR\":/home/jovyan/work \\\n        -e JUPYTER_ENABLE_LAB=yes \\\n        \"$IMAGE_NAME\"\nfi\n\nsleep 5\nopen \"http://localhost:$PORT\"\necho \"Get your token: docker logs $CONTAINER_NAME\"\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/5_Jupyter_AllSparkNotebook.html#advanced-custom-network-specific-version","title":"Advanced: Custom Network + Specific Version","text":"<p>Need to connect Jupyter to a custom Docker network (e.g., <code>dasnet</code>) and use a specific version? Use these scripts.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/5_Jupyter_AllSparkNotebook.html#finding-available-versions","title":"Finding Available Versions","text":"<p>Visit the Docker Hub page and look for tags like: - <code>latest</code> \u2014 Current stable release - <code>ubuntu-22.04</code> \u2014 Latest on Ubuntu 22.04 - <code>x86_64-ubuntu-22.04</code> \u2014 Specific architecture + OS combo - <code>&lt;date&gt;</code> \u2014 Historical releases (e.g., <code>2024-01-15</code>)</p> <p>Example tag: <code>jupyter/all-spark-notebook:x86_64-ubuntu-22.04</code></p> WindowsmacOS <p>Save this as <code>start-jupyter-custom.bat</code>. Edit <code>NETWORK_NAME</code> and <code>IMAGE_VERSION</code> as needed. Double-click or run from Command Prompt:</p> start-jupyter-custom.bat <pre><code>@echo off\nsetlocal enabledelayedexpansion\n\nREM Customize these variables\nset NETWORK_NAME=dasnet\nset IMAGE_VERSION=x86_64-ubuntu-22.04\nset CONTAINER_NAME=jupyter-spark\n\necho Starting Jupyter All-Spark on !NETWORK_NAME! network...\n\nREM Create network if it doesn't exist\ndocker network inspect !NETWORK_NAME! &gt;nul 2&gt;&amp;1\nif !errorlevel! neq 0 (\n    echo Creating !NETWORK_NAME! network...\n    docker network create !NETWORK_NAME!\n) else (\n    echo !NETWORK_NAME! network already exists\n)\n\nREM Clean up old container\ndocker rm -f !CONTAINER_NAME! 2&gt;nul\n\nREM Start container\necho Running container with version !IMAGE_VERSION!...\ndocker run -it ^\n  --name !CONTAINER_NAME! ^\n  --network !NETWORK_NAME! ^\n  -p 8888:8888 ^\n  jupyter/all-spark-notebook:!IMAGE_VERSION!\n\npause\n</code></pre> <p>Save this as <code>start-jupyter-custom.sh</code>. Edit <code>NETWORK_NAME</code> and <code>IMAGE_VERSION</code> as needed. Run: <code>chmod +x start-jupyter-custom.sh &amp;&amp; ./start-jupyter-custom.sh</code>:</p> start-jupyter-custom.sh <pre><code>#!/bin/bash\n\n# Customize these variables\nNETWORK_NAME=\"dasnet\"\nIMAGE_VERSION=\"x86_64-ubuntu-22.04\"\nCONTAINER_NAME=\"jupyter-spark\"\n\necho \"Starting Jupyter All-Spark on $NETWORK_NAME network...\"\n\n# Create network if it doesn't exist\nif ! docker network inspect \"$NETWORK_NAME\" &amp;&gt; /dev/null; then\n    echo \"Creating $NETWORK_NAME network...\"\n    docker network create \"$NETWORK_NAME\"\nelse\n    echo \"$NETWORK_NAME network already exists\"\nfi\n\n# Clean up old container\ndocker rm -f \"$CONTAINER_NAME\" 2&gt;/dev/null\n\n# Start container\necho \"Running container with version $IMAGE_VERSION...\"\ndocker run -it \\\n  --name \"$CONTAINER_NAME\" \\\n  --network \"$NETWORK_NAME\" \\\n  -p 8888:8888 \\\n  \"jupyter/all-spark-notebook:$IMAGE_VERSION\"\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/5_Jupyter_AllSparkNotebook.html#access-your-notebook","title":"Access Your Notebook","text":"<ol> <li>Navigate to <code>http://localhost:8888</code> in your browser</li> <li>On first access, you'll need an authentication token</li> <li>Get your token from terminal output or run:</li> </ol> <pre><code>docker logs jupyter-allspark\n# or if using custom script:\ndocker logs jupyter-spark\n</code></pre> <ol> <li>Look for a URL like: <code>http://127.0.0.1:8888/lab?token=abc123...</code></li> <li>Copy the token and paste into the login page</li> </ol>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/5_Jupyter_AllSparkNotebook.html#use-jupyter-all-spark-notebook-in-vs-code","title":"Use Jupyter All-Spark Notebook in VS Code","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/5_Jupyter_AllSparkNotebook.html#prerequisites","title":"Prerequisites","text":"<ul> <li>VS Code installed with the Jupyter extension</li> <li>Python extension for VS Code</li> <li>Jupyter container running (from Quick Deploy or Advanced section above)</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/5_Jupyter_AllSparkNotebook.html#connect-to-remote-jupyter-kernel","title":"Connect to Remote Jupyter Kernel","text":"<ol> <li>Open Command Palette in VS Code (<code>Ctrl+Shift+P</code> / <code>Cmd+Shift+P</code>)</li> <li>Search for and select Jupyter: Specify local or remote Jupyter server for connections</li> <li>Choose Existing URI and enter: <code>http://localhost:8888</code></li> <li>Paste your authentication token when prompted</li> <li>Click Select another kernel \u2192 Jupyter Kernels \u2192 Choose the running kernel</li> </ol>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/5_Jupyter_AllSparkNotebook.html#create-run-notebooks","title":"Create &amp; Run Notebooks","text":"<ol> <li>Create a new file with <code>.ipynb</code> extension or open an existing notebook</li> <li>Select the remote Jupyter kernel from the kernel picker (top-right)</li> <li>Write and execute cells as normal\u2014all processing happens in your Spark container</li> </ol>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/5_Jupyter_AllSparkNotebook.html#key-benefits","title":"Key Benefits","text":"<p>\u2713 Full IDE features (IntelliSense, debugging, extensions) \u2713 Seamless Spark integration with container isolation \u2713 Work with large datasets without local resource drain \u2713 Version control notebooks alongside your code  </p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/5_Jupyter_AllSparkNotebook.html#troubleshooting","title":"Troubleshooting","text":"<p>If connection fails: - Verify container is running: <code>docker ps</code> - Check logs for token: <code>docker logs jupyter-allspark</code> - Ensure port 8888 is accessible: <code>http://localhost:8888</code> - Try restarting the kernel from VS Code's kernel picker</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/5_Jupyter_AllSparkNotebook.html#create-your-first-notebook","title":"Create Your First Notebook","text":"<ol> <li>In Jupyter Lab, click File \u2192 New \u2192 Notebook</li> <li>Select Python 3 as kernel</li> <li>In the first cell, paste and run:</li> </ol> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"TestApp\").getOrCreate()\nprint(f\"Spark {spark.version} is running!\")\n\n# Simple example: create a dataset\ndata = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")]\ndf = spark.createDataFrame(data, [\"id\", \"name\"])\ndf.show()\n</code></pre> <p>Press Shift + Enter to execute.</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/5_Jupyter_AllSparkNotebook.html#stop-the-container","title":"Stop the Container","text":"<p>When done:</p> <pre><code>docker stop jupyter-allspark\n# or if using custom script:\ndocker stop jupyter-spark\n</code></pre> <p>Your notebooks persist in <code>~/jupyter-notebooks</code> (macOS) or <code>C:\\Users\\YourUsername\\jupyter-notebooks</code> (Windows).</p>"},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/5_Jupyter_AllSparkNotebook.html#architecture-overview","title":"Architecture Overview","text":""},{"location":"DevOps/Docker/ContainerStacks/BigDataStack/5_Jupyter_AllSparkNotebook.html#single-node-vs-multi-node-architecture","title":"Single-Node vs. Multi-Node Architecture","text":"<p>Jupyter All-Spark (This Guide) runs Spark in standalone mode on a single machine: - Driver and executors run in one container - All processing uses local machine resources (RAM, CPU) - No network communication between nodes - Perfect for learning and prototyping</p> <p>Real-World Multi-Node Spark Clusters distribute work across multiple machines: - Driver node: Receives tasks and coordinates job execution - Worker nodes: Execute tasks in parallel across different machines - Cluster Manager (YARN, Kubernetes, Mesos): Allocates resources and schedules work - Network layer: Nodes communicate via TCP/IP, shuffle data across network - Fault tolerance: If a node fails, work redistributes to other nodes - Scalability: Add more nodes to handle larger datasets and workloads</p> <p>Key Differences:</p> Aspect Single-Node (This Guide) Multi-Node Cluster Processing Sequential, limited by one machine Parallel across many machines Data Fits in available RAM Distributed partitions across nodes Fault Tolerance None; node failure = loss Built-in; automatic redistribution Network I/O None Significant data shuffling Setup Docker container, instant Complex infrastructure (Kubernetes, cloud, etc.) Use Case Learning, prototyping Production, big data analytics <p></p>"},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html","title":"Confluent Kafka with KRaft on Docker","text":"<p> Here, I will show you how to setup a Kafka setup on Docker. We will use KRaft rather than Zookeper. We will use a docker-compose.yaml file and setup Kafka broker, Schema Registry, Kafka Connect, Control Center, ksqlDB, and a REST Proxy. I have tested the installation in both Windows and Mac machines with M1 chip. </p>"},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html#step-1-download-the-docker-compose-file","title":"Step 1: Download the Docker Compose file","text":"<p>The first step is to download the docker-compose.yaml file, KRaft version from confluent's github site</p> <p>Note: When choosing between KRaft and ZooKeeper as the metadata service for your Apache Kafka cluster, KRaft is the recommended option.</p> <p></p>"},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html#step-2-run-the-container","title":"Step 2: Run the container","text":"<ul> <li> <p>Open command prompt/terminal and CD to the folder containing the <code>docker-compose.yml</code></p> </li> <li> <p>Run the following command to start all services:<code>docker-compose up -d</code> </p> </li> <li> <p>The Docker Compose will start all the necessary services in the background. Once finished, go to the docker desktop window and see the services</p> </li> </ul> <p></p> <ul> <li>You can access the Control Center at http://localhost:9021 once the container is operational.</li> <li>To create topics and proceed further you can refer to the  this quickstart guide.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html#appendix","title":"Appendix","text":""},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html#connecting-to-kafka-containers","title":"Connecting to Kafka Containers","text":"<p>The container group created using the docker-compose become part of  <code>confluent-kafka_default</code> network group, restricting access from external containers or local machines not in this network. This means, you won't be able to connect to the broker from outside containers or local machine. To connect to Kafka from an external container, add it to the <code>confluent-kafka_default</code> network:</p> <pre><code>docker network connect confluent-kafka_default [external-container-name-or-id]\n</code></pre> <p>After adding, connect to the Kafka broker at <code>broker:29092</code>. Example for Spark:</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"Streaming from Kafka\") \\\n    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') \\\n    .getOrCreate()\n\nstreaming_df = spark.readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"broker:29092) \\\n    .option(\"subscribe\", \"sometopic\") \\\n    .option(\"startingOffsets\", \"earliest\") \\\n    .load()\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html#troubleshooting-broker-connection-issues","title":"Troubleshooting Broker Connection Issues","text":"<p>Inspect the Kafka broker container using <code>docker inspect [broker-container-id]</code>. For detailed network information:</p> <pre><code>docker network inspect [network-name]\n</code></pre> <p>To find the broker's IP address:</p> <pre><code>docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' [container-name-or-id]\n</code></pre> <p>Test connectivity to the broker:</p> <pre><code>nc -vz -w 5 [broker-ip] [listener-port]\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html#error-no-matching-manifest-for-linuxarm64v8","title":"Error: no matching manifest for linux/arm64/v8","text":"<p>We might run into an error like no matching manifest for linux/arm65/v8 this error error indicates that the Docker images specified in the <code>docker-compose.yml</code> file do not have a version compatible with the architecture of our Mac's processor. This error is less likely if you use the KRaft version. I have tested the KRaft versin on both Windows and Mac M1, they both showed no error in architecture or compatibility.</p>"},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html#resolving-port-conflicts-for-kafka-rest-proxy-in-docker","title":"Resolving Port Conflicts for Kafka Rest Proxy in Docker","text":"<p>When deploying Kafka Rest Proxy using Docker Compose, port conflicts are a common issue that can prevent the service from starting successfully. </p> <p></p> <p>Here is a typical example of this error:</p> <pre><code>Cannot start Docker Compose application. Reason: compose [start] exit status 1. Container broker Starting Container broker Started Container schema-registry Starting Container schema-registry Started Container rest-proxy Starting Container connect Starting Container connect Started Error response from daemon: Ports are not available: exposing port TCP 0.0.0.0:8082 -&gt; 0.0.0.0:0: listen tcp 0.0.0.0:8082: bind: An attempt was made to access a socket in a way forbidden by its access permissions.\n</code></pre> <p>Long story short: Use this  modified docker-compose.yml which uses port 8086 instead of 8082</p> <p>Detailed investigation:</p> <ul> <li> <p>Identify Port Usage: Use <code>netstat</code> or <code>lsof</code> to check if the intended port is already in use.</p> </li> <li> <p>For Windows: <code>netstat -ano | findstr :&lt;PORT&gt;</code></p> </li> <li> <p>For Linux/Mac: <code>sudo lsof -i :&lt;PORT&gt;</code> or <code>sudo netstat -tulnp | grep :&lt;PORT&gt;</code></p> <p></p> </li> <li> <p>Modify Docker Compose File: If the port is in use, select an unused port and update the <code>docker-compose.yml</code> file. Change the host port mapping for the <code>rest-proxy</code> service:</p> </li> </ul> <pre><code>ports:\n  - &lt;UNUSED_PORT&gt;:8082 # Change &lt;UNUSED_PORT&gt; to an available port on your host\n</code></pre> <p>e.g:</p> <pre><code>![Alt text](images/image-5.png)\n</code></pre> <ul> <li>Restart Docker Compose: Apply the changes by running:</li> </ul> <pre><code>docker-compose down\ndocker-compose up -d\n</code></pre> <ul> <li>Update Applications: Ensure all applications that interact with Kafka Rest Proxy are updated to use the new port.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html#about-the-docker-composeyml","title":"About the docker-compose.yml","text":"<p>Here is an explanation of the differnet services in the docker-compose.yml</p> <ul> <li>broker: A Kafka broker service with custom environment configurations for topics, listeners, ports, IDs, etc.</li> <li>schema-registry: This service provides a serving layer for your metadata. It is configured to communicate with the Kafka broker and expose port 8081.</li> <li>connect: Kafka Connect with dependencies on the broker and schema-registry, configured to run connectors.</li> <li>control-center: Confluent's web-based tool for managing and monitoring the Kafka ecosystem.</li> <li>ksqldb-server: The server for ksqlDB, streaming SQL engine that enables real-time data processing against Apache Kafka.</li> <li>ksqldb-cli: A command-line interface for interacting with ksqlDB Server.</li> <li>ksql-datagen: A tool for generating sample data for Kafka topics and provides a source of continuously flowing data.</li> <li>rest-proxy: Provides a RESTful interface to Kafka clusters.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html#fully-commented-docker-composeyml","title":"Fully commented docker-compose.yml","text":"<p>If you want to know each step of the docker compose file. I have placed a fully commented docker-compose.yml</p>"},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html#conclusion","title":"Conclusion","text":"<p>You now have a fully functional local Kafka development environment that includes a broker, Schema Registry, Kafka Connect, Control Center, ksqlDB, and a REST Proxy. The KRaft version of the docker compose has been tested in both WIndows and Mac M1 machines.</p>"},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html#further-reading","title":"Further reading","text":"<p>Confluent Documentation. Quick Start. Docker Container</p> <p>Confluent Documentation. Quick Start using CLI</p> <p></p>"},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html#kafka-6-with-zookeeper-on-windows-docker","title":"Kafka 6 with Zookeeper on Windows Docker","text":"<p>In this article, I will show you how to set up Confluent Kafka on Docker in Windows. We will be using a total of 9 containers, and this setup is very stable. You will only need one <code>docker-compose.yml</code> file, and there is no need for a Dockerfile. The setup will install the following Confluent Kafka components:</p> <ul> <li>confluentinc/cp-zookeeper:6.0.1</li> <li>confluentinc/cp-server:6.0.1</li> <li>confluentinc/cp-schema-registry:6.0.1</li> <li>confluentinc/cp-kafka-connect-base:6.0.1</li> <li>confluentinc/cp-enterprise-control-center:6.0.1</li> <li>confluentinc/cp-ksqldb-server:6.0.1</li> <li>confluentinc/cp-ksqldb-cli:6.0.1</li> <li>confluentinc/ksqldb-examples:6.0.1</li> <li>confluentinc/cp-kafka-rest:6.0.1</li> </ul> <p>For the Busy Professionals</p> <ol> <li>Download and unzip the Docker Compose file to a folder on your machine.</li> <li>Open Command Prompt and navigate to the folder by running <code>cd [folder_name]</code>.</li> <li>Execute <code>docker-compose up -d</code> to start the services.</li> </ol> <p>That's it! You will have 9 containers up and running, ready to serve. See the example image below for reference.</p> <p></p> <p></p>"},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html#overview","title":"Overview","text":"<p>Many people struggle with setting up Confluent Kafka on their local machines. While it's easier to use 'templates' on AWS or Azure, setting it up locally can be quite complicated. That\u2019s why I\u2019m sharing this method\u2014it helps you create a stable, fully-working, production-like environment on your local system.</p>"},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html#prerequisites","title":"Prerequisites","text":"<p>Before you start, ensure that you have the following installed: - Docker Desktop: Install it from the Docker website. Docker Compose: It usually comes with Docker Desktop, so you don\u2019t need to install it separately. - Run the command <code>docker network create dasnet</code> in CMD. </p> <p>   Note: I use a network so that all my containers are part of the same network. This is just an extra step. It\u2019s necessary because the Docker Compose file has the dasnet network mentioned everywhere. If you don\u2019t want to use this network, then just remove all occurrences of it from the Docker Compose file.   </p>"},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html#steps-to-create-the-setup","title":"Steps to create the setup","text":"<p>The steps are simple. Just follow these two steps and your setup will be up and running.</p>"},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html#1-create-the-docker-composeyaml","title":"1. Create the <code>docker-compose.yaml</code>","text":"<p>Create a <code>docker-compose.yaml</code> file with the content below inside any folder. Alternatively download the file from the link given at the start of the article.</p> <p>I have verbosely commented the file. Hope you will find it useful.</p> <pre><code># Docker Compose file for setting up a Confluent Platform environment with various services.\n# Author: DDas\n\nversion: '2'  # Specifies the Docker Compose version. Version 2 is stable and compatible with most setups.\n\nservices:  # This section defines all the services (containers) that will be created as part of this setup.\n\n  zookeeper:  # Zookeeper service - essential for managing and coordinating Kafka brokers.\n    image: confluentinc/cp-zookeeper:6.0.1  # Zookeeper version 6.0.1 from Confluent.\n    hostname: zookeeper  # Internal hostname used within the Docker network.\n    container_name: zookeeper  # Name of the container as it will appear in Docker.\n    ports:\n      - \"2181:2181\"  # laptop:container\n    environment:  # Environment variables that configure Zookeeper.\n      ZOOKEEPER_CLIENT_PORT: 2181  # Port where Zookeeper listens for client connections.\n      ZOOKEEPER_TICK_TIME: 2000  # Basic time unit in milliseconds used by Zookeeper.\n      ZOOKEEPER_SERVER_ID: 1\n      ZOOKEEPER_INIT_LIMIT: 5\n      ZOOKEEPER_SYNC_LIMIT: 2\n      ZOOKEEPER_CLIENT_CNXNS: 60\n    networks:\n      - dasnet  # Connecting Zookeeper to the custom network 'dasnet'.\n\n  broker:  # Kafka broker service - handles message publishing and subscribing.\n    image: confluentinc/cp-server:6.0.1  # Kafka broker version 6.0.1 from Confluent.\n    hostname: broker  # Internal hostname used within the Docker network.\n    container_name: broker  # Name of the container as it will appear in Docker.\n    depends_on:  # Ensures Zookeeper is started before the broker.\n      - zookeeper\n    ports:\n      - \"9192:9092\"  # laptop:container\n      - \"9111:9101\"  # laptop:container\n    environment:  # Environment variables that configure the Kafka broker.\n      KAFKA_BROKER_ID: 1  # Unique identifier for the broker.\n      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'  # Address of the Zookeeper service.\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT  # Protocol mapping.\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092  # Advertised listeners.\n      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter  # Metrics reporter.\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1  # Replication factor for the offsets topic.\n      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0  # Delay in rebalancing consumer groups.\n      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1  # Replication factor for the license topic.\n      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1  # Replication factor for the balancer topic.\n      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1  # Minimum in-sync replicas for transaction state log.\n      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1  # Replication factor for the transaction state log.\n      KAFKA_JMX_PORT: 9101  # Port for JMX monitoring.\n      KAFKA_JMX_HOSTNAME: localhost  # Hostname for JMX monitoring.\n      KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:8081  # Schema Registry URL.\n      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:29092  # Bootstrap servers for metrics reporter.\n      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1  # Replication factor for metrics topics.\n      CONFLUENT_METRICS_ENABLE: 'true'  # Enable Confluent metrics.\n      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'  # Customer ID for support.\n      KAFKA_SASL_JAAS_CONFIG: \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"admin\\\" password=\\\"Passw0rd\\\";\"\n\n    networks:\n      - dasnet  # Connecting the Kafka broker to the custom network 'dasnet'.\n\n  schema-registry:  # Schema Registry service - manages Avro schemas for Kafka topics.\n    image: confluentinc/cp-schema-registry:6.0.1  # Schema Registry version 6.0.1 from Confluent.\n    hostname: schema-registry  # Internal hostname used within the Docker network.\n    container_name: schema-registry  # Name of the container as it will appear in Docker.\n    depends_on:  # Ensures the broker is started before the Schema Registry.\n      - broker\n    ports:\n      - \"8181:8081\"  # laptop:container\n    environment:  # Environment variables that configure the Schema Registry.\n      SCHEMA_REGISTRY_HOST_NAME: schema-registry  # Internal hostname for Schema Registry.\n      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker:29092'  # Kafka broker address for Schema Registry.\n      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081  # Listeners for Schema Registry.\n      SCHEMA_REGISTRY_AUTH: \"admin:Passw0rd\"\n\n    networks:\n      - dasnet  # Connecting the Schema Registry to the custom network 'dasnet'.\n\n  connect:  # Kafka Connect service - facilitates integration with external systems.\n    image: confluentinc/cp-kafka-connect-base:6.0.1  # Kafka Connect version 6.0.1 from Confluent.\n    hostname: connect  # Internal hostname used within the Docker network.\n    container_name: kafka-connect  # Name of the container as it will appear in Docker.\n    depends_on:  # Ensures the broker and Schema Registry are started before Kafka Connect.\n      - broker\n      - schema-registry\n    ports:\n      - \"8183:8083\"  # laptop:container\n    environment:  # Environment variables that configure Kafka Connect.\n      CONNECT_BOOTSTRAP_SERVERS: 'broker:29092'  # Kafka broker address for Kafka Connect.\n      CONNECT_REST_ADVERTISED_HOST_NAME: connect  # Hostname for the Kafka Connect REST API.\n      CONNECT_REST_PORT: 8083  # Port for the Kafka Connect REST API.\n      CONNECT_GROUP_ID: kafka-connect  # Group ID for Kafka Connect.\n      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs  # Topic for storing connector configs.\n      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1  # Replication factor for the config storage topic.\n      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000  # Interval for flushing offsets.\n      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets  # Topic for storing connector offsets.\n      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1  # Replication factor for the offset storage topic.\n      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status  # Topic for storing connector statuses.\n      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1  # Replication factor for the status storage topic.\n      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter  # Converter for Kafka Connect keys.\n      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter  # Converter for Kafka Connect values.\n      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081  # Schema Registry URL.\n      CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-6.0.1.jar  # Additional classpath for interceptors.\n      CONNECT_PRODUCER_INTERCEPTOR_CLASSES: \"io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor\"  # Producer interceptor.\n      CONNECT_CONSUMER_INTERCEPTOR_CLASSES: \"io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor\"  # Consumer interceptor.\n      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR  # Logging configuration.\n      CONNECT_PLUGIN_PATH: \"/usr/share/java,/usr/share/confluent-hub-components,/data/connect-jars\"  # Plugin paths.\n      CONNECT_SASL_JAAS_CONFIG: \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"admin\\\" password=\\\"Passw0rd\\\";\"\n\n    volumes:  # Mounting a volume for persistent storage.\n        - ${PWD}/data:/data\n\n    networks:\n      - dasnet  # Connecting Kafka Connect to the custom network 'dasnet'.\n\n  control-center:  # Confluent Control Center service - web interface for managing and monitoring Kafka.\n    image: confluentinc/cp-enterprise-control-center:6.0.1  # Control Center version 6.0.1 from Confluent.\n    hostname: control-center  # Internal hostname used within the Docker network.\n    container_name: control-center  # Name of the container as it will appear in Docker.\n    depends_on:  # Ensures the broker, Schema Registry, and Kafka Connect are started before Control Center.\n      - broker\n      - schema-registry\n      - connect\n      - ksqldb-server\n    ports:\n      - \"9121:9021\"  # laptop:container\n    environment:  # Environment variables that configure the Control Center.\n      CONTROL_CENTER_BOOTSTRAP_SERVERS: 'broker:29092'  # Kafka broker address for Control Center.\n      CONTROL_CENTER_CONNECT_CLUSTER: 'connect:8083'  # Kafka Connect cluster address for Control Center.\n      CONTROL_CENTER_KSQL_KSQLDB1_URL: \"http://ksqldb-server:8088\"  # KSQL server URL for Control Center.\n      CONTROL_CENTER_KSQL_KSQLDB1_ADVERTISED_URL: \"http://localhost:8088\"  # Advertised KSQL server URL.\n      CONTROL_CENTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"  # Schema Registry URL for Control Center.\n      CONTROL_CENTER_REPLICATION_FACTOR: 1  # Replication factor for Control Center's internal topics.\n      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1  # Number of partitions for Control Center's internal topics.\n      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1  # Partitions for monitoring interceptor topic.\n      CONFLUENT_METRICS_TOPIC_REPLICATION: 1  # Replication factor for metrics topics.\n      PORT: 9021  # Internal port for Control Center.\n\n    networks:\n      - dasnet  # Connecting Control Center to the custom network 'dasnet'.\n\n  ksqldb-server:  # KSQL server service - SQL engine for processing real-time streams.\n    image: confluentinc/cp-ksqldb-server:6.0.1  # KSQL server version 6.0.1 from Confluent.\n    hostname: ksqldb-server  # Internal hostname used within the Docker network.\n    container_name: ksqldb-server  # Name of the container as it will appear in Docker.\n    depends_on:  # Ensures the broker and Kafka Connect are started before KSQL server.\n      - broker\n      - connect\n    ports:\n      - \"8188:8088\"  # laptop:container\n    environment:  # Environment variables that configure KSQL server.\n      KSQL_CONFIG_DIR: \"/etc/ksql\"  # Directory for KSQL configuration files.\n      KSQL_BOOTSTRAP_SERVERS: \"broker:29092\"  # Kafka broker address for KSQL server.\n      KSQL_HOST_NAME: ksqldb-server  # Internal hostname for KSQL server.\n      KSQL_LISTENERS: \"http://0.0.0.0:8088\"  # Listeners for KSQL server.\n      KSQL_CACHE_MAX_BYTES_BUFFERING: 0  # Disables caching in KSQL.\n      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"  # Schema Registry URL for KSQL server.\n      KSQL_PRODUCER_INTERCEPTOR_CLASSES: \"io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor\"  # Producer interceptor.\n      KSQL_CONSUMER_INTERCEPTOR_CLASSES: \"io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor\"  # Consumer interceptor.\n      KSQL_KSQL_CONNECT_URL: \"http://connect:8083\"  # Kafka Connect URL for KSQL server.\n      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_REPLICATION_FACTOR: 1  # Replication factor for logging processing topics.\n      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: 'true'  # Auto-create logging processing topics.\n      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: 'true'  # Auto-create logging processing streams.\n\n    networks:\n      - dasnet  # Connecting KSQL server to the custom network 'dasnet'.\n\n  ksqldb-cli:  # KSQL CLI service - command-line interface for interacting with KSQL server.\n    image: confluentinc/cp-ksqldb-cli:6.0.1  # KSQL CLI version 6.0.1 from Confluent.\n    container_name: ksqldb-cli  # Name of the container as it will appear in Docker.\n    depends_on:  # Ensures the broker, Kafka Connect, and KSQL server are started before KSQL CLI.\n      - broker\n      - connect\n      - ksqldb-server\n    entrypoint: /bin/sh  # Entry point for the container to start a shell.\n    tty: true  # Allocate a pseudo-TTY for interactive use.\n\n    networks:\n      - dasnet  # Connecting KSQL CLI to the custom network 'dasnet'.\n\n  ksql-datagen:  # KSQL DataGen service - generates sample data for KSQL.\n    image: confluentinc/ksqldb-examples:6.0.1  # KSQL DataGen version 6.0.1 from Confluent.\n    hostname: ksql-datagen  # Internal hostname used within the Docker network.\n    container_name: ksql-datagen  # Name of the container as it will appear in Docker.\n    depends_on:  # Ensures KSQL server, broker, Schema Registry, and Kafka Connect are started before KSQL DataGen.\n      - ksqldb-server\n      - broker\n      - schema-registry\n      - connect\n    command: \"bash -c 'echo Waiting for Kafka to be ready... &amp;&amp; \\\n                       cub kafka-ready -b broker:29092 1 40 &amp;&amp; \\\n                       echo Waiting for Confluent Schema Registry to be ready... &amp;&amp; \\\n                       cub sr-ready schema-registry 8081 40 &amp;&amp; \\\n                       echo Waiting a few seconds for topic creation to finish... &amp;&amp; \\\n                       sleep 11 &amp;&amp; \\\n                       tail -f /dev/null'\"  # Command to ensure services are ready before starting data generation.\n    environment:  # Environment variables that configure KSQL DataGen.\n      KSQL_CONFIG_DIR: \"/etc/ksql\"  # Directory for KSQL configuration files.\n      STREAMS_BOOTSTRAP_SERVERS: broker:29092  # Kafka broker address for KSQL DataGen.\n      STREAMS_SCHEMA_REGISTRY_HOST: schema-registry  # Schema Registry host for KSQL DataGen.\n      STREAMS_SCHEMA_REGISTRY_PORT: 8081  # Schema Registry port for KSQL DataGen.\n\n    networks:\n      - dasnet  # Connecting KSQL DataGen to the custom network 'dasnet'.\n\n  rest-proxy:  # Kafka REST Proxy service - provides a RESTful interface to Kafka.\n    image: confluentinc/cp-kafka-rest:6.0.1  # Kafka REST Proxy version 6.0.1 from Confluent.\n    hostname: rest-proxy  # Internal hostname used within the Docker network.\n    container_name: rest-proxy  # Name of the container as it will appear in Docker.\n    depends_on:  # Ensures the broker and Schema Registry are started before REST Proxy.\n      - broker\n      - schema-registry\n    ports:\n      - \"8182:8082\"  # laptop:container\n    environment:  # Environment variables that configure the REST Proxy.\n      KAFKA_REST_HOST_NAME: rest-proxy  # Internal hostname for REST Proxy.\n      KAFKA_REST_BOOTSTRAP_SERVERS: 'broker:29092'  # Kafka broker address for REST Proxy.\n      KAFKA_REST_LISTENERS: \"http://0.0.0.0:8082\"  # Listeners for REST Proxy.\n      KAFKA_REST_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'  # Schema Registry URL for REST Proxy.\n\n    networks:\n      - dasnet  # Connecting REST Proxy to the custom network 'dasnet'.\n\nnetworks:  # Network configuration for Docker containers.\n  dasnet:  # Defines the custom network 'dasnet'.\n    external: true  # Indicates that 'dasnet' is a pre-existing external network.\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html#2-start-the-services","title":"2. Start the Services","text":"<ul> <li>CD to <code>docker-compose.yml</code> folder. Run the command  <code>docker-compose up -d</code></li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html#3-check-the-setup","title":"3. Check the setup","text":"<ul> <li>Open Docker Desktop. You should see all the containres up and running</li> </ul> <ul> <li>Open the control centre to see the actual working.</li> </ul> <ul> <li>Go through the tabs to see all the functionalities</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/Kafka/2_Confluent_Kafka.html#component-details","title":"Component Details","text":"<p>Refer to the table for quick info on the the setup:</p> Component Purpose Host Name Ports (Laptop:Container) Key Environment Variables Network Zookeeper Coordinates and manages Kafka brokers. <code>zookeeper</code> 2181:2181 <code>ZOOKEEPER_CLIENT_PORT=2181</code><code>ZOOKEEPER_TICK_TIME=2000</code> <code>dasnet</code> Kafka Broker Handles message publishing and subscribing. <code>broker</code> 9192:90929111:9101 <code>KAFKA_BROKER_ID=1</code><code>KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181</code><code>KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://broker:29092</code> <code>dasnet</code> Schema Registry Manages Avro schemas for Kafka topics. <code>schema-registry</code> 8181:8081 <code>SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS=broker:29092</code><code>SCHEMA_REGISTRY_LISTENERS=http://0.0.0.0:8081</code> <code>dasnet</code> Kafka Connect Integrates Kafka with external systems (e.g., databases). <code>connect</code> 8183:8083 <code>CONNECT_BOOTSTRAP_SERVERS=broker:29092</code><code>CONNECT_REST_PORT=8083</code><code>CONNECT_GROUP_ID=kafka-connect</code> <code>dasnet</code> Control Center Web interface for managing and monitoring Kafka. <code>control-center</code> 9121:9021 <code>CONTROL_CENTER_BOOTSTRAP_SERVERS=broker:29092</code><code>CONTROL_CENTER_CONNECT_CLUSTER=connect:8083</code> <code>dasnet</code> KSQL Server SQL engine for processing real-time data streams. <code>ksqldb-server</code> 8188:8088 <code>KSQL_BOOTSTRAP_SERVERS=broker:29092</code><code>KSQL_LISTENERS=http://0.0.0.0:8088</code><code>KSQL_KSQL_SCHEMA_REGISTRY_URL=http://schema-registry:8081</code> <code>dasnet</code> REST Proxy Provides a RESTful interface to Kafka. <code>rest-proxy</code> 8182:8082 <code>KAFKA_REST_BOOTSTRAP_SERVERS=broker:29092</code><code>KAFKA_REST_LISTENERS=http://0.0.0.0:8082</code> <code>dasnet</code> KSQL DataGen Generates sample data for KSQL testing (optional). <code>ksql-datagen</code> N/A <code>STREAMS_BOOTSTRAP_SERVERS=broker:29092</code><code>STREAMS_SCHEMA_REGISTRY_HOST=schema-registry</code> <code>dasnet</code> <p>All components are connected via the <code>dasnet</code> custom Docker network.</p>"},{"location":"DevOps/Docker/ContainerStacks/MongoDB/3_DockerMongodb.html","title":"MongoDB with Docker","text":""},{"location":"DevOps/Docker/ContainerStacks/MongoDB/3_DockerMongodb.html#create-a-mongodb-container-on-docker","title":"Create a MongoDB Container on Docker","text":"<p>I will show you how to create a MongoDB container easily. We won't use Dockerfile, Docker-compose, or any files. Instead, we'll set up everything using just the command line. We'll use Docker volumes to store the data so it stays even after the container is closed.</p>"},{"location":"DevOps/Docker/ContainerStacks/MongoDB/3_DockerMongodb.html#create-the-network-and-docker-volume","title":"Create the network and docker volume","text":"<pre><code>docker network create spark-network\ndocker volume create mongo_data\n</code></pre>"},{"location":"DevOps/Docker/ContainerStacks/MongoDB/3_DockerMongodb.html#now-run-the-main-command","title":"Now run the main command","text":"<pre><code>docker run -d --name mongoDB --network spark-network -p 27017:27017 -v mongo-data:/data/db  mongo\n</code></pre> <ul> <li><code>-d</code>: Run the container in detached mode.</li> <li><code>--name mongo</code>: Assign the name \"mongo\" to the container.</li> <li><code>--network spark-network</code>: Connect the container to the netowrk you created, can be any network \"spark-network\".</li> <li><code>-p 27017:27017</code>: Map port 27017 on the host to port 27017 in the container.</li> <li><code>-v mongo-data:/data/db</code>: Use a Docker volume named \"mongo-data\" to persist MongoDB data.</li> </ul>"},{"location":"DevOps/Docker/ContainerStacks/MongoDB/3_DockerMongodb.html#connect-to-mongodb","title":"Connect to MongoDB","text":"<p>You can now connect to MongoDB using a MongoDB client or management tool like MongoDB Compass, Robo 3T, or Studio 3T. </p> <p>Here is an example:</p> <p><pre><code>mongodb://localhost:27017\n</code></pre> </p>"},{"location":"DevOps/Docker/ContainerStacks/MongoDB/3_DockerMongodb.html#summary-of-the-setup-and-steps","title":"Summary of the setup and steps","text":"Step by Step Details Pull MongoDB Image First, pull the latest MongoDB image from Docker Hub using this command:  <code>docker pull mongo</code> Run MongoDB Container Run a MongoDB container with this command. It names the container, exposes the port, and sets up a volume for data:  <code>docker run -d --name mongoDB --network cms-network -p 27017:27017 -v mongo-data:/data/db mongo</code> Access MongoDB Shell After starting the MongoDB container, access the MongoDB shell with this command:  <code>docker exec -it my-mongo mongo</code> Connect to MongoDB from an App Connect to your MongoDB container from any app using <code>localhost</code> as the hostname and <code>27017</code> as the port. Add a Docker Volume to MongoDB To persist data, create a Docker volume and attach it to your MongoDB container with these commands:  <code>docker volume create mongo-data</code> <code>docker run --name my-mongo -d -p 27017:27017 -v mongo-data:/data/db mongo</code>  This creates a volume named <code>mongo-data</code> and starts a MongoDB container using this volume for data storage. Mount Host Directory in MongoDB Container You can also mount a directory from your host system into the container to store data. This is useful for backups, restores, or direct access to MongoDB files:  <code>docker run --name my-mongo -d -p 27017:27017 -v /path/on/host:/data/db mongo</code>  Replace <code>/path/on/host</code> with the directory path on your host machine where you want to store MongoDB data. This links the <code>/data/db</code> directory in the container to the specified path on your host. Custom Configuration For custom MongoDB settings, you can mount a configuration file from the host into the container when it runs. Secure MongoDB For production, secure MongoDB with authentication. Start by running the container with the <code>--auth</code> flag to enable security:  <code>docker run --name my-mongo -d -p 27017:27017 -v ~/mongo-data:/data/db mongo --auth</code>  Then, create an admin user inside the MongoDB shell."},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html","title":"Understanding Dockerfile CMD and ENTRYPOINT Instructions","text":"<p>CMD and ENTRYPOINT are important Dockerfile instructions that define what command runs when a Docker container starts. Here, I will try to explain the concepts:</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#what-is-entrypoint","title":"What is ENTRYPOINT?","text":"<p>ENTRYPOINT sets the main process that will run inside the container. For example: <pre><code>ENTRYPOINT [\"/usr/bin/my-app\"]\n</code></pre> In this case, <code>/usr/bin/my-app</code> is the process that will run when the container starts.</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#what-is-cmd","title":"What is CMD?","text":"<p>CMD specifies the default arguments for the ENTRYPOINT process. For instance: <pre><code>ENTRYPOINT [\"/usr/bin/my-app\"]\nCMD [\"help\"]\n</code></pre> Here, <code>help</code> is passed as an argument to <code>/usr/bin/my-app</code>.</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#key-differences-between-entrypoint-and-cmd","title":"Key Differences Between ENTRYPOINT and CMD","text":"<ul> <li>ENTRYPOINT: Defines the main process to run in the container.</li> <li>CMD: Provides default arguments for the ENTRYPOINT process.</li> <li>Override:</li> <li>CMD can be easily overridden by passing arguments in the <code>docker run</code> command.</li> <li>ENTRYPOINT can be changed using the <code>--entrypoint</code> flag in <code>docker run</code>, but this is rarely necessary.</li> </ul>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#docker-ps-error","title":"Docker PS Error","text":""},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#for-our-windows-users","title":"For Our Windows Users:","text":"<ol> <li>Verify Docker's Installation Path: </li> <li>Navigate to <code>C:\\Program Files\\Docker\\Docker\\resources\\bin</code> via your command prompt or PowerShell. </li> <li> <p>While you're there, try running <code>docker ps</code>. If it responds, you're in luck! If not, let's move to the next step.</p> </li> <li> <p>Update the System PATH: </p> </li> <li>Sometimes, Windows isn't aware of where Docker is. We'll need to tell it.</li> <li>Open 'System Properties' by right-clicking on the Windows start button and selecting 'System'.</li> <li>Click on 'Advanced system settings', then choose 'Environment Variables'.</li> <li>Locate the PATH variable under 'System Variables'. Click on it and then select 'Edit'.</li> <li>Add a new entry with the path: <code>C:\\Program Files\\Docker\\Docker\\resources\\bin</code>.</li> <li> <p>Confirm with 'OK'.</p> </li> <li> <p>Using PowerShell to Update the PATH: </p> </li> <li>If you're a fan of PowerShell, you can also add the path using the following command:      <pre><code>[Environment]::SetEnvironmentVariable(\"PATH\", $env:PATH + \";C:\\Program Files\\Docker\\Docker\\resources\\bin\", \"Machine\")\n</code></pre></li> <li>Check if its running now. Just open command prompt and run <code>docker ps</code>. You should get some output. For example:</li> </ol> <p></p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#for-our-mac-users","title":"For Our Mac Users:","text":"<ol> <li>Verify Docker's Installation: </li> <li> <p>Open your terminal and type in <code>docker --version</code>. This ensures that Docker is installed.</p> </li> <li> <p>Is Docker Running? </p> </li> <li> <p>Check if the Docker Desktop application is running. If it's not, fire it up!</p> </li> <li> <p>Update the Shell's PATH: </p> </li> <li> <p>Sometimes, the shell doesn\u2019t know where Docker is located. To fix this:      <pre><code>echo \"export PATH=/usr/local/bin:$PATH\" &gt;&gt; ~/.bash_profile\nsource ~/.bash_profile\n</code></pre></p> </li> <li> <p>Final Check: </p> </li> <li>Close and reopen your terminal, then try <code>docker ps</code>. If all's well, it should work!</li> </ol>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#backup-entire-docker-to-your-laptop","title":"Backup entire docker to your laptop","text":""},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#save-docker-containers-images-and-volumes-on-maclinux","title":"Save Docker Containers, Images, and Volumes on Mac/Linux","text":"<p>To back up Docker containers, images, and volumes on Mac/Linux, you can use the following script:</p> <pre><code>#!/bin/bash\n\n# Create directories to store backups\nmkdir -p docker_image_backups docker_container_backups docker_volume_backups\n\n# Backup Docker images\nfor image in $(docker images --format \"{{.Repository}}:{{.Tag}}\"); do\n  sanitized_image_name=$(echo $image | tr / _)\n  docker save -o docker_image_backups/${sanitized_image_name}.tar $image\ndone\n\n# Backup Docker containers\nfor container in $(docker ps -a --format \"{{.Names}}\"); do\n  docker export -o docker_container_backups/${container}.tar $container\ndone\n\n# Backup Docker volumes\nfor volume in $(docker volume ls --format \"{{.Name}}\"); do\n  docker run --rm -v ${volume}:/volume -v $(pwd)/docker_volume_backups:/backup alpine sh -c \"cd /volume &amp;&amp; tar czf /backup/${volume}.tar.gz .\"\ndone\n\n# Create a single tarball containing all backups\ntar czf docker_backup_$(date +%Y%m%d).tar.gz docker_image_backups docker_container_backups docker_volume_backups\n\necho \"Backup completed successfully!\"\n</code></pre> <p>To run the script:</p> <ol> <li>Save the script to a file, e.g., <code>backup_docker.sh</code>.</li> <li>Make the script executable:</li> </ol> <pre><code>chmod +x backup_docker.sh\n</code></pre> <ol> <li>Run the script:</li> </ol> <pre><code>./backup_docker.sh\n</code></pre> <p>This will create a full backup of all Docker images, containers, and volumes.</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#save-docker-containers-images-and-volumes-on-windows","title":"Save Docker Containers, Images, and Volumes on Windows","text":"<p>To back up Docker containers, images, and volumes on Windows, follow these steps:</p> <ol> <li> <p>Create a folder and save the following content as <code>backup_docker.ps1</code>:</p> <pre><code># Backup Docker Images\ndocker images -q | ForEach-Object { docker save -o \"$($_).tar\" $_ }\n\n# Backup Running Containers\ndocker ps -q | ForEach-Object { docker export -o \"$($_).tar\" $_ }\n\n# Backup Docker Volumes\n$volumes = docker volume ls -q\nforeach ($volume in $volumes) {\n    docker run --rm -v ${volume}:/volume -v $(pwd):/backup ubuntu tar cvf /backup/${volume}_backup.tar /volume\n}\n\n# Backup Docker Configurations\nCopy-Item -Path \"C:\\path\\to\\your\\docker\\configurations\" -Destination \"C:\\path\\to\\your\\backup\\location\" -Recurse\n</code></pre> </li> <li> <p>Open PowerShell with administrative privileges and navigate to the folder you created:</p> <pre><code>cd path\\to\\your\\folder\n</code></pre> </li> <li> <p>Set the execution policy to allow the script to run temporarily:</p> <pre><code>Set-ExecutionPolicy -ExecutionPolicy Bypass -Scope Process\n</code></pre> </li> <li> <p>Run the script:</p> <pre><code>.\\backup_docker.ps1\n</code></pre> </li> </ol> <p>This way you can back up all your Docker containers, images, and volumes to the current folder.</p> <p></p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#docker-common-errors","title":"Docker common errors","text":"<p><code>HTTP code 500) server error - Ports are not available: exposing port TCP 0.0.0.0:50070 -&gt; 0.0.0.0:0: listen tcp 0.0.0.0:50070: bind: An attempt was made to access a socket in a way forbidden by its access permissions.</code></p> <p>Execute command</p> <pre><code>net stop winnat\ndocker start &lt;full container name&gt;\nnet start winnat\n</code></pre>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#orchestration-tools-docker-swarm-vs-kubernetes","title":"Orchestration Tools - Docker Swarm vs Kubernetes","text":"<p>To manage complex applications, many developers use containers. Containers package all the necessary dependencies, making applications portable, fast, secure, scalable, and easy to manage. To handle multiple containers, you need an orchestration tool like Docker Swarm or Kubernetes.</p> <p>Both tools manage containers but have different strengths and weaknesses. This article will help you decide which one is right for you.</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#docker-swarm","title":"Docker Swarm","text":"<p>Docker Swarm, an open-source orchestration tool by Docker, turns multiple Docker instances into a single virtual host. Here are its key components:</p> Component Description Nodes Individual Docker instances. Services and Tasks The applications you run. Load Balancers Distribute requests across nodes. <p>Advantages of Docker Swarm: - Ease of Use: Simple installation and understanding. - Integration: Works seamlessly with Docker CLI. - Automated Load Balancing: Distributes traffic within the cluster automatically.</p> <p>Disadvantages of Docker Swarm: - Limited Functionality: Less powerful compared to Kubernetes. - Basic Automation: Not as robust as Kubernetes.</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#kubernetes","title":"Kubernetes","text":"<p>Kubernetes, developed by Google, offers a more complex structure with nodes, pods, namespaces, and more.</p> Component Description Nodes Worker machines in the cluster. Pods Smallest deployable units, containing one or more containers. Namespaces Logical isolation for resources. <p>Advantages of Kubernetes: - Community Support: Backed by Google, with a large open-source community. - Operating System Support: Works on all OS. - Scalability and Management: Handles large and complex workloads. - Automation and Self-Healing: Automatically scales and repairs itself. - Built-in Monitoring: Includes monitoring tools. - Cloud Support: Available on Google Cloud, Azure, and AWS.</p> <p>Disadvantages of Kubernetes: - Complexity: Difficult to install and learn. - Separate Tools: Requires learning new CLI tools. - Transition: Moving from Docker Swarm can be challenging. - Overhead: Can be overly complex for simple applications.</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#comparing-docker-swarm-and-kubernetes","title":"Comparing Docker Swarm and Kubernetes","text":"Aspect Docker Swarm Kubernetes Installation Easy to install and use. Works with Docker CLI. Complex installation, separate CLI tools. Application Deployment YAML or Docker Compose for services or microservices. More options like namespaces, pods, and deployments. Availability and Scaling High availability, but no automatic scaling. Highly available, fault-tolerant, self-healing, and automatic scaling. Monitoring Requires third-party tools. Built-in monitoring and third-party integrations. Security Uses TLS for security. Supports multiple security protocols (RBAC, SSL/TLS, secrets management). Load Balancing Automatic load balancing using DNS. Uses tools like Nginx Ingress for load balancing."},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#k3s-best-of-both-worlds","title":"K3s, best of both worlds","text":"<p>K3s, a lightweight version of Kubernetes. It gives you the full Kubernetes API without complexity. It's easy to use and CNCF certified. Consider K3s if you want Kubernetes features with simpler management.</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#which-one-should-you-choose","title":"Which One Should You Choose?","text":"<ul> <li>Docker Swarm: Best for beginners and small-scale applications due to its ease of use.</li> <li>Kubernetes: Ideal for complex and large-scale projects requiring robust features and automation.</li> <li>K3s: Suitable if you want Kubernetes features with less complexity.</li> </ul>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#summary","title":"Summary","text":"Feature Docker Swarm Kubernetes K3s Component - Nodes: Individual Docker instances.- Services and Tasks: The applications you run.- Load Balancers: Distribute requests across nodes. - Nodes: Worker and master nodes.- Pods: Smallest deployable units.- Namespaces: Virtual clusters.- Config Maps: Manage configuration. - Nodes: Worker and master nodes.- Pods: Smallest deployable units.- Namespaces: Virtual clusters.- Config Maps: Manage configuration. Advantages - Ease of Use: Simple installation and understanding.- Integration: Works seamlessly with Docker CLI.- Automated Load Balancing: Distributes traffic within the cluster automatically. - Community Support: Backed by Google, large open-source community.- Operating System Support: Works on all OS.- Scalability and Management: Handles large and complex workloads.- Automation and Self-Healing: Automatically scales and repairs itself.- Built-in Monitoring: Comes with monitoring tools.- Cloud Support: Available on Google Cloud, Azure, and AWS. - Lightweight: Easier and faster to set up.- Complete Kubernetes API: Offers all features without extra complexity.- CNCF Certified: Ensures compatibility and support. Disadvantages - Limited Functionality: Less powerful compared to Kubernetes.- Basic Automation: Not as robust as Kubernetes. - Complexity: Difficult to install and learn.- Separate Tools: Requires learning new CLI tools.- Transition: Moving from Docker Swarm can be hard.- Overhead: Can be overly complex for simple applications. - Limited Community Support: Smaller user base compared to Kubernetes.- Fewer Integrations: May not support all third-party tools. Application Deployment Deploy services or microservices using YAML or Docker Compose. Offers more options like namespaces, pods, and deployments. Supports deployments using YAML files with simplified configurations. Availability and Scaling High availability, but no automatic scaling. Highly available, fault-tolerant, self-healing, and automatic scaling. Provides high availability and simple scaling mechanisms. Monitoring Requires third-party tools. Built-in monitoring and third-party integrations. Supports basic monitoring with options for third-party integrations. Security Uses TLS for security. Supports multiple security protocols like RBAC, SSL/TLS, secrets management. Provides essential security features with easier management. Load Balancing Automatic load balancing using DNS. Uses tools like Nginx Ingress for load balancing. Simplified load balancing with integrated tools."},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#common-docker-commands","title":"Common docker commands","text":"Purpose Command \ud83c\udfc3 Run <code>docker run &lt;image&gt;</code> \u23f9\ufe0f Stop <code>docker stop &lt;container&gt;</code> \u25b6\ufe0f Start <code>docker start &lt;container&gt;</code> \ud83d\uddd1\ufe0f Remove Ctr <code>docker rm &lt;container&gt;</code> \ud83d\uddbc\ufe0f Remove Img <code>docker rmi &lt;image&gt;</code> \ud83d\udcc3 List Ctrs <code>docker ps</code> \ud83d\uddbc\ufe0f List Imgs <code>docker images</code> \u2b07\ufe0f Pull <code>docker pull &lt;image&gt;</code> \u2328\ufe0f Exec <code>docker exec &lt;container&gt;</code> \ud83c\udfd7\ufe0f Build <code>docker build -t &lt;tag&gt; .</code> \ud83d\udd0a Logs <code>docker logs &lt;container&gt;</code> \ud83d\udd0d Inspect <code>docker inspect &lt;container_or_image&gt;</code> \ud83d\udcca Stats <code>docker stats &lt;container&gt;</code> \ud83d\udcc1 Volume List <code>docker volume ls</code> \ud83c\udd95 Volume Create <code>docker volume create &lt;volume_name&gt;</code> \ud83d\udeae Volume Remove <code>docker volume rm &lt;volume_name&gt;</code> \ud83c\udf10 Network List <code>docker network ls</code> \ud83c\udf09 Network Create <code>docker network create &lt;network&gt;</code> \ud83d\udce1 Network Connect <code>docker network connect &lt;network&gt; &lt;container&gt;</code> \ud83d\udd0c Network Disconnect <code>docker network disconnect &lt;network&gt; &lt;container&gt;</code> \ud83d\udd04 Pull Latest <code>docker pull &lt;image&gt;:latest</code> \ud83d\udeab Build No Cache <code>docker build --no-cache -t &lt;tag&gt; .</code>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#running-windows-os-as-a-container-in-docker","title":"Running Windows OS as a Container in Docker","text":"<p>When you think of containers, you usually picture a small Linux OS. And you\u2019re right! But did you know that containers can run a small Windows OS too?</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#switching-docker-desktop-to-windows-mode","title":"Switching Docker Desktop to Windows Mode","text":"<p>For most of your tasks, you've likely been using Docker containers, which are typically Linux-based. Running a Windows container might seem unusual. By default, Docker on your Windows machine operates in 'Linux mode.' To run Windows containers, you'll need to switch from this default Linux mode to Windows mode.</p> <p></p> <p>And you can switch back to Linux containers easily.</p> <p></p> <p>Note: If you don't switch back to linux containers mode and try to run a container whose OS is linux you will encounter an error like:</p> <p></p> <p>Note: When you switch to Windows mode, you won\u2019t be able to see your Linux containers.</p> <p></p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#switching-using-command-line","title":"Switching Using Command Line","text":"<p>You can also switch using the command line:</p> <pre><code>&amp; $Env:ProgramFiles\\Docker\\Docker\\DockerCli.exe -SwitchDaemon\n</code></pre>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#further-reading","title":"Further Reading","text":"<p>For more information, check out: Windows Containers Documentation</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#microsoft-base-images","title":"Microsoft Base Images","text":"<p>Microsoft offers several base images to start building your own container images:</p> <ul> <li>Windows: Contains the full set of Windows APIs and system services (excluding server roles).</li> <li>Windows Server: Contains the full set of Windows APIs and system services.</li> <li>Windows Server Core: A smaller image with a subset of Windows Server APIs, including the full .NET framework and most server roles (excluding some like Fax Server).</li> <li>Nano Server: The smallest Windows Server image, supporting .NET Core APIs and some server roles.</li> </ul>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#ready-made-bundles","title":"Ready-Made Bundles","text":"<p>Microsoft offers ready-made bundles that combine Windows Server 2022 with apps like MongoDB.</p> <p>For example, if you need MongoDB on Windows Server 2022, you can use this Dockerfile.</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#other-combinations","title":"Other Combinations","text":"<p>Other combinations you can create include:</p> <ul> <li>aspnet</li> <li>iis</li> <li>Django</li> <li>apache-http-php</li> <li>nginx</li> <li>dotnet35</li> <li>golang</li> <li>nodejs</li> <li>python</li> <li>python-django</li> <li>rails</li> <li>ruby</li> <li>server-jre-8u51-windows-x64</li> <li>mongodb</li> <li>mysql</li> <li>redis</li> <li>sqlite</li> <li>sqlserver-express</li> <li>PowerShellDSC_iis-10.0</li> </ul> <p>Now you know that containers are not just small Linux OSs. They can be a mini Windows OS too!</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#types-of-kubernetes","title":"Types of Kubernetes","text":"<p>Kubernetes is a system for managing containerized applications, but there are many ways to set it up based on your needs. The table below summriazes the various available kubernetes 'brands':</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#for-the-busy-people","title":"For the busy people:","text":"<ul> <li>Development: Minikube, K3d, Docker Desktop Kubernetes, and Kind are geared toward development and testing.</li> <li>Production: Kubeadm, Rancher, K3s, and managed services like Amazon EKS, Google GKE, and Azure AKS are suited for production environments.</li> <li>Mixed Use: K3s and MicroK8s can be used both in development and production, especially in edge computing and IoT.</li> </ul>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#kubernetes-brands","title":"Kubernetes Brands <p>Here's the revised table with the \"Environment\" column removed:</p>    Kubernetes Tool Company/Provider Best For Single Machine Setup Multi-Machine Setup Used By     Minikube Kubernetes Community (CNCF) Development Yes No Individual developers, small startups.   Kubeadm Kubernetes Community (CNCF) Production Yes (single node) Yes Enterprises, cloud service providers.   Rancher SUSE Production Yes (for management) Yes Enterprises, companies managing multi-cloud or hybrid environments.   K3s Rancher (SUSE) Development/Production Yes Yes IoT companies, edge computing solutions, small and medium enterprises.   K3d Rancher (SUSE) Development Yes No Developers, small companies for testing multi-node setups.   Docker Desktop Kubernetes Docker, Inc. Development Yes No Developers using Docker, small teams.   MicroK8s Canonical (Ubuntu) Development/Production Yes Yes IoT and edge computing companies, startups.   Amazon EKS Amazon Web Services (AWS) Production No Yes Large enterprises, companies using AWS.   Google GKE Google Cloud Production No Yes Large enterprises, companies using Google Cloud.   Azure AKS Microsoft Azure Production No Yes Enterprises, companies using Azure services.   OpenShift Red Hat (IBM) Production Yes (for single node) Yes Enterprises needing integrated CI/CD, large companies in regulated industries.   Kind (Kubernetes in Docker) Kubernetes Community (CNCF) Development Yes No Developers, CI/CD pipelines in tech companies.","text":""},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#how-i-push-customized-images-to-docker-hubwebsite","title":"How I push customized Images to Docker Hub(Website)","text":"<p>Recently, I set up a Kafka environment using the base images from Confluent. After tweaking and customizing these images to fit my specific needs, I realized that these modified images should be pushed to Docker Hub so I can easily reuse them later or share them with others. I had 9 images in total, so here\u2019s the process I followed.</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#step-1-log-in-to-docker-hub","title":"Step 1: Log In to Docker Hub","text":"<p>The first thing I did was log in to Docker Hub using my Docker Hub username, <code>dwdas9</code>.</p> <pre><code>docker login\n</code></pre> <p>It asked for my Docker Hub username and password, and once I provided those, I was logged in.</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#step-2-find-the-image-names","title":"Step 2: Find the Image Names","text":"<p>To push the images, I needed to know their names. I used the following command to list all the Docker images on my local machine:</p> <pre><code>docker images\n</code></pre> <p>This command gave me a list of all the images, including their names, tags, and IDs. I picked out the relevant images that I had customized.</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#step-3-tag-the-images","title":"Step 3: Tag the Images","text":"<p>Before I could push the images, I had to tag them with my Docker Hub username and the repository name I wanted them to go into. Here\u2019s how I did it for each of the 9 images:</p> <pre><code>docker tag confluentinc/cp-ksqldb-server:6.0.1 dwdas9/cp-ksqldb-server:v6\ndocker tag confluentinc/cp-kafka-rest:6.0.1 dwdas9/cp-kafka-rest:v6\ndocker tag confluentinc/cp-schema-registry:6.0.1 dwdas9/cp-schema-registry:v6\ndocker tag confluentinc/cp-enterprise-control-center:6.0.1 dwdas9/cp-enterprise-control-center:v6\ndocker tag confluentinc/cp-kafka-connect-base:6.0.1 dwdas9/cp-kafka-connect-base:v6\ndocker tag confluentinc/cp-server:6.0.1 dwdas9/cp-server:v6\ndocker tag confluentinc/cp-zookeeper:6.0.1 dwdas9/cp-zookeeper:v6\ndocker tag confluentinc/ksqldb-examples:6.0.1 dwdas9/ksqldb-examples:v6\ndocker tag confluentinc/cp-ksqldb-cli:6.0.1 dwdas9/cp-ksqldb-cli:v6\n</code></pre> <p>I replaced <code>confluentinc</code> with my username <code>dwdas9</code> and added a custom tag <code>v6</code> to each image.</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#step-4-push-the-images-to-docker-hub","title":"Step 4: Push the Images to Docker Hub","text":"<p>With the images tagged, the next step was to push them to Docker Hub. I ran the following commands:</p> <pre><code>docker push dwdas9/cp-ksqldb-server:v6\ndocker push dwdas9/cp-kafka-rest:v6\ndocker push dwdas9/cp-schema-registry:v6\ndocker push dwdas9/cp-enterprise-control-center:v6\ndocker push dwdas9/cp-kafka-connect-base:v6\ndocker push dwdas9/cp-server:v6\ndocker push dwdas9/cp-zookeeper:v6\ndocker push dwdas9/ksqldb-examples:v6\ndocker push dwdas9/cp-ksqldb-cli:v6\n</code></pre> <p>Docker started uploading each image to the repository. Once the upload was complete, I could see all my modified images on Docker Hub under my account.</p> <p></p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#conclusion","title":"Conclusion","text":"<p>And that\u2019s it! Now, my customized Kafka environment images are safely stored on Docker Hub, ready to be pulled down whenever I need them. If you\u2019re working on something similar, these steps should help you push your images too.</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#troubleshooting-docker-errors","title":"Troubleshooting docker errors","text":"<p>If your Docker container is showing up as orange (in Docker Desktop) or failed to start without giving specific details, it means the container likely encountered an error during startup. Docker doesn\u2019t always show detailed error messages in the UI, but you can retrieve more information using the following methods.</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#steps-to-debug-the-issue","title":"Steps to Debug the Issue","text":""},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#1-check-the-container-logs","title":"1. Check the Container Logs","text":"<p>You can check the logs for the failed container to see what went wrong. Run this command to inspect the container logs:</p> <pre><code>docker logs your-container-name\n</code></pre> <p>This will show you any errors or issues that occurred during the container\u2019s initialization. Look for specific errors related to: - Missing environment variables. - Errors in Spark or system configuration. - Issues with file mounting (e.g., mapping the <code>conf</code> directory).</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#2-inspect-the-container-status","title":"2. Inspect the Container Status","text":"<p>You can inspect the container to get more information about why it failed. Use the <code>docker inspect</code> command:</p> <pre><code>docker inspect your-container-name\n</code></pre> <p>This will provide detailed information about the container's configuration, including the exit code. Look for anything abnormal in the output, especially the <code>State</code> and <code>ExitCode</code> sections.</p>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#3-check-for-file-permissions-and-volume-issues","title":"3. Check for File Permissions and Volume Issues","text":"<p>Sometimes, volume mapping can cause issues, especially when the local directory being mounted doesn\u2019t have the correct permissions, or Docker has trouble accessing it.</p> <p>Make sure that the local <code>mapped-folder</code> folder has the correct permissions for Docker to access it. For example:</p> <ul> <li>Check if the directory exists and has read/write permissions:</li> </ul> <pre><code>ls -ld mapped-folder\n</code></pre> <p>If the directory is not accessible by Docker, try giving it the correct permissions:</p> <pre><code>chmod -R 755 mapped-folder\n</code></pre>"},{"location":"DevOps/Docker/Fundamentals/1_DockerConcepts.html#understanding-docker-container-hostnames","title":"Understanding Docker Container Hostnames","text":"<p>Docker assigns a hostname to each container when it is first created. If you don\u2019t specify a hostname, Docker defaults to using the container\u2019s unique ID, which typically results in an unintuitive, random-looking name. This hostname stays the same even if you stop and start the container again, as long as the container isn\u2019t removed and recreated.</p> <p>To set a more meaningful hostname, you can use the <code>--hostname</code> flag with <code>docker run</code> or define the <code>hostname</code> field in your Docker Compose file during container creation.</p>"},{"location":"DevOps/Docker/Fundamentals/2_VSCode_Docker_Connection.html","title":"VS Code Docker Connection","text":"<p>Here, I will show you how to connect to a container using VS Code to run Python code, create Jupyter notebooks, and more. This setup is very helpful. Containers are essentially Linux OS environments, and you can't log into them directly to install VS Code.</p> <p>The connection is mainly done using the VS Code Dev Containers extension, which is the key component for connectivity. The following sections provide detailed steps to guide you through the process.</p>"},{"location":"DevOps/Docker/Fundamentals/2_VSCode_Docker_Connection.html#summary-of-steps","title":"Summary of Steps","text":"<ol> <li> <p>Install VS Code Dev Containers Extension on your local machine.</p> </li> <li> <p>Open a terminal and run the following commands in the container:    <pre><code>sudo su\nmkdir -p /.vscode-server\nchmod -R 777 /.vscode-server\n</code></pre></p> </li> <li> <p>Attach to the Running Container:    Open a remote window from the bottom left corner in VS Code.</p> </li> <li> <p>Install Jupyter &amp; Python extensions in the container.</p> </li> <li> <p>Install Required Python Packages:    Open a terminal and run the following commands in the container:    <pre><code>sudo su\npip install ipykernel\npip install py4j\n</code></pre></p> </li> </ol>"},{"location":"DevOps/Docker/Fundamentals/2_VSCode_Docker_Connection.html#install-dev-containers-extension","title":"Install Dev Containers Extension","text":"<p>In Visual Studio Code press Ctrl+Shift+X, search  Dev Containers and install. </p>"},{"location":"DevOps/Docker/Fundamentals/2_VSCode_Docker_Connection.html#attach-to-the-running-container","title":"Attach to the running container","text":"<ul> <li> <p>Click the Open Remote Window button in the bottom-left corner of VS Code and select Attach to Running Container from the command palette that appears. </p> </li> <li> <p>Pick your active container from the presented list.</p> </li> </ul> <p></p> <p>Note: Here you will encounter like this, go to the errors section to resolve it</p>"},{"location":"DevOps/Docker/Fundamentals/2_VSCode_Docker_Connection.html#install-jupyter-notebook-support-extension-in-conatiner","title":"Install Jupyter notebook support extension in conatiner","text":"<p>Go to extensions(left pane), search Jupyter, click on Install in container</p> <p></p>"},{"location":"DevOps/Docker/Fundamentals/2_VSCode_Docker_Connection.html#install-python-support-extension-in-container","title":"Install Python Support Extension in Container","text":"<p>Go to extensions(left pane), search Python, click on Install in container</p> <p></p>"},{"location":"DevOps/Docker/Fundamentals/2_VSCode_Docker_Connection.html#install-ipykernel","title":"Install ipykernel","text":"<p>Connect to the container from terminal(or EXEC in Docker container) and run this command:</p> <pre><code>sudo su\npip install ipykernel\n</code></pre> <p>Also, if you try to run a jupyter notebook, using the steps below</p> <p></p> <p>You may be prompted to isntall the extension:</p> <p></p>"},{"location":"DevOps/Docker/Fundamentals/2_VSCode_Docker_Connection.html#install-py4jif-required","title":"Install py4j(if required)","text":""},{"location":"DevOps/Docker/Fundamentals/2_VSCode_Docker_Connection.html#errors","title":"Errors","text":""},{"location":"DevOps/Docker/Fundamentals/2_VSCode_Docker_Connection.html#failed-mkdir-p-vscode-server","title":"failed: mkdir -p /.vscode-server","text":"<p>When trying to attach to a Docker container using the VSCode Dev extension, you may encounter an error during the Installing VS Code Server step:</p> <p></p> <p>Reason: Insufficient permissions for VSCode to create a folder .vscode-server inside the root folder. It cannot create this folder: /.vscode-server.</p> <p>For example, when it runs this command:</p> <p><pre><code>mkdir -p /root/.vscode-server/bin/\n</code></pre> Note: When you attach to a running container, the Dev Container extension installs a remote server in the <code>.vscode-server</code> folder, defaulting to the root location. You can change this property by using:</p> <p>User Settings (JSON): Press <code>Ctrl+Shift+P</code> &gt; \"Preferences: Open User Settings (JSON)\". Add: <pre><code>\"remote.SSH.serverInstallPath\": {\n    \"&lt;host&gt;\": \"/test/location\"\n}\n</code></pre> </p> <p>Settings UI: Go to File &gt; Preferences &gt; Settings, filter by <code>@ext:ms-vscode-remote.remote-ssh install</code>, and under \"Server Install Path\" &gt; Add Item with Item = <code>&lt;host&gt;</code> and Value = <code>/test/location</code>.</p> <p></p>"},{"location":"DevOps/Docker/Fundamentals/2_VSCode_Docker_Connection.html#correct-method","title":"Correct Method","text":"<p>The correct method is to create the folder using the root user and provide permissions to it for the normal user. This way, you can create a normal container without needing to add the root user to the container.</p> <p>To achieve this run the following commands in the container and then try to connect VS code again:</p> <pre><code>sudo su\nmkdir -p /.vscode-server\nchmod -R 777 /.vscode-server\n</code></pre> <p>sometimes, e.g. in debian, just su</p> <p></p> <p>Note: You may not always have su access or password. To resolve it you may have to create a Dockerfile and users inside it with elevated permission. Refer to my BitnamiSparkCluster article to create such containers.</p>"},{"location":"DevOps/Docker/Fundamentals/2_VSCode_Docker_Connection.html#wrong-resolution","title":"Wrong Resolution","text":""},{"location":"DevOps/Docker/Fundamentals/2_VSCode_Docker_Connection.html#add-userroot-method","title":"Add user:root method","text":"<p>If you use a docker-compose file to create the containers you can add <code>user: root</code> to every container. </p> <p></p>"},{"location":"DevOps/Docker/Fundamentals/2_VSCode_Docker_Connection.html#docker-run-u-root-method","title":"docker run -u root method","text":"<p>Alternaively, you can start the container with root. This is feasible only for single containers.</p> <p><code>docker run -u root -it --name myCont theImageFileName /bin/bash</code></p>"},{"location":"DevOps/Docker/Fundamentals/2_VSCode_Docker_Connection.html#no-kernel-visible","title":"No Kernel Visible","text":"<p>Install the Jupyter extension in the container.</p> <p></p> <p>Also, go to the Docker container and install ipykernel:</p> <pre><code>sudo su\npip install ipykernel\n</code></pre> <p></p> <p>Then it will detect the kernel automatically.</p> <p></p>"},{"location":"DevOps/Docker/Fundamentals/2_VSCode_Docker_Connection.html#py4j-error","title":"Py4J Error","text":"<p>While running code, you may get this error:</p> <p></p> <p>Install py4j in the container to resolve it:</p> <pre><code>sudo su\npip install py4j\n</code></pre> <p></p>"},{"location":"Git/1.1_Git_Origin.html","title":"Understanding Git Origin","text":"<p>You clone a repo. You type <code>git push origin main</code>. You see <code>origin</code> everywhere. What is it?</p> <p>It's just a name. A shortcut. A reference to the remote repository on GitHub or GitLab or wherever your code lives.</p>"},{"location":"Git/1.1_Git_Origin.html#what-origin-is","title":"What Origin Is","text":"<p>When you clone a repository, Git automatically creates a remote called <code>origin</code>. It points to the URL you cloned from.</p> <p>Think of it as a bookmark. Instead of typing the full URL every time, you just say <code>origin</code>.</p> <pre><code>flowchart LR\n    A[Your Computer] --&gt;|origin points to| B[GitHub Repository]\n\n    style A fill:#e8f5e9\n    style B fill:#e1f5ff</code></pre> <p>That's it. <code>origin</code> is just a name for the remote location of your code.</p>"},{"location":"Git/1.1_Git_Origin.html#the-basic-commands","title":"The Basic Commands","text":"<p>This is what you'll use every day:</p> <pre><code># Check what origin points to\ngit remote -v\n</code></pre> <p>Output:</p> <pre><code>origin  https://github.com/username/repo.git (fetch)\norigin  https://github.com/username/repo.git (push)\n</code></pre> <p>Two lines. One for fetching. One for pushing. Both point to the same place.</p> <p>Common operations:</p> <pre><code># Fetch from origin\ngit fetch origin\n\n# Pull from origin's main branch\ngit pull origin main\n\n# Push to origin's main branch\ngit push origin main\n</code></pre> <p>You're using <code>origin</code> instead of typing the full URL. That's all it is.</p>"},{"location":"Git/1.1_Git_Origin.html#how-origin-gets-created","title":"How Origin Gets Created","text":"<p>You clone a repo:</p> <pre><code>git clone https://github.com/username/repo.git\n</code></pre> <p>Git automatically does this behind the scenes:</p> <pre><code>git remote add origin https://github.com/username/repo.git\n</code></pre> <p>You didn't have to set it up. Git did it for you.</p> <p>Check it:</p> <pre><code>git remote -v\n</code></pre> <p>There it is. <code>origin</code> pointing to your repo.</p>"},{"location":"Git/1.1_Git_Origin.html#is-origin-special","title":"Is Origin Special?","text":"<p>No. It's just a convention.</p> <p>You could name it anything:</p> <pre><code>git remote add pizza https://github.com/username/repo.git\ngit push pizza main\n</code></pre> <p>That works. But everyone uses <code>origin</code>. It's the standard. Don't be different just to be different.</p> <p>Stick with Convention</p> <p>Use <code>origin</code> for your primary remote. Your team expects it. Tools assume it. Don't fight convention.</p>"},{"location":"Git/1.1_Git_Origin.html#quick-command-reference","title":"Quick Command Reference","text":"Task Command See all remotes <code>git remote -v</code> Add a remote named origin <code>git remote add origin &lt;url&gt;</code> Change origin URL <code>git remote set-url origin &lt;new-url&gt;</code> Remove origin <code>git remote remove origin</code> Rename origin <code>git remote rename origin old-origin</code> Fetch from origin <code>git fetch origin</code> Push to origin <code>git push origin &lt;branch&gt;</code> Pull from origin <code>git pull origin &lt;branch&gt;</code>"},{"location":"Git/1.1_Git_Origin.html#advanced-scenarios","title":"Advanced Scenarios","text":"<p>Everything below is optional. But useful for complex setups.</p>"},{"location":"Git/1.1_Git_Origin.html#working-with-multiple-remotes","title":"Working with Multiple Remotes","text":"Origin vs Upstream (Forked Repositories) <p>You forked a repository. You have two remotes:</p> <ul> <li><code>origin</code> \u2192 Your fork on GitHub</li> <li><code>upstream</code> \u2192 The original repository</li> </ul> <p>Why two remotes?</p> <p>You push to your fork (<code>origin</code>). You pull from the original repo (<code>upstream</code>) to stay up to date.</p> <pre><code>flowchart TB\n    A[Original Repo] --&gt;|You forked it| B[Your Fork]\n    B --&gt;|git clone| C[Your Computer]\n    C --&gt;|git push| B\n    A --&gt;|git fetch upstream| C\n\n    style A fill:#fff3e0\n    style B fill:#e1f5ff\n    style C fill:#e8f5e9</code></pre> <p>Setup:</p> <pre><code># Clone your fork (origin is created automatically)\ngit clone https://github.com/you/repo.git\n\n# Add upstream\ngit remote add upstream https://github.com/original-owner/repo.git\n</code></pre> <p>Check your remotes:</p> <pre><code>git remote -v\n</code></pre> <p>Output:</p> <pre><code>origin    https://github.com/you/repo.git (fetch)\norigin    https://github.com/you/repo.git (push)\nupstream  https://github.com/original-owner/repo.git (fetch)\nupstream  https://github.com/original-owner/repo.git (push)\n</code></pre> <p>Daily workflow:</p> <pre><code># Fetch latest from original repo\ngit fetch upstream\n\n# Update your local main\ngit checkout main\ngit merge upstream/main\n\n# Push to your fork\ngit push origin main\n</code></pre>"},{"location":"Git/1.1_Git_Origin.html#changing-the-origin-url","title":"Changing the Origin URL","text":"Switching from HTTPS to SSH (or Vice Versa) <p>You cloned with HTTPS. Now you want SSH. Or the repo moved. Or you got the URL wrong.</p> <p>Check current URL:</p> <pre><code>git remote -v\n</code></pre> <p>Output:</p> <pre><code>origin  https://github.com/username/repo.git (fetch)\norigin  https://github.com/username/repo.git (push)\n</code></pre> <p>Change to SSH:</p> <pre><code>git remote set-url origin git@github.com:username/repo.git\n</code></pre> <p>Verify:</p> <pre><code>git remote -v\n</code></pre> <p>Output:</p> <pre><code>origin  git@github.com:username/repo.git (fetch)\norigin  git@github.com:username/repo.git (push)\n</code></pre> <p>Done. No need to re-clone.</p>"},{"location":"Git/1.1_Git_Origin.html#adding-origin-to-an-existing-local-repo","title":"Adding Origin to an Existing Local Repo","text":"When You Init Locally First <p>You created a repo on your machine:</p> <pre><code>mkdir my-project\ncd my-project\ngit init\n</code></pre> <p>You made commits. Now you want to push to GitHub.</p> <p>Create repo on GitHub (don't initialize with README).</p> <p>Add origin:</p> <pre><code>git remote add origin https://github.com/username/my-project.git\n</code></pre> <p>Push your code:</p> <pre><code>git branch -M main\ngit push -u origin main\n</code></pre> <p>Your local repo is now connected to GitHub.</p>"},{"location":"Git/1.1_Git_Origin.html#working-with-multiple-origins","title":"Working with Multiple Origins","text":"Deploy to Different Servers <p>You have one codebase. Multiple deployment targets.</p> <ul> <li><code>origin</code> \u2192 GitHub for version control</li> <li><code>production</code> \u2192 Production server</li> <li><code>staging</code> \u2192 Staging server</li> </ul> <p>Add multiple remotes:</p> <pre><code>git remote add origin https://github.com/username/repo.git\ngit remote add production user@prod-server:/var/www/repo.git\ngit remote add staging user@staging-server:/var/www/repo.git\n</code></pre> <p>Check all remotes:</p> <pre><code>git remote -v\n</code></pre> <p>Output:</p> <pre><code>origin      https://github.com/username/repo.git (fetch)\norigin      https://github.com/username/repo.git (push)\nproduction  user@prod-server:/var/www/repo.git (fetch)\nproduction  user@prod-server:/var/www/repo.git (push)\nstaging     user@staging-server:/var/www/repo.git (fetch)\nstaging     user@staging-server:/var/www/repo.git (push)\n</code></pre> <p>Push to specific remote:</p> <pre><code># Push to GitHub\ngit push origin main\n\n# Deploy to staging\ngit push staging main\n\n# Deploy to production\ngit push production main\n</code></pre>"},{"location":"Git/1.1_Git_Origin.html#removing-and-re-adding-origin","title":"Removing and Re-adding Origin","text":"Fixing a Messed Up Remote <p>Your origin is broken. Wrong URL. Corrupted. Whatever.</p> <p>Remove it:</p> <pre><code>git remote remove origin\n</code></pre> <p>Verify it's gone:</p> <pre><code>git remote -v\n# Should show nothing or only other remotes\n</code></pre> <p>Add it back:</p> <pre><code>git remote add origin https://github.com/username/repo.git\n</code></pre> <p>Verify:</p> <pre><code>git remote -v\n</code></pre>"},{"location":"Git/1.1_Git_Origin.html#renaming-origin","title":"Renaming Origin","text":"Using a Different Name <p>You want to call it something else. Maybe you have multiple repos with different conventions.</p> <p>Rename origin to github:</p> <pre><code>git remote rename origin github\n</code></pre> <p>Now you push to github:</p> <pre><code>git push github main\n</code></pre> <p>But again: don't do this unless you have a good reason. Stick with <code>origin</code>.</p>"},{"location":"Git/1.1_Git_Origin.html#real-world-scenarios","title":"Real-World Scenarios","text":""},{"location":"Git/1.1_Git_Origin.html#scenario-1-you-cloned-someones-repo","title":"Scenario 1: You Cloned Someone's Repo","text":"<p>You want to contribute. You fork their repo. You clone your fork.</p> <pre><code># Fork on GitHub first, then:\ngit clone https://github.com/you/their-repo.git\ncd their-repo\n\n# Add upstream\ngit remote add upstream https://github.com/them/their-repo.git\n\n# Check remotes\ngit remote -v\n</code></pre> <p>Output:</p> <pre><code>origin    https://github.com/you/their-repo.git (fetch)\norigin    https://github.com/you/their-repo.git (push)\nupstream  https://github.com/them/their-repo.git (fetch)\nupstream  https://github.com/them/their-repo.git (push)\n</code></pre> <p>Now you can:</p> <ul> <li>Push to your fork: <code>git push origin feature/new-thing</code></li> <li>Pull from original: <code>git fetch upstream</code></li> </ul>"},{"location":"Git/1.1_Git_Origin.html#scenario-2-repository-moved","title":"Scenario 2: Repository Moved","text":"<p>Your company moved from GitHub to GitLab. Same repo, different URL.</p> <pre><code># Check current origin\ngit remote -v\n\n# Update the URL\ngit remote set-url origin https://gitlab.com/company/repo.git\n\n# Verify\ngit remote -v\n</code></pre> <p>Your local repo now points to the new location. No need to re-clone.</p>"},{"location":"Git/1.1_Git_Origin.html#scenario-3-you-have-no-origin","title":"Scenario 3: You Have No Origin","text":"<p>You ran <code>git init</code> locally. No remote. You want to add one.</p> <pre><code># Create repo on GitHub, then:\ngit remote add origin https://github.com/username/new-repo.git\n\n# Push\ngit push -u origin main\n</code></pre>"},{"location":"Git/1.1_Git_Origin.html#common-mistakes","title":"Common Mistakes","text":"<p>Don't Remove Origin Without a Backup</p> <p>If you remove <code>origin</code> and forget the URL, you can't push. Make sure you know the URL before removing.</p> <p>Wrong: <pre><code>git remote remove origin\n# Oh no, what was the URL?\n</code></pre></p> <p>Right: <pre><code># Save the URL first\ngit remote -v\n# Copy the URL\ngit remote remove origin\n# Now you can add it back if needed\n</code></pre></p> <p>Origin is Not Always Main Remote</p> <p>In forked repos, <code>upstream</code> might be more important than <code>origin</code>. Don't assume <code>origin</code> is the source of truth.</p>"},{"location":"Git/1.1_Git_Origin.html#best-practices","title":"Best Practices","text":"<p>Do This</p> <ul> <li>Always check remotes after cloning: <code>git remote -v</code></li> <li>Use <code>origin</code> for your primary remote</li> <li>Use <code>upstream</code> for the original repo in forks</li> <li>Name other remotes clearly: <code>production</code>, <code>staging</code>, etc.</li> <li>Verify URL before pushing to a new remote</li> </ul> <p>Quick Health Check</p> <p>When you start work in a repo: <pre><code>git remote -v\ngit status\ngit fetch origin\n</code></pre></p> <p>You'll know exactly where your code lives and if you're synced.</p>"},{"location":"Git/1.1_Git_Origin.html#whats-next","title":"What's Next?","text":"<p>You understand <code>origin</code>. It's just a name. A reference to your remote repository.</p> <p>Next up: branches. How local branches track remote branches. What <code>origin/main</code> means. How tracking works.</p> <p>But now you know: when you see <code>origin</code>, it's just pointing to GitHub (or wherever your code is stored).</p>"},{"location":"Git/1.2_Git_Fetch.html","title":"Understanding Git Fetch","text":"<p>You're working on your feature. You want to know what changed on remote. But you don't want to mess with your local files. You don't want to merge anything yet. Just check what's new.</p> <p>That's <code>git fetch</code>.</p> <p></p>"},{"location":"Git/1.2_Git_Fetch.html#what-git-fetch-does","title":"What Git Fetch Does","text":"<p>It downloads information from remote. New commits. New branches. New tags. Everything.- Retrieves the latest changes (commits, branches, tags) from a remote repository.</p> <p>But it doesn't change your files. Doesn't touch your working directory. Doesn't merge anything.- It\u2019s safe: you can fetch as often as you like without worrying about overwriting local changes.</p> <p>It downloads information from remote. New commits. New branches. New tags. Everything.</p> <ul> <li>Retrieves the latest changes (commits, branches, tags) from a remote repository.</li> <li>Updates your local metadata (remote tracking branches), but does not merge or apply these changes to your working branches.</li> <li>But it doesn't change your files. Doesn't touch your working directory. Doesn't merge anything.</li> <li>It\u2019s safe: you can fetch as often as you like without worrying about overwriting local changes.</li> </ul> <p>Think of it as reading the news. You're gathering information. Not taking action yet.&gt; Analogy: If <code>git pull</code> is like receiving new mail and reading it right away, <code>git fetch</code> is like receiving new mail and putting it aside to read later.</p>"},{"location":"Git/1.2_Git_Fetch.html#2-common-use-cases","title":"2. Common Use Cases","text":"<pre><code>flowchart LR\n    A[Remote Repository] --&gt;|git fetch| B[Local .git]\n    B -.-&gt;|No changes| C[Working Directory]\n    style A fill:#e1f5ff\n    style B fill:#e8f5e9\n    style C fill:#f0f0f0</code></pre> <ol> <li>Check for new commits on the remote before merging changes locally.</li> <li>Review differences between your local branch and the remote branch.</li> <li>Fetch updates from multiple remotes in complex setups (e.g., <code>origin</code> + <code>upstream</code>).</li> </ol> <p>Your working directory stays clean. You can review changes and decide what to do next.</p>"},{"location":"Git/1.2_Git_Fetch.html#3-basic-usage","title":"3. Basic Usage","text":""},{"location":"Git/1.2_Git_Fetch.html#31-fetch-from-the-default-remote","title":"3.1 Fetch from the Default Remote","text":"<p>This is what you'll use most of the time:</p> <p><pre><code>git fetch\n</code></pre> - Fetches updates from all configured remotes (commonly <code>origin</code>). - If you have only one remote, it\u2019s effectively the same as <code>git fetch origin</code>.</p>"},{"location":"Git/1.2_Git_Fetch.html#32-fetch-from-a-specific-remote","title":"3.2 Fetch from a Specific Remote","text":"<p>Fetches all branches from <code>origin</code>. Updates your local knowledge of remote branches.</p> <p><pre><code>git fetch origin\n</code></pre> - Fetches the latest updates only from the remote named <code>origin</code>. - Updates all branches in <code>origin</code> (e.g., <code>origin/main</code>, <code>origin/feature-xyz</code>). - Does not update local branches automatically.</p>"},{"location":"Git/1.2_Git_Fetch.html#33-fetch-only-a-specific-branch","title":"3.3 Fetch Only a Specific Branch","text":"<p>Fetch only the <code>main</code> branch from the remote named <code>origin</code>:</p> <p><pre><code>git fetch origin main\n</code></pre> - Fetches only the <code>main</code> branch from <code>origin</code>. - Updates only <code>origin/main</code>, without touching other branches.</p> <p>After fetching, check what's new:</p> <p><pre><code>git log main..origin/main --oneline\n</code></pre> This shows commits that exist on <code>origin/main</code> but not on your local <code>main</code>.</p>"},{"location":"Git/1.2_Git_Fetch.html#4-differences-at-a-glance","title":"4. Differences at a Glance","text":"Command Fetches Updates From Updates Which Branches? Use Case <code>git fetch</code> All remotes All remote tracking branches Check updates from all remotes <code>git fetch origin</code> <code>origin</code> only All branches in <code>origin</code> Fetch all branches from <code>origin</code> <code>git fetch origin main</code> <code>origin</code> only Only <code>origin/main</code> Quickly fetch updates just for the <code>main</code> branch"},{"location":"Git/1.2_Git_Fetch.html#fetch-vs-pull","title":"Fetch vs Pull","text":"<p>People confuse these. They're different.</p> <ul> <li><code>git fetch</code>: Good if you have multiple remotes or want to update all remote branches.</li> <li><code>git fetch origin</code>: Focuses on fetching all branches from <code>origin</code> only.</li> <li><code>git fetch origin main</code>: Ideal if you care only about the <code>main</code> branch.</li> </ul> <pre><code>graph TB\n    subgraph \"git fetch origin\"\n        A1[Remote] --&gt;|Download info| B1[.git updates]\n        B1 -.-&gt;|Files untouched| C1[Working Dir]\n    end\n    subgraph \"git pull origin main\"\n        A2[Remote] --&gt;|Download + Merge| B2[.git updates]\n        B2 --&gt;|Files changed| C2[Working Dir]\n    end\n    style B1 fill:#e8f5e9\n    style C1 fill:#f0f0f0\n    style B2 fill:#fff4e1\n    style C2 fill:#ffcdd2</code></pre> Command What it does Safe? <code>git fetch origin</code> Downloads info. Doesn't change files \u2705 Always safe <code>git pull origin main</code> Fetches AND merges. Changes files \u26a0\ufe0f Can cause conflicts <p>Best Practice</p> <p>Fetch first. Review changes. Then decide to pull or merge.</p>"},{"location":"Git/1.2_Git_Fetch.html#5-advanced-options","title":"5. Advanced Options","text":""},{"location":"Git/1.2_Git_Fetch.html#51-fetch-all-remotes","title":"5.1 Fetch All Remotes","text":"<p><pre><code>git fetch --all\n</code></pre> - Retrieves updates from every remote (e.g., <code>origin</code>, <code>upstream</code>).</p>"},{"location":"Git/1.2_Git_Fetch.html#52-prune-deleted-branches","title":"5.2 Prune Deleted Branches","text":"<p><pre><code>git fetch --prune\n</code></pre> - Removes remote-tracking branches in your local repo that no longer exist on the remote.</p> <p>Combine both:</p> <p><pre><code>git fetch --all --prune\n</code></pre> - Keeps your local copy of remote branches clean.</p>"},{"location":"Git/1.2_Git_Fetch.html#53-verbose-output","title":"5.3 Verbose Output","text":"<p><pre><code>git fetch --verbose\n</code></pre> - Shows detailed information about what\u2019s being fetched.</p>"},{"location":"Git/1.2_Git_Fetch.html#6-morning-routine","title":"6. Morning Routine","text":"<p>Here's how you'll actually use fetch:</p> <p><pre><code>git fetch --all --prune\ngit status\ngit log main..origin/main --oneline\n</code></pre> You start work. Check what changed overnight.</p> <p>Or use <code>git diff</code>:</p> <p><pre><code>git diff main origin/main\n</code></pre> - Shows exact code differences.</p>"},{"location":"Git/1.2_Git_Fetch.html#7-applying-fetched-changes","title":"7. Applying Fetched Changes","text":"<p>Since fetching alone doesn\u2019t modify your local branches, you can:</p> <ol> <li>Pull (merge automatically)     <pre><code>git pull origin main\n</code></pre></li> <li>Merge (manual merge)     <pre><code>git merge origin/main\n</code></pre></li> <li>Rebase <pre><code>git rebase origin/main\n</code></pre></li> </ol> <p>Now you're synced. Create your feature branch from updated <code>main</code>.</p>"},{"location":"Git/1.2_Git_Fetch.html#8-example-scenarios","title":"8. Example Scenarios","text":""},{"location":"Git/1.2_Git_Fetch.html#81-quick-check-before-pulling","title":"8.1 Quick Check Before Pulling","text":"<ol> <li>Fetch the latest changes:     <pre><code>git fetch origin\n</code></pre></li> <li>Inspect what\u2019s new:     <pre><code>git log main..origin/main --oneline\n</code></pre></li> <li>Pull if needed:     <pre><code>git pull origin main\n</code></pre></li> </ol> <p>Now your branch includes the latest from main. No surprises during code review.</p>"},{"location":"Git/1.2_Git_Fetch.html#82-multiple-remotes","title":"8.2 Multiple Remotes","text":"<p>You fork a repository, so you have: - <code>origin</code> \u2192 Your fork. - <code>upstream</code> \u2192 The original repo.</p> <p>Fetch updates from both:</p> <pre><code>git fetch origin\ngit fetch upstream\n</code></pre> <p>Merge changes from <code>upstream</code> if necessary.</p>"},{"location":"Git/1.2_Git_Fetch.html#83-checking-on-teammates-branch","title":"8.3 Checking on Teammate's Branch","text":"<p>Your teammate is working on <code>feature/payment</code>. You want to see their latest work.</p> <p><pre><code>git fetch origin feature/payment\ngit log origin/feature/payment --oneline -10\ngit diff main origin/feature/payment\n</code></pre> - Fetches only the specified branch from <code>origin</code>. - You didn't checkout anything. Didn't change your working directory. Just reviewed their work.</p>"},{"location":"Git/1.2_Git_Fetch.html#9-key-takeaways","title":"9. Key Takeaways","text":"<ul> <li><code>git fetch</code> is a safe way to update your local knowledge of remote branches without modifying your local branches.</li> <li>Use <code>git fetch --all --prune</code> to keep your local copy of remote branches clean.</li> <li>Use <code>git fetch origin &lt;branch&gt;</code> if you only need updates for a specific branch.</li> <li>If you want those fetched changes in your local branch, merge, pull, or rebase.</li> </ul>"},{"location":"Git/1.2_Git_Fetch.html#quick-command-reference","title":"Quick Command Reference","text":"Task Command Fetch all branches from origin <code>git fetch origin</code> Fetch specific branch <code>git fetch origin branch-name</code> Fetch from all remotes <code>git fetch --all</code> Check what's new on main <code>git log main..origin/main --oneline</code> See code differences <code>git diff main origin/main</code> Check if you're behind <code>git status</code> <p>Happy Fetching! \ud83d\ude80</p>"},{"location":"Git/1.2_Git_Fetch.html#advanced-usage","title":"Advanced Usage","text":"<p>Everything below is optional. But useful for complex scenarios.</p>"},{"location":"Git/1.2_Git_Fetch.html#fetch-commands-explained","title":"Fetch Commands Explained","text":"Different Ways to Fetch <p>Fetch everything</p> <pre><code>git fetch\n</code></pre> <p>Fetches from all configured remotes. If you only have <code>origin</code>, same as <code>git fetch origin</code>.</p> <p>Fetch from specific remote</p> <pre><code>git fetch origin\n</code></pre> <p>Fetches all branches from <code>origin</code>. This is what you'll use 90% of the time.</p> <p>Fetch specific branch</p> <pre><code>git fetch origin main\n</code></pre> <p>Only fetches the <code>main</code> branch. Faster if you only care about one branch.</p> <p>Fetch all remotes</p> <pre><code>git fetch --all\n</code></pre> <p>If you have multiple remotes (like <code>origin</code> and <code>upstream</code>), this fetches from all of them.</p> <p>Comparison:</p> Command Fetches From Updates <code>git fetch</code> All remotes All branches <code>git fetch origin</code> origin only All origin branches <code>git fetch origin main</code> origin only Only main <code>git fetch --all</code> All remotes All branches from all remotes"},{"location":"Git/1.2_Git_Fetch.html#cleaning-up-stale-branches","title":"Cleaning Up Stale Branches","text":"Using --prune to Remove Dead Branches <p>Your teammate deleted <code>feature/old-work</code> on remote. But your local git still tracks <code>origin/feature/old-work</code>.</p> <p>See stale branches:</p> <pre><code>git branch -r\n</code></pre> <p>You'll see branches that don't exist on remote anymore.</p> <p>Clean them up:</p> <pre><code>git fetch --prune\n</code></pre> <p>Or automatically prune every time:</p> <pre><code>git config --global fetch.prune true\n</code></pre> <p>Now every <code>git fetch</code> will remove stale branches.</p> <p>Combined command:</p> <pre><code>git fetch --all --prune\n</code></pre> <p>Fetches from all remotes AND removes dead branches. Clean and efficient.</p>"},{"location":"Git/1.2_Git_Fetch.html#working-with-multiple-remotes","title":"Working with Multiple Remotes","text":"Fetching from Upstream (Forked Repos) <p>You forked a repository. You have two remotes:</p> <ul> <li><code>origin</code> \u2192 Your fork on GitHub</li> <li><code>upstream</code> \u2192 The original repository</li> </ul> <p>Setup upstream (one time):</p> <pre><code>git remote add upstream https://github.com/original-owner/repo.git\n</code></pre> <p>Check your remotes:</p> <pre><code>git remote -v\n</code></pre> <p>Output:</p> <pre><code>origin    https://github.com/you/repo.git (fetch)\norigin    https://github.com/you/repo.git (push)\nupstream  https://github.com/original-owner/repo.git (fetch)\nupstream  https://github.com/original-owner/repo.git (push)\n</code></pre> <p>Fetch from both:</p> <pre><code>git fetch origin\ngit fetch upstream\n</code></pre> <p>Or fetch from all at once:</p> <pre><code>git fetch --all\n</code></pre> <p>Update your fork with upstream changes:</p> <pre><code># Fetch latest from upstream\ngit fetch upstream\n\n# Checkout your main\ngit checkout main\n\n# Merge upstream's main\ngit merge upstream/main\n\n# Push to your fork\ngit push origin main\n</code></pre> <p>Now your fork is synced with the original repo.</p>"},{"location":"Git/1.2_Git_Fetch.html#viewing-what-changed","title":"Viewing What Changed","text":"Inspecting Fetched Changes <p>You fetched. Now what? How do you see what changed?</p> <p>See commits on origin/main that you don't have:</p> <pre><code>git log main..origin/main --oneline\n</code></pre> <p>Output:</p> <pre><code>a3f2c1d Fix authentication bug\nb7e9f3a Add password validation\nc4d8e2f Update README\n</code></pre> <p>See commits you have that origin doesn't:</p> <pre><code>git log origin/main..main --oneline\n</code></pre> <p>If this shows commits, you're ahead of remote.</p> <p>See code differences:</p> <pre><code>git diff main origin/main\n</code></pre> <p>Shows actual line-by-line changes.</p> <p>See only which files changed:</p> <pre><code>git diff --name-only main origin/main\n</code></pre> <p>Output:</p> <pre><code>auth.js\nREADME.md\npackage.json\n</code></pre> <p>See commit graph:</p> <pre><code>git log --oneline --graph main origin/main\n</code></pre> <p>Visual representation of branch history.</p>"},{"location":"Git/1.2_Git_Fetch.html#fetching-tags","title":"Fetching Tags","text":"Working with Tags <p>Tags are version markers. Like <code>v1.0.0</code>, <code>v2.0.0</code>.</p> <p>Fetch all tags:</p> <pre><code>git fetch --tags\n</code></pre> <p>List all tags:</p> <pre><code>git tag\n</code></pre> <p>Checkout a specific version:</p> <pre><code>git checkout v1.0.0\n</code></pre> <p>You're now in \"detached HEAD\" state, viewing the code at version 1.0.0.</p> <p>Get back to your branch:</p> <pre><code>git checkout main\n</code></pre>"},{"location":"Git/1.2_Git_Fetch.html#verbose-output","title":"Verbose Output","text":"See What's Happening During Fetch <p>Want to see exactly what git is doing?</p> <pre><code>git fetch --verbose\n</code></pre> <p>Or even more detail:</p> <pre><code>git fetch --verbose --progress\n</code></pre> <p>Output:</p> <pre><code>remote: Counting objects: 25, done.\nremote: Compressing objects: 100% (15/15), done.\nremote: Total 25 (delta 10), reused 20 (delta 5)\nUnpacking objects: 100% (25/25), done.\nFrom https://github.com/username/repo\n   a3f2c1d..b7e9f3a  main       -&gt; origin/main\n   c4d8e2f..f1a5b9c  develop    -&gt; origin/develop\n</code></pre>"},{"location":"Git/1.2_Git_Fetch.html#real-world-scenarios","title":"Real-World Scenarios","text":""},{"location":"Git/1.2_Git_Fetch.html#scenario-1-before-starting-work","title":"Scenario 1: Before Starting Work","text":"<p>You're about to start a new feature. First, sync your knowledge.</p> <pre><code># Fetch latest info\ngit fetch origin\n\n# Check if main moved\ngit log main..origin/main --oneline\n</code></pre> <p>If main has new commits:</p> <pre><code>git checkout main\ngit pull origin main\ngit checkout -b feature/new-work\n</code></pre> <p>Now you're branching off the latest code.</p>"},{"location":"Git/1.2_Git_Fetch.html#scenario-2-during-code-review","title":"Scenario 2: During Code Review","text":"<p>Someone requested changes on your PR. Meanwhile, main moved ahead.</p> <pre><code># Fetch latest\ngit fetch origin\n\n# Update main\ngit checkout main\ngit pull origin main\n\n# Update your feature branch\ngit checkout feature/your-work\ngit merge main\n\n# Push updated branch\ngit push origin feature/your-work\n</code></pre>"},{"location":"Git/1.2_Git_Fetch.html#scenario-3-checking-teammates-work","title":"Scenario 3: Checking Teammate's Work","text":"<p>Your teammate asked you to review their branch before they open a PR.</p> <pre><code># Fetch their branch\ngit fetch origin feature/their-work\n\n# Compare to main\ngit log main..origin/feature/their-work --oneline\n\n# See the code diff\ngit diff main origin/feature/their-work\n</code></pre> <p>You reviewed without touching your working directory.</p>"},{"location":"Git/1.2_Git_Fetch.html#common-mistakes","title":"Common Mistakes","text":"<p>Don't Confuse Fetch and Pull</p> <p>Wrong: <pre><code># Thinking this will update your files\ngit fetch origin main\n# Your files didn't change!\n</code></pre></p> <p>Right: <pre><code># Fetch to check\ngit fetch origin main\n# Then pull to apply\ngit pull origin main\n</code></pre></p> <p>Remote Branches Are Read-Only</p> <p>You can't checkout and work on <code>origin/main</code> directly.</p> <p>Wrong: <pre><code>git checkout origin/main\n# You're in detached HEAD!\n</code></pre></p> <p>Right: <pre><code>git checkout main\ngit pull origin main\n# Now you're on your local main\n</code></pre></p>"},{"location":"Git/1.2_Git_Fetch.html#best-practices","title":"Best Practices","text":"<p>Do This</p> <ul> <li>Fetch before starting work</li> <li>Fetch before creating a PR</li> <li>Use <code>git fetch origin</code> as your default</li> <li>Review changes before pulling</li> <li>Set up auto-prune: <code>git config --global fetch.prune true</code></li> </ul> <p>Pro Workflow</p> <p>Make this your morning routine: <pre><code>git fetch --all --prune\ngit status\ngit log main..origin/main --oneline\n</code></pre></p> <p>You'll know exactly what changed overnight.</p>"},{"location":"Git/1.2_Git_Fetch.html#whats-next","title":"What's Next?","text":"<p>You understand fetch. You know how to check for changes without risking your local work.</p> <p>Next up: <code>git pull</code>, <code>git merge</code>, and <code>git rebase</code>. How to actually apply those fetched changes.</p> <p>But fetch is your safety net. Always fetch first. Review. Then act.</p>"},{"location":"Git/1.3_Git_Pull.html","title":"Understanding Git Pull","text":"<p>You're working on a project. Your teammate pushed changes. You need them. You type <code>git pull</code>.</p> <p>What just happened?</p> <p>Git pull does two things: fetch + merge. It grabs the latest code from the remote and merges it into your current branch.</p> <p>One command. Two operations.</p>"},{"location":"Git/1.3_Git_Pull.html#what-git-pull-does","title":"What Git Pull Does","text":"<pre><code>flowchart LR\n    A[Remote Repository] --&gt;|1. Fetch| B[Local Repository]\n    B --&gt;|2. Merge| C[Working Directory]\n\n    style A fill:#fff3e0\n    style B fill:#e1f5ff\n    style C fill:#e8f5e9</code></pre> <p>Step 1: Fetch - Download commits from remote Step 2: Merge - Combine them with your local branch</p> <p>It's <code>git fetch</code> + <code>git merge</code> in one command.</p>"},{"location":"Git/1.3_Git_Pull.html#the-daily-command","title":"The Daily Command","text":"<p>This is what you'll use most:</p> <pre><code>git pull\n</code></pre> <p>That's it. Git pulls from the current branch's remote tracking branch.</p> <p>You're on <code>main</code>, it pulls from <code>origin/main</code>. You're on <code>feature/login</code>, it pulls from <code>origin/feature/login</code>.</p> <p>More explicit version:</p> <pre><code>git pull origin main\n</code></pre> <p>This pulls from <code>origin/main</code> specifically. Use this when you want to be clear about what you're pulling.</p>"},{"location":"Git/1.3_Git_Pull.html#pull-vs-fetch","title":"Pull vs Fetch","text":"Command What It Does When to Use <code>git fetch</code> Downloads commits, doesn't merge When you want to check changes first <code>git pull</code> Downloads commits AND merges When you trust the changes and want them now <p>Think of it like this:</p> <ul> <li><code>git fetch</code> \u2192 \"Show me what's new\"</li> <li><code>git pull</code> \u2192 \"Give me what's new and merge it\"</li> </ul> <pre><code>flowchart TB\n    A[Remote has new commits] --&gt; B{What do you want?}\n    B --&gt;|Just look| C[git fetch]\n    B --&gt;|Get and merge| D[git pull]\n\n    C --&gt; E[Review changes]\n    E --&gt; F[Manually merge later]\n\n    D --&gt; G[Automatic merge]\n\n    style C fill:#e1f5ff\n    style D fill:#e8f5e9</code></pre>"},{"location":"Git/1.3_Git_Pull.html#common-workflows","title":"Common Workflows","text":""},{"location":"Git/1.3_Git_Pull.html#morning-routine","title":"Morning Routine","text":"<p>You start your day. You need the latest code.</p> <pre><code>git checkout main\ngit pull\n</code></pre> <p>Now you're synced. Everyone's changes from yesterday are in your local repo.</p>"},{"location":"Git/1.3_Git_Pull.html#before-starting-new-work","title":"Before Starting New Work","text":"<p>You're about to create a feature branch. Make sure main is up to date first.</p> <pre><code>git checkout main\ngit pull\ngit checkout -b feature/new-thing\n</code></pre> <p>You branched from the latest code. No surprises later.</p>"},{"location":"Git/1.3_Git_Pull.html#updating-your-feature-branch","title":"Updating Your Feature Branch","text":"<p>You've been working for days. Main has moved forward. You need those changes.</p> <pre><code># You're on feature/login\ngit checkout main\ngit pull\ngit checkout feature/login\ngit merge main\n</code></pre> <p>Or use rebase (cleaner history):</p> <pre><code>git checkout main\ngit pull\ngit checkout feature/login\ngit rebase main\n</code></pre>"},{"location":"Git/1.3_Git_Pull.html#quick-update-while-working","title":"Quick Update While Working","text":"<p>You're on a feature branch. You want the latest from that branch's remote.</p> <pre><code>git pull\n</code></pre> <p>Git automatically pulls from the tracking branch.</p>"},{"location":"Git/1.3_Git_Pull.html#quick-command-reference","title":"Quick Command Reference","text":"Task Command Pull from current tracking branch <code>git pull</code> Pull from specific remote/branch <code>git pull origin main</code> Pull with rebase instead of merge <code>git pull --rebase</code> Pull all branches <code>git pull --all</code> Pull and don't commit merge <code>git pull --no-commit</code> Pull and show what changed <code>git pull -v</code>"},{"location":"Git/1.3_Git_Pull.html#advanced-usage","title":"Advanced Usage","text":"<p>Everything below is for specific situations.</p> Pull with Rebase (Cleaner History) <p>Normal <code>git pull</code> creates a merge commit. It's messy.</p> <p>Standard pull: <pre><code>git pull\n</code></pre></p> <p>Creates: <pre><code>*   Merge branch 'main' of origin into main\n|\\  \n| * Commit from teammate\n* | Your local commit\n</code></pre></p> <p>Pull with rebase: <pre><code>git pull --rebase\n</code></pre></p> <p>Creates: <pre><code>* Your local commit (moved on top)\n* Commit from teammate\n</code></pre></p> <p>Linear history. No merge commit. Cleaner.</p> <p>Make it default: <pre><code>git config --global pull.rebase true\n</code></pre></p> <p>Now <code>git pull</code> will always rebase instead of merge.</p> Pull from Different Remote <p>You have multiple remotes. You want to pull from a specific one.</p> <pre><code># Pull from upstream instead of origin\ngit pull upstream main\n</code></pre> <p>Or pull from a teammate's fork: <pre><code>git remote add teammate https://github.com/teammate/repo.git\ngit pull teammate feature/their-work\n</code></pre></p> Pull All Remotes <p>You have multiple remotes configured. You want to fetch all of them.</p> <pre><code>git pull --all\n</code></pre> <p>This fetches from all remotes but only merges the current branch's tracking branch.</p> <p>More control: <pre><code># Fetch from all remotes\ngit fetch --all\n\n# Then merge specific branch\ngit merge origin/main\n</code></pre></p> Pull Without Auto-Merge <p>You want to pull but review changes before merging.</p> <pre><code>git pull --no-commit\n</code></pre> <p>Git fetches and attempts to merge but doesn't commit. You can review and then:</p> <pre><code># Accept the merge\ngit commit\n\n# Or abort\ngit merge --abort\n</code></pre> Pull and Overwrite Local Changes (Force) <p>You messed up. You want to throw away local changes and pull fresh.</p> <p>This Deletes Your Local Work</p> <p>Make sure you don't need your local changes.</p> <pre><code># Save the branch name\ngit fetch origin\ngit reset --hard origin/main\n</code></pre> <p>Your local <code>main</code> now matches <code>origin/main</code> exactly. Local changes are gone.</p> <p>Less destructive version: <pre><code># Stash your changes first\ngit stash\ngit pull\n# Decide if you want them back\ngit stash pop\n</code></pre></p> Pull Specific Files <p>You don't want to pull everything. Just specific files.</p> <p>Can't do it with pull. But you can do it with fetch + checkout:</p> <pre><code>git fetch origin\ngit checkout origin/main -- path/to/file.txt\n</code></pre> <p>This updates only that file from the remote.</p>"},{"location":"Git/1.3_Git_Pull.html#real-world-scenarios","title":"Real-World Scenarios","text":""},{"location":"Git/1.3_Git_Pull.html#scenario-1-merge-conflict-during-pull","title":"Scenario 1: Merge Conflict During Pull","text":"<p>You pull. Git says there's a conflict.</p> <pre><code>git pull\n</code></pre> <p>Output: <pre><code>Auto-merging src/app.js\nCONFLICT (content): Merge conflict in src/app.js\nAutomatic merge failed; fix conflicts and then commit the result.\n</code></pre></p> <p>What to do:</p> <ol> <li>Open the conflicted file</li> <li>Look for conflict markers:</li> </ol> <pre><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n// Your code\nfunction login() { }\n=======\n// Their code\nfunction authenticate() { }\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; origin/main\n</code></pre> <ol> <li>Fix it manually:</li> </ol> <pre><code>function authenticate() { }\n</code></pre> <ol> <li>Stage and commit:</li> </ol> <pre><code>git add src/app.js\ngit commit\n</code></pre> <p>Done. Conflict resolved.</p>"},{"location":"Git/1.3_Git_Pull.html#scenario-2-you-have-uncommitted-changes","title":"Scenario 2: You Have Uncommitted Changes","text":"<p>You try to pull but Git blocks you:</p> <pre><code>git pull\n</code></pre> <p>Output: <pre><code>error: Your local changes to the following files would be overwritten by merge:\n    src/app.js\nPlease commit your changes or stash them before you merge.\n</code></pre></p> <p>Option 1: Commit your changes</p> <pre><code>git add .\ngit commit -m \"WIP: working on feature\"\ngit pull\n</code></pre> <p>Option 2: Stash your changes</p> <pre><code>git stash\ngit pull\ngit stash pop\n</code></pre> <p>Your changes are back. If there's a conflict, fix it manually.</p>"},{"location":"Git/1.3_Git_Pull.html#scenario-3-pull-failed-midway","title":"Scenario 3: Pull Failed Midway","text":"<p>The pull started but failed. Now you're stuck.</p> <pre><code>git status\n</code></pre> <p>Output: <pre><code>You have unmerged paths.\n</code></pre></p> <p>Abort the merge:</p> <pre><code>git merge --abort\n</code></pre> <p>Back to where you started. Try again or fix the issue.</p>"},{"location":"Git/1.3_Git_Pull.html#common-mistakes","title":"Common Mistakes","text":"<p>Pulling Without Committing Local Work</p> <p>Wrong: <pre><code># You have changes\ngit pull  # Error!\n</code></pre></p> <p>Right: <pre><code># Commit first\ngit add .\ngit commit -m \"Save work\"\ngit pull\n</code></pre></p> <p>Or stash: <pre><code>git stash\ngit pull\ngit stash pop\n</code></pre></p> <p>Pulling the Wrong Branch</p> <p>You're on <code>main</code> but you pull from <code>develop</code>: <pre><code>git checkout main\ngit pull origin develop  # Wrong!\n</code></pre></p> <p>Now <code>main</code> has <code>develop</code> code. Fix it: <pre><code>git reset --hard origin/main\n</code></pre></p> <p>Pulling Without Checking Current Branch</p> <pre><code>git pull  # Which branch am I on?\n</code></pre> <p>Always check first: <pre><code>git branch  # Shows current branch\ngit pull\n</code></pre></p>"},{"location":"Git/1.3_Git_Pull.html#best-practices","title":"Best Practices","text":"<p>Do This</p> <ul> <li>Check your branch before pulling: <code>git status</code></li> <li>Commit or stash changes before pulling</li> <li>Pull from <code>main</code> before starting new work</li> <li>Use <code>git pull --rebase</code> for cleaner history</li> <li>Review merge conflicts carefully</li> </ul> <p>Daily Workflow</p> <p>Start every work session: <pre><code>git checkout main\ngit pull\ngit checkout feature/your-branch\ngit merge main  # Or rebase\n</code></pre></p> <p>You're synced. No surprises.</p>"},{"location":"Git/1.3_Git_Pull.html#fast-forward-vs-merge","title":"Fast-Forward vs Merge","text":"<p>When you pull, Git might \"fast-forward\" or create a merge commit.</p> <p>Fast-forward (no merge commit):</p> <pre><code>Before pull:\n* Your local main\n|\n* Commit A\n\nAfter pull (fast-forward):\n* Remote commit C\n* Remote commit B\n* Commit A\n</code></pre> <p>Your branch just moves forward. No merge needed.</p> <p>Merge commit (you had local commits):</p> <pre><code>Before pull:\n* Your local commit\n|\n* Commit A\n\nAfter pull:\n*   Merge commit\n|\\\n| * Remote commit\n* | Your local commit\n|/\n* Commit A\n</code></pre> <p>Git creates a merge commit to combine both histories.</p>"},{"location":"Git/1.3_Git_Pull.html#pull-vs-clone","title":"Pull vs Clone","text":"Command What It Does When to Use <code>git clone</code> Copy entire repository for first time When you don't have the repo yet <code>git pull</code> Update existing repository When you already have the repo <p>You clone once. You pull many times.</p>"},{"location":"Git/1.3_Git_Pull.html#whats-next","title":"What's Next?","text":"<p>You understand <code>git pull</code>. It's fetch + merge in one command.</p> <p>Next up: <code>git push</code>. How to send your changes to the remote. What happens when you push. When to force push (and when not to).</p> <p>But now you know: <code>git pull</code> gets the latest code and merges it into your branch.</p>"},{"location":"Git/1.4_Git_Push.html","title":"Understanding Git Push","text":"<p>You made changes. You committed them. Now you want them on GitHub. You type <code>git push</code>.</p> <p>Your local commits go to the remote repository. Your teammates can see them. CI/CD can deploy them.</p> <p>One command. Your code is shared.</p>"},{"location":"Git/1.4_Git_Push.html#what-git-push-does","title":"What Git Push Does","text":"<pre><code>flowchart LR\n    A[Working Directory] --&gt;|git commit| B[Local Repository]\n    B --&gt;|git push| C[Remote Repository]\n\n    style A fill:#e8f5e9\n    style B fill:#e1f5ff\n    style C fill:#fff3e0</code></pre> <p>Step 1: You commit locally Step 2: You push to remote Result: Remote repository has your commits</p> <p>Push moves commits from your machine to GitHub (or GitLab, Bitbucket, etc.).</p>"},{"location":"Git/1.4_Git_Push.html#the-daily-command","title":"The Daily Command","text":"<p>This is what you'll use most:</p> <pre><code>git push\n</code></pre> <p>That's it. Git pushes to the current branch's remote tracking branch.</p> <p>You're on <code>main</code>, it pushes to <code>origin/main</code>. You're on <code>feature/login</code>, it pushes to <code>origin/feature/login</code>.</p> <p>First time pushing a new branch:</p> <pre><code>git push -u origin feature/new-branch\n</code></pre> <p>The <code>-u</code> sets up tracking. After this, just use <code>git push</code>.</p> <p>More explicit version:</p> <pre><code>git push origin main\n</code></pre> <p>This pushes to <code>origin/main</code> specifically.</p>"},{"location":"Git/1.4_Git_Push.html#push-workflow","title":"Push Workflow","text":""},{"location":"Git/1.4_Git_Push.html#creating-and-pushing-a-feature-branch","title":"Creating and Pushing a Feature Branch","text":"<p>You create a branch. You make commits. You push.</p> <pre><code># Create branch\ngit checkout -b feature/login\n\n# Make changes\ngit add .\ngit commit -m \"Add login feature\"\n\n# Push (first time)\ngit push -u origin feature/login\n</code></pre> <p>Now <code>feature/login</code> exists on GitHub. Your teammates can see it.</p> <p>Subsequent pushes:</p> <pre><code># Make more changes\ngit add .\ngit commit -m \"Fix login bug\"\n\n# Push (no -u needed)\ngit push\n</code></pre>"},{"location":"Git/1.4_Git_Push.html#pushing-to-main","title":"Pushing to Main","text":"<p>You're on main. You made commits. You push.</p> <pre><code>git checkout main\ngit add .\ngit commit -m \"Update README\"\ngit push\n</code></pre> <p>Done. Main on GitHub has your changes.</p>"},{"location":"Git/1.4_Git_Push.html#pushing-after-merging","title":"Pushing After Merging","text":"<p>You merged a feature branch into main. Now push.</p> <pre><code>git checkout main\ngit merge feature/login\ngit push\n</code></pre> <p>The merge commit goes to GitHub.</p>"},{"location":"Git/1.4_Git_Push.html#quick-command-reference","title":"Quick Command Reference","text":"Task Command Push current branch <code>git push</code> Push and set upstream (first time) <code>git push -u origin branch-name</code> Push to specific remote/branch <code>git push origin main</code> Push all branches <code>git push --all</code> Push tags <code>git push --tags</code> Force push (dangerous) <code>git push --force</code> Force push (safer) <code>git push --force-with-lease</code> Delete remote branch <code>git push origin --delete branch-name</code>"},{"location":"Git/1.4_Git_Push.html#advanced-usage","title":"Advanced Usage","text":"<p>Everything below is for specific situations.</p> Force Push (When You Rewrote History) <p>You rebased. You amended. You changed commits. Now push fails:</p> <pre><code>git push\n</code></pre> <p>Output: <pre><code>! [rejected]        main -&gt; main (non-fast-forward)\nerror: failed to push some refs\n</code></pre></p> <p>Git won't let you push because histories diverged.</p> <p>Force Push Overwrites Remote History</p> <p>This deletes commits on the remote. Make sure you know what you're doing.</p> <p>Option 1: Force push (dangerous) <pre><code>git push --force\n</code></pre></p> <p>Remote branch is now updated. Any commits that were there are gone.</p> <p>Option 2: Force with lease (safer) <pre><code>git push --force-with-lease\n</code></pre></p> <p>This fails if someone else pushed after your last fetch. Safer.</p> <p>When to force push: - Working on your own feature branch - After rebasing or amending commits - After squashing commits</p> <p>When NOT to force push: - On <code>main</code> or <code>develop</code> (shared branches) - On branches others are working on - If you're not sure what you're doing</p> Pushing a New Branch <p>You created a branch locally. It doesn't exist on remote.</p> <pre><code>git checkout -b feature/new-thing\ngit add .\ngit commit -m \"Start new feature\"\n</code></pre> <p>Push it: <pre><code>git push -u origin feature/new-thing\n</code></pre></p> <p>The <code>-u</code> (or <code>--set-upstream</code>) creates the branch on remote and sets up tracking.</p> <p>After this, just use: <pre><code>git push\n</code></pre></p> Push All Branches <p>You have multiple local branches. You want to push all of them.</p> <pre><code>git push --all\n</code></pre> <p>Every local branch goes to the remote.</p> <p>Be careful: This includes old branches you might not want to share.</p> Push Tags <p>You created tags. They don't auto-push with commits.</p> <pre><code># Create a tag\ngit tag v1.0.0\n\n# Push it\ngit push origin v1.0.0\n</code></pre> <p>Push all tags: <pre><code>git push --tags\n</code></pre></p> Delete Remote Branch <p>You merged a feature branch. You want to delete it from GitHub.</p> <pre><code>git push origin --delete feature/old-branch\n</code></pre> <p>The remote branch is gone. Local branch still exists:</p> <pre><code># Delete local too\ngit branch -d feature/old-branch\n</code></pre> Push to Different Remote <p>You have multiple remotes. You want to push to a specific one.</p> <pre><code># Push to origin\ngit push origin main\n\n# Push to upstream\ngit push upstream main\n\n# Push to production server\ngit push production main\n</code></pre> Push Without Specifying Branch (Dangerous) <p>Old Git versions had dangerous default behavior. They pushed ALL matching branches.</p> <p>Check your config: <pre><code>git config --global push.default\n</code></pre></p> <p>Should be: <pre><code>simple  # or current\n</code></pre></p> <p>If it's not set: <pre><code>git config --global push.default simple\n</code></pre></p> <p>Now <code>git push</code> only pushes the current branch.</p> Dry Run (See What Would Be Pushed) <p>You want to check before pushing.</p> <pre><code>git push --dry-run\n</code></pre> <p>Output shows what would happen. Nothing actually pushed.</p>"},{"location":"Git/1.4_Git_Push.html#real-world-scenarios","title":"Real-World Scenarios","text":""},{"location":"Git/1.4_Git_Push.html#scenario-1-push-rejected-someone-else-pushed","title":"Scenario 1: Push Rejected (Someone Else Pushed)","text":"<p>You try to push. Git blocks you:</p> <pre><code>git push\n</code></pre> <p>Output: <pre><code>! [rejected]        main -&gt; main (fetch first)\nerror: failed to push some refs\nhint: Updates were rejected because the remote contains work that you do not have locally.\n</code></pre></p> <p>What happened: Someone pushed while you were working.</p> <p>Fix it:</p> <pre><code># Pull their changes\ngit pull\n\n# If there's a conflict, fix it\n# Then push\ngit push\n</code></pre> <p>Or use rebase for cleaner history:</p> <pre><code>git pull --rebase\ngit push\n</code></pre>"},{"location":"Git/1.4_Git_Push.html#scenario-2-pushed-to-wrong-branch","title":"Scenario 2: Pushed to Wrong Branch","text":"<p>You're on <code>develop</code> but you thought you were on <code>main</code>. You pushed.</p> <pre><code>git branch  # Oh no, I'm on develop!\n</code></pre> <p>Fix it:</p> <p>If you pushed to the wrong branch but haven't messed up anything yet:</p> <pre><code># Go to correct branch\ngit checkout main\n\n# Cherry-pick the commits\ngit cherry-pick &lt;commit-hash&gt;\n\n# Push to correct branch\ngit push\n\n# Remove from wrong branch (if needed)\ngit checkout develop\ngit reset --hard origin/develop\ngit push --force\n</code></pre>"},{"location":"Git/1.4_Git_Push.html#scenario-3-accidentally-pushed-sensitive-data","title":"Scenario 3: Accidentally Pushed Sensitive Data","text":"<p>You committed an API key. You pushed it. Now it's on GitHub.</p> <p>Act Fast</p> <p>Secrets on GitHub are compromised. Rotate them immediately.</p> <p>Remove it:</p> <pre><code># Remove from history (using BFG or git filter-branch)\n# This is complex, see GitHub docs\n\n# Or revert the commit and push\ngit revert &lt;commit-hash&gt;\ngit push\n</code></pre> <p>Then: - Rotate the secret - Use environment variables - Add secrets to <code>.gitignore</code></p>"},{"location":"Git/1.4_Git_Push.html#scenario-4-push-keeps-failing","title":"Scenario 4: Push Keeps Failing","text":"<p>Every push fails with authentication errors.</p> <pre><code>git push\n</code></pre> <p>Output: <pre><code>fatal: Authentication failed\n</code></pre></p> <p>Fix it:</p> <p>HTTPS with token: <pre><code>git remote -v  # Check URL\ngit remote set-url origin https://&lt;TOKEN&gt;@github.com/user/repo.git\n</code></pre></p> <p>Switch to SSH: <pre><code>git remote set-url origin git@github.com:user/repo.git\n</code></pre></p> <p>Make sure your SSH keys are set up.</p>"},{"location":"Git/1.4_Git_Push.html#common-mistakes","title":"Common Mistakes","text":"<p>Force Pushing to Shared Branches</p> <p>Never do this: <pre><code>git checkout main\ngit push --force  # NEVER on main!\n</code></pre></p> <p>You just deleted commits your teammates might have based their work on. You'll break their repos.</p> <p>Only force push to: - Your own feature branches - Branches you're the only one working on</p> <p>Pushing Without Pulling First</p> <p>You have old code. You push. It fails.</p> <pre><code># Pull first\ngit pull\n\n# Then push\ngit push\n</code></pre> <p>Pushing Unfinished Work to Main</p> <p>Don't push broken code to main. Use feature branches.</p> <p>Wrong: <pre><code>git checkout main\ngit commit -m \"WIP half-done feature\"\ngit push  # Breaks main!\n</code></pre></p> <p>Right: <pre><code>git checkout -b feature/wip\ngit commit -m \"WIP half-done feature\"\ngit push -u origin feature/wip\n</code></pre></p>"},{"location":"Git/1.4_Git_Push.html#best-practices","title":"Best Practices","text":"<p>Do This</p> <ul> <li>Pull before pushing: <code>git pull</code></li> <li>Push feature branches, not directly to main</li> <li>Write clear commit messages before pushing</li> <li>Use <code>--force-with-lease</code> instead of <code>--force</code></li> <li>Check what branch you're on: <code>git branch</code></li> <li>Never force push to shared branches</li> </ul> <p>Before Pushing Checklist</p> <pre><code># 1. Check your branch\ngit branch\n\n# 2. Check what you're pushing\ngit log origin/main..HEAD\n\n# 3. Pull first\ngit pull\n\n# 4. Push\ngit push\n</code></pre>"},{"location":"Git/1.4_Git_Push.html#push-vs-pull","title":"Push vs Pull","text":"Command Direction What It Does <code>git pull</code> Remote \u2192 Local Get changes from remote <code>git push</code> Local \u2192 Remote Send changes to remote <p>You pull to get updates. You push to share your work.</p>"},{"location":"Git/1.4_Git_Push.html#protected-branches","title":"Protected Branches","text":"<p>Some branches can't be pushed to directly. GitHub blocks you:</p> <pre><code>git push origin main\n</code></pre> <p>Output: <pre><code>remote: error: GH006: Protected branch update failed\n</code></pre></p> <p>What this means: Main is protected. You need a pull request.</p> <p>Fix it: 1. Push to a feature branch 2. Open a pull request on GitHub 3. Get it reviewed 4. Merge it</p> <pre><code>git checkout -b feature/update\ngit push -u origin feature/update\n# Now open PR on GitHub\n</code></pre>"},{"location":"Git/1.4_Git_Push.html#upstream-tracking","title":"Upstream Tracking","text":"<p>When you push with <code>-u</code>, Git sets up tracking:</p> <pre><code>git push -u origin feature/login\n</code></pre> <p>Now Git knows: - <code>feature/login</code> (local) \u2192 <code>origin/feature/login</code> (remote)</p> <p>Check tracking:</p> <pre><code>git branch -vv\n</code></pre> <p>Output: <pre><code>* feature/login  abc123 [origin/feature/login] Latest commit\n  main           def456 [origin/main] Another commit\n</code></pre></p> <p>The <code>[origin/feature/login]</code> shows tracking.</p> <p>Why it matters: After setting upstream, just use <code>git push</code> and <code>git pull</code>.</p>"},{"location":"Git/1.4_Git_Push.html#whats-next","title":"What's Next?","text":"<p>You understand <code>git push</code>. It sends your commits to the remote repository.</p> <p>Next up: branches. Remote tracking branches. What <code>origin/main</code> means. How to work with remote branches effectively.</p> <p>But now you know: <code>git push</code> shares your work. <code>git pull</code> gets others' work. That's collaboration.</p>"},{"location":"Git/1.5_Git_Log_Online.html","title":"Understanding Git Log --oneline","text":"<p>You type <code>git log --oneline</code>. You see a wall of text. Hashes. Branch names. Messages.</p> <p>What does it all mean?</p> <p>Let's decode it.</p>"},{"location":"Git/1.5_Git_Log_Online.html#the-command","title":"The Command","text":"<pre><code>git log --oneline\n</code></pre> <p>This shows your commit history. One line per commit. Compact. Easy to scan.</p> <p>Example output:</p> <pre><code>7d6850f (HEAD -&gt; feature/jun7_2, origin/feature/jun7_2) changes\na3b2c1d (origin/main, origin/HEAD, main) Update README\n5e4f3g2 Add login feature\n</code></pre> <p>Each line tells a story. Let's break it down.</p>"},{"location":"Git/1.5_Git_Log_Online.html#decoding-a-single-line","title":"Decoding a Single Line","text":"<p>Take this line:</p> <pre><code>7d6850f (HEAD -&gt; feature/jun7_2, origin/feature/jun7_2) changes\n</code></pre> <p>Three parts. Each part tells you something important.</p> <pre><code>flowchart LR\n    A[7d6850f] --&gt; B[Commit Hash]\n    C[HEAD \u2192 feature/jun7_2, origin/feature/jun7_2] --&gt; D[Pointers/Refs]\n    E[changes] --&gt; F[Commit Message]\n\n    style B fill:#e1f5ff\n    style D fill:#fff3e0\n    style F fill:#e8f5e9</code></pre>"},{"location":"Git/1.5_Git_Log_Online.html#part-1-the-commit-hash","title":"Part 1: The Commit Hash","text":"<p><code>7d6850f</code></p> <p>This is the commit ID. A unique identifier for this specific snapshot of your code.</p> <p>Think of it as a serial number. Every commit gets one. It's permanent. It never changes.</p> <p>Full hash vs short hash:</p> <pre><code># Full hash (40 characters)\n7d6850f2a3b4c5d6e7f8g9h0i1j2k3l4m5n6o7p8\n\n# Short hash (7 characters, shown in --oneline)\n7d6850f\n</code></pre> <p>Git shows the short version. It's usually enough to identify a commit.</p> <p>Why it matters:</p> <p>You use this hash to refer to specific commits:</p> <pre><code># Checkout this commit\ngit checkout 7d6850f\n\n# Show what changed in this commit\ngit show 7d6850f\n\n# Cherry-pick this commit\ngit cherry-pick 7d6850f\n</code></pre>"},{"location":"Git/1.5_Git_Log_Online.html#part-2-the-pointers-the-important-stuff","title":"Part 2: The Pointers (The Important Stuff)","text":"<p><code>(HEAD -&gt; feature/jun7_2, origin/feature/jun7_2)</code></p> <p>This is where the magic happens. These are pointers (also called refs). They're labels pointing to this commit.</p> <p>Let's decode each one:</p> HEAD - Where You Are Right Now <p><code>HEAD</code> is special. It tells you: \"This is where you're working.\"</p> <p>It's your current position in the repository. Your working directory matches this commit.</p> <p>Think of it like this: - You're reading a book - <code>HEAD</code> is your bookmark - It shows which page (commit) you're on</p> <p>When you make a new commit: - It gets added after HEAD - HEAD moves to the new commit - The branch pointer moves too</p> HEAD -&gt; feature/jun7_2 - You're On This Branch <p>The arrow <code>-&gt;</code> is critical. It means:</p> <p>HEAD is pointing to the branch <code>feature/jun7_2</code>, not directly to the commit.</p> <p>What this tells you: - You have <code>feature/jun7_2</code> checked out - You're working on this branch - New commits will go on this branch - The branch pointer will move with HEAD</p> <p>Visual: <pre><code>HEAD \u2192 feature/jun7_2 \u2192 7d6850f (commit)\n</code></pre></p> <p>HEAD points to the branch. The branch points to the commit.</p> feature/jun7_2 - Your Local Branch <p>This is a local branch. It exists on your computer.</p> <p>No prefix like <code>origin/</code>. Just the branch name. That means it's local.</p> <p>Local branch: - Lives on your machine - You can commit to it - You can change it - You can delete it</p> <p>Right now, this branch points to commit <code>7d6850f</code>.</p> origin/feature/jun7_2 - The Remote's State <p>This is a remote-tracking branch.</p> <p>It's your local copy of where the branch is on the remote server (GitHub).</p> <p>Breaking it down: - <code>origin</code> \u2192 The remote server (GitHub, GitLab, etc.) - <code>feature/jun7_2</code> \u2192 The branch name on that server - Together: \"The state of <code>feature/jun7_2</code> on origin\"</p> <p>Important: This is read-only. You don't commit to it directly.</p> <p>It updates when you: - <code>git fetch</code> \u2192 Updates remote-tracking branches - <code>git pull</code> \u2192 Fetch + merge - <code>git push</code> \u2192 After pushing, fetch updates it</p> <p>It tells you: \"This is where the remote branch was last time I checked.\"</p>"},{"location":"Git/1.5_Git_Log_Online.html#part-3-the-commit-message","title":"Part 3: The Commit Message","text":"<p><code>changes</code></p> <p>This is what you typed when you committed:</p> <pre><code>git commit -m \"changes\"\n</code></pre> <p>It's supposed to explain what this commit does. (Though \"changes\" is a terrible message. Be more specific.)</p> <p>Better messages:</p> <pre><code>git commit -m \"Add login feature\"\ngit commit -m \"Fix null pointer in user service\"\ngit commit -m \"Update README with installation steps\"\n</code></pre>"},{"location":"Git/1.5_Git_Log_Online.html#what-this-line-really-tells-you","title":"What This Line Really Tells You","text":"<p>Let's put it all together:</p> <pre><code>7d6850f (HEAD -&gt; feature/jun7_2, origin/feature/jun7_2) changes\n</code></pre> <p>Translation:</p> <p>You're currently on the local branch <code>feature/jun7_2</code>. This branch points to commit <code>7d6850f</code>. The remote-tracking branch <code>origin/feature/jun7_2</code> also points to the same commit. The commit message is \"changes\".</p> <p>The key insight:</p> <p>Your local branch and the remote branch are in sync. They're pointing to the same commit.</p> <p>What this means:</p> <ul> <li>\u2705 No unpushed commits (local is not ahead)</li> <li>\u2705 No unpulled commits (remote is not ahead)</li> <li>\u2705 You're synced with the remote</li> </ul>"},{"location":"Git/1.5_Git_Log_Online.html#common-patterns","title":"Common Patterns","text":""},{"location":"Git/1.5_Git_Log_Online.html#pattern-1-local-is-ahead","title":"Pattern 1: Local Is Ahead","text":"<pre><code>c5d4e3f (HEAD -&gt; feature/login) Add password hashing\nb4a3c2d (origin/feature/login) Add login form\n</code></pre> <p>What this means: - Your local branch has commit <code>c5d4e3f</code> - Remote branch is still at <code>b4a3c2d</code> - You have unpushed commits</p> <p>Fix it: <pre><code>git push\n</code></pre></p>"},{"location":"Git/1.5_Git_Log_Online.html#pattern-2-remote-is-ahead","title":"Pattern 2: Remote Is Ahead","text":"<pre><code>b4a3c2d (HEAD -&gt; feature/login) Add login form\nc5d4e3f (origin/feature/login) Add password hashing\n</code></pre> <p>This won't happen with <code>git log --oneline</code> because it shows YOUR history. But <code>git log --oneline --all</code> would show it.</p> <p>What this means: - Remote has commits you don't have - You need to pull</p> <p>Fix it: <pre><code>git pull\n</code></pre></p>"},{"location":"Git/1.5_Git_Log_Online.html#pattern-3-youre-on-main","title":"Pattern 3: You're On Main","text":"<pre><code>a3b2c1d (HEAD -&gt; main, origin/main, origin/HEAD) Update README\n</code></pre> <p>What this means: - You're on main - Local and remote main are synced - <code>origin/HEAD</code> points here too (default branch on remote)</p>"},{"location":"Git/1.5_Git_Log_Online.html#pattern-4-detached-head","title":"Pattern 4: Detached HEAD","text":"<pre><code>7d6850f (HEAD, origin/feature/jun7_2) changes\n</code></pre> <p>Notice: <code>HEAD</code> is there, but no arrow <code>-&gt;</code> to a branch.</p> <p>What this means: - You're in \"detached HEAD\" state - You're looking at a commit directly, not through a branch - If you make commits, they'll be orphaned</p> <p>Fix it: <pre><code># Create a branch here\ngit checkout -b new-branch-name\n\n# Or go back to a branch\ngit checkout main\n</code></pre></p>"},{"location":"Git/1.5_Git_Log_Online.html#quick-reference","title":"Quick Reference","text":"Output What It Means <code>7d6850f</code> Commit hash (short version) <code>HEAD</code> Your current position <code>HEAD -&gt; branch</code> You're on this branch <code>branch</code> (no prefix) Local branch <code>origin/branch</code> Remote-tracking branch <code>origin/HEAD</code> Default branch on remote Message text Commit message"},{"location":"Git/1.5_Git_Log_Online.html#useful-variations","title":"Useful Variations","text":"Show More Commits <p>By default, <code>git log</code> shows a lot. You can limit it:</p> <pre><code># Show last 5 commits\ngit log --oneline -5\n\n# Show last 10 commits\ngit log --oneline -10\n</code></pre> Show All Branches <p>See commits from all branches, not just current:</p> <pre><code>git log --oneline --all\n</code></pre> <p>Now you see commits from every branch.</p> Show Graph <p>Visualize branch structure:</p> <pre><code>git log --oneline --graph\n</code></pre> <p>Output: <pre><code>* 7d6850f (HEAD -&gt; feature/login) Add login\n* 5e4f3g2 Start login feature\n| * a3b2c1d (main) Update README\n|/\n* 2d1c3b4 Initial commit\n</code></pre></p> <p>The lines show branch relationships.</p> Show All Branches with Graph <p>The ultimate commit history view:</p> <pre><code>git log --oneline --graph --all --decorate\n</code></pre> <p>Shows: - All branches - Graph visualization - All decorations (branch names, HEAD, etc.)</p> <p>Make it an alias: <pre><code>git config --global alias.lg \"log --oneline --graph --all --decorate\"\n</code></pre></p> <p>Now just type: <pre><code>git lg\n</code></pre></p> Show Commits by Author <p>Filter by who made commits:</p> <pre><code>git log --oneline --author=\"John\"\n</code></pre> <p>Shows only John's commits.</p> Show Commits in Date Range <pre><code># Last week\ngit log --oneline --since=\"1 week ago\"\n\n# Last month\ngit log --oneline --since=\"1 month ago\"\n\n# Specific date range\ngit log --oneline --since=\"2024-01-01\" --until=\"2024-01-31\"\n</code></pre>"},{"location":"Git/1.5_Git_Log_Online.html#common-scenarios","title":"Common Scenarios","text":""},{"location":"Git/1.5_Git_Log_Online.html#scenario-1-checking-if-youre-synced","title":"Scenario 1: Checking If You're Synced","text":"<p>You want to know: Do I need to push? Do I need to pull?</p> <pre><code>git log --oneline -1\n</code></pre> <p>Output: <pre><code>7d6850f (HEAD -&gt; feature/login, origin/feature/login) Add login\n</code></pre></p> <p>Analysis: Local and remote point to same commit. You're synced.</p> <p>If you see: <pre><code>a1b2c3d (HEAD -&gt; feature/login) Fix bug\n7d6850f (origin/feature/login) Add login\n</code></pre></p> <p>Analysis: Local is ahead. You need to push.</p>"},{"location":"Git/1.5_Git_Log_Online.html#scenario-2-finding-a-specific-commit","title":"Scenario 2: Finding a Specific Commit","text":"<p>You remember changing something last week. Find it:</p> <pre><code>git log --oneline --since=\"1 week ago\" --grep=\"login\"\n</code></pre> <p>This shows commits from last week with \"login\" in the message.</p>"},{"location":"Git/1.5_Git_Log_Online.html#scenario-3-checking-branch-divergence","title":"Scenario 3: Checking Branch Divergence","text":"<p>Your branch diverged from main. How many commits apart?</p> <pre><code># Show commits on your branch not in main\ngit log --oneline main..feature/login\n\n# Show commits on main not in your branch\ngit log --oneline feature/login..main\n</code></pre>"},{"location":"Git/1.5_Git_Log_Online.html#best-practices","title":"Best Practices","text":"<p>Do This</p> <ul> <li>Use <code>git log --oneline</code> to get quick overview</li> <li>Add <code>--graph</code> to see branch structure</li> <li>Add <code>--all</code> to see all branches</li> <li>Check before pushing: Are you synced?</li> <li>Use meaningful commit messages (not \"changes\")</li> </ul> <p>Make It Your Default</p> <p>Create an alias: <pre><code>git config --global alias.lo \"log --oneline\"\n</code></pre></p> <p>Now just type: <pre><code>git lo\n</code></pre></p>"},{"location":"Git/1.5_Git_Log_Online.html#whats-next","title":"What's Next?","text":"<p>You understand <code>git log --oneline</code>. It shows commit history in a compact format.</p> <p>Next up: branches in depth. How to create them. How to merge them. How to rebase them. Everything about branching.</p> <p>But now you know: that line in <code>git log --oneline</code> tells you exactly where you are and where your branches are pointing.</p>"},{"location":"Git/1.6_Pull_Requests.html","title":"Understanding Pull Requests (PRs)","text":"<p>You hear \"Pull Request\" everywhere. PR this. PR that. Open a PR. Merge the PR.</p> <p>But here's the confusing part: Pull Request is a terrible name.</p> <p>You're not requesting to pull. You're requesting someone to merge your code into their branch.</p> <p>It should be called a \"Merge Request.\" (GitLab got it right.)</p>"},{"location":"Git/1.6_Pull_Requests.html#what-a-pull-request-really-is","title":"What a Pull Request Really Is","text":"<p>A Pull Request is you saying:</p> <p>\"Hey, I made changes on my branch. Please review them and merge them into main.\"</p> <p>That's it. You're asking someone(or yourself) to merge your feature branch into 'a' branch (usually <code>main</code>).</p> <pre><code>flowchart LR\n    A[Your Feature Branch] --&gt;|Pull Request| B[Review Process]\n    B --&gt;|Approved| C[Merge into Main]\n\n    style A fill:#e1f5ff\n    style B fill:#fff3e0\n    style C fill:#e8f5e9</code></pre> <p>Not a pull. It's a merge request with a review process.</p>"},{"location":"Git/1.6_Pull_Requests.html#the-name-confusion","title":"The Name Confusion","text":"Why Is It Called 'Pull Request'? <p>GitHub named it from the repository owner's perspective.</p> <p>Your perspective: \"I want to merge my changes into main.\"</p> <p>Repository owner's perspective: \"Please pull my changes into your repository.\"</p> <p>The owner \"pulls\" your changes in. Hence, \"Pull Request.\"</p> <p>But it's still confusing because: - You use <code>git pull</code> to get changes - You use <code>git push</code> to send changes - But you \"open a pull request\" to ask for a merge</p> <p>What others call it: - GitHub: Pull Request (PR) - GitLab: Merge Request (MR) \u2190 Better name - Bitbucket: Pull Request (PR) - Azure DevOps: Pull Request (PR)</p> <p>Just remember: PR = \"Please merge my code\" request.</p>"},{"location":"Git/1.6_Pull_Requests.html#the-basic-flow","title":"The Basic Flow","text":"<p>This is what happens with every PR:</p> Step 1: Create a Feature Branch <p>You don't work on <code>main</code> directly. You create a branch.</p> <pre><code>git checkout main\ngit pull\ngit checkout -b feature/login\n</code></pre> <p>Now you're on <code>feature/login</code>.</p> Step 2: Make Changes and Commit <p>You write code. You commit it.</p> <pre><code># Make changes to files\ngit add .\ngit commit -m \"Add login feature\"\n</code></pre> <p>Maybe you make more commits:</p> <pre><code>git add .\ngit commit -m \"Fix login bug\"\n\ngit add .\ngit commit -m \"Add tests for login\"\n</code></pre> <p>All commits are on <code>feature/login</code>.</p> Step 3: Push Your Branch <p>Push your feature branch to GitHub.</p> <pre><code>git push -u origin feature/login\n</code></pre> <p>Your branch is now on GitHub. But it's not merged into <code>main</code> yet.</p> Step 4: Open a Pull Request <p>Go to GitHub. You'll see a yellow banner:</p> <pre><code>feature/login had recent pushes\n[Compare &amp; pull request]\n</code></pre> <p>Click it. Fill in: - Title: What your PR does - Description: Why you made these changes - Reviewers: Who should review it</p> <p>Click \"Create pull request.\"</p> <p>Now you wait. Someone needs to review your code.</p> Step 5: Code Review <p>Reviewers look at your code. They might:</p> <ul> <li>Approve it: \"Looks good!\"</li> <li>Request changes: \"Fix this bug first\"</li> <li>Comment: \"Why did you do it this way?\"</li> </ul> <p>You respond. You make more commits if needed:</p> <pre><code># Fix review comments\ngit add .\ngit commit -m \"Address review comments\"\ngit push\n</code></pre> <p>The PR updates automatically with your new commits.</p> Step 6: Merge the PR <p>Once approved, someone clicks \"Merge pull request\" on GitHub.</p> <p>Your <code>feature/login</code> branch merges into <code>main</code>.</p> <p>Done. Your code is in main. Everyone has it.</p> Step 7: Delete the Feature Branch <p>The branch is merged. You don't need it anymore.</p> <p>On GitHub: Click \"Delete branch\" after merging.</p> <p>Locally: <pre><code>git checkout main\ngit pull\ngit branch -d feature/login\n</code></pre></p> <p>Clean slate. Ready for the next feature.</p>"},{"location":"Git/1.6_Pull_Requests.html#quick-command-reference","title":"Quick Command Reference","text":"Task Command/Action Create feature branch <code>git checkout -b feature/name</code> Push branch <code>git push -u origin feature/name</code> Open PR Go to GitHub, click \"Compare &amp; pull request\" Update PR with changes <code>git commit</code> + <code>git push</code> Sync with main during PR <code>git pull origin main</code> or <code>git rebase main</code> Delete local branch after merge <code>git branch -d feature/name</code> Delete remote branch <code>git push origin --delete feature/name</code>"},{"location":"Git/1.6_Pull_Requests.html#common-confusion-cleared","title":"Common Confusion Cleared","text":"Confusion 1: Pull Request vs git pull <p>They're completely different.</p> Term What It Is Command/Action <code>git pull</code> Git command to fetch and merge <code>git pull</code> Pull Request GitHub feature to request code review Click button on GitHub <p>git pull: You get code from remote. Pull Request: You ask others to merge your code.</p> <p>Different things. Same word \"pull.\" Confusing.</p> Confusion 2: Do I Need to Pull Before Opening a PR? <p>No, but you should sync with main first.</p> <p>Before opening a PR:</p> <pre><code># Make sure main is up to date\ngit checkout main\ngit pull\n\n# Update your feature branch\ngit checkout feature/login\ngit merge main\n\n# Or rebase for cleaner history\ngit rebase main\n\n# Then push\ngit push\n</code></pre> <p>Now your PR has the latest code from main. Fewer conflicts.</p> Confusion 3: Can I Open a PR Without Pushing? <p>No. PRs are on GitHub. Your branch must be on GitHub.</p> <pre><code># This doesn't work - branch is only local\ngit commit -m \"Add feature\"\n# Open PR \u2190 Can't, GitHub doesn't have your branch\n\n# This works\ngit commit -m \"Add feature\"\ngit push -u origin feature/login\n# Now open PR on GitHub\n</code></pre> Confusion 4: Who Merges the PR? <p>Depends on your team's rules.</p> <ul> <li>Option 1: Reviewer merges after approving</li> <li>Option 2: You merge after getting approval</li> <li>Option 3: Auto-merge after tests pass</li> </ul> <p>Check your team's process.</p> Confusion 5: Can I Keep Committing After Opening a PR? <p>Yes! The PR updates automatically.</p> <pre><code># PR is already open\ngit add .\ngit commit -m \"Fix typo\"\ngit push\n</code></pre> <p>GitHub adds this commit to your existing PR. Reviewers see it immediately.</p>"},{"location":"Git/1.6_Pull_Requests.html#real-world-workflow","title":"Real-World Workflow","text":""},{"location":"Git/1.6_Pull_Requests.html#scenario-1-your-first-pr","title":"Scenario 1: Your First PR","text":"<p>You're new. You want to contribute.</p> <pre><code># 1. Fork the repo on GitHub\n\n# 2. Clone your fork\ngit clone https://github.com/you/repo.git\ncd repo\n\n# 3. Create feature branch\ngit checkout -b feature/fix-typo\n\n# 4. Make changes\n# Edit files...\ngit add .\ngit commit -m \"Fix typo in README\"\n\n# 5. Push\ngit push -u origin feature/fix-typo\n\n# 6. Open PR on GitHub\n# Go to your fork, click \"Compare &amp; pull request\"\n# Make sure base is: original-owner/repo main\n# And compare is: you/repo feature/fix-typo\n\n# 7. Wait for review\n\n# 8. After merge, update your fork\ngit checkout main\ngit pull upstream main\ngit push origin main\n</code></pre>"},{"location":"Git/1.6_Pull_Requests.html#scenario-2-pr-has-conflicts","title":"Scenario 2: PR Has Conflicts","text":"<p>You opened a PR. Someone else merged code. Now you have conflicts.</p> <p>GitHub shows: <pre><code>This branch has conflicts that must be resolved\n</code></pre></p> <p>Fix it:</p> <pre><code># Update your branch with latest main\ngit checkout feature/login\ngit pull origin main\n</code></pre> <p>If there are conflicts:</p> <pre><code># Fix conflicts in files\n# Look for &lt;&lt;&lt;&lt;&lt;&lt;&lt; markers\n\n# Stage resolved files\ngit add .\ngit commit -m \"Resolve merge conflicts\"\n\n# Push\ngit push\n</code></pre> <p>PR updates. Conflicts are gone.</p>"},{"location":"Git/1.6_Pull_Requests.html#scenario-3-reviewer-requests-changes","title":"Scenario 3: Reviewer Requests Changes","text":"<p>You got feedback. Need to make changes.</p> <pre><code># You're on feature/login\n# Make the requested changes\ngit add .\ngit commit -m \"Address review feedback: improve error handling\"\ngit push\n</code></pre> <p>The PR updates. Reviewer gets notified. They review again.</p>"},{"location":"Git/1.6_Pull_Requests.html#scenario-4-keeping-pr-updated-during-long-review","title":"Scenario 4: Keeping PR Updated During Long Review","text":"<p>Your PR is open for days. Main keeps moving forward.</p> <p>Keep your branch updated:</p> <pre><code># Get latest main\ngit checkout main\ngit pull\n\n# Update your feature branch\ngit checkout feature/login\ngit merge main\n\n# Or rebase (cleaner)\ngit rebase main\n\n# Push (force push if you rebased)\ngit push --force-with-lease\n</code></pre> <p>Your PR stays current with main.</p>"},{"location":"Git/1.6_Pull_Requests.html#pr-best-practices","title":"PR Best Practices","text":"<p>Do This</p> <ul> <li>Small PRs: Easier to review. Aim for &lt; 400 lines changed</li> <li>Clear title: \"Add login feature\" not \"Update stuff\"</li> <li>Good description: Explain what and why</li> <li>One feature per PR: Don't mix unrelated changes</li> <li>Update often: Keep your branch synced with main</li> <li>Respond to reviews: Answer questions, make requested changes</li> <li>Add tests: Include tests with your code</li> <li>Check CI: Make sure tests pass before requesting review</li> </ul> <p>Writing Good PR Descriptions</p> <p>Bad: <pre><code>Updated code\n</code></pre></p> <p>Good: <pre><code>Add user authentication feature\n\nWhat:\n- Implemented login/logout\n- Added password hashing\n- Created user session management\n\nWhy:\n- Required for accessing protected resources\n- Improves security with bcrypt hashing\n\nTesting:\n- Added unit tests for auth functions\n- Tested manually with different users\n\nCloses #123\n</code></pre></p>"},{"location":"Git/1.6_Pull_Requests.html#advanced-pr-concepts","title":"Advanced PR Concepts","text":"Draft Pull Requests <p>You're not ready for review yet. But you want to show progress.</p> <p>Create a Draft PR:</p> <p>On GitHub, click \"Create pull request\" dropdown \u2192 \"Create draft pull request\"</p> <p>What it means: - Shows your work in progress - Can't be merged yet - People can comment but won't formally review - You convert to \"Ready for review\" when done</p> <p>When to use: - Getting early feedback - CI/CD testing before review - Showing progress to team</p> PR Templates <p>Your repo can have a PR template. It auto-fills the description.</p> <p>Create <code>.github/pull_request_template.md</code>:</p> <pre><code>## Description\nWhat does this PR do?\n\n## Type of Change\n- [ ] Bug fix\n- [ ] New feature\n- [ ] Breaking change\n\n## Testing\nHow was this tested?\n\n## Checklist\n- [ ] Tests added\n- [ ] Documentation updated\n- [ ] No linting errors\n</code></pre> <p>Now every PR has this template. Consistent PRs across the team.</p> Automated Checks on PRs <p>PRs can trigger:</p> <ul> <li>Tests: Unit tests, integration tests</li> <li>Linting: Code style checks</li> <li>Security scans: Check for vulnerabilities</li> <li>Build checks: Make sure code builds</li> </ul> <p>GitHub shows: <pre><code>\u2713 All checks passed\n</code></pre></p> <p>Or: <pre><code>\u2717 Some checks failed\n</code></pre></p> <p>Can't merge until checks pass (if required).</p> Protected Branches and Required Reviews <p>Repositories can protect <code>main</code>:</p> <ul> <li>Require PR: Can't push directly to main</li> <li>Require reviews: Need X approvals before merge</li> <li>Require tests: All checks must pass</li> <li>No force push: Can't overwrite history</li> </ul> <p>Check settings: GitHub repo \u2192 Settings \u2192 Branches \u2192 Branch protection rules</p> Force Pushing to PR Branch <p>You rebased your feature branch. Now you need to force push.</p> <pre><code>git rebase main\ngit push --force-with-lease\n</code></pre> <p>What happens to PR: - PR updates with new commits - Old commit history is rewritten - Comments on old commits might be hidden</p> <p>Don't Force Push if Others Are Working on Your Branch</p> <p>You'll mess up their work. Only force push to branches you own.</p> Squashing Commits <p>Your PR has 20 commits. You want to merge as 1 commit.</p> <p>On GitHub: When merging, choose \"Squash and merge\"</p> <p>All your commits become one commit in main.</p> <p>Before: <pre><code>* Fix typo\n* Fix another typo\n* Actually fix the bug\n* Add tests\n* Fix tests\n* More fixes\n</code></pre></p> <p>After squash: <pre><code>* Add login feature (#123)\n</code></pre></p> <p>Clean history in main.</p> Reverting a Merged PR <p>You merged a PR. It broke production. You need to undo it.</p> <p>On GitHub: Go to the merged PR \u2192 Click \"Revert\"</p> <p>This creates a new PR that undoes all changes from the original PR.</p> <p>Merge this new PR. The code is back to before.</p>"},{"location":"Git/1.6_Pull_Requests.html#merge-strategies","title":"Merge Strategies","text":"<p>When you click \"Merge pull request\" on GitHub, you have options:</p> Create a Merge Commit <p>Default option. Creates a merge commit.</p> <p>History looks like: <pre><code>*   Merge pull request #123 from feature/login\n|\\\n| * Add login tests\n| * Implement login\n|/\n* Previous commit on main\n</code></pre></p> <p>Pros: Preserves full history, shows branch structure Cons: History can get messy with many PRs</p> Squash and Merge <p>Combines all commits into one.</p> <p>History looks like: <pre><code>* Add login feature (#123)\n* Previous commit on main\n</code></pre></p> <p>Pros: Clean linear history Cons: Loses individual commit messages</p> Rebase and Merge <p>Replays commits from feature branch onto main.</p> <p>History looks like: <pre><code>* Add login tests\n* Implement login\n* Previous commit on main\n</code></pre></p> <p>Pros: Linear history, keeps individual commits Cons: Rewrites commit SHAs</p> <p>Your team chooses which strategy to use.</p>"},{"location":"Git/1.6_Pull_Requests.html#common-pr-mistakes","title":"Common PR Mistakes","text":"<p>Opening PR from Main</p> <p>Wrong: <pre><code>git checkout main\n# Make changes\ngit commit\ngit push\n# Open PR from main\n</code></pre></p> <p>Don't work on main. Use feature branches.</p> <p>Right: <pre><code>git checkout -b feature/fix\n# Make changes\ngit commit\ngit push\n# Open PR from feature/fix\n</code></pre></p> <p>Giant PRs (500+ Lines Changed)</p> <p>Reviewers won't read it carefully. Too much to review.</p> <p>Break it up: - PR 1: Database changes - PR 2: API changes - PR 3: UI changes</p> <p>Smaller PRs = better reviews = fewer bugs.</p> <p>Not Testing Before Opening PR</p> <p>Your PR breaks tests. CI fails. Reviewer can't approve.</p> <p>Before opening PR: <pre><code># Run tests locally\nnpm test\n# or\npytest\n# or whatever your test command is\n\n# Make sure they pass\n# Then push and open PR\n</code></pre></p> <p>Merging Your Own PR Without Review</p> <p>You opened a PR. You merged it yourself immediately.</p> <p>Why it's bad: No one checked your code. Bugs slip through.</p> <p>Exception: Tiny docs fixes, emergency hotfixes (with team agreement).</p>"},{"location":"Git/1.6_Pull_Requests.html#pr-etiquette","title":"PR Etiquette","text":"<p>Being a Good PR Author</p> <ul> <li>Respond to reviews within 24 hours</li> <li>Don't take feedback personally</li> <li>Explain your reasoning if you disagree</li> <li>Thank reviewers for their time</li> <li>Keep PRs updated with main</li> <li>Mark conversations as resolved after fixing</li> </ul> <p>Being a Good Reviewer</p> <ul> <li>Review within 24-48 hours</li> <li>Be constructive, not critical</li> <li>Explain why, not just what's wrong</li> <li>Approve small fixes, don't nitpick</li> <li>Test the code if possible</li> <li>Say what's good, not just what's bad</li> </ul>"},{"location":"Git/1.6_Pull_Requests.html#whats-next","title":"What's Next?","text":"<p>You understand Pull Requests. They're merge requests with a review process.</p> <p>Next up: branches in depth. Remote tracking branches. What <code>origin/main</code> really means. How local and remote branches sync.</p> <p>But now you know: PR is just a fancy way to say \"Please review and merge my code.\"</p>"},{"location":"Git/1_Hello_Git.html","title":"Getting Started with Git","text":"<p>Every project needs version control. Git is that version control. There are others - Mercurial, Subversion, Visual Source Safe. But git won. It's free, open source, distributed, can work offline and large brands like GitHub, GitLab, BitBucket, or Azure DevOps use git. This section will focus on some important Git concepts you need to know.</p> <p>So, how do we define Git? Git is a version control system that solves the chaos of many people working on the same project\u2014like overwritten files or version confusion\u2014by acting as a 'sophisticated save button' that keeps a detailed history of every change and provides commands to safely 'combine' everyone's work.</p>"},{"location":"Git/1_Hello_Git.html#install-git-and-configure-vs-code-to-use-it","title":"Install Git and configure VS Code to use it","text":"<p>This is actually pretty simple. The very first thing you always need is this software called Git. This is the software. So to install it, just visit this link, click download (I\u2019m talking about Windows here), run the installer, and go next \u2192 next \u2192 next. Below are some snapshots of the installer screens that you may or may not see in the exact same order.</p>"},{"location":"Git/1_Hello_Git.html#selecting-components","title":"Selecting components","text":""},{"location":"Git/1_Hello_Git.html#choosing-the-default-editor-used-by-git","title":"Choosing the default editor used by Git","text":"<p>I choose VS Code, because it makes life easier.</p> <p></p>"},{"location":"Git/1_Hello_Git.html#adjusting-the-name-of-the-initial-branch-in-new-repositories","title":"Adjusting the name of the initial branch in new repositories","text":"<p>I choose \u201cOverride\u201d and set it to main.</p> <p></p>"},{"location":"Git/1_Hello_Git.html#adjusting-the-path","title":"Adjusting the PATH","text":"<p>I usually pick the recommended option.</p> <p></p>"},{"location":"Git/1_Hello_Git.html#and-so-on","title":"And so on\u2026","text":"<p>Just keep going with the default or recommended options.</p> <p></p> <p>Finally, you should see the \u201cSetup has finished installing Git\u201d message.</p> <p></p>"},{"location":"Git/1_Hello_Git.html#verify-that-git-works","title":"Verify that Git works","text":"<p>After everything is installed, just open Command Prompt and type:</p> <pre><code>git\n</code></pre> <p>Press Enter.</p> <p>This is the fastest and simplest way to see if Git works. Make sure Git is added to your PATH so you don\u2019t need any rocket science to run it from the command prompt. It will make things much easier later.</p>"},{"location":"Git/1_Hello_Git.html#clone-a-repository","title":"Clone a repository","text":"<p>Great job. Yey! You installed git and it works with your VS Code. Now,  The first thing you always do (apart from git setup etc) is clone a repository. Your team will already have a repository on some git system (Github/Azure Devops/Gitlab). What this cloning does is simply copies files from the cloud to your machine.</p>"},{"location":"Git/1_Hello_Git.html#basic-clone-syntax","title":"Basic Clone Syntax","text":"HTTPSSSH <p><pre><code>git clone https://github.com/username/repo-ontario.git\n</code></pre> Most common method. Works everywhere.</p> <p><pre><code>git clone git@github.com:username/repo-ontario.git\n</code></pre> No password prompts. Requires SSH key setup.</p> <p>Note: The <code>.git</code> extension is optional. </p>"},{"location":"Git/1_Hello_Git.html#different-ways-to-clone","title":"Different Ways to Clone","text":"Custom Folder NameSpecific BranchShallow CloneSpecific Tag/ReleaseSingle Branch Only <p><pre><code>git clone https://github.com/username/repo-ontario.git repo-belgium\n</code></pre> Creates a folder called <code>repo-belgium</code> instead of <code>repo-ontario</code>. Useful when the repo name is generic.</p> <p><pre><code>git clone -b development https://github.com/username/repo-ontario.git\n</code></pre> Checks out the <code>development</code> branch instead of <code>main</code>. Saves you from doing <code>git checkout development</code> after cloning.</p> <p><pre><code>git clone --depth 1 https://github.com/username/repo-ontario.git\n</code></pre> Downloads only the latest commit. Way faster. Perfect for CI/CD or when you don't need history.</p> <p><pre><code>git clone --branch v1.0.0 https://github.com/username/repo-ontario.git\n</code></pre> Gets the code at version 1.0.0. The <code>--branch</code> flag works for tags too.</p> <p><pre><code>git clone --depth 1 --single-branch https://github.com/username/repo-ontario.git\n</code></pre> Downloads only one branch. Even faster for huge repos.</p>"},{"location":"Git/1_Hello_Git.html#what-really-happens-when-you-run-git-clone","title":"What Really Happens When You Run <code>git clone</code>?","text":"Behind the Scenes: The Complete Clone Process <p>You type <code>git clone https://github.com/someone/repo-ontario.git</code> and boom\u2014a whole project appears. But what's actually happening?</p> <p>Phase 1: The Setup</p> <pre><code>flowchart LR\n    A[\"git clone\"] --&gt; B[\"Create folder\"]\n    B --&gt; C[\"Initialize .git/\"]\n    C --&gt; D[\"Add remote 'origin'\"]\n    style A fill:#e1f5ff\n    style D fill:#e8f5e9</code></pre> <p>Git creates a folder and initializes a hidden <code>.git</code> folder inside it. This <code>.git</code> folder is the brain\u2014it contains everything Git knows about your project.</p> Inside the .git Folder <ul> <li>config \u2192 Repository settings</li> <li>refs/ \u2192 Branch and tag references</li> <li>objects/ \u2192 All the data (commits, files, everything)</li> <li>HEAD \u2192 Points to your current branch</li> <li>hooks/ \u2192 Automation scripts</li> </ul> <p>Git then adds a remote called <code>origin</code> to <code>.git/config</code>:</p> .git/config<pre><code>[remote \"origin\"]\n    url = https://github.com/someone/repo-ontario.git\n    fetch = +refs/heads/*:refs/remotes/origin/*\n</code></pre> <p>What does that fetch line mean?</p> <p>\"For every branch on the remote (<code>refs/heads/*</code>), create a local tracking reference (<code>refs/remotes/origin/*</code>).\"</p> <p>So <code>main</code> \u2192 <code>origin/main</code>, <code>dev</code> \u2192 <code>origin/dev</code>, and so on.</p> <p>Phase 2: The Download</p> <pre><code>flowchart LR\n    A[\"GitHub Server\"] --&gt;|\"Download everything\"| B[\".git/objects/\"]\n    B --&gt; C[\"Create origin/main&lt;br/&gt;origin/dev&lt;br/&gt;origin/feature\"]\n    style A fill:#fff3e0\n    style C fill:#e8f5e9</code></pre> <p>Git downloads everything: every commit ever made, every branch, every tag, every file version. You're getting a time machine, not just the latest code.</p> How Git Stores Data <p>It compresses all this using SHA-1 hashing and stores it in <code>.git/objects/</code>\u2014like a library where every book has a unique barcode. Want a specific commit? Git looks up its hash and pulls it instantly.</p> <p>Git creates read-only snapshots for all remote branches:</p> <ul> <li><code>origin/main</code> \u2192 What <code>main</code> looks like on GitHub</li> <li><code>origin/dev</code> \u2192 What <code>dev</code> looks like on GitHub</li> <li><code>origin/feature</code> \u2192 What <code>feature</code> looks like on GitHub</li> </ul> <p>These are Read-Only</p> <p>You can view them but can't edit them. They update only when you run <code>git fetch</code> or <code>git pull</code>.</p> <p>Exception: Shallow Clones</p> <p>If you used <code>--depth 1</code>, Git only downloads that one commit. No history, no other branches (unless you specified them).</p> <p>Phase 3: The Checkout</p> <pre><code>flowchart LR\n    A[\"Create local main\"] --&gt; B[\"Link main \u2192 origin/main\"]\n    B --&gt; C[\"Extract files to&lt;br/&gt;working directory\"]\n    style A fill:#e1f5ff\n    style C fill:#e8f5e9</code></pre> <p>Git creates your local branch (either <code>main</code> or whatever you specified with <code>-b</code>) and links it to its remote counterpart. This is called tracking.</p> <p>Why Tracking Matters</p> <p>It means <code>git pull</code> and <code>git push</code> know where to sync without you specifying every time. It's like speed dial\u2014one command, it knows where to go.</p> <p>Finally, Git extracts the latest commit's files into your working directory. Now you see actual code, README files, everything.</p> <p></p>"},{"location":"Git/1_Hello_Git.html#advanced-clone-scenarios","title":"Advanced Clone Scenarios","text":"<p>Bare Repository (No Working Files)</p> <p><pre><code>git clone --bare https://github.com/username/repo-ontario.git\n</code></pre> Creates just the <code>.git</code> folder without any working files. Used for setting up mirrors or servers.</p> <p>Different Remote Name</p> <p><pre><code>git clone -o upstream https://github.com/username/repo-ontario.git\n</code></pre> Instead of <code>origin</code>, your remote is called <code>upstream</code>. Useful when working with forks.</p>"},{"location":"Git/2_Git_Branching.html","title":"Creating Your First Feature Branch","text":"<p>You cloned the repo yesterday. Today you got a ticket. Build a new feature. Let's call it user authentication.</p> <p>You don't work on <code>main</code>. Never work on <code>main</code>. Create a branch.</p>"},{"location":"Git/2_Git_Branching.html#the-daily-workflow","title":"The Daily Workflow","text":"<p>This is what you'll do every single day. The basic loop.</p> Step 1: Sync with main <p>Before you do anything, sync up. Someone might have pushed changes overnight. Working across multiple computers? In a large team? Your local <code>main</code> can get out of sync fast.</p> <p>The Simple Case (Everything's Clean)</p> <pre><code>git checkout main\ngit pull origin main\n</code></pre> <p>This works 80% of the time. But what about the other 20%?</p> Step 2: Create your branch <pre><code>git checkout -b feature/user-auth\n</code></pre> <p>This creates a new branch called <code>feature/user-auth</code> and switches to it.</p> <p>One command. Two actions.</p> <p>Branch only exists locally</p> <p>Right now, this branch is only on your machine. Remote doesn't know about it yet.</p> Step 3: Make changes <p>You write code. Edit files. Add new files. Normal work.</p> <p>Let's say you created <code>auth.js</code> and modified <code>app.js</code>.</p> <p>Check what changed:</p> <pre><code>git status\n</code></pre> <p>You'll see:</p> <pre><code>On branch feature/user-auth\nChanges not staged for commit:\n  modified:   app.js\n\nUntracked files:\n  auth.js\n</code></pre> Step 4: Stage your changes <pre><code>git add .\n</code></pre> <p>This stages everything. Or stage specific files:</p> <pre><code>git add auth.js app.js\n</code></pre> Step 5: Commit <pre><code>git commit -m \"Add user authentication module\"\n</code></pre> <p>A commit is a snapshot. You're saving the current state.</p> Step 6: Push to remote (first time) <pre><code>git push -u origin feature/user-auth\n</code></pre> <p>The -u flag is critical</p> <p><code>-u</code> stands for <code>--set-upstream</code>. It does two things:</p> <ol> <li>Pushes your branch to remote</li> <li>Sets up tracking between local and remote</li> </ol> <p>You only need <code>-u</code> the first time.</p> <p>You'll see output like:</p> <pre><code>Enumerating objects: 5, done.\nTo https://github.com/username/repository.git\n * [new branch]      feature/user-auth -&gt; feature/user-auth\nBranch 'feature/user-auth' set up to track remote branch 'feature/user-auth' from 'origin'.\n</code></pre> <p>Your branch is now on remote. Your team can see it.</p> Step 7: Keep working <p>You're not done. You need to add password validation. Make the changes. Then:</p> <pre><code>git add .\ngit commit -m \"Add password validation\"\ngit push\n</code></pre> <p>No more -u needed</p> <p>Notice? Just <code>git push</code>. No <code>-u origin feature/user-auth</code>.</p> <p>That's the magic of <code>-u</code>. You set it up once. Git remembers.</p> <pre><code>graph TB\n    A[Edit files] --&gt; B[git add .]\n    B --&gt; C[git commit -m 'msg']\n    C --&gt; D{First push?}\n    D -- Yes --&gt; E[git push -u origin branch]\n    D -- No --&gt; F[git push]\n    E --&gt; G[Edit more]\n    F --&gt; G\n    G --&gt; A\n\n    style E fill:#ffebee\n    style F fill:#c8e6c9</code></pre> Step 8: Open a pull request <p>Your feature is done. Time to merge it into <code>main</code>.</p> <ol> <li>Go to GitHub/GitLab/Azure DevOps</li> <li>You'll see a banner: \"feature/user-auth had recent pushes\"</li> <li>Click \"Compare &amp; pull request\"</li> <li>Write a good title and description</li> <li>Click \"Create pull request\"</li> </ol> <p>Your team will review. After approval, someone will merge.</p>"},{"location":"Git/2_Git_Branching.html#common-scenarios-solutions","title":"Common Scenarios &amp; Solutions","text":"Scenario 1: You accidentally made commits on main <p>You forgot to create a branch. You committed directly to <code>main</code>. Now <code>git pull</code> fails.</p> <pre><code>git pull origin main\n</code></pre> <p>Error:</p> <pre><code>error: Your local changes to the following files would be overwritten by merge:\n    app.js\nPlease commit your changes or stash them before you merge.\n</code></pre> <p>Solution: Save your work, then reset</p> <pre><code># 1. Create a branch from your current position\ngit branch feature/accidental-work\n\n# 2. Switch to main\ngit checkout main\n\n# 3. Hard reset to match remote\ngit reset --hard origin/main\n\n# 4. Your work is safe on feature/accidental-work\ngit checkout feature/accidental-work\n</code></pre> <p>Your commits are saved on the new branch. Your <code>main</code> is clean.</p> Scenario 2: You have uncommitted changes on main <p>You edited files on <code>main</code>. Now you can't pull.</p> <p>Option 1: Stash and pull</p> <pre><code># Save your changes temporarily\ngit stash\n\n# Pull latest\ngit pull origin main\n\n# Create a branch\ngit checkout -b feature/new-work\n\n# Get your changes back\ngit stash pop\n</code></pre> <p>Option 2: Discard the changes</p> <p>If you don't need those changes:</p> <pre><code># Discard all uncommitted changes\ngit checkout -- .\n\n# Pull latest\ngit pull origin main\n</code></pre> Scenario 3: Your main diverged from origin/main <p>You're on a different computer. Or someone force-pushed. Your <code>main</code> has different commits than remote.</p> <p>Check if you're diverged:</p> <pre><code>git fetch origin\ngit status\n</code></pre> <p>You'll see:</p> <pre><code>Your branch and 'origin/main' have diverged,\nand have 3 and 5 different commits each, respectively.\n</code></pre> <p>Solution: Hard reset to remote</p> <pre><code># Make sure you're on main\ngit checkout main\n\n# Fetch latest info\ngit fetch origin\n\n# Hard reset to match remote exactly\ngit reset --hard origin/main\n</code></pre> <p>This discards your local commits</p> <p>If you have commits you want to keep, save them first: <pre><code>git branch backup-main\ngit reset --hard origin/main\n</code></pre></p> Scenario 4: You have untracked files blocking the pull <p>You created new files. Git pull wants to bring in files with the same names.</p> <p>Error:</p> <pre><code>error: The following untracked working tree files would be overwritten by merge:\n    config.json\nPlease move or remove them before you merge.\n</code></pre> <p>Solution 1: Move the files</p> <pre><code># Rename your file\nmv config.json config.json.backup\n\n# Pull\ngit pull origin main\n</code></pre> <p>Solution 2: Force clean (if you don't need the files)</p> <pre><code># Remove all untracked files\ngit clean -fd\n\n# Pull\ngit pull origin main\n</code></pre> <p>git clean is destructive</p> <p><code>-f</code> = force, <code>-d</code> = remove directories too. This deletes untracked files permanently.</p> Nuclear Option: Start completely fresh <p>Nothing works. You want to discard everything and match remote exactly.</p> <p>Method 1: Hard reset + clean</p> <pre><code># Fetch latest from remote\ngit fetch origin\n\n# Hard reset main to match origin/main\ngit checkout main\ngit reset --hard origin/main\n\n# Remove all untracked files and directories\ngit clean -fdx\n</code></pre> <p><code>-f</code> = force <code>-d</code> = directories <code>-x</code> = ignored files too (like node_modules, .env)</p> <p>Method 2: Delete and re-clone</p> <p>When even the nuclear option fails:</p> <pre><code># Go up one directory\ncd ..\n\n# Delete the entire repo\nrm -rf repository-name\n\n# Clone fresh\ngit clone https://github.com/username/repository-name.git\ncd repository-name\n</code></pre> <p>This is the ultimate reset. Everything local is gone.</p>"},{"location":"Git/2_Git_Branching.html#verification-is-my-main-clean","title":"Verification: Is my main clean?","text":"<p>After syncing, verify you're good:</p> <pre><code># Check status\ngit status\n</code></pre> <p>You should see:</p> <pre><code>On branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\n</code></pre> <p>Check if you match remote exactly:</p> <pre><code>git fetch origin\ngit log main..origin/main --oneline\n</code></pre> <p>If this shows nothing, you're in sync. If it shows commits, you're behind:</p> <pre><code>git pull origin main\n</code></pre> <p>Check for uncommitted changes:</p> <pre><code>git diff\n</code></pre> <p>Should show nothing.</p>"},{"location":"Git/2_Git_Branching.html#quick-decision-tree","title":"Quick Decision Tree","text":"<pre><code>flowchart TD\n    A[Need to sync main] --&gt; B{git status clean?}\n    B -- Yes --&gt; C[git pull origin main]\n    B -- No --&gt; D{Uncommitted changes?}\n    D -- Yes --&gt; E[git stash]\n    D -- No --&gt; F{Untracked files?}\n    E --&gt; C\n    F -- Yes --&gt; G[git clean -fd]\n    F -- No --&gt; H{Local commits?}\n    G --&gt; C\n    H -- Yes --&gt; I[git reset --hard origin/main]\n    H -- No --&gt; J[Nuclear: clean -fdx]\n    I --&gt; C\n    J --&gt; C\n\n    style C fill:#e8f5e9\n    style I fill:#ffebee\n    style J fill:#d32f2f,color:#fff</code></pre>"},{"location":"Git/2_Git_Branching.html#common-errors-fixes","title":"Common Errors &amp; Fixes","text":"Error What it means Fix <code>Your local changes would be overwritten</code> You have uncommitted changes <code>git stash</code> or <code>git checkout -- .</code> <code>Your branch has diverged</code> Your commits differ from remote <code>git reset --hard origin/main</code> <code>untracked working tree files</code> New local files conflict <code>git clean -fd</code> <code>Cannot pull with rebase</code> Outdated git config <code>git pull origin main --no-rebase</code> <code>fatal: Not possible to fast-forward</code> Commits on both sides <code>git reset --hard origin/main</code> <p>Best Practice: Always fetch first</p> <p>Get into the habit: <pre><code>git fetch origin\ngit status\ngit pull origin main\n</code></pre></p> <p><code>git fetch</code> downloads info without changing your files. You can see what's coming before you pull.</p>"},{"location":"Git/2_Git_Branching.html#working-on-a-teammates-branch","title":"Working on a Teammate's Branch","text":"<p>Your teammate created <code>feature/payment-gateway</code>. You need to work on it.</p> <p>First, fetch the latest:</p> <pre><code>git fetch origin\n</code></pre> <p>Then checkout the branch:</p> <pre><code>git checkout feature/payment-gateway\n</code></pre> <p>Git is smart. If the branch exists on remote but not locally, this command automatically:</p> <ol> <li>Creates your local version</li> <li>Sets up tracking</li> </ol> <p>No <code>-b</code> needed. No <code>-u</code> needed.</p> <p>Make your changes. Commit. Push.</p> <pre><code>git add .\ngit commit -m \"Add Stripe integration\"\ngit push\n</code></pre>"},{"location":"Git/2_Git_Branching.html#branch-naming-conventions","title":"Branch Naming Conventions","text":"<p>Your team probably has rules. Common patterns:</p> Feature BranchesBug Fix BranchesRefactor BranchesTicket-Based Naming <p><pre><code>feature/user-authentication\nfeature/payment-gateway\nfeature/dark-mode\n</code></pre> For new functionality.</p> <p><pre><code>bugfix/login-timeout\nfix/api-error-handling\nhotfix/critical-security-patch\n</code></pre> For fixing issues. <code>hotfix</code> is for urgent production fixes.</p> <p><pre><code>refactor/database-layer\nrefactor/cleanup-utils\n</code></pre> For code improvements without changing behavior.</p> <p><pre><code>JIRA-1234-user-auth\nfeature/PROJ-567-add-export\n</code></pre> Including ticket numbers for traceability.</p>"},{"location":"Git/2_Git_Branching.html#quick-command-reference","title":"Quick Command Reference","text":"Task Command Create and switch to branch <code>git checkout -b feature/name</code> Switch to existing branch <code>git checkout branch-name</code> Pull latest changes <code>git pull origin main</code> Stage changes <code>git add .</code> Commit <code>git commit -m \"message\"</code> First push <code>git push -u origin feature/name</code> Subsequent pushes <code>git push</code> List local branches <code>git branch</code> Delete local branch <code>git branch -d feature/name</code> Delete remote branch <code>git push origin --delete feature/name</code>"},{"location":"Git/2_Git_Branching.html#advanced-topics","title":"Advanced Topics","text":"<p>Everything below is optional. But useful to know.</p>"},{"location":"Git/2_Git_Branching.html#understanding-gits-three-states","title":"Understanding Git's Three States","text":"How Git Thinks About Your Files <pre><code>flowchart LR\n    A[Working Directory] --&gt;|git add| B[Staging Area]\n    B --&gt;|git commit| C[Local Repository]\n    C --&gt;|git push| D[Remote Repository]\n\n    style A fill:#ffebee\n    style B fill:#fff3e0\n    style C fill:#e8f5e9\n    style D fill:#e1f5ff</code></pre> <p>Working Directory \u2192 Your actual files. What you see in VS Code.</p> <p>Staging Area \u2192 A holding zone. Files you've marked with <code>git add</code>. Think of it as a shopping cart before checkout.</p> <p>Local Repository \u2192 Committed snapshots. After <code>git commit</code>, your changes are saved in <code>.git/objects/</code>.</p> <p>Remote Repository \u2192 GitHub/GitLab/Azure DevOps. After <code>git push</code>, your changes are visible to the team.</p>"},{"location":"Git/2_Git_Branching.html#switching-branches-with-uncommitted-changes","title":"Switching Branches with Uncommitted Changes","text":"Using git stash <p>You're on <code>feature/user-auth</code>. Your boss needs an urgent fix on <code>main</code>. But you have uncommitted changes.</p> <p>Git won't let you switch:</p> <pre><code>error: Your local changes would be overwritten by checkout.\nPlease commit your changes or stash them before you switch branches.\n</code></pre> <p>Option 1: Commit your work</p> <pre><code>git add .\ngit commit -m \"WIP: Authentication half done\"\ngit checkout main\n</code></pre> <p><code>WIP</code> stands for \"Work In Progress\".</p> <p>Option 2: Stash your changes</p> <pre><code>git stash\ngit checkout main\n# Fix the urgent issue\n# Commit and push\n\n# Come back\ngit checkout feature/user-auth\ngit stash pop\n</code></pre> <p>Stash saves your changes temporarily. Your working directory is clean. When you come back, <code>git stash pop</code> restores everything.</p> <p>Stash commands:</p> Command What it does <code>git stash</code> Stash current changes <code>git stash pop</code> Apply latest stash and remove it <code>git stash list</code> Show all stashes <code>git stash apply</code> Apply stash but keep it in stack <code>git stash drop</code> Delete latest stash"},{"location":"Git/2_Git_Branching.html#viewing-branches","title":"Viewing Branches","text":"Branch Listing Commands <p>List local branches</p> <pre><code>git branch\n</code></pre> <p>Output:</p> <pre><code>  feature/payment-gateway\n* feature/user-auth\n  main\n</code></pre> <p>The <code>*</code> shows your current branch.</p> <p>List remote branches</p> <pre><code>git branch -r\n</code></pre> <p>List all branches</p> <pre><code>git branch -a\n</code></pre> <p>See more details</p> <pre><code>git branch -v\n</code></pre> <p>Shows the last commit on each branch:</p> <pre><code>  feature/payment-gateway  a3f2c1d Add Stripe integration\n* feature/user-auth        b7e9f3a Add password validation\n  main                     c4d8e2f Update README\n</code></pre>"},{"location":"Git/2_Git_Branching.html#deleting-branches","title":"Deleting Branches","text":"How to Delete Branches <p>Your feature is merged. Clean up.</p> <p>Delete local branch</p> <pre><code>git checkout main\ngit branch -d feature/user-auth\n</code></pre> <p>Safe delete. Git won't let you delete if there are unmerged changes.</p> <p>Force delete</p> <pre><code>git branch -D feature/user-auth\n</code></pre> <p>Use when you really want to throw away the branch.</p> <p>Force delete is destructive</p> <p><code>-D</code> doesn't check if your changes are merged. Use carefully.</p> <p>Delete remote branch</p> <pre><code>git push origin --delete feature/user-auth\n</code></pre>"},{"location":"Git/2_Git_Branching.html#common-branching-workflows","title":"Common Branching Workflows","text":"GitHub Flow - Simple &amp; Popular <p>One main branch. Short-lived feature branches.</p> <pre><code>gitGraph\n    commit\n    commit\n    branch feature/login\n    checkout feature/login\n    commit\n    commit\n    checkout main\n    merge feature/login\n    commit\n    branch feature/payment\n    checkout feature/payment\n    commit\n    commit\n    checkout main\n    merge feature/payment\n    commit</code></pre> <p>The flow:</p> <ol> <li>Branch from <code>main</code></li> <li>Work on feature</li> <li>Open pull request</li> <li>Code review</li> <li>Merge to <code>main</code></li> <li>Deploy immediately</li> </ol> <p>When to use</p> <ul> <li>Fast-moving projects</li> <li>Continuous deployment</li> <li>Small to medium teams</li> </ul> Git Flow - Structured Releases <p>Multiple long-lived branches. More structured.</p> <pre><code>gitGraph\n    commit\n    branch develop\n    checkout develop\n    commit\n    branch feature/login\n    checkout feature/login\n    commit\n    commit\n    checkout develop\n    merge feature/login\n    branch release/1.0\n    checkout release/1.0\n    commit\n    checkout main\n    merge release/1.0 tag: \"v1.0\"\n    checkout develop\n    merge release/1.0</code></pre> <p>The branches:</p> <ul> <li><code>main</code> \u2192 Production code. Always deployable.</li> <li><code>develop</code> \u2192 Integration branch. Next release code.</li> <li><code>feature/*</code> \u2192 New features. Branch from <code>develop</code>.</li> <li><code>release/*</code> \u2192 Release preparation. Branch from <code>develop</code>.</li> <li><code>hotfix/*</code> \u2192 Emergency fixes. Branch from <code>main</code>.</li> </ul> <p>When to use</p> <ul> <li>Scheduled releases</li> <li>Multiple versions in production</li> <li>Enterprise projects</li> </ul>"},{"location":"Git/2_Git_Branching.html#understanding-merge-conflicts","title":"Understanding Merge Conflicts","text":"How Conflicts Happen &amp; Resolution <p>Two people edit the same file. Same line. Different changes. Both push to remote.</p> <p>The second person gets a conflict.</p> <p>What a conflict looks like</p> <pre><code>function login(username, password) {\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n    return authenticateUser(username, password);\n=======\n    return validateAndAuthenticate(username, password);\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/bob\n}\n</code></pre> <ul> <li><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</code> \u2192 What's in the target branch (usually <code>main</code>)</li> <li><code>=======</code> \u2192 The divider</li> <li><code>&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/bob</code> \u2192 What's in your branch</li> </ul> <p>Resolving a conflict</p> <ol> <li>Open the file</li> <li>Decide what to keep</li> <li> <p>Edit the file. Remove the conflict markers.</p> <pre><code>function login(username, password) {\n    return validateAndAuthenticate(username, password);\n}\n</code></pre> </li> <li> <p>Stage the resolved file</p> <pre><code>git add login.js\n</code></pre> </li> <li> <p>Complete the merge</p> <pre><code>git commit -m \"Resolve merge conflict in login.js\"\n</code></pre> </li> </ol> <p>Avoiding conflicts</p> <ul> <li>Pull often</li> <li>Communicate with your team</li> <li>Small commits</li> <li>Modular code</li> </ul>"},{"location":"Git/2_Git_Branching.html#whats-next","title":"What's Next?","text":"<p>You've got branching down. You know the daily workflow. You understand staging, committing, pushing.</p> <p>Next up: Pull Requests. Code reviews. How to merge into <code>main</code>. The collaboration workflow.</p> <p>Practice this workflow a few times. It'll become muscle memory.</p>"},{"location":"Git/3_Git_Issues.html","title":"Git Troubleshooting Scenarios","text":"<p>Here are some common Git troubleshooting scenarios I've encountered, along with explanations and fixes.</p>"},{"location":"Git/3_Git_Issues.html#git-branch-not-showing-in-vs-code","title":"Git Branch Not Showing in VS Code","text":""},{"location":"Git/3_Git_Issues.html#the-problem","title":"The Problem","text":"<p>I could see my branch <code>feature/DasSomeFeatureName</code> in Azure DevOps, but VS Code wasn't showing it in the branch selector.</p> <p></p>"},{"location":"Git/3_Git_Issues.html#the-fix","title":"The Fix","text":"<pre><code>git fetch --all\n</code></pre> <p>Then checkout the branch:</p> <pre><code>git checkout feature/DasSomeFeatureName\n</code></pre>"},{"location":"Git/3_Git_Issues.html#why-this-happens","title":"Why This Happens","text":"<p>VS Code's refresh button only syncs branches you're already tracking. It doesn't check for new remote branches. When someone creates a branch remotely (web interface, another machine, teammate), your local Git doesn't know about it until you fetch.</p>"},{"location":"Git/3_Git_Issues.html#quick-reference","title":"Quick Reference","text":"<pre><code># Get all remote branches\ngit fetch --all\n\n# See what's available\ngit branch -a\n\n# Switch to the branch\ngit checkout feature/DasSomeFeatureName\n</code></pre>"},{"location":"Git/3_Git_Issues.html#key-takeaway","title":"Key Takeaway","text":"<p>VS Code's refresh \u2260 <code>git fetch</code>. If a remote branch isn't showing up, fetch first.</p>"},{"location":"Git/3_Git_Issues.html#understanding-git-reset-when-hard-reset-doesnt-work","title":"Understanding Git Reset: When Hard Reset Doesn't Work","text":""},{"location":"Git/3_Git_Issues.html#the-problem_1","title":"The Problem","text":"<p>I had this weird situation. I work on multiple machines with the same GitHub repo. Sometimes I work on one machine, push changes, and then don't touch another machine for weeks. When I finally get back to that old machine, I prefer doing a hard reset instead of merging or dealing with conflicts. Just wipe everything and start fresh with whatever's on GitHub, right?</p> <p>But here's what happened - I did a <code>git reset --hard origin/main</code> and I was still seeing old content. Multiple times. Like, what's going on? I thought hard reset was supposed to be the nuclear option that just obliterates everything local and matches the remote. But it wasn't working.</p>"},{"location":"Git/3_Git_Issues.html#what-i-was-doing-the-wrong-way","title":"What I Was Doing (The Wrong Way)","text":"<pre><code># I was doing this:\ngit reset --hard origin/main\ngit checkout main\n# Still seeing old content... tried again\ngit reset --hard origin/main\n# Still old content!\n</code></pre>"},{"location":"Git/3_Git_Issues.html#the-missing-piece","title":"The Missing Piece","text":"<p>Then I ran <code>git fetch origin</code> and boom - suddenly Git downloaded 342 objects. The output showed:</p> <pre><code>From https://github.com/dwdas9/home\na46af0f..b6044b6 main -&gt; origin/main\n</code></pre> <p>Wait, what? <code>origin/main</code> just moved from <code>a46af0f</code> to <code>b6044b6</code>? That's when it hit me.</p>"},{"location":"Git/3_Git_Issues.html#the-fundamental-misunderstanding","title":"The Fundamental Misunderstanding","text":"<p>Here's what I forgot: <code>origin/main</code> is not the actual GitHub repository. It's a local copy of where GitHub was the last time you asked.</p> <p>Let me break this down:</p>"},{"location":"Git/3_Git_Issues.html#what-git-actually-stores-locally","title":"What Git Actually Stores Locally","text":"<p>When you have a Git repo, you actually have several things:</p> <ol> <li>Your working directory - the actual files you see and edit</li> <li>Your local branches - like <code>main</code>, <code>feature-branch</code>, etc.</li> <li>Remote-tracking branches - like <code>origin/main</code>, <code>origin/develop</code>, etc.</li> </ol> <p>That third one is the key. <code>origin/main</code> is just Git's memory of where the remote <code>main</code> branch was. It's a snapshot, a cached reference. Not a live connection.</p>"},{"location":"Git/3_Git_Issues.html#when-does-this-memory-get-updated","title":"When Does This Memory Get Updated?","text":"<p>Only when you explicitly talk to GitHub: - <code>git fetch</code> - updates your memory - <code>git pull</code> - updates your memory AND merges - <code>git push</code> - sends your changes (also updates memory)</p>"},{"location":"Git/3_Git_Issues.html#what-i-was-actually-doing","title":"What I Was Actually Doing","text":"<pre><code>git reset --hard origin/main\n</code></pre> <p>This command means: \"Reset my working directory and local branch to match <code>origin/main</code>\"</p> <p>But my <code>origin/main</code> was pointing to a commit from weeks ago because I hadn't fetched in weeks! So I was just resetting to old content, over and over.</p> <p>It's like asking someone \"What time is it?\" and they tell you what time it was the last time they checked their watch three days ago. You keep asking, they keep saying \"3:47 PM Tuesday\" because they haven't looked at their watch since then.</p>"},{"location":"Git/3_Git_Issues.html#the-right-way-to-hard-reset","title":"The Right Way to Hard Reset","text":""},{"location":"Git/3_Git_Issues.html#method-1-fetch-first-then-reset","title":"Method 1: Fetch First, Then Reset","text":"<pre><code># Step 1: Update your knowledge of what's on GitHub\ngit fetch origin\n\n# Step 2: Now reset to that updated information\ngit reset --hard origin/main\n</code></pre>"},{"location":"Git/3_Git_Issues.html#method-2-pull-with-force-one-command","title":"Method 2: Pull with Force (One Command)","text":"<pre><code># This does fetch + reset in one go\ngit pull --force origin main\n</code></pre>"},{"location":"Git/3_Git_Issues.html#method-3-the-safest-comprehensive-reset","title":"Method 3: The Safest Comprehensive Reset","text":"<pre><code># Fetch all remote branches\ngit fetch origin\n\n# Make sure you're on the right branch\ngit checkout main\n\n# Reset to match remote exactly\ngit reset --hard origin/main\n\n# Clean any untracked files/directories (optional but thorough)\ngit clean -fd\n</code></pre>"},{"location":"Git/3_Git_Issues.html#key-learnings","title":"Key Learnings","text":""},{"location":"Git/3_Git_Issues.html#1-remote-tracking-branches-are-local-references","title":"1. Remote-Tracking Branches are Local References","text":"<p><code>origin/main</code> lives on your computer. It's not magical or live-updated. It's just Git's bookmark saying \"last time I checked, GitHub's main branch was here.\"</p>"},{"location":"Git/3_Git_Issues.html#2-fetch-updates-your-mental-model","title":"2. Fetch Updates Your Mental Model","text":"<p><code>git fetch</code> is like syncing your calendar. It doesn't change your schedule (working directory), it just updates your knowledge of what everyone else is doing.</p>"},{"location":"Git/3_Git_Issues.html#3-the-git-three-way-relationship","title":"3. The Git Three-Way Relationship","text":"<p>Think of it as three separate entities: - GitHub's actual repository (remote server) - Your remote-tracking branches (<code>origin/main</code> - local cache) - Your local branches (<code>main</code> - what you work on)</p> <p>When you reset to <code>origin/main</code>, you're resetting to #2, not #1. You need fetch to sync #2 with #1 first.</p>"},{"location":"Git/3_Git_Issues.html#4-why-this-matters-more-with-multiple-machines","title":"4. Why This Matters More with Multiple Machines","text":"<p>When you work on Machine A, push changes, then go to Machine B weeks later: - GitHub has all your new changes - Machine B's <code>origin/main</code> still points to the old commit from weeks ago - Machine B has no idea anything changed until you fetch</p> <p>If you skip the fetch step, you're resetting to outdated information.</p>"},{"location":"Git/3_Git_Issues.html#other-related-commands-to-understand","title":"Other Related Commands to Understand","text":""},{"location":"Git/3_Git_Issues.html#git-fetch-vs-git-pull","title":"git fetch vs git pull","text":"<pre><code># Just updates origin/main reference, doesn't touch your files\ngit fetch origin\n\n# Updates origin/main AND merges it into your current branch\ngit pull origin main\n</code></pre>"},{"location":"Git/3_Git_Issues.html#checking-whats-different","title":"Checking What's Different","text":"<pre><code># See what's different between local and remote (after fetching)\ngit log HEAD..origin/main\n\n# Show all remote branches and their status\ngit branch -r\n</code></pre>"},{"location":"Git/3_Git_Issues.html#force-pull-vs-force-push","title":"Force Pull vs Force Push","text":"<pre><code># Force pull - discard local changes, match remote\ngit fetch origin\ngit reset --hard origin/main\n\n# Force push - dangerous! Overwrites remote with local\ngit push --force origin main  # Be careful with this!\n</code></pre>"},{"location":"Git/3_Git_Issues.html#when-to-use-hard-reset","title":"When to Use Hard Reset","text":"<p>Hard reset is perfect for: - Getting a clean slate when you've been experimenting - Syncing a rarely-used machine with the main repo - Recovering from a messy local state - When you don't care about local changes at all</p> <p>But remember: Always fetch first so you're resetting to the actual latest code, not to your outdated memory of what the latest code was.</p>"},{"location":"Git/3_Git_Issues.html#the-command-i-should-have-run","title":"The Command I Should Have Run","text":"<p>Instead of repeatedly doing: <pre><code>git reset --hard origin/main  # Wrong - resetting to old cached reference\n</code></pre></p> <p>I should have done: <pre><code>git fetch origin              # First - update the cache\ngit reset --hard origin/main  # Then - reset to updated cache\n</code></pre></p> <p>Or just: <pre><code>git pull --force origin main  # Does both in one step\n</code></pre></p>"},{"location":"Git/3_Git_Issues.html#conclusion","title":"Conclusion","text":"<p>Git is a distributed version control system, and \"distributed\" means your local repo is independent. <code>origin/main</code> is just your local copy of a reference, not a live connection. Understanding this distinction is crucial for avoiding confusion when working across multiple machines.</p> <p>The lesson? Fetch before you reset. Always update your local references before using them to reset your working state. Otherwise you're just resetting to old information, which is exactly what happened to me.</p>"},{"location":"Git/3_Git_Rebase.html","title":"Git Rebase: Handling Divergent Branches","text":"<p>When you try to push changes and Git says your branch has diverged, it's protecting you from losing work. Let's understand why this happens and how to fix it cleanly.</p>"},{"location":"Git/3_Git_Rebase.html#the-problem","title":"The Problem","text":"<p>You're working on a feature branch, make some commits, and try to push. Suddenly:</p> <pre><code>$ git push\nTo https://github.com/dwdas9/home.git\n ! [rejected]        feature/25.10.25 -&gt; feature/25.10.25 (fetch first)\nerror: failed to push some refs to 'https://github.com/dwdas9/home.git'\n</code></pre> <p>Push Rejected</p> <p>Git is telling you: \"The remote has changes you don't have locally. Pull first!\"</p> <p>So you run <code>git pull</code>, but instead of fixing things, you get another error:</p> <pre><code>$ git pull\nfatal: Need to specify how to reconcile divergent branches.\n</code></pre> <p>What Git is Really Asking</p> <p>\"Your local and remote branches have diverged. How do you want to combine them?\"</p> <ul> <li><code>git config pull.rebase false</code> \u2192 merge (create a merge commit)</li> <li><code>git config pull.rebase true</code> \u2192 rebase (replay your commits on top)</li> <li><code>git config pull.ff only</code> \u2192 fast-forward only (only works if you haven't committed)</li> </ul> <p>This guide will help you choose the right strategy.</p>"},{"location":"Git/3_Git_Rebase.html#understanding-divergent-branches","title":"Understanding Divergent Branches","text":"<p>Think of it like two editors working on the same document simultaneously. Eventually, you need to reconcile the changes.</p> <p></p> Breaking Down the Diagram <ul> <li>Commits A, B, C: Shared history between local and remote</li> <li>Commit C: Where the branches diverged (common ancestor)</li> <li>Commits E, F (red): Changes pushed to remote while you worked locally</li> <li>Commit D (yellow): Your local commit that hasn't been pushed</li> </ul> <p>Git doesn't know which version is \"correct\"\u2014both have valid work!</p>"},{"location":"Git/3_Git_Rebase.html#common-causes","title":"Common Causes","text":"Multiple DevelopersMultiple MachinesCI/CD AutomationForce Push <p>You and a teammate both work on <code>feature/auth</code>. They push first. When you try to push, your branches have diverged.</p> <p>You push from your work laptop, then commit from home (forgetting to pull first). Divergence!</p> <p>Your pipeline makes automated commits (version bumps, lockfiles) while you're coding locally.</p> <p>Someone force-pushed to the branch, rewriting history. Your local branch is now out of sync.</p>"},{"location":"Git/3_Git_Rebase.html#three-solutions","title":"Three Solutions","text":"<p>Git offers three ways to reconcile divergent branches. Let's explore when to use each.</p>"},{"location":"Git/3_Git_Rebase.html#1-merge-strategy","title":"1. Merge Strategy","text":"<p>Creates a new \"merge commit\" with two parents, preserving both histories.</p> <p>When to Use Merge</p> <p>Best for: Main/shared branches where you want full transparency</p> <p>Example: Working on <code>main</code> with your team. You want to see exactly when features were integrated.</p> <pre><code>git config pull.rebase false\ngit pull  # Creates a merge commit\ngit push\n</code></pre> Merge Advantages <ul> <li>Preserves complete history</li> <li>Shows when branches were integrated</li> <li>Non-destructive (doesn't rewrite history)</li> <li>Good for collaborative branches</li> </ul>"},{"location":"Git/3_Git_Rebase.html#2-rebase-strategy-recommended","title":"2. Rebase Strategy \u2b50 Recommended","text":"<p>Replays your commits on top of the latest remote commits, creating a linear history.</p> <p>When to Use Rebase</p> <p>Best for: Feature branches where you're the primary developer</p> <p>Example: Working solo on <code>feature/login</code>. You want clean, linear history.</p> <pre><code>git config pull.rebase true\ngit pull  # Replays your commits on top\ngit push\n</code></pre> Rebase Advantages <ul> <li>Clean, linear history</li> <li>Easier to understand commit flow</li> <li>No extra merge commits cluttering history</li> <li>Perfect for feature branches</li> </ul> <p>Golden Rule of Rebase</p> <p>Never rebase commits that you've already pushed to a shared branch where others might have based work on them. Only rebase local commits or personal feature branches.</p>"},{"location":"Git/3_Git_Rebase.html#3-fast-forward-only","title":"3. Fast-Forward Only","text":"<p>Only updates if your local branch is simply \"behind\" (no local commits).</p> <p>When to Use Fast-Forward</p> <p>Best for: When you want to be extra cautious and avoid automatic merges</p> <p>Example: You cloned a repo and haven't made changes. Just want to update.</p> <pre><code>git config pull.ff only\ngit pull  # Works only if you haven't committed locally\n</code></pre> Why It Often Fails <p>Fast-forward only works when:</p> <ul> <li>Remote has new commits</li> <li>You have NO new local commits</li> </ul> <p>In our divergent branch scenario, you HAVE local commits (Commit D), so this fails with: <pre><code>fatal: Not possible to fast-forward, aborting.\n</code></pre></p>"},{"location":"Git/3_Git_Rebase.html#solving-the-problem-step-by-step","title":"Solving the Problem: Step-by-Step","text":"<p>Let's fix the divergent branch issue from our example.</p>"},{"location":"Git/3_Git_Rebase.html#quick-solution-feature-branch","title":"Quick Solution (Feature Branch)","text":"<p>For a personal feature branch, rebase is cleanest:</p> <pre><code># 1. Set rebase as the strategy\ngit config pull.rebase true\n\n# 2. Pull with rebase\ngit pull\n\n# 3. Push your changes\ngit push\n</code></pre> <p>Done!</p> <p>Your commits are now replayed on top of the remote changes. Clean, linear history!</p>"},{"location":"Git/3_Git_Rebase.html#handling-merge-conflicts","title":"Handling Merge Conflicts","text":"<p>Sometimes your changes and remote changes touch the same lines:</p> <pre><code>$ git pull\nAuto-merging myfile.py\nCONFLICT (content): Merge conflict in myfile.py\nerror: could not apply 396de08... changes\nResolve all conflicts manually, mark them as resolved with\n\"git add/rm &lt;conflicted_files&gt;\", then run \"git rebase --continue\".\n</code></pre> Conflict Resolution Steps <p>1. Open the conflicting file(s)</p> <p>Git marks conflicts with special markers: <pre><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n# Remote version (what's on GitHub)\nprint(\"Hello from remote\")\n=======\n# Your version (your local commit)\nprint(\"Hello from local\")\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; 396de08 (changes)\n</code></pre></p> <p>2. Edit to resolve the conflict <pre><code># Resolved version - keep what you need\nprint(\"Hello from both versions\")\n</code></pre></p> <p>3. Stage the resolved file <pre><code>git add myfile.py\n</code></pre></p> <p>4. Continue the rebase <pre><code>git rebase --continue\n</code></pre></p> <p>5. Push your changes <pre><code>git push\n</code></pre></p> <p>Need to Abort?</p> <p>If things go wrong during conflict resolution: <pre><code>git rebase --abort  # Returns to state before rebase\n</code></pre></p>"},{"location":"Git/3_Git_Rebase.html#best-practices","title":"Best Practices","text":"<p>Minimize divergent branches with these habits:</p>"},{"location":"Git/3_Git_Rebase.html#always-pull-before-pushing","title":"Always Pull Before Pushing","text":"<pre><code># Make it a habit\ngit pull\ngit push\n</code></pre>"},{"location":"Git/3_Git_Rebase.html#set-a-default-strategy","title":"Set a Default Strategy","text":"<p>Choose your preferred method globally:</p> Rebase (Recommended)MergeFast-Forward Only <pre><code>git config --global pull.rebase true\n</code></pre> <pre><code>git config --global pull.rebase false\n</code></pre> <pre><code>git config --global pull.ff only\n</code></pre>"},{"location":"Git/3_Git_Rebase.html#additional-tips","title":"Additional Tips","text":"<p>Team Coordination</p> <ul> <li>Communicate when working on the same branch</li> <li>Use separate feature branches per developer</li> <li>Consider branch protection rules</li> </ul> <p>Commit Frequently</p> <ul> <li>Don't let your local branch get too far ahead</li> <li>Smaller, frequent pushes reduce conflict chances</li> <li>Push daily (or more often)</li> </ul> <p>Use Pull Requests</p> <p>For main/shared branches, use PRs instead of direct pushes. This prevents accidental divergence.</p>"},{"location":"Git/3_Git_Rebase.html#quick-reference","title":"Quick Reference","text":"Strategy Command Best For Result Rebase <code>git pull --rebase</code> Feature branches, solo work Linear history Merge <code>git pull --no-rebase</code> Shared branches, teams Preserves full history FF Only <code>git pull --ff-only</code> Safe updates, no local commits Simple updates only Common Commands <pre><code># Set default strategy\ngit config pull.rebase true   # or false, or ff-only\n\n# Pull with specific strategy (one-time)\ngit pull --rebase\ngit pull --no-rebase\ngit pull --ff-only\n\n# During conflict resolution\ngit rebase --continue   # After resolving conflicts\ngit rebase --abort      # Undo the rebase\ngit rebase --skip       # Skip current commit\n\n# View branch status\ngit status\ngit log --oneline --graph --all\n</code></pre>"},{"location":"Git/3_Git_Rebase.html#key-takeaways","title":"Key Takeaways","text":"<p>Remember</p> <ul> <li>Divergent branches are normal\u2014they happen when local and remote histories differ</li> <li>Three solutions: Merge, Rebase, or Fast-Forward (when possible)</li> <li>For feature branches: Use rebase for clean, linear history</li> <li>For shared branches: Use merge to preserve complete history</li> <li>Always pull before pushing to minimize divergence</li> </ul> <p>Real-World Solution</p> <p>For the scenario we started with: <pre><code>git config pull.rebase true\ngit pull\ngit push\n</code></pre> Clean history maintained, changes successfully pushed!</p>"},{"location":"Git/3_Git_Rebase.html#learn-more","title":"Learn More","text":"<ul> <li>Git Documentation: Rebasing</li> <li>Atlassian: Merging vs Rebasing</li> <li>GitHub: Resolving Merge Conflicts</li> </ul>"},{"location":"Git/4_Git_Detached_Head.html","title":"Git Detached HEAD: Why VS Code Shows a Commit Hash Instead of Branch Name","text":"<p>The Problem</p> <p>You run <code>git checkout origin/main</code> expecting VS Code to show \"main\" at the bottom left, but instead it shows something like <code>353d709</code>. What's going on?</p> <p></p>"},{"location":"Git/4_Git_Detached_Head.html#whats-happening","title":"What's Happening","text":"<p>Detached HEAD State</p> <p>You're in a detached HEAD state. Here's what happened:</p> <ul> <li><code>origin/main</code> is a reference to the remote branch, not your local branch</li> <li>When you checkout <code>origin/main</code>, you're literally sitting on a commit, not on a branch</li> <li>VS Code is being honest\u2014you're not on any branch, you're on commit <code>353d709</code></li> <li>Git even warns you: \"You are in 'detached HEAD' state\"</li> </ul> <p>Think of it this way</p> <p><code>origin/main</code> is like a bookmark to where the remote branch is. You're checking out the bookmark, not the actual branch.</p>"},{"location":"Git/4_Git_Detached_Head.html#the-solution","title":"The Solution","text":"<p>Simple Fix</p> <p>Stop checking out <code>origin/main</code>. Check out your local <code>main</code> branch instead:</p> <pre><code>git checkout main\ngit pull\n</code></pre> <p>That's it. Now VS Code will show \"main\" because you're actually on the main branch.</p> <p></p> Quick Fix Commands <p>If you're already in detached HEAD state:</p> <pre><code># Just switch to your local main branch\ngit checkout main\n\n# Then pull if needed\ngit pull\n</code></pre> Don't Have a Local Main Branch? <p>If <code>git checkout main</code> gives you an error, create it:</p> <pre><code>git checkout -b main origin/main\n</code></pre> <p>This creates a local <code>main</code> branch that tracks <code>origin/main</code>.</p> <p>The Bottom Line</p> <p>Always checkout local branches (<code>main</code>), never remote references (<code>origin/main</code>) unless you specifically want to inspect that commit without being on a branch.</p>"},{"location":"Git/Syllabus.html","title":"Syllabus","text":""},{"location":"Git/Syllabus.html#git-tutorial-syllabus-command-mastery-from-basics-to-advanced","title":"Git Tutorial Syllabus: Command Mastery from Basics to Advanced","text":""},{"location":"Git/Syllabus.html#learn-git-through-commands-practical-real-world-focused","title":"Learn Git Through Commands - Practical, Real-World Focused","text":""},{"location":"Git/Syllabus.html#module-1-getting-started","title":"Module 1: Getting Started","text":"<ol> <li>Setup &amp; First Commands</li> <li>Installing Git</li> <li><code>git config --global user.name \"Your Name\"</code></li> <li><code>git config --global user.email \"your@email.com\"</code></li> <li>Essential configuration for associating work with your identity</li> <li>Quick Win: Configure Git in 2 minutes</li> </ol>"},{"location":"Git/Syllabus.html#module-2-core-commands-the-daily-essentials","title":"Module 2: Core Commands - The Daily Essentials","text":"<ol> <li>Repository Basics</li> <li><code>git init</code> - Create a new repository</li> <li><code>git clone [url]</code> - Download existing project</li> <li>Concept: What is a repository (local vs remote)?</li> <li> <p>Exercise: Clone a real project from GitHub</p> </li> <li> <p>The Fundamental Workflow</p> </li> <li><code>git status</code> - Check state of working directory</li> <li><code>git add [file]</code> or <code>git add .</code> - Stage changes</li> <li><code>git commit -m \"message\"</code> - Save snapshot locally</li> <li><code>git commit -a -m \"message\"</code> - Stage and commit tracked files</li> <li><code>git commit --amend</code> - Modify last commit</li> <li>Concept: Working Directory \u2192 Staging Area \u2192 Repository</li> <li>VS Code: Using Source Control panel</li> <li> <p>Project: Build a portfolio website with meaningful commits</p> </li> <li> <p>Viewing History</p> </li> <li><code>git log</code> - View commit history</li> <li><code>git log --oneline</code> - Condensed view</li> <li><code>git log --graph --oneline --all</code> - Visual branch history</li> <li><code>git show [commit]</code> - View specific commit</li> <li><code>git diff</code> - See unstaged changes</li> <li><code>git diff --staged</code> - See staged changes</li> <li>VS Code: Using Timeline and GitLens</li> </ol>"},{"location":"Git/Syllabus.html#module-3-branching-commands","title":"Module 3: Branching Commands","text":"<ol> <li>Branch Operations</li> <li><code>git branch</code> - List branches</li> <li><code>git branch [branch-name]</code> - Create new branch</li> <li><code>git checkout [branch-name]</code> - Switch branches</li> <li><code>git checkout -b [branch-name]</code> - Create and switch</li> <li><code>git switch [branch-name]</code> - Modern way to switch branches</li> <li><code>git branch -d [branch-name]</code> - Delete branch</li> <li><code>git branch -D [branch-name]</code> - Force delete</li> <li>Concept: Why branches matter (parallel development)</li> <li>VS Code: Bottom-left branch switcher</li> <li> <p>Scenario: Feature development workflow</p> </li> <li> <p>Merging Commands</p> </li> <li><code>git merge [branch-name]</code> - Integrate changes</li> <li><code>git merge --no-ff [branch-name]</code> - Force merge commit</li> <li><code>git merge --abort</code> - Cancel merge during conflicts</li> <li>Concept: Fast-forward vs three-way merge</li> <li>Handling Conflicts: Manual resolution</li> <li>VS Code: Conflict resolution interface</li> <li>Exercise: Create and resolve conflicts intentionally</li> </ol>"},{"location":"Git/Syllabus.html#module-4-remote-repository-commands","title":"Module 4: Remote Repository Commands","text":"<ol> <li>Understanding Remotes</li> <li><code>git remote -v</code> - List remotes</li> <li><code>git remote add origin [url]</code> - Add remote</li> <li><code>git remote remove [name]</code> - Remove remote</li> <li><code>git remote rename [old] [new]</code> - Rename remote</li> <li>Concept: Origin (your remote repository)</li> <li> <p>Concept: Upstream (original when forked)</p> </li> <li> <p>Syncing Commands - Push, Pull, Fetch</p> </li> <li><code>git push [remote] [branch]</code> - Upload commits</li> <li><code>git push -u origin [branch]</code> - Set upstream and push</li> <li><code>git push --force-with-lease</code> - Safe force push</li> <li><code>git pull</code> - Fetch + Merge automatically</li> <li><code>git fetch</code> - Download without merging (safer)</li> <li><code>git fetch --all</code> - Fetch from all remotes</li> <li>Concept: Fetch downloads changes safely, Pull fetches and merges</li> <li>When to use what: Fetch for safety, Pull for quick updates</li> <li> <p>Project: Push portfolio to GitHub</p> </li> <li> <p>GitHub Workflow Commands</p> </li> <li><code>git clone [url]</code> - Download repository</li> <li>Fork workflow (on GitHub)</li> <li>Creating Pull Requests</li> <li>VS Code: GitHub Pull Requests extension</li> </ol>"},{"location":"Git/Syllabus.html#module-5-undo-fix-commands-essential-survival","title":"Module 5: Undo &amp; Fix Commands (Essential Survival)","text":"<ol> <li> <p>Undoing Changes</p> <ul> <li><code>git restore [file]</code> - Discard working directory changes</li> <li><code>git restore --staged [file]</code> - Unstage files</li> <li><code>git reset HEAD [file]</code> - Unstage (older way)</li> <li><code>git reset --soft HEAD~1</code> - Undo commit, keep changes staged</li> <li><code>git reset --mixed HEAD~1</code> - Undo commit, unstage changes</li> <li><code>git reset --hard HEAD~1</code> - Undo commit, discard changes</li> <li><code>git revert [commit]</code> - Create new commit that undoes changes</li> <li>Concept: Reset vs Revert (when to use which)</li> <li>Common Scenarios: \"I committed to wrong branch!\"</li> <li>VS Code: Undo last commit option</li> </ul> </li> <li> <p>Stash Commands</p> <ul> <li><code>git stash</code> - Temporarily save changes</li> <li><code>git stash save \"message\"</code> - Stash with description</li> <li><code>git stash list</code> - View all stashes</li> <li><code>git stash pop</code> - Apply and remove latest stash</li> <li><code>git stash apply</code> - Apply without removing</li> <li><code>git stash apply stash@{n}</code> - Apply specific stash</li> <li><code>git stash drop</code> - Remove stash</li> <li><code>git stash clear</code> - Remove all stashes</li> <li>Use Case: Switch branches quickly without committing incomplete work</li> <li>VS Code: Stash operations in Source Control</li> </ul> </li> </ol>"},{"location":"Git/Syllabus.html#module-6-advanced-commands-power-user","title":"Module 6: Advanced Commands (Power User)","text":"<ol> <li> <p>Rebase Commands</p> <ul> <li><code>git rebase [branch]</code> - Replay commits on new base</li> <li><code>git rebase -i HEAD~n</code> - Interactive rebase (edit history)</li> <li><code>git rebase --continue</code> - Continue after resolving conflicts</li> <li><code>git rebase --abort</code> - Cancel rebase</li> <li><code>git rebase --skip</code> - Skip problematic commit</li> <li>Concept: Rebase creates linear history, avoids merge commits</li> <li>Golden Rule: Never rebase public/shared commits</li> <li>Interactive Rebase: Squash, reorder, edit, or drop commits</li> <li>When: Cleaning feature branch before merging</li> <li>Exercise: Squash multiple commits into one</li> </ul> </li> <li> <p>Cherry-pick Commands</p> <ul> <li><code>git cherry-pick [commit]</code> - Apply specific commit to current branch</li> <li><code>git cherry-pick [commit1] [commit2]</code> - Multiple commits</li> <li><code>git cherry-pick [commit1]..[commit2]</code> - Range of commits</li> <li><code>git cherry-pick --no-commit [commit]</code> - Apply without committing</li> <li><code>git cherry-pick --continue</code> - Continue after conflict</li> <li><code>git cherry-pick --abort</code> - Cancel operation</li> <li>Use Case: Apply bug fix from one branch to another without merging everything</li> <li>Scenario: Hotfix to multiple versions</li> </ul> </li> <li> <p>Reflog Commands (The Safety Net)</p> <ul> <li><code>git reflog</code> - View history of HEAD movements</li> <li><code>git reflog show [branch]</code> - Reflog for specific branch</li> <li><code>git reset --hard HEAD@{n}</code> - Recover to previous state</li> <li><code>git checkout HEAD@{n}</code> - View previous state</li> <li><code>git branch [name] HEAD@{n}</code> - Create branch from reflog</li> <li>Concept: Reflog logs every action that modified HEAD - your undo history</li> <li>Use Case: Recover accidentally deleted commits or branches</li> <li>Exercise: \"Accidentally\" delete commits, then recover them</li> </ul> </li> </ol>"},{"location":"Git/Syllabus.html#module-7-inspection-comparison-commands","title":"Module 7: Inspection &amp; Comparison Commands","text":"<ol> <li> <p>Advanced Inspection</p> <ul> <li><code>git log --author=\"name\"</code> - Filter by author</li> <li><code>git log --since=\"2 weeks ago\"</code> - Time-based filtering</li> <li><code>git log --grep=\"keyword\"</code> - Search commit messages</li> <li><code>git log -p</code> - Show diff in each commit</li> <li><code>git log -- [file]</code> - History of specific file</li> <li><code>git blame [file]</code> - See who changed each line</li> <li><code>git show [commit]:[file]</code> - View file at specific commit</li> <li><code>git diff [branch1]..[branch2]</code> - Compare branches</li> <li><code>git diff [commit1] [commit2]</code> - Compare commits</li> <li>VS Code: GitLens for inline blame and history</li> </ul> </li> <li> <p>Finding Issues</p> <ul> <li><code>git bisect start</code> - Start binary search for bugs</li> <li><code>git bisect bad</code> - Mark current as bad</li> <li><code>git bisect good [commit]</code> - Mark known good commit</li> <li><code>git bisect reset</code> - End bisect session</li> <li>Use Case: Find which commit introduced a bug</li> </ul> </li> </ol>"},{"location":"Git/Syllabus.html#module-8-cleaning-maintenance-commands","title":"Module 8: Cleaning &amp; Maintenance Commands","text":"<ol> <li>Cleanup Commands<ul> <li><code>git clean -n</code> - Preview what will be removed</li> <li><code>git clean -f</code> - Remove untracked files</li> <li><code>git clean -fd</code> - Remove untracked files and directories</li> <li><code>git clean -fx</code> - Include ignored files</li> <li><code>git gc</code> - Garbage collection (optimize repository)</li> <li><code>git prune</code> - Remove unreachable objects</li> </ul> </li> </ol>"},{"location":"Git/Syllabus.html#module-9-collaboration-workflow-commands","title":"Module 9: Collaboration &amp; Workflow Commands","text":"<ol> <li> <p>Tags &amp; Releases</p> <ul> <li><code>git tag</code> - List tags</li> <li><code>git tag [tag-name]</code> - Create lightweight tag</li> <li><code>git tag -a [tag-name] -m \"message\"</code> - Annotated tag</li> <li><code>git tag -d [tag-name]</code> - Delete local tag</li> <li><code>git push origin [tag-name]</code> - Push tag to remote</li> <li><code>git push origin --tags</code> - Push all tags</li> <li>Concept: Semantic versioning (v1.0.0, v1.1.0, v2.0.0)</li> </ul> </li> <li> <p>Workflow Patterns</p> <ul> <li>GitHub Flow: main + feature branches + PRs</li> <li>Git Flow: main + develop + feature/release/hotfix</li> <li><code>.gitignore</code> file essentials</li> <li>Commit message conventions</li> <li>Branch naming strategies</li> <li>Case Study: How professional teams work</li> </ul> </li> </ol>"},{"location":"Git/Syllabus.html#module-10-configuration-aliases","title":"Module 10: Configuration &amp; Aliases","text":"<ol> <li>Productivity Commands<ul> <li><code>git config --list</code> - View all settings</li> <li><code>git config --global alias.st status</code> - Create aliases</li> <li><code>git config --global alias.co checkout</code></li> <li><code>git config --global alias.br branch</code></li> <li><code>git config --global alias.lg \"log --graph --oneline\"</code></li> <li>Useful Aliases: Speed up common operations</li> <li>Setting default editor</li> <li>Setting merge tool</li> </ul> </li> </ol>"},{"location":"Git/Syllabus.html#module-11-real-world-problem-solving","title":"Module 11: Real-World Problem Solving","text":"<ol> <li> <p>Common Problems &amp; Solutions</p> <ul> <li>\"Detached HEAD state\" - What and how to fix</li> <li>\"Can't push - rejected\" - Understanding and fixing</li> <li>\"Merge conflicts\" - Resolving strategies</li> <li>\"Accidentally committed sensitive data\" - Removing from history</li> <li>\"Lost commits\" - Using reflog to recover</li> <li>\"Wrong branch\" - Moving commits between branches</li> <li>Troubleshooting Guide: Step-by-step solutions</li> </ul> </li> <li> <p>Complete Workflow Scenarios</p> <ul> <li>Solo developer workflow</li> <li>Team collaboration (multiple developers)</li> <li>Open source contribution (fork \u2192 PR)</li> <li>Emergency hotfix procedure</li> <li>Release management</li> <li>Hands-on: Practice all scenarios</li> </ul> </li> </ol>"},{"location":"Git/Syllabus.html#bonus-quick-reference","title":"Bonus: Quick Reference","text":"<ol> <li>Command Cheat Sheet<ul> <li>Daily commands quick reference</li> <li>VS Code keyboard shortcuts</li> <li>Git aliases compilation</li> <li>Command comparison table</li> <li>Downloadable PDF: Printable cheat sheet</li> </ul> </li> </ol>"},{"location":"Git/Syllabus.html#what-makes-this-syllabus-command-focused","title":"What Makes This Syllabus Command-Focused:","text":"<p>\u2705 Top 12 Essential Commands covered comprehensively \u2705 Command-First Approach: Every lesson teaches actual commands \u2705 Practical Examples: Real scenarios for each command \u2705 VS Code Integration: IDE shortcuts alongside terminal commands \u2705 Progressive Complexity: Basic \u2192 Intermediate \u2192 Advanced \u2705 Problem-Solving Focus: Common issues and their command solutions \u2705 Hands-On: Every module has practical exercises  </p> <p>Total: 23 lessons, ~70+ commands mastered, 10-12 hours of focused learning</p> <p>The MkDocs YAML:</p> <pre><code>- Git:\n      - 1. Getting Started:\n          - Setup &amp; First Commands: 'DevOps/Git/1_Setup_First_Commands.md'\n\n      - 2. Core Commands - Daily Essentials:\n          - Repository Basics (init, clone): 'DevOps/Git/2_Repository_Basics.md'\n          - The Fundamental Workflow (status, add, commit): 'DevOps/Git/2.1_Fundamental_Workflow.md'\n          - Viewing History (log, show, diff): 'DevOps/Git/2.2_Viewing_History.md'\n\n      - 3. Branching Commands:\n          - Branch Operations (branch, checkout, switch): 'DevOps/Git/3_Branch_Operations.md'\n          - Merging Commands (merge, conflicts): 'DevOps/Git/3.1_Merging_Commands.md'\n\n      - 4. Remote Repository Commands:\n          - Understanding Remotes (remote commands): 'DevOps/Git/4_Understanding_Remotes.md'\n          - Push, Pull, Fetch - Syncing Commands: 'DevOps/Git/4.1_Push_Pull_Fetch.md'\n          - GitHub Workflow Commands: 'DevOps/Git/4.2_GitHub_Workflow.md'\n\n      - 5. Undo &amp; Fix Commands:\n          - Undoing Changes (restore, reset, revert): 'DevOps/Git/5_Undoing_Changes.md'\n          - Stash Commands: 'DevOps/Git/5.1_Stash_Commands.md'\n\n      - 6. Advanced Commands:\n          - Rebase Commands: 'DevOps/Git/6_Rebase_Commands.md'\n          - Cherry-pick Commands: 'DevOps/Git/6.1_Cherry_Pick_Commands.md'\n          - Reflog Commands - The Safety Net: 'DevOps/Git/6.2_Reflog_Commands.md'\n\n      - 7. Inspection &amp; Comparison:\n          - Advanced Inspection Commands: 'DevOps/Git/7_Advanced_Inspection.md'\n          - Finding Issues (bisect): 'DevOps/Git/7.1_Finding_Issues.md'\n\n      - 8. Cleaning &amp; Maintenance:\n          - Cleanup Commands (clean, gc, prune): 'DevOps/Git/8_Cleanup_Commands.md'\n\n      - 9. Collaboration &amp; Workflow:\n          - Tags &amp; Releases Commands: 'DevOps/Git/9_Tags_Releases.md'\n          - Workflow Patterns &amp; Best Practices: 'DevOps/Git/9.1_Workflow_Patterns.md'\n\n      - 10. Configuration &amp; Productivity:\n          - Configuration &amp; Aliases: 'DevOps/Git/10_Config_Aliases.md'\n\n      - 11. Real-World Problem Solving:\n          - Common Problems &amp; Command Solutions: 'DevOps/Git/11_Common_Problems.md'\n          - Complete Workflow Scenarios: 'DevOps/Git/11.1_Workflow_Scenarios.md'\n\n      - 12. Quick Reference:\n          - Command Cheat Sheet: 'DevOps/Git/12_Command_Cheat_Sheet.md'\n</code></pre>"},{"location":"M365/DocumentumToSharePoint.html","title":"Documentum","text":""},{"location":"M365/DocumentumToSharePoint.html#case-study-documentum-to-sharepoint-online-and-azure-blob","title":"Case study: Documentum to SharePoint Online and Azure Blob","text":"<p>For one of my clients, the Document Archving and ECM system used on-prem Documentum. This system was connected with  a large number of field offices scanning/importing contet through OpenText Intelligent Capture. The total licensing and  maintenace cost was exorbitant. There was non HA and DR. Moreover many departments used simple filesytem for archiving. When advising this client on enhancing their content management system, the focus was on overcoming the challenges posed by Documentum, particularly in terms of cost, integration difficulties, and user experience issues. The recommended strategy involved transitioning to SharePoint Online for active content management and Azure Blob Storage for archiving. This approach resulted in many benefits aligned with the client's need to reduce 'technical debt'.</p>"},{"location":"M365/DocumentumToSharePoint.html#challenges-with-documentum","title":"Challenges with Documentum","text":"<ul> <li>High Costs: Licensing, hardware maintenance, and support costs were unnecessarily high.</li> <li>Integration and User Experience Issues: Difficulties in integrating with the Office 365 and Azure ecosystems, coupled with user experience challenges due to frequent maintenance issues with the Java applet in WebTop.</li> </ul>"},{"location":"M365/DocumentumToSharePoint.html#the-strategic-shift","title":"The Strategic Shift","text":"<ul> <li>To SharePoint Online for Live Content: For its cost-effectiveness, scalability, and seamless integration within the Office 365 ecosystem.</li> <li>To Azure Blob for Archiving: For its scalability and cost-efficiency in handling large volumes of archived(structured/Unstructured) data. There are many tiers, the hot(low latency) cool(infrequent access)</li> </ul>"},{"location":"M365/DocumentumToSharePoint.html#key-benefits-of-the-transition","title":"Key Benefits of the Transition","text":"<ol> <li>Enhanced Mapping Features: SharePoint Online supports advanced mapping of Documentum structures, including advanced library configuration.</li> <li>Improved Access Control: Superior ACL settings in SharePoint Online mirror Documentum's capabilities, offering more refined permission management.</li> <li>No Database Constraints: Unlike previous SharePoint versions, SharePoint Online has no SQL server database size limit, eliminating on-prem system capacity issues.</li> <li>Advanced Content Management: Supports the management of linked documents, renditions, and contentless objects, making the migration process straightforward.</li> <li>Direct Exports with OpenText: OpenText Intellinget Capture's Microsoft SharePoint Export - Exports documents and data directly to SharePoint.</li> <li>ADF abd AZCopy : Using these tools it was easily possible to move data to Azure blob storage.</li> </ol> <p>By moving to SharePoint Online and Azure Blob Storage, the client resolved their current issues and also used advanced features and future-proofed their system.</p>"},{"location":"M365/LicensingExamples.html","title":"Licensing","text":""},{"location":"M365/LicensingExamples.html#m365-example-licensing","title":"M365 Example licensing:","text":"<p>I went to a shop, and the prices of the goods were so complicated that I just threw some cash down and told the shopkeeper to give me the best I could get for that amount. That's the story here: Microsoft licensing is so complex that these tables will give you some idea of what you would get if you shelled out $1,000 or more.</p> <p>Note: These tables are just here to give you some idea. Prices change, you might get discounts, or new features might come in. So, this is just to give you a basic idea. Don't take this table to the Microsoft sales guy and say that it says this.</p>"},{"location":"M365/LicensingExamples.html#10000-budget","title":"$10,000 Budget","text":"Product Annual Cost Per User Number of Users Features Power Automate $180 55 Unlimited flows, standard &amp; premium connectors Power Apps $480 20 Unlimited apps per user, standard &amp; premium connectors Power BI Pro $120 83 Self-service analytics, collaboration, and sharing Copilot (AI Builder) $500 (estimate) 20 AI capabilities in apps &amp; workflows Teams (Microsoft 365 Business Basic) $60 166 Chat, file sharing, online meetings, collaboration Outlook (Microsoft 365 Business Basic) $60 166 Email, calendar, contacts, 1TB OneDrive storage"},{"location":"M365/LicensingExamples.html#50000-budget","title":"$50,000 Budget","text":"Product Annual Cost Per User Number of Users Features Power Automate $180 277 Unlimited flows, standard &amp; premium connectors Power Apps $480 104 Unlimited apps per user, standard &amp; premium connectors Power BI Pro $120 416 Self-service analytics, collaboration, and sharing Copilot (AI Builder) $500 (estimate) 100 AI capabilities in apps &amp; workflows Teams (Microsoft 365 Business Basic) $60 833 Chat, file sharing, online meetings, collaboration Outlook (Microsoft 365 Business Basic) $60 833 Email, calendar, contacts, 1TB OneDrive storage"},{"location":"M365/LicensingExamples.html#100000-budget","title":"$100,000 Budget","text":"Product Annual Cost Per User Number of Users Features Power Automate $180 555 Unlimited flows, standard &amp; premium connectors Power Apps $480 208 Unlimited apps per user, standard &amp; premium connectors Power BI Pro $120 833 Self-service analytics, collaboration, and sharing Copilot (AI Builder) $500 (estimate) 200 AI capabilities in apps &amp; workflows Teams (Microsoft 365 Business Basic) $60 1666 Chat, file sharing, online meetings, collaboration Outlook (Microsoft 365 Business Basic) $60 1666 Email, calendar, contacts, 1TB OneDrive storage"},{"location":"M365/LicensingExamples.html#m365-features-microsoft-365-business-basic","title":"M365 Features (Microsoft 365 Business Basic)","text":"<ul> <li>Teams: Chat, file sharing, online meetings, collaboration.</li> <li>Outlook: Email, calendar, contacts, 1TB OneDrive storage.</li> </ul>"},{"location":"M365/LicensingExamples.html#summary-of-inclusions-for-each-budget","title":"Summary of Inclusions for Each Budget","text":"<ul> <li>$10,000 Budget: Smaller team with basic Power Automate, Power Apps, Power BI Pro, and M365 Business Basic features.</li> <li>$50,000 Budget: Medium-sized team with more extensive access to Power Platform tools and M365 Business Basic features.</li> <li>$100,000 Budget: Larger team with comprehensive access to Power Platform tools and M365 Business Basic features.</li> </ul>"},{"location":"M365/SPMT.html","title":"SPMT","text":""},{"location":"M365/SPMT.html#migrating-to-microsoft-365-with-the-sharepoint-migration-tool-spmt","title":"Migrating to Microsoft 365 with the SharePoint Migration Tool (SPMT)","text":"<p> If you're looking to move your content from on-site SharePoint locations to the cloud with Microsoft 365, there's a free and user-friendly tool just for you. The SharePoint Migration Tool (SPMT) is here to simplify the process of transferring your SharePoint Server sites and content. </p>"},{"location":"M365/SPMT.html#what-can-be-migrated","title":"What Can Be Migrated?","text":"<p>The table below contains the items which can be migrated using SPMT.</p> Supported Feature Description Migration Sources On-premises fileshares Supports local and network fileshares migration. SharePoint Server versions Migrates from SharePoint Server 2010, 2013, 2016, and 2019. Content Types File, folder, list items Migrates files, folders, and lists. Pages Migrates pages in the site asset library. Permissions &amp; Security Permissions Sets file share and SharePoint permissions separately. Site Features &amp; Components Managed metadata and taxonomy Supports content types and term store migration; requires admin permissions. Navigation and icons Preserves and migrates site navigation for out-of-box sites. Site features Supports a wide range of site features. SharePoint web parts Supports SharePoint web parts migration. Site migration Migrates \"out-of-the-box\" SharePoint sites without coding or third-party tools. Site description Migrates site descriptions. Workflows &amp; Automation OOTB Workflows to Power Automate Migrates SharePoint Server 2010 OOTB workflows to Power Automate. SPD Workflows to Power Automate Migrates SharePoint Server 2010 and 2013 Designer workflows to Power Automate. List, library, content-type workflows Migrates list, library, and content-type workflows (excluding site workflows). Workflow definitions and associations Migrates workflow definitions and associations, not history data. Additional Features Incremental Supports incremental migration by rerunning tasks later. Microsoft Teams Allows selection of Teams and channels for migration. Taxonomy migration Manages metadata and taxonomy in incremental updates; off by default. Bulk migration Allows bulk migration via JSON or CSV for numerous sources. Versions Lets you choose what file history to preserve."},{"location":"M365/SPMT.html#lets-get-started","title":"Let's get started","text":""},{"location":"M365/SharePoint2007FarmUpgrade.html","title":"Case Study: SharePoint Farm Design for a Media Company","text":"<p>I wokred as a SharePoint Architect for an farm deployment project for a prominent media company, this document captures the essence of our SharePoint Farm design project. Developed in collaboration with our networking and infrastructure teams, it outlines the deployment strategy employed and the considerations made to ensure a robust SharePoint environment.</p> <p>From SharePoint 2007 through to SharePoint 2016, the landscape of SharePoint has evolved, introducing several features and enhancements like miniRoles etc. However, the fundamental architecture focusing on high availability (HA) remains unchanged.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#background","title":"Background","text":"<p>Marxxx wants to redesign their existing Microsoft Office SharePoint based portal application which includes around 10 site collections and also wants to improve and simplify the ability to manage content.  </p> <p>Objectives of redesign </p> <ul> <li>Deploy the best fit infrastructure and system architecture </li> <li>Using generic content types with optimum reusability </li> <li>Using consistent use of document management</li> <li>Using  a collaboration platform with self-service a minimal learning curve </li> <li>Standardize enterprise content management utilizing built-in SharePoint tools </li> <li>Implement a MOSS Governance policy that will define Roles and Responsibilities, Policies, Processes,  Deployment Strategies and Site Structure</li> <li>Design and deploy High Availability solution with no single point of failure</li> <li>Integration of plug-in and proprietary applications based on MOSS</li> </ul>"},{"location":"M365/SharePoint2007FarmUpgrade.html#project-overview-understanding","title":"Project Overview &amp; Understanding","text":"<p>Marxxx has implemented the SharePoint to meet different organizational needs. However, the implementation lacks best practices and scope for future enhancements. On generic terms organizations adopting SharePoint face a variety of tasks \u2013 from planning, strategy, infrastructure and architecture design, UI Design, migration, and to development. All these tasks imply flexible infrastructural baseline before actual work starts. However, in reality we face the outdated environment and miss-configured farms that are not ready to implement new requirements. In such cases, baseline architecture becomes foundation stone of all SharePoint projects.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#current-system-study","title":"Current system study","text":""},{"location":"M365/SharePoint2007FarmUpgrade.html#current-system-study_1","title":"Current System study","text":"<p>HXX has detailed discussions with the Marxxx management team to understand the needs to revamp &amp; redesign the current SharePoint implementation. Marxxx provided the VPN connection with site level permission to view the replica of the production sites without sensitive data. HXX provided a document to Marxxx on how to remove sensitive data and backup/restore the sites in development area for HCL\u2019s assessment. HXX followed the standard process of studying current system. The following section would highlight the key findings of the system study.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#current-system-assessment","title":"Current System Assessment","text":"<p>HXX's assessment findings based on current Marxxx SharePoint implementation are:</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#governance","title":"Governance","text":"<ul> <li>No defined SharePoint resource governance matrix.</li> <li>Seems to be no clear business owners of sites and accountability lies on only technical team.</li> <li>Governance document exist but that is incomplete (only 2 pages) and doesn\u2019t seem to serve the purpose of SharePoint governance. It is out-dated. </li> <li>It seems SharePoint administration process is not defined and there seems to be no dedicated administrator for ongoing maintenance and implement governance best practices. </li> <li>No application usage &amp; customization policies exist </li> </ul>"},{"location":"M365/SharePoint2007FarmUpgrade.html#infrastructure-platform","title":"Infrastructure &amp; Platform","text":"<ul> <li>Current SharePoint installation is not implemented for High Availability. No business continuity plans defined. </li> <li>Single Point of failure. No Disaster Recovery farm exists. </li> <li>Server boxes are out-dated for future upgrade of SharePoint and virtualization technologies. </li> <li>Sandbox environment is not yet configured.</li> <li>Development environment consists of sensitive data which ideally should NOT reside in any environments other than production environment.</li> <li>Minimal integration observed with other Enterprise Web Applications and Data </li> </ul>"},{"location":"M365/SharePoint2007FarmUpgrade.html#information-management","title":"Information Management","text":"<ul> <li>Current taxonomy was developed without hierarchy \u2013 flat structure </li> <li>There are few lists/libraries implemented which are not used or obsolete in current sites and are eating up storage space. </li> <li>Contents scattered in File Systems as well \u2013 might be critical contents. No Expiration policies for contents.</li> <li>No Information Management documentation are available which narrates existing SharePoint implementation like taxonomy, design document etc,. Neither any user reference manuals.</li> <li>There is no ready reference on lists/libraries which contain sensitive information. </li> <li>Some custom ASPX pages are used without using SharePoint publishing feature. </li> <li>Content and document types are not being leveraged.</li> <li>Content publishing processes and policies do not exist. </li> </ul>"},{"location":"M365/SharePoint2007FarmUpgrade.html#usability","title":"Usability","text":"<ul> <li>Navigation is not structured in distinct, easily recognizable groups nor is it consistent in structure.</li> <li>MyMarvel Home page UI is good, however that the site menus and contents are not clearly identifiable in the glittery background.</li> <li>Quick links are not present in all sites to facilitate better navigation through different sites</li> <li>Broken Links present.</li> </ul>"},{"location":"M365/SharePoint2007FarmUpgrade.html#sharepoint-farm-maintenance-deployment","title":"SharePoint Farm Maintenance &amp; Deployment","text":"<ul> <li>Dedicated Web application created for each Site Collection in current environment without proper reason or Isolation needs</li> <li>All custom DLLs are placed in GAC should ideally be in \\BIN folder as per Microsoft Best Practices.</li> <li>Backup is done, but no archiving strategy and plan found. </li> <li>No true business process management or enterprise-level workflow automation solutions are in place. Might be present offline \u2013 not known. </li> </ul>"},{"location":"M365/SharePoint2007FarmUpgrade.html#planning-sharepoint-server-farm","title":"Planning SharePoint Server Farm","text":""},{"location":"M365/SharePoint2007FarmUpgrade.html#capacity-planning","title":"Capacity Planning","text":"<p>Assuming that 1000 users access the SharePoint web sites and the content size will grow to 2 TB in future, the following is the Capacity Planning based on Microsoft\u2019s System Center Capacity Planner 2007:</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#usage-profile-characteristics","title":"Usage profile characteristics","text":"<p>Figure 1: Usage profile of SharePoint sites.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#topology-recommendation-by-capacity-planner","title":"Topology recommendation by Capacity Planner","text":"<p>Figure 2: Topology diagram as per MS System Center Capacity Planner 2007</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#hardware-recommendation-by-capacity-planner","title":"Hardware recommendation by Capacity Planner","text":""},{"location":"M365/SharePoint2007FarmUpgrade.html#hardware-profile-for-the-servers","title":"Hardware profile for the servers","text":"<p>The server configuration details are based on Microsoft\u2019s System Center Capacity Planner 2007:</p> Total number of clients/users: 1000 Number of servers: 6 Number of SAN connections: 1"},{"location":"M365/SharePoint2007FarmUpgrade.html#web-front-end-servers","title":"Web Front End Servers","text":"<p>Server: Web Front End 1</p> Configuration Details Processor 2.20 GHz Quad Core Processor Minimum memory 8.0 GB Disk DiskArray 1\\Volume 1 (File System), 146 GB RAID 1 (2 x 146.00 GB SCSI 10,000 RPM) DiskArray 1\\Volume 2 (File System), 360 GB RAID 5 (6 x 72.00 GB SCSI 10,000 RPM) NIC 1 x 1,000 Mb/s Roles Web Front End; Query Server <p>Server: Web Front End 2</p> Configuration Details Processor 2.20 GHz Quad Core Processor Minimum memory 8.0 GB Disk DiskArray 1\\Volume 1 (File System), 146 GB RAID 1 (2 x 146.00 GB SCSI 10,000 RPM) DiskArray 1\\Volume 2 (File System), 360 GB RAID 5 (6 x 72.00 GB SCSI 10,000 RPM) NIC 1 x 1,000 Mb/s Roles Web Front End"},{"location":"M365/SharePoint2007FarmUpgrade.html#application-servers","title":"Application Servers","text":"<p>Server: Index Server &amp; Query Server</p> Configuration Details Processor 2.20 GHz Quad Core Processor Minimum memory 8.0 GB Disk DiskArray 1\\Volume 1 (File System), 1500 GB RAID 5 (6 x 300.00 GB SCSI 15,000 RPM) NIC 1 x 1,000 Mb/s Roles Index &amp; Query"},{"location":"M365/SharePoint2007FarmUpgrade.html#database-sql-servers","title":"Database (SQL) Servers","text":"<p>Server: SQL Server Cluster (Failover)</p> Configuration Details Processor 2.20 GHz Quad Core Processor Minimum memory 16.0 GB Disk No local disk devices NIC 1 x 1,000 Mb/s SAN connections 2 x 4 Gb/s SANs SAN Array\\Volume 3 (Log Files), 600 GB RAID 10 (4 x 300.00 GB SCSI 15,000 RPM) SAN Array\\Volume 4 (Data Files), 5400 GB RAID 10 (36 x 300.00 GB SCSI 15,000 RPM) Roles SQL Server (Clustered) <p>Server: SQL Server Cluster (Primary)</p> Configuration Details Processor 2.20 GHz Quad Core Processor Minimum memory 16.0 GB Disk No local disk devices NIC 1 x 1,000 Mb/s SAN connections 2 x 4 Gb/s SANs SAN Array\\Volume 1 (Log Files), 600 GB RAID 10 (4 x 300.00 GB SCSI 15,000 RPM) SAN Array\\Volume 2 (Data Files), 5400 GB RAID 10 (36 x 300.00 GB SCSI 15,000 RPM) Roles SQL Server"},{"location":"M365/SharePoint2007FarmUpgrade.html#san","title":"SAN","text":"<p>SAN: SAN Array</p> Configuration Details Disk SAN Array\\Volume 1 (Log Files), 600 GB RAID 10 (4 x 300.00 GB SCSI 15,000 RPM) SAN Array\\Volume 2 (Data Files), 5400 GB RAID 10 (36 x 300.00 GB SCSI 15,000 RPM) SAN Array\\Volume 3 (Log Files), 600 GB RAID 10 (4 x 300.00 GB SCSI 15,000 RPM) SAN Array\\Volume 4 (Data Files), 5400 GB RAID 10 (36 x 300.00 GB SCSI 15,000 RPM)"},{"location":"M365/SharePoint2007FarmUpgrade.html#software-stack","title":"Software Stack","text":"Software / Tools - Internet Explorer 6.0 and above - Microsoft Office SharePoint Server 2007 - Microsoft Office SharePoint Designer 2007 - Microsoft Visual Studio 2008 - Microsoft Exchange Server - Microsoft SQL Server 2008 R2 - Microsoft Windows Server 2003 R2 - Microsoft ForeFront"},{"location":"M365/SharePoint2007FarmUpgrade.html#browser-support","title":"Browser Support","text":""},{"location":"M365/SharePoint2007FarmUpgrade.html#about-browser-support","title":"About browser support","text":"<p>Microsoft Office SharePoint Server 2007 supports several Web browsers that are commonly used. However, there are certain browsers that might cause some Office SharePoint Server 2007 functionality to be downgraded, limited, or available only through alternative steps. In some cases, functionality might be unavailable for noncritical administrative tasks.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#levels-of-browser-support","title":"Levels of browser support","text":"<p>Web browser support is divided into two levels: level 1 and level 2. Although administrative tasks on SharePoint sites are optimized for level 1 browser, Office SharePoint Server 2007 also provides support for other browsers that are commonly used. To ensure complete access to all the functionality, it is recommended to use level 1 browser. </p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#level-1-web-browsers","title":"Level 1 Web browsers","text":"<p>Level 1 Web browsers take advantage of advanced features provided by ActiveX controls and provide the most complete user experience. Level 1 browsers offer full functionality on all SharePoint sites, including the SharePoint Central Administration Web site. Level 1 browsers are:</p> <ul> <li>Microsoft Internet Explorer 6.x (32-bit)</li> <li>Windows Internet Explorer 7.x (32-bit)</li> </ul>"},{"location":"M365/SharePoint2007FarmUpgrade.html#system-architecture-design-recommendations","title":"System Architecture &amp; Design Recommendations","text":"<p>Microsoft Office SharePoint Server 2007 provides the flexibility to meet many different deployment solution goals. This includes guidance that would help Marxxx in:</p> <ul> <li>Determine the number of server in farm required to meet the solution goals.</li> <li>Plan for the relationships between servers in the farm.</li> <li>Plan for Extranet -facing server farm.</li> <li>Design server-farm topologies to meet availability goals.</li> </ul>"},{"location":"M365/SharePoint2007FarmUpgrade.html#overall-design-goals","title":"Overall Design Goals","text":"<p>The conceptualized design has been prepared considering the Marvel\u2019s need for SharePoint farm implementation. The key design goals are mentioned below:</p> <ul> <li>Use the minimum number of server farms to host various SharePoint web sites typically required by a Marxxx for their intranet and extranet, considering performance and high availability.</li> <li>Create a framework for designing a scalable environment. Design decisions for individual applications do not prevent the addition of other applications in future. For example, an initial deployment would include ten site collections that comprises of document center, document workspace, collaborative team sites or publishing sites that compose an intranet (team sites, My Sites, and published intranet content). By using a similar logical architecture design, Marxxx can add applications to the solution without affecting the design of the initial applications. In other words, the design does not incorporate design choices that limit the use of the environment.</li> <li>Provide access for several classes of users without compromising the security of the content within the disparate applications. Users from different network zones (both internal and external) with different authentication providers can participate in collaboration. Also, users can only access the content they are intended to access. By following a similar logical architecture design, Marxxx can create the opportunity to provide access to users in multiple locations and with different objectives.</li> <li>Ensure that the design can be used in an extranet environment. Deliberate design choices are made to ensure that the server farms can be securely deployed in a perimeter network (DMZ).</li> </ul> <p>The rest of this article discusses each of the physical components that appear in the landscape model in the following section and discusses the design choices that are applied to the model. The purpose of this approach is to demonstrate the different ways in which logical architecture components can be configured based on the Marvel\u2019s SharePoint Farm needs. </p> <p>## System Landscape </p> <p></p> <p>The Landscape has been divided in three zones.</p> <ul> <li>Extranet/Internet</li> <li>Perimeter Network (DMZ)</li> <li>Corporate Network (Intranet)</li> </ul> Components Position Quantity Role Cisco Firewall Internet 1 (Optional) Hardware Firewall ISA 2006 Server <p>Perimeter Network &amp; </p><p>Corporate Network</p> 2 Software Firewall Web Front End Server Perimeter Network 2 WFE + Query Server Application Server Corporate Network 2 1 App Server + 1 Index Server SQL Server Corporate Network 2 Database Server SAN Array Corporate Network 1 SAN Storage <p>## Farm Design Consideration</p> <p>HXX has kept the following key design consideration at core while designing the framework of the proposed solution. To attain maximum redundancy with a minimum number of servers, deploy an additional application server to the middle tier for load balancing application server roles that are designed to be redundant. This server farm topology consists of six servers. The query role is installed to the front-end Web servers to achieve redundancy.</p> <p></p> <p>This topology protects these server roles from direct user connections and optimizes the performance of the overall farm when compared to smaller farms. The SQL server would be a two node clustered installation.</p> <p>Office SharePoint Server 2007 supports scalable server farms for capacity, performance and availability. Typically, capacity is the first consideration in determining the number of server computers to start with. After factoring in performance, availability also plays a role in determining both the number of servers and the size or capacity of the server computers in a server farm.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#high-availability-for-web-front-ends","title":"High Availability for Web Front Ends","text":"<p>For high availability in a MOSS environment, HXX proposes to use a collection of servers supporting multiple SharePoint sites. A server farm militating against the effects of unexpected downtime in addition to downtime that is related to ongoing maintenance (such as operating system updates etc.). The HXX suggested server farm for Marxxx workloads that builds in availability will consists of six servers (please see the figure 3). </p> <p>There will be four servers for MOSS 2007, two dedicated to Web Server along with Query role, while the other two servers will share the Application services, and Indexing roles. All of the Web Server will be load balanced using NLB. SQL Server 2008 will be clustered across two servers in a primary-secondary cluster.</p> <p>#### NLB Network Load Balancer (NLB) is useful for ensuring that stateless applications, such as a Web server running Internet Information Services (IIS), are scalable by adding additional servers as the load increases. </p> <p>Windows NLB works in two different modes \u2013 Uni-cast and Multicast. When in unicast mode, NLB replaces the network card's original MAC address. When in multicast mode, NLB adds the new virtual MAC to the network card, but also keeps the card's original MAC address.</p> <p>Multiple network adapters in multicast mode for Marvel\u2019s new farm:</p> <p>This model is suitable for a cluster in which ordinary network communication among cluster hosts is necessary and in which there is heavy dedicated traffic from outside the cluster subnet to specific cluster hosts.</p> <p>Advantages</p> <ul> <li>Because there are at least two network adapters, overall network performance is typically enhanced.</li> <li>Ordinary network communication among cluster hosts is permitted.</li> </ul> <p>Disadvantages</p> <ul> <li>This model requires a second network adapter.</li> <li>Some routers might not support the use of a multicast media access control (MAC) address. This only affects the Network Load Balancing/MAC address (not all MAC addresses) and only when dynamic ARP replies are sent by the cluster to the router. not all MAC addresses</li> </ul>"},{"location":"M365/SharePoint2007FarmUpgrade.html#application-servers-redundancy","title":"Application Servers Redundancy","text":"<p>The baseline server topology design depends on the requirements for redundancy of application server roles. This matrix below describes the application server roles relative to their redundancy options. </p> <p>Application server roles for Office SharePoint Server 2007 fall into two categories:</p> <ul> <li>Roles that can be redundant </li> <li>Roles that cannot be redundant </li> </ul> Application server role Redundancy Allowed Query Yes Index No Windows SharePoint Services 3.0 search No Excel Calculation Services Yes Office Project Server 2007 Yes"},{"location":"M365/SharePoint2007FarmUpgrade.html#index-role","title":"Index Role","text":"<p>Microsoft has recommended as a best practice, having a dedicated index server available to crawl data and that should not be part of the potentially load-balanced front-end servers that actually serve up the web pages to end-users. The reason for this is indexing requires significant computing resources both on the indexing machine and the machines being crawled.  Microsoft has also recommended having a dedicated Query server if the crawling content is more than 500 GB, this would alleviate the potential performance issues.  </p> <p>Refer the proposed SharePoint System Landscape, the dedicated Index server will perform the index function of the farm in order to manage crawling of searchable content and will create index files.\u00a0</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#query-role","title":"Query Role.","text":"<p>The Query role would be combined with the web front end servers in a load balanced fashion. This will provide redundancy for Query server role.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#redundant-application-server","title":"Redundant Application Server","text":"<p>The Query role would be combined with the web front end servers in a load balanced fashion. This will provide redundancy for Query server role.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#availability-failover-for-database-server","title":"Availability &amp; Failover for Database Server","text":"<p>The database server role affects the availability of the solution more than any other role. If a Web server or an application server fails, these roles can quickly be restored or redeployed. However, if a database server fails, the solution depends on restoring the database server. This can potentially include rebuilding the database server and then restoring data from the backup media. In this case, you can potentially lose any new or changed data dating back to the last backup job, depending on how SQL Server 2008 is configured. Additionally, the solution will be completely unavailable for the time it takes to restore the database server role. </p> <p>Microsoft enables SQL Server database availability with the SQL Server Always on Technologies program. This consists of</p> <ul> <li>Database mirroring </li> <li>Failover clustering </li> </ul>"},{"location":"M365/SharePoint2007FarmUpgrade.html#database-clustering-recommendation","title":"Database Clustering Recommendation","text":"<p>The proposed design depicts Failover Clustering among two SQL Server 2008 database nodes in the system landscape, which provides availability support for a complete instance of SQL Server. The following diagram shows the Database Failover cluster for SQL Server.</p> <p></p> <p>The failover cluster is a combination of one or more nodes or servers, with two or more shared disks. It will appear as a single instance, but has functionality that provides failover from one node to another if the current node becomes unavailable. Office SharePoint Server 2007 references the cluster as a whole, so that failover is automatic and seamless from the perspective of SharePoint Products and Technologies.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#scalability","title":"Scalability","text":"<p>The architecture is inherently scalable as all product components and integration products are inherently distributed and support scalability by distribution in addition to scale-outs and scale-ups. This would help Marxxx in future to upkeep the system for better performance when the number of users increases.  </p> <p>Scale Up Constraints:</p> <p>The SharePoint implementation in the new farm will be adaptable to version upgrade (SharePoint 2010) only when the software (O/S and Database) installed are x64. Microsoft recommends, as a part of implementation Best Practices, that all production environment have x64 hardware and software for performance benefits. </p> <p>Though there would be x64 hardware support in the new production farm, the design is based on x86 software stack as available with Marxxx at present. </p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#storage-area-network","title":"Storage Area Network","text":"<p>Storage Area Network (SAN) would be used to handle the bulk data storage. All the SQL Server files such as .mdf(primary data file), .ndf(secondary data files), .ldf(log files) etc. would be stored in the SAN. </p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#network-components","title":"Network Components","text":""},{"location":"M365/SharePoint2007FarmUpgrade.html#firewall","title":"Firewall","text":"<p>A firewall is a software- or hardware-based barrier against unauthorized access to your network. Firewalls inspect all the traffic going through them.</p> <p>Configuration of a Firewall that Works</p> <ul> <li>Opt for a hardware firewall, such as Cisco. Hardware firewalls are more powerful, and give you more control over protecting your Web traffic than a software-based firewall. </li> <li>If Spam Protection is available on your firewall, enable it. Spam firewalls are proving popular in keeping the nastier emails out (viruses, botnets). </li> <li>Standardize all wireless connections on WPA. Not only does the WPA Protocol have better security than WEP, but this way you can recognize (and block) if someone else tries to break in using WEP or an unencrypted connection. </li> </ul> <p>Much of perimeter security is addressed by the firewall. However, with the increased risk from DoS attacks and malware scripts worming their ways past firewalls, additional security products are now needed for which Microsoft ForeFront is recommended.</p> <p>ForeFront is a suite of business security products, designed to cover every aspect of network infrastructure. Server operations, client PCs, gateways, network edge\u2014everything\u2019s protected.</p> <p>ForeFront includes: </p> <ul> <li>ForeFront Security for Exchange Server </li> <li>ForeFront Security for SharePoint </li> <li>ForeFront Client Security (FCS) \u2013 Malware protection for every client PC. </li> <li>Intelligent Application Gateway (IAG) \u2013 Remotely-connecting PCs must verify their compliance before gaining server access. </li> <li>Internet Security and Acceleration (ISA) Server 2006 \u2013 Manages security for VPN connections and remotely-accessed applications.</li> </ul> <p>Advantage of ForeFront</p> <ul> <li>Protect Exchange and MOSS 2007. Both servers deal with data coming and going. All day, every day. Cover them both with ForeFront Security for Exchange Server and ForeFront Security for SharePoint.</li> <li>Uniform Reporting. Instead of trying to piece together six reports from six different applications, you can read ForeFront reports. Collected from all components into one, so problems are quickly spotted. </li> <li>A wall against the next attack. Virus and malware attacks are becoming more creative. And sneakier. Since it\u2019s designed to watch the edges of your network, ForeFront will catch new break-in attempts before they find an exploit. </li> </ul> <p>Forefront goes beyond an antivirus on a desktop. It\u2019s an entire web of security spread out through all network systems, covering all access points in and out.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#dns-configuration","title":"DNS Configuration","text":"<p>After setting up SharePoint Portal Server and creating the portal site, you must create a public DNS entry to map the public (external) FQDN to the IP address for the public (external) interface of the external ISA Server 2006 computer. The URL containing the FQDN is the URL that users will use to access the portal site across the extranet.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#active-directory-configuration","title":"Active Directory Configuration","text":"<p>From a MOSS 2007 point of view the Microsoft Split back-to-back topology scenario has been followed; MOSS web front-end servers are located in the perimeter network while the application and database servers reside in the internal network.</p> <p>A one-way Active Directory trust exists between the perimeter domain and the internal domain. This is a non-transitive trust where the perimeter domain trusts the internal domain but not the inverse. This serves two main purposes; it allows windows authentication and delegation to occur between services in the perimeter domain and the internal domain and at the same time protects sensitive data such as payroll databases on the internal network.</p> <p>Forest with a single domain is proposed:</p> <ul> <li>perimeter.corp.local (perimeter production domain, contains accounts for all members and external collaborators)</li> <li>Authenticates users from both perimeter domain and internal domain via domain trust</li> </ul> <p>A one-way domain trust is configured between the corp.com domain and the perimeter.corp.local domain. This is done to allow internal accounts to be used in the perimeter domain which in turn enables windows authentication to be used when accessing backend resources such as SQL databases. This trust means that deploying MOSS web front end servers is as straight forward as adding the web front end role, all IIS configuration is completed by MOSS and remains valid in both the internal and perimeter domains.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#farm-security-considerations","title":"Farm Security Considerations","text":"<p>SharePoint Server-farm security will include the following tasks:</p> <p>Secure communication planning \u2014 decide which methods of secure communication are most appropriate for Marxxx landscape.   Server hardening \u2014 plan specific hardening settings for each of the server roles in the new server farm.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#communication-in-server-farm","title":"Communication in Server Farm","text":"<p>Secure communication between the components of Marxxx SharePoint Farm deployment is a vital element of in-depth security architecture to ensure business critical information is not intercepted and misappropriated during its flow from user request to Web Server to Application Server and Database Server. It\u2019s important to apply secure communication techniques for both inside and outside the firewall. Secure communication provides privacy and integrity of data.</p> <p>Privacy ensures that data remains private and confidential, and that it cannot be viewed by eavesdroppers armed with network-monitoring software. Privacy is usually provided by means of encryption.</p> <p>Integrity ensures that data is protected from accidental or deliberate (malicious) modification while in transit. Secure communication channels must provide integrity of data. Integrity is usually provided by using Message Authentication Codes (MACs).</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#server-server-communication","title":"Server-Server Communication","text":"<p>In this deployment scenario, the communications between servers is done using IPSec (Internet Protocol Security), which is an open standard framework that specifies encryption and packet-filtering techniques to enhance network security.</p> <p>IPSec is a transport-layer mechanism through which ensures the confidentiality and integrity of TCP/IP-based communications between servers. IPSec is completely transparent to applications because encryption, integrity, and authentication services are implemented at the transport level.</p> <p>Please refer MOSS Installation Manual \u2013 Part 2 Appendix B for instructions to setup IPSec. </p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#client-server-communication","title":"Client-Server Communication","text":"<p>SSL is generally recommended to secure communications between users and servers when sensitive information must be secured. SSL can be configured to require server authentication or both server and client authentication.</p> <p>SSL can decrease the performance of the network. There are several common guidelines that can be used to optimize pages that use SSL. First, use SSL only for pages that require it. This includes pages that contain or capture sensitive data, such as passwords or other personal data. </p> <p>SSL should be used only if the following conditions are true:</p> <ul> <li>To encrypt the page data.</li> <li>To guarantee data is sent to the intended web server.</li> </ul>"},{"location":"M365/SharePoint2007FarmUpgrade.html#server-hardening","title":"Server Hardening","text":"<p>The process of \u201cServer Hardening\u201d deals with activities to make servers in the SharePoint farm less vulnerable to attack. This section describes hardening characteristics for Web servers, application servers and database servers.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#web-application-server-hardening","title":"Web &amp; Application Server Hardening","text":"<p>The following table summarizes the Microsoft Patterns and Practices guidance for securing Web Server</p> Task Applies To Details Patches and Updates <p>- WFE servers</p><p>- Application servers</p> <p>Ports</p><p></p> <p>- WFE servers</p><p>- Application servers</p> <p>- Limit Internet-facing ports to port 80 for HTTP and port 443 for HTTPS.</p><p>- Use IPSec to encrypt or restrict intranet communication.</p><p>\u2003(Refer the complete list below)</p> Sites and Virtual drives <p>- WFE servers</p><p>- Application servers hosting Central Admin</p> <p>- Move website to non-system drive.</p><p>- Restrict Web permissions in the IIS meta-base.</p><p>- Remove FrontPage Server extensions.</p><p></p> Protocols <p>- WFE servers</p><p>- Application servers</p> <p>- Configure TCP/IP stack to mitigate the threat from network denial of service attacks.</p><p>- Disable the NetBIOS and SMB protocols if not used.</p> Shares <p>- WFE servers</p><p>- Application servers</p> <p>- Remove all unnecessary shared folders.</p><p>- Restrict access to any required shares.</p> IIS Lockdown <p>- WFE servers</p><p>- Application servers hosting Central Admin</p> - Install URLScan to block requests that contain unsafe characters. It is installed automatically when IISLockdown is run. <p>The following list depicts the important ports:</p> <ul> <li>TCP 80, TCP 443 (SSL)</li> <li>Direct-hosted SMB (TCP/UDP 445) \u2014 disable this port if not used</li> <li>NetBIOS over TCP/IP (NetBT) (TCP/UDP ports 137, 138, 139) \u2014 disable this port if not used</li> </ul> <p>Ports required for communication between Web servers and service applications (the default is HTTP):</p> <ul> <li>HTTP binding: 32843</li> <li>HTTPS binding: 32844</li> <li>net.tcp binding: 32845 (only if a third party has implemented this option for a service application)</li> </ul> <p>Ports required for synchronizing profiles between SharePoint and Active Directory on the server that runs the Forefront Identity Management agent:</p> <ul> <li>TCP/5725</li> <li>TCP/UDP 389 (LDAP service)</li> <li>TCP/UDP 88 (Kerberos)</li> <li>TCP/UDP 53 (DNS)</li> <li>UDP 464 (Kerberos Change Password)</li> <li>UDP port 1434 and TCP port 1433\u00a0\u2014 default ports for SQL Server communication. If these ports are blocked on the SQL Server computer (recommended) and databases are installed on a named instance, configure a SQL Server client alias for connecting to the named instance.</li> <li>Block external access to the port that is used for the Central Administration site.</li> <li>TCP/25 (SMTP for e-mail integration)</li> </ul>"},{"location":"M365/SharePoint2007FarmUpgrade.html#database-server-hardening","title":"Database Server Hardening","text":"<p>SharePoint Server 2007 has built-in support for protection against SQL Injection attacks; however, some steps must be taken to protect data from network eavesdropping, password cracking and unauthorized access.</p> <p>A default instance of SQL Server listens for connection on TCP port 1433. If a client computer is unable to connect to the database on TCP port 1433, it queries SQL Server Resolution Service on UDP port 1434. These standard ports are well known and the use of standard configuration enables the SQL Server Resolution Service to be targeted by malicious software. To protect for these the following steps need to be followed:</p> <p>Blocking of TCP port 1433  Blocking of UDP port 1434  Configuring SQL Server to listen to non-standard port  Configure SQL Server client aliases on all Web Server and application servers  Stop SQL Server Browser service to further secure</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#secure-topology-design-checklists","title":"Secure Topology Design Checklists","text":""},{"location":"M365/SharePoint2007FarmUpgrade.html#server-topology-design","title":"Server topology design","text":"<p>Review the following checklist to ensure that your plans meet the criteria for a secure server topology design.</p> The topology incorporates dedicated front-end Web servers. Servers that host application server roles and database server roles are protected from direct user access. The SharePoint Central Administration site is hosted on a dedicated application server, such as the index server. <p>### Network topology design  Review the following checklist to ensure that your plans meet the criteria for a secure networking topology design.</p> All servers within the farm reside within a single data center and on the same vLAN. Access is allowed through a single point of entry, which is a firewall. For a more secure environment, the farm is separated into three tiers (front-end Web, application, and database), which are separated by routers or firewalls at each vLAN boundary."},{"location":"M365/SharePoint2007FarmUpgrade.html#planning-for-extranet-topology","title":"Planning for Extranet Topology","text":""},{"location":"M365/SharePoint2007FarmUpgrade.html#intranet-extranet-zones","title":"Intranet &amp; Extranet Zones","text":"<p>An extranet environment is a private network that is securely extended to share part of an organization's information or processes with remote employees, external partners, or customers.</p> <p>The following table describes the benefits that the extranet provides for each group.</p> Group Benefits Remote employees Remote employees can access corporate information and electronic resources anywhere, anytime, and any place, without requiring a virtual private network (VPN). Remote employees include:<ul><li>Traveling sales employees.</li><li>Employees working from home offices or customer sites.</li><li>Geographically dispersed virtual teams.</li></ul> External partners External partners can participate in business processes and collaborate with employees of your organization. You can use an extranet to help enhance the security of data in the following ways:<ul><li>Apply appropriate security and user-interface components to isolate partners and to segregate internal data.</li><li>Authorize partners to use only sites and data that are necessary for their contributions.</li><li>Restrict partners from viewing other partners\u2019 data.</li></ul>You can optimize processes and sites for partner collaboration in the following ways:<ul><li>Enable employees of your organization and partner employees to view, change, add, and delete content to promote successful results for both companies.</li><li>Configure alerts to notify users when content changes or to start a workflow.</li></ul> Customers Publish branded, targeted content to partners and customers in the following ways:<ul><li>Target content based on product line or by customer profile.</li><li>Segment content by implementing separate site collections within a farm.</li><li>Limit content access and search results based on audience.</li></ul>"},{"location":"M365/SharePoint2007FarmUpgrade.html#internet-facing-topology","title":"Internet-facing Topology","text":"<p>You can plan to host extranet content inside your corporate network and make it available through an edge firewall, or you can isolate the server farm inside a perimeter network. </p> <p>Together, these are the content and collaboration sites that employees will use on a day-to-day basis. Individually, each of these applications represents a distinct type of content. Each type of content: - Emphasizes different features of Office SharePoint Server 2007. - Hosts data with different data characteristics. - Is subject to a different usage profile. - Requires a different permissions management strategy.</p> <p>Consequently, design choices for each of these applications are intended to optimize the performance and security for each application. </p> <p>The use of a single Shared Services Provider (SSP) brings these three applications together to provide: - Navigation across the applications. - Enterprise-wide search. - Shared profile data. </p> <p>The following figure shows the three applications that make up the corporate intranet.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#split-back-to-back-topology","title":"Split back-to-back topology","text":"<p>This topology splits the farm between the perimeter and corporate networks. The computers running Microsoft SQL Server database software are hosted inside the corporate network. Web servers are located in the perimeter network. The application server computers can be hosted in either the perimeter network or the corporate network. </p> <p></p> <p></p> <p>If the server farm is split between the perimeter network and the corporate network with the database servers located inside the corporate network, a domain trust relationship is required if Windows accounts are used to access SQL Server. In this scenario, the perimeter domain must trust the corporate domain. If SQL authentication is used, a domain trust relationship is not required. </p> <ul> <li>To optimize search performance and crawling, the application servers has been kept inside the corporate network with the database servers. Do not add the query role to the index server if the query role also is located on other servers in the farm. If you place Web servers in the perimeter network and application servers inside the corporate network, you must configure a one-way trust relationship in which the perimeter network domain trusts the corporate network domain. This one-way trust relationship is required in this scenario to support inter-server communication within the farm, regardless of whether you are using Windows authentication or SQL authentication to access SQL Server.</li> </ul>"},{"location":"M365/SharePoint2007FarmUpgrade.html#advantages","title":"Advantages","text":"<ul> <li>Computers running SQL Server are not hosted inside the perimeter network.</li> <li>Farm components both within the corporate network and the perimeter network can share the same databases.</li> <li>Content can be isolated to a single farm inside the corporate network, which simplifies sharing and maintaining content across the corporate network and the perimeter network.</li> <li>With a separate Active Directory infrastructure, external user accounts can be created without affecting the internal corporate directory.</li> </ul>"},{"location":"M365/SharePoint2007FarmUpgrade.html#disadvantages","title":"Disadvantages","text":"<ul> <li>Complexity of the solution is greatly increased.</li> <li>Inter-farm communication is typically split across two domains.</li> </ul>"},{"location":"M365/SharePoint2007FarmUpgrade.html#disaster-recovery-recommendations","title":"Disaster Recovery Recommendations","text":""},{"location":"M365/SharePoint2007FarmUpgrade.html#disaster-recovery","title":"Disaster Recovery","text":"<p>As part of the requirement, some kind replication needs to be established between the two sites so as to ensure the availability of data at all the sites. In case of failure of any one of the site the other must be able to take over the entire operations. So as to cater to these requirements, a site replication methodology is designed for Marxxx as shown in the below diagram. The proposed solution comprises of two sites Primary Site and DR Site. Between the Primary Site and the DR Site, data will be replicated asynchronously over the Dark FC/WAN IP network Connectivity. </p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#disaster-recovery-strategy","title":"Disaster Recovery Strategy","text":"<p>Figure 5: DC-DR Before</p> <p>Two logical farms:</p> <ul> <li>Only content databases can be successfully mirrored to failover farm.</li> <li>A separate configuration database must be maintained on the failover farm.</li> <li>All customizations must be done on both farms.</li> <li>Patches must be individually applied to both farms.</li> <li>SSP databases can be backed up and restored to the failover farm.</li> <li>Consult with your SAN vendor to determine whether you can use SAN replication or another supported mechanism to provide availability across data centers.</li> </ul> <p></p> <p>Figure 6: DC-DR After</p> <p>On failover, traffic is redirected to the secondary farm.</p> <ul> <li>On failover, you must attach mirrored content databases to the Web applications running in the failover farm.</li> <li>Over time, you can either fail back or turn the primary farm into the failover farm.</li> </ul>"},{"location":"M365/SharePoint2007FarmUpgrade.html#disaster-recovery-configuration-process","title":"Disaster Recovery Configuration Process","text":"<ul> <li>Disaster recovery solution for web content: Use SharePoint QA server for Disaster Recovery, first export the specific site from backup to a blank site and then extract the specific content that needs to be restored. Do not use production server for disaster recovery because the recovery process will overwrite the existing contents.</li> <li>Rationale: SharePoint\u2019s built-in recovery solution does not allow granular file/item level recovery. Entire site needs to be restored to recover a single file/item</li> <li> <p>Technical Details: Use STSADM to restore the site to QA web server from an existing backup and then recover the file/item</p> </li> <li> <p>Disaster recovery solution for index servers: </p> </li> <li>Solution1: <ul> <li>The only clean and recommended way to recover from an index corruption is to completely rebuild the index on all the servers in the farm</li> </ul> </li> <li>Solution 2:<ul> <li>Build a new index server</li> <li>Disable and stop ossearch service on the index server</li> <li>Check if the propagation status is idle</li> <li>Copy the Portal_Content catalog  from the query server to the indexer in the same location</li> </ul> </li> </ul>"},{"location":"M365/SharePoint2007FarmUpgrade.html#asynchronous-database-mirroring","title":"Asynchronous database mirroring","text":"<p>There are two mirroring operating modes</p> <p>Synchronous mode -   when a session starts, the mirror server synchronizes the mirror database together with the principal database as quickly as possible. As soon as the databases are synchronized, a transaction is committed on both partners, at the cost of increased transaction latency.  Asynchronous mode - as soon as the principal server sends a log record to the mirror server, the principal server sends a confirmation to the client. It does not wait for an acknowledgement from the mirror server. This means that transactions commit without waiting for the mirror server to write the log to disk. Such asynchronous operation enables the principal server to run with minimum transaction latency, at the potential risk of some data loss.</p> <p>All database mirroring sessions support only one principal server and one mirror server. This configuration is shown in the following illustration.</p> <p></p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"M365/SharePoint2007FarmUpgrade.html#production-dc-dr","title":"Production DC &amp; DR","text":"<p>Figure 5: Production Environment</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#non-production-environments","title":"Non-Production Environments","text":""},{"location":"M365/SharePoint2007FarmUpgrade.html#development-environment","title":"Development Environment","text":"<p>SharePoint development environment configuration depends on the processes, type of engagements and type of work. The most popular solution that addresses the development scenarios is using local SharePoint farm, separated from the production servers, with the single installations of the SharePoint server on the development boxes. This provides isolation for builds, tests, and debugging across different teams, projects and production environments. The local environment is mostly isolated by development box and is installed on the host server or on virtual server. The following procedure is an overview of the steps that are required to create a typical SharePoint development environment.</p> <p>Development Box installation</p> <ul> <li>Use Windows Server, Visual Studio, and SQL Server.\u00a0Windows Server 2008, VS 2008 and .NET 3.5, SQL 2008, with TFS 2008 is officially supported environment for SharePoint development. Advantage of Windows 2008 is that it is fast in virtualized environments.</li> <li>Install SharePoint on development boxes and prefer not to connect to existed farm instances used on other stages. Development environment should stay apart, to develop and tests in isolated environment.</li> </ul>"},{"location":"M365/SharePoint2007FarmUpgrade.html#qa-environment","title":"QA Environment","text":"<p>The following diagram depicts the most common development environment, which is recommended by \u201cSharePoint Guidance patterns &amp; practices\u201d team.</p> <p></p> <p>Stand-alone SharePoint environment for development, unit testing and debugging of SharePoint project. Runs continuous integration and builds verification tests before deploying the SharePoint solutions to the test environment.  Source Control/Build Server to build SharePoint packages (WSP) and to deploy solution to test environment.  The test environment performs user acceptance testing, manual functional testing, automated functional testing, system testing, security testing, and performance testing. After the solution meets production requirements, the SharePoint solutions are deployed to the staging environment.  The Staging server uses to test the \u201cproduction-ready\u201d solution in an environment that closely resembles the production environment. The purpose of this environment is to identify any potential deployment issues. Although the Staging environment is optional for smaller applications where deployment failures are not critical</p> <p>The staging environment represents the target production environment as closely as possible from the perspective of topology (for example, the server farm and database structure) and components (for example, the inclusion of the Microsoft Active Directory service and load balancing, where applicable).</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#virtualization-of-environments","title":"Virtualization of Environments","text":"<p>Virtualization in SharePoint farm is one of the key design factors that simplify server availability by providing number of additional servers that might not be available over physical server models, or solution become very expensive. Microsoft officially supports SharePoint farm in virtualized environment since mid 2007. The following virtualizations technologies are supported: Hyper-V and 3rd party providers like VMware.</p> <p>One of the key factors for virtualization is that performance of virtualized farm is competitive to the physical farm. Microsoft tests shows: 7.2% less throughput on virtual Web roles with 8GB of RAM than a physical Web role server with 32GB of RAM; 4.4% slower in the page response time on the Hyper-V Web front-end than the physical server;</p> <p></p> <p>Figure 6: Virtual Development Environment</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#virtualize-sharepoint-web-role","title":"Virtualize SharePoint Web Role","text":"<p>Web front-end servers are responsible for the content rendering and have comparatively lower memory requirements and disk activity than other roles, what makes them is an ideal candidate for virtualization.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#choose-disk-type-for-query-role","title":"Choose disk type for Query Role","text":"<p>The query role is responsible for a search performed by users is a good candidate for virtualization. The disk type choice for this role depends on the size of propagated index and the rate of index propagation. The recommendation for the large indexes and the farm with the high rate of the updated information to use a physical disk volume that is dedicated for the individual query server, rather than a virtual disk file.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#consider-using-index-role-on-physical-server","title":"Consider using Index Role on physical server","text":"<p>The Index server role in a SharePoint farm is often the most memory-intensive role, what makes it less ideal candidate for virtualization. Virtualized Index server role might be appropriate for development environment, small farm or farm with small content usage. Take into account, that index can vary from 10% to 30% of the total size of the documents being indexed. For the large indexes (above 200 GB) consider using physical disk volume that is dedicated to the individual query server, rather than virtual disk. For large farms with big amount of crawled data use physical Index server role due to large memory requirements and high disk I/O activity.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#do-not-virtualize-database-role","title":"Do not virtualize Database role","text":"<p>SharePoint database role is the least appropriate role for virtualization in production scenarios, mainly due to the highest amount of disk I/O activity and very high memory and processor requirements. However, it is very common to see the SQL Server virtualized in test farms, quality assurance (QA) farms, and smaller SharePoint environments.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#do-i-need-to-virtualize-application-role","title":"Do I need to virtualize Application role?","text":"<p>The decision of virtualizations the application roles, such Excel Services and InfoPath Services, depends on the roles usage. Those roles can be easily virtualized, because they are similar to Web Roles and mostly CPU intensive. When necessary, those servers can be easily moved to dedicated physical servers.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#virtualized-scenario-sample","title":"Virtualized scenario sample","text":"<p>The following picture demonstrates the common virtualized scenario of SharePoint Farm.</p> <p>Common deployment scenarios for the SQL role in a SharePoint farm may have multiple farms, both physical and virtual, use a single database server or database cluster, further increasing the amount of resources consumed by the role. For example, in the picture above, the sample SharePoint environment illustrated maintains a two-server SQL cluster that is used by several virtual farms and one production farm.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#use-proper-number-of-cpu","title":"Use proper number of CPU","text":"<p>Do not use more virtual CPUs than physical CPUs on the virtual host computer \u2013 this will cause performance issues, because the hypervisor software has to swap out CPU contexts. The best performance can be realized if the number of virtual processors allocated to running guests does not exceed the number of logical processors (physical processors multiplied by the number of cores) on the host. For example, a four processor quad-core server will be able to allocate up to 16 virtual processors across its running sessions without any significant performance impact. Note that this only applies to sessions that are physically running simultaneously.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#use-proper-amount-of-ram","title":"Use proper amount of RAM","text":"<p>Plan to allocate the memory on virtual sessions according the next rule \u2013 divide the total amount of RAM in the server by the number of logical processors (physical processors multiplied by number of cores) in the host server. This will align allocated memory along with NUMA sessions. Otherwise it will provide performance issues. In some testing, a virtual SharePoint Web server role with an allocation of 32GB of RAM actually performed worse than a virtual server with an allocation of 8GB of RAM.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#plan-to-use-physical-drives","title":"Plan to use physical drives","text":"<p>In virtual scenarios front-end Web servers or Query servers disk performance is not as important as it would be physicals servers of the Index role or a SQL Server database. A fixed-size virtual disk typical provides better performance than a dynamically-sized disk. If disk speed is a high priority, consider adding physical drives to the host computer. Add new virtual hard drive and map it to an unused physical drive on the host. This configuration, called a \u201cpass-through disk\u201d, is likely to give the best overall disk throughput.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#consider-using-hardware-load-balancing","title":"Consider using hardware load balancing","text":"<p>Hardware load balancing provides the best performance, comparing with the software load balancing. It offloads CPU and I/O pressure from the WFE\u2019s to hardware layer thereby improving availability of resources to SharePoint. Examples of Hardware: F5 BIG IP, Citrix Netscaler, Cisco Content Switch. Software load balancing examples are Windows Network load balancing, Round Robin load balancing with DNS. It is a trade-off between cost and performance.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#be-careful-with-snapshot-feature-on-virtual-servers","title":"Be careful with snapshot feature on virtual servers","text":"<p>Using snapshots for the backup might cause you troubles, because SharePoint timer service might be \u00a0unsynchronized during the snapshot process, and once the snapshot is finished, errors or inconsistencies can arise. So, consider backups over the snapshots for the production environments.</p>"},{"location":"M365/SharePoint2007FarmUpgrade.html#measure-virtualized-environment-performance-only-for-hyper-v","title":"Measure virtualized environment performance (only for Hyper-V)","text":"<p>After completing your virtualized environment installation and configuration, it's crucial to measure how fast your environment operates and optimize it for the best performance. Here are the key parameters to measure:</p> <ol> <li>Processor Performance</li> <li>Counter: <code>\\HYPER-V HYPERVISOR LOGICAL Processor(_Total)% Total Run Time</code></li> <li> <p>Results: </p> <ul> <li>&lt;60% Utilization is fine.</li> <li>60%-89% indicates caution.</li> <li> <p>90% signals significant performance degradation.</p> </li> </ul> </li> <li> <p>Memory Performance</p> </li> <li>Counter: <code>\\MEMORY\\AVAILABLE MBytes</code></li> <li>This measures the physical memory available to processes running on the computer, as a percentage of the total physical memory installed on the computer.</li> <li> <p>Results:</p> <ul> <li>50% utilization is fine.</li> <li>10% and below is critical.</li> </ul> </li> <li> <p>Disk Performance</p> </li> <li>Counters:<ul> <li>Read Latency: <code>\\LOGICAL DISK(*)\\AVG. DISK SEC/READ</code></li> <li>Write Latency: <code>\\LOGICAL DISK(*)\\AVG. DISK SEC/WRITE</code></li> </ul> </li> <li>These measure disk latency on the Hyper-V host operating system.</li> <li>Results:<ul> <li>Up to 15ms is fine.</li> <li>15ms-25ms indicates a warning.</li> <li>Greater than 26ms is critical.</li> </ul> </li> </ol>"},{"location":"M365/SharePoint2016FarmUpgrade.html","title":"Background","text":"<p>Hello, readers. In this article I will share my experience architecting a farm topology for a SharePoint 2016 on-prem, in-country farm deployment project.</p> <p>The project's goal was to revamp the client's existing Microsoft Office SharePoint setup. Our preliminary assessment revealed a stark absence of best practice implementations in their topology design. Furthermore, we encountered issues with their CMS system and various other elements of their SharePoint portals. Without further delay, let's dive into the details.</p>"},{"location":"M365/SharePoint2016FarmUpgrade.html#project-overview-understanding","title":"Project Overview &amp; Understanding","text":"<p>The client has deployed SharePoint to satisfy various organizational needs. Despite this, the implementation is lacking in best practices and future enhancement opportunities. Common challenges for organizations adopting SharePoint include planning, strategy, infrastructure, architecture design, UI Design, migration, and development. These tasks demand a flexible infrastructure as a prerequisite. Yet, often, organizations encounter outdated and improperly configured environments that hinder new implementations. Thus, the baseline architecture is critical for the success of all SharePoint projects.</p>"},{"location":"M365/SharePoint2016FarmUpgrade.html#objectives-of-redesign","title":"Objectives of Redesign","text":"<ul> <li>Deploy the best fit infrastructure and system architecture.</li> <li>Utilize generic content types with optimum reusability.</li> <li>Ensure consistent use of document management.</li> <li>Leverage a collaboration platform that minimizes the learning curve through self-service.</li> <li>Standardize enterprise content management using built-in SharePoint tools.</li> <li>Implement a MOSS Governance policy to define Roles and Responsibilities, Policies, Processes, Deployment Strategies, and Site Structure.</li> <li>Design and deploy a High Availability solution without any single point of failure.</li> <li>Integrate plug-in and proprietary applications based on MOSS.</li> </ul>"},{"location":"M365/SharePoint2016FarmUpgrade.html#current-system-study","title":"Current System Study","text":"<p>The main team working on this project engaged in thorough discussions with the The Client management team to comprehend the requirements for revamping and redesigning the current SharePoint setup. The Client facilitated this by providing VPN access with site-level permissions for us to review the production sites' replicas, minus any sensitive data. We has documented how to remove sensitive data and manage the backup/restore process for site assessments in the development environment, adhering to standard practices during the current system study.</p>"},{"location":"M365/SharePoint2016FarmUpgrade.html#current-system-assessment","title":"Current System Assessment","text":""},{"location":"M365/SharePoint2016FarmUpgrade.html#governance","title":"Governance","text":"<ul> <li>Absence of a defined SharePoint resource governance matrix.</li> <li>Lack of clear business ownership for sites, with accountability resting solely on the technical team.</li> <li>Existence of an incomplete and outdated governance document.</li> <li>Undefined SharePoint administration process and absence of a dedicated administrator for maintenance and governance best practices implementation.</li> <li>Lack of application usage and customization policies.</li> </ul>"},{"location":"M365/SharePoint2016FarmUpgrade.html#infrastructure-platform","title":"Infrastructure &amp; Platform","text":"<ul> <li>Non-implementation of SharePoint for High Availability and lack of business continuity plans.</li> <li>Presence of single points of failure and absence of a Disaster Recovery farm.</li> <li>Outdated server boxes unsuitable for future SharePoint upgrades and virtualization technologies.</li> <li>Non-configured sandbox environment.</li> <li>Presence of sensitive data in the development environment, which should only reside in the production environment.</li> <li>Minimal integration with other Enterprise Web Applications and Data.</li> </ul>"},{"location":"M365/SharePoint2016FarmUpgrade.html#information-management","title":"Information Management","text":"<ul> <li>Development of a flat structure taxonomy without hierarchy.</li> <li>Presence of unused or obsolete lists/libraries consuming storage space.</li> <li>Scattered contents in File Systems potentially holding critical contents without expiration policies.</li> <li>Absence of Information Management documentation describing existing SharePoint implementation.</li> <li>Lack of reference for lists/libraries containing sensitive information.</li> <li>Use of custom ASPX pages outside of the SharePoint publishing feature.</li> <li>Underutilization of content and document types.</li> <li>Nonexistence of content publishing processes and policies.</li> </ul>"},{"location":"M365/SharePoint2016FarmUpgrade.html#usability","title":"Usability","text":"<ul> <li>Unstructured navigation lacking distinct, easily recognizable groups and consistency.</li> <li>The UI of Home page is appealing, but site menus and contents are not easily identifiable against a glittery background.</li> <li>Absence of quick links across all sites for better navigation.</li> <li>Presence of broken links.</li> </ul>"},{"location":"M365/SharePoint2016FarmUpgrade.html#sharepoint-farm-maintenance-deployment","title":"SharePoint Farm Maintenance &amp; Deployment","text":"<ul> <li>Creation of dedicated web applications for each Site Collection without clear rationale or isolation needs.</li> <li>Placement of all custom DLLs in GAC, contrary to Microsoft Best Practices recommending the \\BIN folder.</li> <li>Execution of backups without an archiving strategy and plan.</li> <li>Absence of true business process management or enterprise-level workflow automation solutions, possibly available offline.</li> </ul>"},{"location":"M365/SharePoint2016FarmUpgrade.html#system-architecture-design-recommendations","title":"System Architecture &amp; Design Recommendations","text":"<p>Microsoft Office SharePoint Server 2016 provides the flexibility to meet many different deployment solution goals. This includes guidance that would help the client in: -   Determine the number of server in farm required to meet the solution goals. -   Plan for the relationships between servers in the farm. -   Plan for Extranet -facing server farm. -   Design server-farm topologies to meet availability goals.</p>"},{"location":"M365/SharePoint2016FarmUpgrade.html#available-architectural-models","title":"Available architectural models","text":"<p>The table below summarizes the different approaches to deploying and managing SharePoint 2016, ranging from fully cloud-based SaaS solutions to traditional on-premises deployments.</p> Model Description SharePoint Online/SaaS You consume SharePoint through a Software as a Service (SaaS) with an Office 365 subscription. SharePoint is always up to date, but you are responsible for managing SharePoint itself. SharePoint Hybrid Combines SharePoint Online with a SharePoint Server 2016 farm, deployed either in Azure or on-premises. Incorporates SharePoint Online services into your overall SharePoint offering, starts building SaaS management skills in your organization, and moves your SharePoint Server 2016 sites and apps to the cloud at your own pace. SharePoint in Azure/IaaS Extends your on-premises environment into Azure Infrastructure as a Service (IaaS) for production, disaster recovery, or dev/test SharePoint Server 2016 farms. SharePoint on-premises Plans, deploys, maintains, and customizes your SharePoint Server 2016 farm in a datacenter that you maintain."},{"location":"M365/SharePoint2016FarmUpgrade.html#license-models","title":"License models","text":"Deployment Model Licensing Requirements SharePoint Online Assign licenses to Azure AD user accounts from your Office 365 subscription, no additional licenses needed SharePoint Hybrid - Office 365: Subscription model, no additional licenses needed - On-premises: Windows Server 2012 R2 or Windows Server 2016 - On-premises: SQL Server 2016 or SQL Server 2014 SP1 or later - On-premises: SharePoint Server 2016 License - On-premises: SharePoint Server 2016 Client Access License SharePoint Server 2016 in Azure (IaaS) - Azure subscription - SharePoint Server 2016 License - SharePoint Server 2016 Client Access License SharePoint Server 2016 On-premises - Windows Server 2016 or Windows Server 2012 R2 - SQL Server 2016 or SQL Server 2014 SP1 or later - SharePoint Server 2016 License - SharePoint Server 2016 Client Access License"},{"location":"M365/SharePoint2016FarmUpgrade.html#topology-recommendation","title":"Topology Recommendation","text":"<p>As the client wants to have an in-country farm due to data regulations. We recommend the following farm topology for a high-availability, on-premises SharePoint 2016 farm with six servers, using the latest features such as MinRole</p> <p></p> <p>Front-End with Distributed Cache Servers: - Server 1 &amp; 2: These servers will handle all the user requests and serve the web pages. They will also host the Distributed Cache service, which is crucial for speeding up the retrieval of data and improving performance by caching frequently accessed information.</p> <p>Application with Search Servers: - Server 3 &amp; 4: These servers will run backend service applications and host the Search service. They handle the processing tasks that support the front-end servers, such as the Search service, which indexes content and processes search queries.</p> <p>Database Servers: - Server 5 &amp; 6: These will be the SQL Server databases configured in a high-availability cluster using SQL Server AlwaysOn Availability Groups. They store all the content and configurations for the SharePoint farm.</p>"},{"location":"M365/SharePoint2016FarmUpgrade.html#services-installed","title":"Services Installed:","text":"<p>Front-End with Distributed Cache Servers (Server 1 &amp; 2): - Access Services - Business Data Connectivity Service - Managed Metadata Web Service - User Profile Service - Distributed Cache - Microsoft SharePoint Foundation Web Application - And other front-end related services...</p> <p>Application with Search Servers (Server 3 &amp; 4): - App Management Service - Business Data Connectivity Service - Machine Translation Service - Managed Metadata Web Service - Search Host Controller Service - Search Query and Site Settings Service - Secure Store Service - User Profile Service - And other application services...</p> <p>Database Servers (Server 5 &amp; 6): - SQL Server with AlwaysOn Availability Groups configured for all SharePoint databases. </p>"},{"location":"M365/SharePoint2016FarmUpgrade.html#high-availability-explanation","title":"High Availability Explanation:","text":"<p>Front-End Servers: - Having two front-end servers ensures that if one goes down, the other can continue to serve user requests without interruption. Network Load Balancer (NLB) would be used to distribute the requests evenly between the two servers.</p> <p>Application with Search Servers: - Two servers with application and search services offer redundancy for these critical components of the SharePoint infrastructure. If one server fails, the other can take over the services without impacting the availability of the SharePoint farm.</p> <p>Database Servers: - SQL Server AlwaysOn Availability Groups provide high availability for the databases. In the event of a database server failure, the other node in the AlwaysOn group will take over, ensuring the SharePoint farm's data remains accessible. This setup provides both high availability and disaster recovery.</p> <p>The suggested SharePoint 2016 farm topology is designed to minimize single points of failure, ensuring that user access is uninterrupted, search functionality remains operational, and data is consistently available even in the event of server outages. The use of MinRole ensures that each server is optimized for its role, improving performance and reliability. The distribution of roles across multiple servers, along with the redundancy built into each layer (front-end, application, and database), achieves a highly available environment that aligns with SharePoint 2016's infrastructure advancements.</p>"},{"location":"M365/SharePoint2016FarmUpgrade.html#disaster-recovery-recommendations","title":"Disaster recovery Recommendations","text":"<p>Implement a \"stretched farm\" where there will be two data centres configured as a single farm. This is possible if two data centres are in close proximity, connected with higg bandwidth fiber optiks link. The bandwidth requiremnt, recommended by Microsoft for such topology is:</p> <ul> <li>Intra-farm latency of &lt;1ms (one way), 99.9% of the time over a period of ten minutes.</li> <li>The bandwidth speed must be at least 1 gigabit per second. The recommended topology from Microsoft would look something like as shown below:</li> </ul> <p></p>"},{"location":"M365/SharePointEvents.html","title":"Events","text":""},{"location":"M365/SharePointEvents.html#sharepoint-2007-event-receiver-project","title":"SharePoint 2007 Event Receiver Project","text":"<p>In this article I will show you how to Develop an event receiver in C# to handle item added, updated, or deleted events in a specific SharePoint document library. This article can be used from SharePoint 2007 till SharePoint 2013 without much changes. </p>"},{"location":"M365/SharePointEvents.html#event-receivers","title":"Event Receivers","text":"<p>Using Event Receiver we can write custom code(e.g. send a mail) whenever something is done to a list or library item. Note: Nowadays SharePoint is moving towards Webhooks as it is lightweight and more efficient.</p>"},{"location":"M365/SharePointEvents.html#event-receivers-types","title":"Event Receivers Types","text":"<ul> <li>Synchronous event receivers: These receivers run before or after the event. E.g. Before SharePoint saves an updated item(Before Item Updated)</li> <li>Asynchronous event receivers: This work after an event has happened. E.g. Sene a mail after an event is added.</li> </ul>"},{"location":"M365/SharePointEvents.html#lets-get-started","title":"Let's get started","text":""},{"location":"M365/SharePointEvents.html#step-1-create-sharepoint-document-library","title":"Step 1: Create SharePoint Document Library","text":"<ol> <li>Open your SharePoint site using a web browser.</li> <li>Navigate to the desired site where you want to create the document library.</li> <li>Click on \"Site Actions\" -&gt; \"View All Site Content\" -&gt; \"Create\".</li> <li>Select \"Document Library\" from the list of available templates.</li> <li>Provide a name for the document library (e.g., \"Custom Documents\").</li> <li>Click \"Create\" to create the document library.</li> </ol>"},{"location":"M365/SharePointEvents.html#step-2-create-visual-studio-project","title":"Step 2: Create Visual Studio Project","text":"<ol> <li>Open Visual Studio 2008 or later.</li> <li>Create a new SharePoint project:</li> <li>File -&gt; New -&gt; Project.</li> <li>Select \"SharePoint\" from the installed templates.</li> <li>Choose \"Empty SharePoint Project\" and provide a name (e.g., \"SharePointEventReceiverProject\").</li> <li>Click \"OK\" to create the project.</li> </ol>"},{"location":"M365/SharePointEvents.html#step-3-add-event-receiver-class","title":"Step 3: Add Event Receiver Class","text":"<ol> <li>Right-click on the project in Solution Explorer.</li> <li>Select \"Add\" -&gt; \"New Item\".</li> <li>Choose \"Event Receiver\" and provide a name for the class (e.g., \"CustomDocumentLibraryEventReceiver\").</li> <li>Click \"Add\" to add the event receiver class to the project.</li> </ol>"},{"location":"M365/SharePointEvents.html#step-4-implement-event-receiver-logic","title":"Step 4: Implement Event Receiver Logic","text":"<pre><code>//Das, 2010\n\nusing Microsoft.SharePoint;\n\nnamespace SharePointEventReceiverProject\n{\n    public class CustomDocumentLibraryEventReceiver : SPItemEventReceiver\n    {\n        public override void ItemAdded(SPItemEventProperties properties)\n        {\n            base.ItemAdded(properties);\n            // Write your custom code here\n            // Example: Send email notification or update metadata\n        }\n\n        public override void ItemUpdated(SPItemEventProperties properties)\n        {\n            base.ItemUpdated(properties);\n            // Write your custom code here\n            // Example: Log changes or trigger workflows\n        }\n\n        public override void ItemDeleted(SPItemEventProperties properties)\n        {\n            base.ItemDeleted(properties);\n            // Perform actions when an item is deleted from the document library\n            // Example: Archive deleted items or update related records\n        }\n    }\n}\n</code></pre>"},{"location":"M365/SharePointEvents.html#step-5-deploy-using-feature","title":"Step 5: Deploy using Feature","text":"<ol> <li>Right-click on the project in Solution Explorer.</li> <li>Select \"Add\" -&gt; \"New Item\".</li> <li>Choose \"Feature\" and provide a name for the feature (e.g., \"CustomEventReceiverFeature\").</li> <li>Open the feature XML file (Feature.xml) and add an EventReceiver element to specify the event receiver class.</li> <li>Set the ReceiverAssembly and ReceiverClass attributes to reference the event receiver assembly and class.</li> <li>Build the SharePoint project to generate the event receiver assembly (.dll).</li> <li>Deploy the event receiver assembly to the SharePoint server using the feature:</li> <li>Activate the feature at the site collection or site level where the document library is located.</li> <li>Verify that the event receiver is attached to the document library.</li> </ol>"},{"location":"M365/SharePointFarmConsolidation.html","title":"Introduction","text":"<p>Here, I am sharing about a project which I was part of acting as a consultant for architecture.</p>"},{"location":"M365/SharePointFarmConsolidation.html#purpose","title":"Purpose","text":"<p>The client wants to setup a new consolidated SharePoint farm in its European Data Center (EDC) for its intranet and extranet SharePoint applications in an efficient and cost effective manner. This new farm should be implemented in such a way so that it provides high availability (HA), Scalability and Disaster Recovery (DR).</p>"},{"location":"M365/SharePointFarmConsolidation.html#existing-system-specification","title":"Existing System Specification","text":""},{"location":"M365/SharePointFarmConsolidation.html#current-system-study","title":"Current System study","text":"<p>Our team had detailed discussions with the client's management team to understand the needs to consolidate the existing SharePoint environments to new SharePoint environments in a more (cost) efficient and more stable manner. We followed the standard process of studying current system. The following section would highlight the key findings of the system study:</p>"},{"location":"M365/SharePointFarmConsolidation.html#farm-scenario","title":"Farm Scenario","text":"Particular Ceva portal Ceva NET Extranet Farm Topology One SharePoint 2007 farm in Florida consisting of 2 servers  (WFE and 2 App roles combined),  using a SQL Server cluster One SharePoint 2007 farm in Texasconsisting of 2 WFE and 2 App servers, using SQL Server 2005 cluster. One WSS 3.0 server;  located in Texas  containing few 100 sites. Sites 200 team sites, 15 apps (from Notes). 1000 sites, multilingual,  custom apps (30% custom, 70% standard). Few hundred sites. Storage 50 gb 100 gb unspecified Users 500 \u2013 2000 24000 unspecified Average Requests 400 45000 unspecified NLB No Hardware NLB used. Uses Windows (software) NLB. No Hardware NLB used. Uses Windows (software) NLB. No Hardware NLB used. Uses Windows (software) NLB. Business Data Catalogue(BDC) Not configured Not configured Not configured."},{"location":"M365/SharePointFarmConsolidation.html#current-system-assessment","title":"Current System Assessment","text":"<p>Our assessment findings based on current client's SharePoint implementation are:</p> <ul> <li>Current SharePoint installation is not implemented for High Availability. No business continuity plans defined.</li> <li>Single Point of failure. No Disaster Recovery farm exists.</li> <li>Absence of redundancy for some of the server roles.</li> <li>Absence of scalability for some of the server roles.</li> </ul>"},{"location":"M365/SharePointFarmConsolidation.html#general-farm-requirements","title":"General Farm Requirements","text":"<ul> <li>To design, build, test and deploy the new SharePoint 2007 farm in the EDC meant for intranet/extranet MOSS 2007 applications.</li> <li>To Perform English-only out of the box installation.</li> <li>To configure search, user profile sync etc.</li> <li>To incorporate scalability, High availability and Disaster recovery.</li> <li>To implement a supported full backup/restore solution, preferably with NetBackup (VERITAS/Symantec).</li> <li>To provide a migration plan.</li> <li>To help with test migrations.</li> </ul>"},{"location":"M365/SharePointFarmConsolidation.html#high-availability-ha-and-disaster-recovery-dr-requirements","title":"High Availability (HA) and Disaster Recovery (DR) Requirements","text":"<p>The following parameters are to be met in for High availability and Disaster Recovery:</p> Parameter CEVANet Gold --- --- Monitoring 24x7 on desk Availability 99.35% \u201cAllowed\u201d downtime per month Approximately 4.5 hours Recovery time (in the event of a complete failure this is the RTO) 8 hours Recovery Point Objective (RPO) 120 minutes Successful failover testing Once every 12 months Successful Disaster Recovery Testing Once every 12 months Successful Testing of a Restore from a Backup Once every 12 months Disaster Recovery Capacity 80%"},{"location":"M365/SharePointFarmConsolidation.html#assumptions-dependencies","title":"Assumptions &amp; Dependencies","text":""},{"location":"M365/SharePointFarmConsolidation.html#assumptions","title":"Assumptions","text":"<ol> <li>Development and (user) acceptance environments are already available.</li> <li>SharePoint 2007 SP2 with SQL Server 2008 R2 setup is already available and is clustered and mirrored across two data centers.</li> <li>Hardware ordering/installation and OS installation will be done by Ceva and are available within 4 weeks after ordering.</li> <li>SQL Server storage and resources are available and high available with disaster recovery.</li> <li>The reverse proxy will support SharePoint (possible impact on authentication and/or requires additional configuration)</li> </ol>"},{"location":"M365/SharePointFarmConsolidation.html#scope","title":"Scope","text":"<p>Below activities will be performed to accomplish the task.</p> <ol> <li>Discovery<ol> <li>Analyze the existing SharePoint 2007 farms to find the application size, no. of site collections, database size, and user base.</li> </ol> </li> <li>Design<ol> <li>Prepare Hardware BOM.</li> <li>Prepare Software BOM.</li> <li>Prepare Technical Architecture Specifications.</li> <li>Prepare Execution Environment Design.</li> <li>Prepare DR Plan.</li> <li>Prepare Backup/Restore Plan.</li> </ol> </li> <li>Build<ol> <li>MOSS Installation and Farm Setup.</li> <li>Backup/Restore Implementation.</li> <li>DR Implementation.</li> <li>Prepare the install and configuration guide.</li> <li>Prepare the Technology Policies and Procedures.</li> <li>Prepare the Scalability Matrix.</li> </ol> </li> <li>Migration Support<ol> <li>Provide any Farm related support during application migration.</li> </ol> </li> <li>Test &amp; Deploy<ol> <li>Prepare the Test Plan to test the SharePoint Farm.</li> <li>Execute the Tests.</li> <li>Provide any Farm related support during the application Go Live.</li> </ol> </li> </ol>"},{"location":"M365/SharePointFarmConsolidation.html#high-level-infrastructure-layout","title":"High-level infrastructure layout","text":"<p>Below image shows the proposed high-level structure of the Farm</p> <p></p>"},{"location":"M365/SharePointFormsOrPowerApps.html","title":"Forms vs PowerApps","text":""},{"location":"M365/SharePointFormsOrPowerApps.html#choosing-power-apps-forms-for-sharepoint-when-and-why","title":"Choosing Power Apps Forms for SharePoint: When and Why?","text":"<p>In the dynamic world of digital transformation, SharePoint has long been a staple for organizations seeking efficient content management and collaboration. However, with the advent of Power Apps, Microsoft introduced a powerful tool that further expands SharePoint's capabilities, especially in custom form development. Understanding when to use Power Apps-based forms for SharePoint can significantly impact how your organization manages data, automates processes, and enhances user interaction.</p>"},{"location":"M365/SharePointFormsOrPowerApps.html#the-sharepoint-context","title":"The SharePoint Context","text":"<p>SharePoint lists and libraries are fundamental components that store and organize data, ranging from simple contact lists to complex project management trackers. The default forms provided by SharePoint for adding, viewing, or editing items in these lists are straightforward but often lack the flexibility for customization or business logic implementation needed by many organizations.</p>"},{"location":"M365/SharePointFormsOrPowerApps.html#enter-power-apps","title":"Enter Power Apps","text":"<p>Power Apps is a part of Microsoft's Power Platform, designed to build custom apps and forms without the need for deep programming knowledge. When integrated with SharePoint, Power Apps elevates the customization level of forms, enabling tailored user experiences, complex data validations, and dynamic content presentation.</p>"},{"location":"M365/SharePointFormsOrPowerApps.html#when-to-use-power-apps-for-sharepoint-forms","title":"When to Use Power Apps for SharePoint Forms","text":"<p>1. Enhanced Customization Requirements: If your organization needs more than simple field additions\u2014such as conditional visibility, customized layouts, or integration with external data sources\u2014Power Apps is your go-to solution.</p> <p>2. Improved User Experience: For scenarios where user interaction with SharePoint data requires a more intuitive and visually appealing interface, Power Apps forms can provide a significantly enhanced user experience.</p> <p>3. Complex Business Logic: When you need to incorporate complex business logic into your forms\u2014like dynamic dropdowns based on previous selections or data validation that goes beyond the basic\u2014Power Apps allows for these sophisticated scenarios.</p> <p>4. Mobile Accessibility: If accessing SharePoint forms on mobile devices is a priority, Power Apps offers a responsive design that can adjust to various screen sizes, making it an ideal choice for a mobile-friendly interface.</p> <p>5. Integration with Other Services: For forms that require pulling in data from or pushing data to other services and applications within the Microsoft ecosystem or external APIs, Power Apps provides a robust set of connectors and the ability to create custom connections.</p>"},{"location":"M365/SharePointFormsOrPowerApps.html#considerations-before-transitioning","title":"Considerations Before Transitioning","text":"<p>While Power Apps offers impressive capabilities, it's essential to consider the learning curve, licensing requirements, and the potential need for ongoing maintenance and updates. Ensure your team has the skills\u2014or the willingness to learn\u2014before fully committing to Power Apps for your SharePoint forms.</p>"},{"location":"M365/SharePointFormsOrPowerApps.html#conclusion","title":"Conclusion","text":"<p>The decision to use Power Apps-based forms for SharePoint hinges on the need for customization, the desire for a better user experience, and the requirement to integrate complex business logic. By leveraging Power Apps, organizations can transform their SharePoint sites into more dynamic, interactive, and efficient digital workspaces. Always weigh the benefits against the complexity and resources required to maintain these solutions to ensure they align with your organization's goals and capabilities.</p> <p>\u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com</p>"},{"location":"M365/SharePointMiniRole.html","title":"SharePoint 2016 MinRole","text":""},{"location":"M365/SharePointMiniRole.html#before-2016","title":"Before 2016:","text":"<p>Say, you had two servers. You could install any service on either of them, in any combination. E,g,:</p> <ul> <li>Server 1: Web Front-End + Search + Central Administration</li> <li>Server 2: Indexing + Application Services</li> </ul> <p>This freedom, and lack of guidance, gave a lot of flexibility. But, you had to be careful about performance issues.</p>"},{"location":"M365/SharePointMiniRole.html#sharepoint-20162019","title":"SharePoint 2016/2019:","text":"<p>MinRole is just a best combination of services. Now adminstrator know beforehand, what mix is best. It's not just advice, its shown as a radio  button during installation.</p> <p>Also, like before, you can still install 'anything anywhere' by clicking 'Custom' MiniRole optioin. This is how the new installer shows them:</p> <p></p>"},{"location":"M365/SharePointMiniRole.html#dedicated-minrole","title":"Dedicated MinRole:","text":"<ul> <li>Front-End</li> <li>Application</li> <li>Distributed Cache</li> <li>Search</li> </ul> <p> From Microsoft Site</p>"},{"location":"M365/SharePointMiniRole.html#special-minirole","title":"Special MiniRole:","text":"<ul> <li>Single-Farm: All-in-one. Before called, Standalone Install mode. Used for Dev.</li> <li>Custom: Anything-anywhere. The-way-you-like-it</li> </ul> <p> From Microsoft Site</p>"},{"location":"M365/SharePointMiniRole.html#shared-miniroles","title":"Shared MiniRoles:","text":"<ul> <li>Front-End + Distributed Cache</li> <li>Application +  Search</li> </ul> <p> From Microsoft Site</p> <p>Long story short: MinRole is just 'best mix of SharePoint services' as per Microsoft.</p>"},{"location":"M365/SharePointVersionEvolution.html","title":"Evolution","text":""},{"location":"M365/SharePointVersionEvolution.html#sharepoint-version-evolution","title":"SharePoint Version Evolution","text":"<p>SharePoint has evolved significantly since its inception, transforming from a simple document management tool to a comprehensive platform for collaboration, content management, and much more. Let's dive straight into the technical journey of SharePoint's evolution, highlighting key versions and features.</p>"},{"location":"M365/SharePointVersionEvolution.html#sharepoint-portal-server-2001","title":"SharePoint Portal Server 2001","text":"<ul> <li>Introduction: Launched as a document management and team collaboration tool.</li> <li>Key Features: Document libraries, web portals for information sharing.</li> </ul>"},{"location":"M365/SharePointVersionEvolution.html#sharepoint-portal-server-2003","title":"SharePoint Portal Server 2003","text":"<ul> <li>Enhancements: Improved integration with Microsoft Office, introduction of site collections, and better content management capabilities.</li> <li>Key Features: Personal sites, audience targeting.</li> </ul>"},{"location":"M365/SharePointVersionEvolution.html#microsoft-office-sharepoint-server-2007-moss-2007","title":"Microsoft Office SharePoint Server 2007 (MOSS 2007)","text":"<ul> <li>Major Overhaul: Introduced the modern SharePoint architecture, including features like workflows, web parts, and enterprise content management.</li> <li>Key Features: Enhanced search, content types, and integration with Windows Workflow Foundation.</li> </ul>"},{"location":"M365/SharePointVersionEvolution.html#sharepoint-2010","title":"SharePoint 2010","text":"<ul> <li>Focus on User Experience: Introduced the Ribbon interface, enhancing usability. Added features like sandboxed solutions and service applications.</li> <li>Key Features: SharePoint Designer enhancements, Visual Studio integration, and the introduction of Business Connectivity Services.</li> </ul>"},{"location":"M365/SharePointVersionEvolution.html#sharepoint-2013","title":"SharePoint 2013","text":"<ul> <li>Social and Search: Emphasized social features and improved search functionality. Introduced the app model for custom development.</li> <li>Key Features: Community site, improved content search, and cross-site publishing.</li> </ul>"},{"location":"M365/SharePointVersionEvolution.html#sharepoint-2016","title":"SharePoint 2016","text":"<ul> <li>Hybrid Cloud Integration: Focused on hybrid capabilities, allowing seamless integration between SharePoint Online (Office 365) and on-premises SharePoint.</li> <li>Key Features: MinRole architecture for optimized server roles, durable links, and improved file storage.</li> </ul>"},{"location":"M365/SharePointVersionEvolution.html#sharepoint-2019","title":"SharePoint 2019","text":"<ul> <li>Modern Experience: Continued the shift towards a more modern user experience introduced in SharePoint Online, with modern sites, lists, and libraries.</li> <li>Key Features: Modern search experience, SharePoint Home page, and mobile-friendly design.</li> </ul>"},{"location":"M365/SharePointVersionEvolution.html#sharepoint-online-part-of-office-365microsoft-365","title":"SharePoint Online (Part of Office 365/Microsoft 365)","text":"<ul> <li>Cloud-First: Continuously updated, providing the latest features and integrations with the Microsoft ecosystem.</li> <li>Key Features: Modern team sites, communication sites, integration with Power Platform (PowerApps, Power Automate), and continuous updates based on user feedback and technological advancements.</li> </ul> <p>Now, lets put what we learnt so far in tabular format</p> Feature SharePoint 2007 SharePoint 2010 SharePoint 2013 SharePoint 2016 SharePoint 2019 User Interface Classic UI Ribbon UI Ribbon UI, Metro UI Ribbon UI, Metro UI Modern UI, Communication Sites Social Features Limited social features My Sites, Activity Feeds Community Sites, Microblogging Community Sites, Microblogging Yammer Integration, News Web Parts Mobile Compatibility Limited mobile support Improved mobile support Responsive design, Touch-friendly UI Responsive design, Touch-friendly UI Responsive design, Mobile App Workflow Capabilities Basic workflows with SharePoint Designer Improved workflows with SharePoint Designer Improved workflows with SharePoint Designer Improved workflows with SharePoint Designer Improved workflows with SharePoint Designer Search Functionality Basic search functionality Enterprise search capabilities Improved search relevance, refinement Hybrid search, Improved relevance Hybrid search, Intelligent search Cloud Integration Limited integration with Office 365 Improved integration with Office 365 Hybrid deployment options Hybrid deployment options Hybrid deployment options Business Intelligence Basic BI features PerformancePoint Services, Excel Services Power View, PowerPivot, Power Map Power View, PowerPivot, Power Map Power BI Integration App Model N/A Sandboxed Solutions, SharePoint App Store SharePoint App Model, App Catalog SharePoint App Model, App Catalog SharePoint App Model, App Catalog Site Templates Limited site templates Enhanced site templates Improved site templates Improved site templates Improved site templates Compliance and Records Management Basic compliance features Records Center, Document Sets Improved compliance features Improved compliance features Improved compliance features Hybrid Deployment N/A N/A Hybrid deployment options Hybrid deployment options Hybrid deployment options Customization Options Features, Web Parts, Custom Code Features, Web Parts, Custom Code Features, Apps, Client-Side Solutions Features, Apps, Client-Side Solutions Features, Apps, Client-Side Solutions Development Models Server-Side Solutions (C#, ASP.NET) Server-Side Solutions (C#, ASP.NET) Server-Side and Client-Side Solutions Server-Side and Client-Side Solutions Server-Side and Client-Side Solutions Windows Server Versions Windows Server 2003, Windows Server 2008 Windows Server 2008 R2, Windows Server 2012 Windows Server 2012, Windows Server 2012 R2 Windows Server 2012 R2, Windows Server 2016 Windows Server 2016, Windows Server 2019 SQL Server Versions SQL Server 2005, SQL Server 2008 SQL Server 2008 R2, SQL Server 2012 SQL Server 2012, SQL Server 2014 SQL Server 2014, SQL Server 2016 SQL Server 2016, SQL Server 2017 SharePoint Designer Versions SharePoint Designer 2007 SharePoint Designer 2010 SharePoint Designer 2013 SharePoint Designer 2013 SharePoint Designer 2013 <p>\u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com</p>"},{"location":"M365/SharePointVsOtherECM.html","title":"vs Other ECM","text":""},{"location":"M365/SharePointVsOtherECM.html#sharepoint-vs-opentext-documentum-and-ibm-filenet-for-ecm","title":"SharePoint vs OpenText Documentum, and IBM FileNet for ECM","text":"<p>When selecting an Enterprise Content Management (ECM) platform, SharePoint, OpenText Documentum, and IBM FileNet emerge as notable choices, each with unique strengths.</p> <ul> <li> <p>Integration &amp; Scalability: SharePoint excels in the Microsoft ecosystem, offering seamless integration with Office 365. Documentum and FileNet provide strong integration with enterprise systems but might need more customization.</p> </li> <li> <p>Customization &amp; Flexibility: All three platforms allow significant customization. SharePoint is user-friendly with numerous templates; Documentum and FileNet offer deep customization but require more technical expertise.</p> </li> <li> <p>Security &amp; Compliance: SharePoint utilizes Microsoft's security framework, making it secure for sensitive data. Documentum and FileNet are preferred in regulated industries for their robust security and compliance features.</p> </li> <li> <p>Collaboration: SharePoint shines in collaboration, integrating well with Microsoft Teams. Documentum and FileNet may need third-party tools for similar functionality.</p> </li> <li> <p>Cost &amp; ROI: SharePoint could offer better ROI for Microsoft users. Documentum and FileNet may have higher initial costs but excel in managing complex workflows in regulated fields.</p> </li> <li> <p>User Experience: SharePoint is aiming for user-friendliness, while Documentum and FileNet, though powerful, might demand more training.</p> </li> <li> <p>Use Cases:</p> </li> <li>SharePoint: Suitable for integrated ECM with strong collaboration, especially in Microsoft-centric environments.</li> <li>Documentum: Ideal for regulated industries needing stringent content management.</li> <li>FileNet: Fits businesses focusing on advanced workflow and content lifecycle management, like in finance and insurance.</li> </ul> <p>Conclusion: The choice between SharePoint, Documentum, and FileNet hinges on your organization's specific needs, compliance requirements, and tech ecosystem. SharePoint is versatile and collaboration-focused, while Documentum and FileNet excel in complex management needs within regulated sectors. Consider your organizational goals and budget to select the ECM that best supports your strategic objectives.</p> <p>\u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com</p>"},{"location":"M365/WSS3DocumentUpload.html","title":"WSS3","text":""},{"location":"M365/WSS3DocumentUpload.html#document-migration-to-sharepoint-wss-30-using-cnet","title":"Document Migration to SharePoint WSS 3.0 Using C#.NET","text":"<p>Our client had a large number of documents saved on their network drives and wanted to move them to SharePoint WSS 3.0. They were excited about using the new features of WSS 3.0 to make document management easier and more efficient.</p>"},{"location":"M365/WSS3DocumentUpload.html#the-challenge","title":"The Challenge","text":"<p>However, the migration task came with several technical challenges. The large volume of documents made the transfer process time-consuming and complex. Important metadata like creation and modification dates, along with author information, was getting lost during the migration, disrupting record-keeping and compliance. The need to rename files and folders for better organization and consistency was not easy to handle. Additionally, SharePoint's restrictions on file and folder names caused issues with invalid characters, needing manual intervention.</p>"},{"location":"M365/WSS3DocumentUpload.html#a-custom-solution","title":"A Custom Solution","text":"<p>To solve these problems, we developed a custom C# solution using the SharePoint Object Model. This approach allowed us to automate the upload process, ensure metadata integrity, provide flexibility in renaming, and handle invalid characters efficiently.</p>"},{"location":"M365/WSS3DocumentUpload.html#key-features-of-the-solution","title":"Key Features of the Solution","text":"<p>The solution included automated uploads that allowed bulk document uploads directly from network drives to SharePoint. We also developed custom code to preserve important metadata like the original creation date, modification date, and author information. The solution provided functionality to rename files and folders as per the client's needs, and it included automated detection and correction of invalid characters in filenames.</p>"},{"location":"M365/WSS3DocumentUpload.html#the-c-code-snippet","title":"The C# Code Snippet","text":"<p>Below is a simplified version of the C# code snippet that was used:</p> <pre><code>using System;\nusing Microsoft.SharePoint;\n\nnamespace SharePointMigrationHelper\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            string siteUrl = \"http://sharepointserver/sites/mrvl\";\n            string libraryName = \"Documents\";\n            string filePath = @\"C:\\networkdrive\\document.docx\";\n            string fileName = \"document.docx\";\n\n            // Example metadata\n            DateTime creationDate = new DateTime(2020, 1, 1);\n            string createdBy = \"CreatorName\";\n\n            using (SPSite site = new SPSite(siteUrl))\n            {\n                using (SPWeb web = site.OpenWeb())\n                {\n                    SPFolder libraryFolder = web.Folders[libraryName];\n                    byte[] fileContent = System.IO.File.ReadAllBytes(filePath);\n\n                    SPFile uploadedFile = libraryFolder.Files.Add(fileName, fileContent, true);\n\n                    SPListItem item = uploadedFile.Item;\n                    item[\"Created\"] = creationDate;\n                    item[\"Author\"] = web.EnsureUser(createdBy);\n                    item.Update();\n                }\n            }\n\n            Console.WriteLine(\"Document uploaded successfully with metadata.\");\n        }\n    }\n}\n</code></pre>"},{"location":"M365/WSS3DocumentUpload.html#conclusion","title":"Conclusion","text":"<p>The migration of documents to SharePoint WSS 3.0 allowed the client to use the robust features of WSS 3.0, making it easier for them to move to MOSS 2007 later.</p> <p>\u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com</p>"},{"location":"M365/oAuthSharePointPython.html","title":"OAuth Python","text":""},{"location":"M365/oAuthSharePointPython.html#how-to-authenticate-from-your-local-python-setup-with-sharepoint-online","title":"How to authenticate from your local python setup with SharePoint online","text":"<p>The ability to integrate your local python enviornment with SharePoint online can have lot of advantages. Imagine you have a bunch of JSON files and want to put the information from those files into a SharePoint document library. With Python, you can easily write a program to do that.</p> <p>This article will show you a few ways to connect your Python environment to SharePoint Online.</p>"},{"location":"M365/oAuthSharePointPython.html#authenticating-to-sharepoint-online-with-username-and-password-in-python-challenges-with-mfa","title":"Authenticating to SharePoint Online with Username and Password in Python: Challenges with MFA","text":"<p>Authenticating to SharePoint Online directly using a username and password from a Python script can be straightforward for basic scripts and automation tasks. However, this approach faces significant challenges, especially in environments where Modern Authentication policies, including Multi-Factor Authentication (MFA), are enforced. </p> <p>For example, I had MFA enabled and I tried to connect with SharePoint onlien from my local VS Code using python. Install the Office365-REST-Python-Client Library</p> <pre><code>pip install Office365-REST-Python-Client\n</code></pre> <p></p> <p>Then, I tried to authanticate</p> <pre><code>from office365.runtime.auth.authentication_context import AuthenticationContext\nfrom office365.sharepoint.client_context import ClientContext\n\n# SharePoint site URL\nsite_url = \"https://yourdomain.sharepoint.com/sites/yoursite\"\n\n# Your Office 365 credentials\nusername = \"yourusername@yourdomain.com\"\npassword = \"yourpassword\"\n\nctx_auth = AuthenticationContext(site_url)\nif ctx_auth.acquire_token_for_user(username, password):\n    ctx = ClientContext(site_url, ctx_auth)\n    web = ctx.web\n    ctx.load(web)\n    ctx.execute_query()\n    print(f\"Authenticated as: {web.properties['Title']}\")\nelse:\n    print(\"Authentication failed.\")\n</code></pre> <p>This failed:</p> <p></p> <p><code>Conclusion</code></p> <p>Nowadays, MFA is everywhere, most organizations enforce MFA, hence while direct username/password authentication is theoritically possible, chances are it will fail 99% of the time.</p>"},{"location":"M365/oAuthSharePointPython.html#connecting-to-sharepoint-online-from-python-using-oauth-as-a-solution","title":"Connecting to SharePoint Online from Python Using OAuth as a solution","text":"<p>We saw how our basic authentication method using username/password failed. Hence, Modern authentication via OAuth is recommended for secure access. Follow the steps below to connect with SharePoint online using oAuth authantication. oAuth is also used to connect to a lot of other Offie 365 and Azure services. Its very popular.</p>"},{"location":"M365/oAuthSharePointPython.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Python: Ensure Python is installed on your system. The examples that I show you will need Python 3.6 or higher</li> <li>Azure AD App Registration: You will also need access to Azure with sufficient permission to register an application in Azure Active Directory (Azure AD) for OAuth authentication.</li> </ul>"},{"location":"M365/oAuthSharePointPython.html#step-1-register-an-application-in-azure-ad","title":"Step 1: Register an Application in Azure AD","text":"<ol> <li>Sign in to the Azure portal and navigate to Azure Active Directory &gt; App registrations &gt; New registration.</li> <li>Register your app by providing a name and selecting supported account types. The redirect URI (Web) can be <code>http://localhost</code> for testing purposes.</li> <li>Capture the Application (client) ID and Directory (tenant) ID from the Overview page after registration.</li> <li>Under Certificates &amp; secrets, generate a new client secret and note it down.</li> </ol>"},{"location":"M365/oAuthSharePointPython.html#step-2-grant-permissions-to-the-app","title":"Step 2: Grant Permissions to the App","text":"<ol> <li>Navigate to API permissions &gt; Add a permission &gt; APIs my organization uses &gt; SharePoint.</li> <li>Choose Delegated permissions and add permissions like <code>Sites.Read.All</code> (adjust based on your needs).</li> <li>Click \"Add permissions\" and ensure an administrator grants consent if required.</li> </ol>"},{"location":"M365/oAuthSharePointPython.html#step-3-install-required-python-libraries","title":"Step 3: Install Required Python Libraries","text":"<p>Install the <code>requests</code> library to make HTTP requests and <code>msal</code> for Microsoft Authentication Library support.</p> <pre><code>pip install requests msal\n</code></pre>"},{"location":"M365/oAuthSharePointPython.html#step-4-authenticate-using-oauth","title":"Step 4: Authenticate Using OAuth","text":"<pre><code>from msal import ConfidentialClientApplication\n\n# Replace these with your Azure AD app registration details\nclient_id = 'YOUR_APP_CLIENT_ID'\nclient_secret = 'YOUR_APP_CLIENT_SECRET'\ntenant_id = 'YOUR_TENANT_ID'\n\nauthority_url = f'https://login.microsoftonline.com/{tenant_id}'\nresource_url = 'https://graph.microsoft.com'\nredirect_uri = 'http://localhost'\n\n# Initialize the MSAL confidential client\napp = ConfidentialClientApplication(\n    client_id,\n    authority=authority_url,\n    client_credential=client_secret,\n)\n\n# Acquire token for SharePoint\ntoken_response = app.acquire_token_for_client(scopes=[f'{resource_url}/.default'])\n\n# Extract the access token\naccess_token = token_response.get('access_token', None)\nif not access_token:\n    raise Exception(\"Failed to acquire token. Check your credentials and permissions.\")\n\nprint(\"Successfully authenticated.\")\n</code></pre>"},{"location":"M365/oAuthSharePointPython.html#step-5-interact-with-sharepoint-online","title":"Step 5: Interact with SharePoint Online","text":"<p>With the access token, you can now make authenticated requests to SharePoint Online. Here is a small example that shows how to list the titles of all SharePoint sites in your organization using the Microsoft Graph API.</p> <pre><code>import requests\n\n# The endpoint to list all sites\nurl = f'{resource_url}/v1.0/sites'\n\n# Headers for the request\nheaders = {\n    'Authorization': f'Bearer {access_token}',\n    'Accept': 'application/json',\n}\n\nresponse = requests.get(url, headers=headers)\nif response.status_code == 200:\n    sites = response.json()\n    for site in sites.get('value', []):\n        print(site.get('displayName'))\nelse:\n    print(f\"Failed to retrieve sites: {response.status_code}\")\n</code></pre>"},{"location":"M365/oAuthSharePointPython.html#takeaways","title":"Takeaways","text":"<p>This article showed you how to connect to SharePoint Online from a local Python environment using OAuth for authentication. By following these steps, you've registered an application in Azure AD, granted it necessary permissions, authenticated using the Microsoft Authentication Library (MSAL), and interacted with SharePoint Online via the Microsoft Graph API.</p> <p>Remember, OAuth provides a secure and robust method for authenticating and interacting with Microsoft services. Knowing about oAuth can be a valuable skill if you want to work with microsoft resources.</p> <p>\u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com</p>"},{"location":"MKdocs/Github_Deployment_Guide.html","title":"Deploying MkDocs Material to GitHub Pages","text":"<p>This guide walks you through setting up an MkDocs site with Material theme and deploying it automatically to GitHub Pages using GitHub Actions.</p>"},{"location":"MKdocs/Github_Deployment_Guide.html#initial-setup","title":"Initial Setup","text":"<p>Start by creating a GitHub repository for your project, then clone it locally and open it in VS Code. Navigate to the repository folder in your terminal and create a Python virtual environment:</p> <pre><code>python -m venv venvmac\nsource venvmac/bin/activate  # For Linux or macOS\n</code></pre> <p>Install MkDocs Material along with the necessary extensions:</p> <pre><code>pip install --no-cache-dir mkdocs-material pymdown-extensions mkdocs-glightbox\n</code></pre> <p>The <code>mkdocs-material</code> package is the main theme, while <code>pymdown-extensions</code> and <code>mkdocs-glightbox</code> add enhanced markdown features and image lightbox functionality.</p>"},{"location":"MKdocs/Github_Deployment_Guide.html#creating-your-site","title":"Creating Your Site","text":"<p>Initialize a new MkDocs site in the current directory:</p> <pre><code>mkdocs new .\n</code></pre> <p>This creates a basic folder structure:</p> <pre><code>.\n\u251c\u2500 docs/\n\u2502  \u2514\u2500 index.md\n\u2514\u2500 mkdocs.yml\n</code></pre> <p>Open <code>mkdocs.yml</code> and configure the Material theme with your site details:</p> <pre><code>site_name: My Documentation\nsite_url: https://yourusername.github.io/your-repo\ntheme:\n  name: material\n</code></pre> <p>Add your content by creating markdown files in the <code>docs/</code> folder. Test your site locally with:</p> <pre><code>mkdocs serve\n</code></pre> <p>This launches a local server at <code>http://127.0.0.1:8000</code> where you can preview changes in real-time.</p>"},{"location":"MKdocs/Github_Deployment_Guide.html#automated-deployment-with-github-actions","title":"Automated Deployment with GitHub Actions","text":"<p>To automatically deploy your site whenever you push changes, create a GitHub Actions workflow file at <code>.github/workflows/ci.yml</code>:</p> <pre><code>name: ci\non:\n  push:\n    branches:\n      - master\n      - main\npermissions:\n  contents: write\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Configure Git Credentials\n        run: |\n          git config user.name github-actions[bot]\n          git config user.email 41898282+github-actions[bot]@users.noreply.github.com\n      - uses: actions/setup-python@v5\n        with:\n          python-version: 3.x\n      - run: echo \"cache_id=$(date --utc '+%V')\" &gt;&gt; $GITHUB_ENV\n      - uses: actions/cache@v4\n        with:\n          key: mkdocs-material-${{ env.cache_id }}\n          path: .cache\n          restore-keys: |\n            mkdocs-material-\n      - run: pip install mkdocs-material mkdocs-glightbox\n      - run: mkdocs gh-deploy --force\n</code></pre> <p>This workflow triggers on every push to <code>main</code> or <code>master</code>, builds your site, and deploys it to the <code>gh-pages</code> branch. Before this works, you need to configure GitHub Pages in your repository settings.</p> <p>Go to Settings \u2192 Pages and set the source to the <code>gh-pages</code> branch with <code>/(root)</code> as the folder. Don't select <code>main/(root)</code>\u2014MkDocs Material generates the site in a separate <code>gh-pages</code> branch, not directly from your source files.</p> <p>Once configured, push your changes and watch the Actions tab. After the workflow completes, your site will be live at <code>https://yourusername.github.io/your-repo</code>.</p>"},{"location":"MKdocs/Github_Deployment_Guide.html#troubleshooting-missing-navigation","title":"Troubleshooting: Missing Navigation","text":"<p>After deploying your MkDocs site, you might find that only the index page appears without any navigation menu. This common issue occurs when GitHub Pages is pointing to the wrong branch. Here's how the deployment process works and how to fix it.</p>"},{"location":"MKdocs/Github_Deployment_Guide.html#understanding-the-gh-pages-branch","title":"Understanding the gh-pages Branch","text":"<p>When you push changes to your <code>main</code> branch, the GitHub Actions workflow automatically builds your MkDocs site and pushes the generated HTML files to a separate branch called <code>gh-pages</code>. This branch contains the final static website that GitHub Pages serves to visitors.</p> <pre><code>main branch:                          gh-pages branch:\n docs/           (Markdown)         index.html      (Built HTML)\n mkdocs.yml      (Config)           assets/         (CSS, JS, images)\n .github/        (Workflow)         ...             (Other static files)\n</code></pre> <p>The <code>gh-pages</code> branch is automatically managed by the <code>mkdocs gh-deploy --force</code> command in your workflow. You should never modify this branch directlylet GitHub Actions handle it.</p>"},{"location":"MKdocs/Github_Deployment_Guide.html#the-fix","title":"The Fix","text":"<p>The solution is simple: configure GitHub Pages to use the <code>gh-pages</code> branch. Navigate to your repository's Settings \u2192 Pages and set:</p> <ul> <li>Source: \"Deploy from a branch\"</li> <li>Branch: <code>gh-pages</code></li> <li>Folder: <code>/(root)</code></li> </ul> <p></p> <p>Important: Selecting <code>main/(root)</code> won't work with MkDocs Material because the workflow builds to a different branch.</p> <p>After changing this setting, trigger a rebuild to ensure everything deploys correctly:</p> <pre><code>git commit --allow-empty -m \"Trigger rebuild\"\ngit push origin main\n</code></pre> <p>Watch the Actions tab in GitHub for the workflow to complete, then visit your site at <code>https://&lt;username&gt;.github.io/&lt;repository&gt;/</code>. The navigation should now appear properly.</p> <p>If GitHub Pages isn't configured correctly, you may also encounter this deployment error:</p> <p></p> <pre><code>Error: Creating Pages deployment failed\nError: HttpError: Not Found\nError: Failed to create deployment (status: 404)\nEnsure GitHub Pages has been enabled: https://github.com/&lt;username&gt;/&lt;repository&gt;/settings/pages\n</code></pre> <p>This 404 error means GitHub Actions can't deploy because the Pages service isn't enabled or the source branch isn't set. Go to Settings \u2192 Pages, ensure Source is set to \"Deploy from a branch\" with <code>gh-pages/(root)</code> selected, then re-run the workflow.</p>"},{"location":"MKdocs/Github_Deployment_Guide.html#best-practices","title":"Best Practices","text":"<p>Keep your workflow simple: store all source content in <code>main</code>, let GitHub Actions build and deploy automatically, and never touch the <code>gh-pages</code> branch manually. If your site doesn't update after pushing changes, check the workflow logs in the Actions tab for any build errors.</p>"},{"location":"MKdocs/Parallel_Local_Testing.html","title":"Testing MKDocs Locally","text":"<p>This website is made using Material for MKDocs framework. Its essentially few markdown files which I create and the framework compiles them into html pages and puts in a github branch, from where GitHub pages serves them.</p> <p>So far so good. But, everytime I make a change to the website, I need to push the changes to GitHub and wait for it to build. This can be time-consuming and frustrating, especially if you are making a lot of changes.</p> <p>To make the process easier, you can test your MKDocs site locally before deploying it to GitHub Pages. Let me show you how.</p> <p>Note: If the venv is already created, all you have to do is run the two commands command <code>source venvmac/bin/activate</code>  and then <code>mkdocs serve</code> and your page will be available at <code>http://127.0.0.1:8000/</code>.</p>"},{"location":"MKdocs/Parallel_Local_Testing.html#quick-start","title":"Quick Start","text":"Step 1:  Create the script <code>createvenv.command</code> with the following content: <p>Just create a file and copy the content below into it. You can name the file anything you want, but make sure to keep the <code>.command</code> extension. This is a shell script that will set up a virtual environment for you. This is for mac OS.</p> <pre><code>        #!/bin/bash\n\n        # Can run anytime. Safe. Run it to make a fully working mkdocs venv with no integration issue with current docs.\n        # This script sets up a venv and MkDocs environment for macOS.\n\n        #This version is for macOS. For Windows, use createvenv.bat. Just go to the folder and double click the file.\n\n        echo \"Setting up MkDocs environment...\"\n\n        # Step 1: Clean up any existing virtual environment\n        if [ -d \"venvmac\" ]; then\n            echo \"Removing existing virtual environment...\"\n            rm -rf venvmac\n        fi\n\n        # Step 2: Create new virtual environment\n        echo \"Creating new virtual environment...\"\n        python3 -m venv venvmac\n\n        # Step 3: Activate virtual environment\n        echo \"Activating virtual environment...\"\n        source venvmac/bin/activate\n\n        # Step 4: Upgrade pip and install packages\n        echo \"Installing required packages...\"\n        pip install --upgrade pip\n        pip install --no-cache-dir mkdocs-material pymdown-extensions mkdocs-glightbox\n\n        # Step 5: Verify installation\n        echo \"Verifying installation...\"\n        python --version\n        pip list | grep mkdocs\n\n        echo \"Setup complete! Virtual environment is activated.\"\n        echo \"To activate this environment later, run: source venvmac/bin/activate\"\n\n        # Keep terminal window open\n        read -p \"Press Enter to close...\"\n</code></pre> Step 2:  Run these commands in terminal: <pre><code>chmod +x createvenv.command\n./createvenv.command\n</code></pre> Step 3:  Run these commands in terminal: <pre><code>mkdocs build\nmkdocs serve\n</code></pre> <p>Your site will be available at <code>http://127.0.0.1:8000/</code>. Any changes you make to the markdown files will be immediately visible in the browser.</p>"},{"location":"MKdocs/Parallel_Local_Testing.html#troubleshooting-sidebarnavigation-not-updating","title":"Troubleshooting Sidebar/Navigation Not Updating","text":"<p>Sometimes, changes made to <code>mkdocs.yml</code> (such as updating the sidebar navigation) may not appear in your local site. This is often caused by an old <code>mkdocs serve</code> process running in the background, browser cache issues, or stale build files in the <code>site</code> folder.</p> <p>How to check and resolve:</p> <ol> <li>Check for old mkdocs serve processes: <ul> <li>In VS Code, check the Terminal panel for any terminals that are already running <code>mkdocs serve</code> or Python processes. Stop or close these terminals before starting a new one.  </li> <li>On Windows, open Task Manager and look for any <code>python.exe</code> or <code>mkdocs</code> processes.  </li> <li>Alternatively, use PowerShell: <pre><code>Get-Process | Where-Object { $_.ProcessName -match \"python|mkdocs\" }\n</code></pre></li> </ul> </li> <li>Stop all old mkdocs/Python processes: <ul> <li>In Task Manager, right-click and end the processes.  </li> <li>In PowerShell: <pre><code>Get-Process | Where-Object { $_.ProcessName -match \"python|mkdocs\" } | Stop-Process\n</code></pre></li> </ul> </li> <li>Delete the <code>site</code> folder: <ul> <li>Manually delete the folder, or use PowerShell: <pre><code>Remove-Item -Recurse -Force site\n</code></pre></li> </ul> </li> <li>Restart mkdocs serve: <ul> <li>Open a new terminal and run <code>mkdocs serve</code> again.</li> </ul> </li> <li>Clear your browser cache: <ul> <li>Old cached pages may show outdated navigation. Refresh or clear cache before reloading the site.</li> </ul> </li> </ol> <p>Summary: Always ensure only one <code>mkdocs serve</code> process is running, your browser cache is cleared, and the <code>site</code> folder is rebuilt to see the latest navigation changes.</p>"},{"location":"MKdocs/Parallel_Local_Testing.html#common-issues","title":"Common Issues","text":"<p>OSError</p> <p>OSError: [Errno 48] Address already in use. </p> <p>This happens when your browser is already using the default port (8000). You can either stop that server or change the port for mkdocs by using the command <code>mkdocs serve -p 8001</code> to run it on port 8001.</p>"},{"location":"Microsoft-Fabric/DataFactory.html","title":"Data Factory in Microsoft Fabric","text":""},{"location":"Microsoft-Fabric/DataFactory.html#background","title":"Background","text":"<p>ADF is a very important commponent of Fabric. It is 100% the same old ADF in the new Fabric Platform.</p>"},{"location":"Microsoft-Fabric/DataFactory.html#pipelines-in-microsoft-fabric","title":"Pipelines in Microsoft Fabric","text":""},{"location":"Microsoft-Fabric/DataFactory.html#understand-pipelines","title":"Understand Pipelines","text":"<ul> <li>A Pipeline is like a workflow for ingesting and transforming data.</li> <li>Using the GUI we can build complex pipelines with very less coding.</li> </ul>"},{"location":"Microsoft-Fabric/DataFactory.html#core-pipeline-concepts","title":"Core Pipeline Concepts","text":""},{"location":"Microsoft-Fabric/DataFactory.html#activities-executable-tasks-in-a-sequence-two-types","title":"Activities: Executable tasks in a sequence. Two types:","text":"<ul> <li>Data Transformation: Transfers and transforms data (e.g., Copy Data, Data Flow, Notebook, Stored Procedure).</li> <li>Control Flow: Implements loops, conditional branching, and manages variables.</li> </ul>"},{"location":"Microsoft-Fabric/DataFactory.html#parameters","title":"Parameters","text":"<p>Enable specific values for each run, increasing reusability.</p>"},{"location":"Microsoft-Fabric/DataFactory.html#pipeline-runs","title":"Pipeline Runs","text":"<p>Executed on-demand or scheduled. Unique run ID for tracking and reviewing each execution.</p>"},{"location":"Microsoft-Fabric/DataFactory.html#canvas-for-desinign-piplines","title":"Canvas for desinign piplines","text":"<p>Fabric offers a Canvas where you can build complex pipeliens without much coding:</p> <p></p>"},{"location":"Microsoft-Fabric/DataFactory.html#the-copy-data-activity","title":"The Copy Data Activity","text":"<p>The Copy Data is the most important activity in data pipelines. Some pipelines only contain one Copy Data activity, thats all!</p> <p>When to use?</p> <p>Use the Copy Data activity to move data without transformations or to import raw data. For transformations or merging data, use a Data Flow (Gen2) activity with Power Query to define and include multiple transformation steps in a pipeline.</p>"},{"location":"Microsoft-Fabric/DataFactory.html#the-copy-data-tool","title":"The Copy Data Tool","text":""},{"location":"Microsoft-Fabric/DataFactory.html#pipeline-templates","title":"Pipeline Templates","text":"<p>To create a pipeline based on a template on the start page choose Templates</p> <p></p> <p>You will see templates like this:</p> <p></p>"},{"location":"Microsoft-Fabric/DataFactory.html#run-and-monitor-pipelines","title":"Run and monitor pipelines","text":"<p>You can run a pipeline, schedule it and view the run history from the GUI</p> <p></p>"},{"location":"Microsoft-Fabric/DataFactory.html#dataflows","title":"Dataflows","text":"<p>A way to import and transform data with Power Query Online.</p>"},{"location":"Microsoft-Fabric/DataFactory.html#when-you-choose-dataflows","title":"When you choose DataFlows","text":"<p>You need to connect to and transform data to be loaded into a Fabric lakehouse. You aren't comfortable using Spark notebooks, so decide to use Dataflows Gen2. How would you complete this task? </p> <p>Answer: Create a Dataflow Gen2 to transform data &gt; add your lakehouse as the data destination.</p> <p>You can either use Dataflows by iteself or add dataflows in pipelines.</p>"},{"location":"Microsoft-Fabric/DataFactory.html#pipeline-copy-vs-dataflows-vs-spark","title":"Pipeline Copy Vs DataFlows Vs Spark","text":"Property Pipeline Copy Activity Dataflow Gen 2 Spark Use Case Data lake and data warehouse migration, data ingestion, lightweight transformation Data ingestion, data transformation, data wrangling, data profiling Data ingestion, data transformation, data processing, data profiling Code Written No code, low code No code, low code Code Data Volume Low to high Low to high Low to high Development Interface Wizard, canvas Power Query Notebook, Spark job definition Sources 30+ connectors 150+ connectors Hundreds of Spark libraries Destinations 18+ connectors (Lakehouse, Azure SQL database, Azure Data explorer, Azure Synapse analytics) Hundreds of Spark libraries Transformation Complexity Low: lightweight (type conversion, column mapping, merge/split files, flatten hierarchy) Low to high: 300+ transformation functions Low to high: support for native Spark and open-source libraries"},{"location":"Microsoft-Fabric/DataScience.html","title":"Data Science","text":""},{"location":"Microsoft-Fabric/DataScience.html#common-machine-learning-models","title":"Common machine learning models","text":"<ul> <li>Classification: Will the customer stay or leave?</li> <li>Regression: What will the product cost?</li> <li>Clustering: Group similar customers together.</li> <li>Forecasting: What will sales be next month?</li> </ul>"},{"location":"Microsoft-Fabric/DataScience.html#lets-get-started","title":"Let's get started","text":"<p>Go to your workspace and click on Data Science on the left corner icon</p> <p></p> <p>Now from the landing page click on Notebook</p> <p></p> <p>Now your notebook will open, You can add your lakehouse from the left side pane:</p> <p></p> <p>In the notebook, paste this read-made microsoft code:</p> <pre><code># Azure storage access info for open dataset diabetes\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"mlsamples\"\nblob_relative_path = \"diabetes\"\nblob_sas_token = r\"\" # Blank since container is Anonymous access\n\n# Set Spark config to access  blob storage\nwasbs_path = f\"wasbs://%s@%s.blob.core.windows.net/%s\" % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\"fs.azure.sas.%s.%s.blob.core.windows.net\" % (blob_container_name, blob_account_name), blob_sas_token)\nprint(\"Remote blob path: \" + wasbs_path)\n\n# Spark read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\n</code></pre> <p>Fabric automatically creates the spark session and the df is created. Now lets see the df</p> <p>Enter <code>python display(df)</code> in the next cell to see the output</p> <p>With display(df) the output will show two options Table and Chart. Select Chart</p> <p></p> <p>Then you can customize the chart as required</p> <p></p>"},{"location":"Microsoft-Fabric/DataScience.html#data-wrangler-tool","title":"Data Wrangler Tool","text":"<p>The Data Wrangler tool is available at the top of your notebook. First, create a dataframe, then use Data Wrangler to clean it and generate PySpark or Pandas code. It's a useful tool that saves you from writing a lot of code.</p> <p>To open Data Wrangler, click on its icon  and select the dataframe you created. Make sure your session is active.</p> <p>For example, to create a new column, choose Create Column from Formula and provide the details. This formula is only for pandas DF. The corresponding code will be generated automatically!</p> <p></p> <p>Finally after cleaning Fabric will create a function and add it to the main notebook:</p> <p></p>"},{"location":"Microsoft-Fabric/DataScience.html#further-steps","title":"Further steps","text":""},{"location":"Microsoft-Fabric/DataScience.html#train-machine-learning-models","title":"Train Machine Learning Models","text":""},{"location":"Microsoft-Fabric/DataScience.html#train-a-regression-model","title":"Train a Regression Model","text":"<ol> <li>Split Data:      <pre><code>from sklearn.model_selection import train_test_split\n\nX, y = df_clean[['AGE','SEX','BMI','BP','S1','S2','S3','S4','S5','S6']].values, df_clean['Y'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n</code></pre></li> <li>Create a new experiment <code>diabetes-regression</code>:      <pre><code>import mlflow\nexperiment_name = \"diabetes-regression\"\nmlflow.set_experiment(experiment_name)\n</code></pre></li> <li>Train Model:      <pre><code>from sklearn.linear_model import LinearRegression\n\nwith mlflow.start_run():\n   mlflow.autolog()\n   model = LinearRegression()\n   model.fit(X_train, y_train)\n</code></pre></li> </ol>"},{"location":"Microsoft-Fabric/DataScience.html#train-a-classification-model","title":"Train a Classification Model","text":"<ol> <li>Split Data:      <pre><code>from sklearn.model_selection import train_test_split\n\nX, y = df_clean[['AGE','SEX','BMI','BP','S1','S2','S3','S4','S5','S6']].values, df_clean['Risk'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n</code></pre></li> <li>Create a new experiment <code>diabetes-classification</code>:     <pre><code>import mlflow\nexperiment_name = \"diabetes-classification\"\nmlflow.set_experiment(experiment_name)\n</code></pre></li> <li>Train Model:      <pre><code>from sklearn.linear_model import LogisticRegression\n\nwith mlflow.start_run():\n    mlflow.sklearn.autolog()\n    model = LogisticRegression(C=1/0.1, solver=\"liblinear\").fit(X_train, y_train)\n</code></pre></li> </ol>"},{"location":"Microsoft-Fabric/DataScience.html#explore-your-experiments","title":"Explore Your Experiments","text":"<ul> <li>Navigate to your workspace.</li> <li>Open the \"diabetes-regression\" experiment.     </li> <li>Review Run metrics.</li> <li>Click <code>Save run as ML Model</code> to save it         </li> </ul>"},{"location":"Microsoft-Fabric/DataWareHouse.html","title":"Data Warehouse in Microsoft Fabric","text":""},{"location":"Microsoft-Fabric/DataWareHouse.html#background","title":"Background","text":"<p>If earth is a database then Sun is a warehouse. Fabric's warehouse is unique - built on the Lakehouse(Delta format). You can use Full T-SQL.</p> <p></p>"},{"location":"Microsoft-Fabric/DataWareHouse.html#two-types-of-warehouse-available-in-microsoft-fabric","title":"Two types of Warehouse available in Microsoft Fabric","text":"<p>In Microsoft Fabric, there are two types of warehouses</p> <ol> <li>SQL Endpoint:</li> <li>The SQL Endpoint is auto-generated when you create a Lakehouse in Fabric.</li> <li>It serves as an analytics endpoint, allowing you to query data within the Lakehouse using T-SQL (Transact-SQL) language and the TDS (Tabular Data Stream) protocol.</li> <li>Each Lakehouse has one SQL analytics endpoint, and a workspace can have multiple Lakehouses.</li> <li>The SQL analytics endpoint exposes Delta tables from the Lakehouse as SQL tables, making them accessible via T-SQL queries.</li> <li>It also automatically creates a default Power BI semantic model based on the Lakehouse objects' naming conventions.</li> <li>No user action is required to create a SQL analytics endpoint; it is generated automatically during Lakehouse creation.</li> <li>Behind the scenes, the SQL analytics endpoint leverages the same engine as the Warehouse, ensuring high performance and low latency for SQL queries.</li> <li> <p>Automatic metadata discovery keeps SQL metadata up to date without user intervention.</p> </li> <li> <p>Synapse Data Warehouse:</p> </li> <li>The Synapse Data Warehouse is a SQL engine designed specifically for querying and transforming data within the Data Lake (OneLake).</li> <li>It provides full transactional support, including DDL (Data Definition Language) and DML (Data Manipulation Language) queries.</li> <li>With the Synapse Data Warehouse, you can perform cross-database queries and seamlessly transition from read-only operations to building business logic on your OneLake data.</li> <li>It complements the Lakehouse by enabling more complex analytics scenarios.</li> </ol>"},{"location":"Microsoft-Fabric/DataWareHouse.html#fact-and-dimension-table-concepts","title":"Fact and Dimension Table Concepts","text":"<p>Fact tables have the numbers you want to look at, like a receipt. They have a lot of rows and are the main source of data for analysis. They're like the \"what\" you are measuring.</p> <p></p> <p>Dimension tables have details about those numbers, like a restaurant menu. They have fewer rows and give context to the data in the fact tables.</p> <p></p> <p>Examples:</p> Column Fact Table Dimension Table Sales Amount \u2714 Order Quantity \u2714 Product ID \u2714 Customer ID \u2714 Transaction Date \u2714 Product Name \u2714 Customer Name \u2714 Supplier ID \u2714 Supplier Name \u2714 Discount Rate \u2714 Revenue \u2714 Store Location \u2714 Category \u2714 Time (Hour, Day, Month, Year) \u2714 Payment Method \u2714"},{"location":"Microsoft-Fabric/DataWareHouse.html#surrogate-keys-and-alternte-keys","title":"Surrogate keys and alternte keys","text":"<p>Surrogate key: A unique key for each row. Like a cop's badge number. Its unique in the police department.</p> <p>Alternate key: Its like a key that identifies the person in the whole ecosystem. Like a passport number of the cop - unique in the nation.</p> <p></p>"},{"location":"Microsoft-Fabric/DataWareHouse.html#how-data-is-ingested-into-a-warehouse-in-fabric","title":"How data is ingested into a warehouse in Fabric?","text":"<p>Data is ingested using: Pipelines, Dataflows, cross-database querying, and the COPY INTO command.</p>"},{"location":"Microsoft-Fabric/DataWareHouse.html#copy-into-syntax","title":"COPY into syntax","text":"<pre><code>COPY INTO dbo.apple \nFROM 'https://abc/xxx.csv' WITH ( \n            FILE_TYPE = 'CSV'\n            ,CREDENTIAL = ( \n                IDENTITY = 'Shared Access Signature'\n                , SECRET = 'xxx'\n                )\n            ,FIRSTROW = 2\n            )\nGO\n</code></pre>"},{"location":"Microsoft-Fabric/DataWareHouse.html#fabric-datawarehouse-interface","title":"Fabric Datawarehouse interface","text":""},{"location":"Microsoft-Fabric/DataWareHouse.html#visual-query-in-fabric","title":"Visual Query in Fabric","text":"<p>Here I will show you how easy it is to create a left-outer join of Two tables - DimProduct &amp; FactSalesOrder</p> <p>Just drag both the tables on to the canvas then perform the steps as shown in the image below</p> <p></p> <p>Then select the required column. Here we selected ProductName.</p> <p></p> <p>Then create a Power BI Reports quickly:</p> <p></p>"},{"location":"Microsoft-Fabric/DataWareHouse.html#appendix","title":"Appendix","text":""},{"location":"Microsoft-Fabric/DataWareHouse.html#special-types-of-dimension-tables","title":"Special Types of Dimension Tables","text":"<p>Special types of dimensions provide additional context and enable more comprehensive data analysis. Let's explore this with an example from a popular online retail company, \"ShopEZ.\"</p>"},{"location":"Microsoft-Fabric/DataWareHouse.html#time-dimensions","title":"Time Dimensions","text":"<p>Time dimensions provide information about the time period in which an event occurred. This table enables data analysts to aggregate data over temporal intervals. For example, a time dimension might include columns for the year, quarter, month, and day in which a sales order was placed.</p> <p>Example: Sales Analysis at ShopEZ</p> <p>ShopEZ wants to analyze its sales performance to optimize inventory and marketing strategies. The time dimension table allows them to aggregate sales data over different periods.</p> <ul> <li>Year: 2023</li> <li>Quarter: Q1</li> <li>Month: January</li> <li>Day: 15</li> </ul> <p>With these time dimensions, ShopEZ can easily aggregate sales data to see trends like:</p> <ul> <li>Increased sales during holiday seasons</li> <li>Monthly sales growth</li> <li>Quarterly performance comparison</li> </ul>"},{"location":"Microsoft-Fabric/DataWareHouse.html#slowly-changing-dimensions-scd","title":"Slowly Changing Dimensions (SCD)","text":"<p>Slowly changing dimensions track changes to dimension attributes over time, like changes to a customer's address or a product's price. They are crucial in a data warehouse because they allow users to analyze and understand changes to data over time.</p> <p>Example: Customer Loyalty Program at Wallmart</p> <p>ShopEZ runs a loyalty program where customers' membership tiers can change based on their purchase history. Tracking these changes accurately over time is essential for targeted marketing and personalized offers.</p> <p>Scenario: Change in Customer's Membership Tier</p> <ul> <li>Original Record (2022): </li> <li>Customer ID: 456</li> <li>Name: Sarah Lee</li> <li>Membership Tier: Silver</li> <li> <p>Join Date: 2021-05-10</p> </li> <li> <p>Updated Record (2023):</p> </li> <li>Customer ID: 456</li> <li>Name: Sarah Lee</li> <li>Membership Tier: Gold</li> <li>Join Date: 2021-05-10</li> </ul> <p>In a slowly changing dimension scenario, ShopEZ's data warehouse can handle this change using different SCD types:</p> <ul> <li>Type 1 (Overwrite): The old membership tier is overwritten with the new tier. This approach is straightforward but loses historical data.</li> <li>Customer ID: 456</li> <li>Name: Sarah Lee</li> <li>Membership Tier: Gold</li> <li> <p>Join Date: 2021-05-10</p> </li> <li> <p>Type 2 (Historical Tracking): A new record is created for Sarah to preserve the history of changes. This method adds a new row for each change and typically includes an effective date range.</p> </li> <li>Record 1:<ul> <li>Customer ID: 456</li> <li>Name: Sarah Lee</li> <li>Membership Tier: Silver</li> <li>Join Date: 2021-05-10</li> <li>End Date: 2023-01-14</li> </ul> </li> <li> <p>Record 2:</p> <ul> <li>Customer ID: 456</li> <li>Name: Sarah Lee</li> <li>Membership Tier: Gold</li> <li>Join Date: 2021-05-10</li> <li>Start Date: 2023-01-15</li> </ul> </li> <li> <p>Type 3 (Limited History): The old value is stored in additional columns, allowing some history tracking but limited to a predefined number of changes.</p> </li> <li>Customer ID: 456</li> <li>Name: Sarah Lee</li> <li>Current Membership Tier: Gold</li> <li>Previous Membership Tier: Silver</li> <li>Join Date: 2021-05-10</li> </ul>"},{"location":"Microsoft-Fabric/DataWareHouse.html#real-life-impact","title":"Real-Life Impact","text":"<p>Using time dimensions, ShopEZ can identify that sales peak during certain times, such as Black Friday or Christmas. This insight helps them plan inventory, staffing, and marketing campaigns more effectively.</p> <p>With slowly changing dimensions, ShopEZ can track changes in customer behavior and preferences over time. For example, they can see that Sarah Lee upgraded her membership tier from Silver to Gold, indicating increased engagement and spending.</p>"},{"location":"Microsoft-Fabric/DirectLake.html","title":"DirectLake Mode in Fabric","text":"<p>DirectLake is the third and newest data handling technique in Microsoft Fabric for Power BI. Before understanding DirectLake, let's first look at the other two modes:</p> <p>Let's say you have 10 GB of data stored in a SQL Server and you want to create a dashboard. Before DirectLake, you had two options to handle the data:</p>"},{"location":"Microsoft-Fabric/DirectLake.html#import-mode","title":"Import Mode <p>In Import mode, Power BI reads the data from your SQL Server, compresses it (e.g., from 10 GB to 1 GB), and stores it as columns in <code>*.pbix</code> files (Power BI Desktop) or <code>*.idf</code> files (Power BI service).</p> <p>When you create a report and apply filters, sums, etc., Power BI will smartly load only the required data into RAM, perform the calculations, and generate the visuals. However, everything happens on the Power BI side, cutting you off from your original SQL Server. If your SQL Server has new data, you have two options: reload all the data (Full refresh) or load only the new data (Incremental refresh, available in Power BI Premium).</p>","text":""},{"location":"Microsoft-Fabric/DirectLake.html#directquery-mode","title":"DirectQuery Mode <p>If you always want the freshest data on your dashboard, you can use DirectQuery mode. In DirectQuery mode, Power BI doesn\u2019t import the data but directly queries the SQL Server. Every Power BI report generates a DAX query, regardless of the mode. In DirectQuery mode, DAX queries are translated into SQL, sent to the SQL Server, executed there, and the results are returned.</p> <p>This ensures you always see the freshest data. However, network bandwidth, large data volumes, and frequent interactions might result in slowness or out-of-memory issues.</p> <p>To summarize: - Import Mode: Compresses and stores data locally, making reports fast and interactive, but requires periodic refreshes to get new data. - DirectQuery Mode: Provides always-fresh data by querying the live SQL Server directly, but may be slower and more resource-intensive.</p>","text":""},{"location":"Microsoft-Fabric/DirectLake.html#directlake-mode","title":"DirectLake Mode <p>DirectLake mode is similar to Import mode, but it uses .parquet files directly instead of .idf files. The key requirement is that the data must be in OneLake as .parquet files. This avoids the need to save data from SQL Server into new .idf files, preventing data duplication.</p> <p>If you have 10GB of data for reporting, Import mode loads the entire dataset into memory (smartly), even if you don't need all the columns. In contrast, DirectLake mode only loads the data related to the columns used in your report, making it more efficient.</p> <p>If the data size exceeds a certain limit, DirectLake mode automatically switches to DirectQuery mode.</p> <p>Regarding refresh: Any changes in the Delta Lake files are automatically detected, and the report datasets are refreshed intelligently. Only the metadata is refreshed and synced, making DirectLake refreshes very quick, often taking less than 10 seconds. In Import mode, the refresh loads the data into memory, which takes longer.</p>","text":""},{"location":"Microsoft-Fabric/DirectLake.html#pros-and-cons-of-directlake-personal-views","title":"Pros and Cons of DirectLake - Personal Views","text":""},{"location":"Microsoft-Fabric/DirectLake.html#directlake-mode-does-not-replace-any-mode","title":"DirectLake Mode DOES NOT replace any mode <p>Fabric has Import Mode and DirectQuery Mode. DirectLake is just the third and newest option. It provides near-real-time reports like Import Mode but not real real-time like DirectQuery.</p>","text":""},{"location":"Microsoft-Fabric/DirectLake.html#it-is-not-the-best-of-both-worlds","title":"It is not the best of both worlds <p>This mode is unique, but it\u2019s not faster than Import mode and doesn\u2019t replace DirectQuery's special features.</p>","text":""},{"location":"Microsoft-Fabric/DirectLake.html#directlake-vs-import-mode","title":"DirectLake Vs Import Mode <p>DirectLake can be as good as Import mode but not better. It can sometimes be slower compared to Import mode.</p> <p>Import mode and DirectLake take the same amount of memory. DirectLake uses the same 'Import Required Columns in Memory' approach like Import mode. So it\u2019s not special.</p> <p>The only difference is in Import Mode, when the memory is less, it gives an out-of-memory exception, and in DirectLake, it switches to DirectQuery. This happens if your data is around 300 GB.</p>","text":""},{"location":"Microsoft-Fabric/DirectLake.html#data-modeling-has-limitations-in-directlake","title":"Data Modeling has limitations in DirectLake <p>You cannot use calculated columns, calculated tables, and MDX user hierarchies in DirectLake. This impacts Excel\u2019s user experience when consuming semantic models published on Power BI. Also, DirectLake models created in Fabric have case-sensitive collation.</p>","text":""},{"location":"Microsoft-Fabric/DirectLake.html#directlake-requires-real-physical-tables-no-room-for-views","title":"DirectLake requires real physical tables - no room for views <p>When you use views in DirectLake, the mode changes to DirectQuery. To use 100% DirectLake, all your tables must be real. This might require duplicating your data.</p> <p>Note: We have option of creating shortcuts. Which comes as a relief.  But, I haven't tested the performance of shortcut data.</p> <p>Hence, if you already use Import Mode or DirectQuery and everything works fine, there are no substantial benefits to moving to DirectLake.</p>","text":""},{"location":"Microsoft-Fabric/DirectLake.html#practical-scenario-dp-600-question","title":"Practical Scenario (DP-600 Question) <p>You have a Fabric tenant with a workspace named Workspace1, assigned to an F64 capacity, containing a lakehouse with one billion historical sales records, receiving up to 10,000 new or updated sales records every 15 minutes. You plan to build a custom Microsoft Power BI semantic model and reports from this data, requiring the best report performance and near-real-time data reporting.</p> <p>Which Power BI semantic model storage mode should you use?</p> <p>Answer: DirectLake</p>","text":""},{"location":"Microsoft-Fabric/DirectLake.html#how-microsoft-advertises-it","title":"How Microsoft Advertises It <p>DirectLake storage mode offers near-real-time (NRT) access to data with performance close to Import storage mode, surpassing DirectQuery in terms of speed for large datasets. While DirectQuery provides NRT access, it can slow down with large datasets. Import Mode, though fast, requires data to be loaded into Power BI's memory, lacking NRT capabilities. Currently, DirectLake tables cannot be mixed with other table types (Import, DirectQuery, or Dual) within the same model, and composite models are not supported yet.</p> <p></p> <p>For further details, you can refer to the Microsoft Fabric Direct Lake overview.</p>","text":""},{"location":"Microsoft-Fabric/E2EProject.html","title":"E2E Project","text":""},{"location":"Microsoft-Fabric/E2EProject.html#concepts-we-will-learn","title":"Concepts we will learn","text":"<p>Here you will understand two concepts of ETL:</p> <p>Medallion architecture for data management: Star Schema:</p>"},{"location":"Microsoft-Fabric/E2EProject.html#project-summary","title":"Project summary","text":"<p>Here we will first import data(few .csv files) into bronze layer(Simple folders). Then clean and transform it and load it into Silver Delta Table. Then create star schema and load the data from silver to Gold Delta Tables.</p> <p>You have successfully taken data from your bronze layer, transformed it, and loaded it into a silver Delta table. Now you\u2019ll use a new notebook to transform the data further, model it into a star schema, and load it into gold Delta tables.</p> <p>In data processing, you often hear about Bronze, Silver, and Gold layers, which are part of the Medallion Architecture.</p> <p></p> <p>Here I will show the use of Medallion architecture in a simple straightforward ETL project:</p> <ol> <li>Import data files into the Bronze layer (Lakehouse folder).</li> <li>Define a schema and create a dataframe from the .csv files.</li> <li>Clean the data and add new columns as needed.</li> <li>Manually create a Silver Delta Table with the same schema as the dataframe.</li> <li>Perform an upsert operation on the Delta table, which means updating existing records based on certain conditions and adding new records if no match is found.</li> <li>Explore data in the silver layer using the SQL endpoint</li> </ol>"},{"location":"Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse.html","title":"ETL with OPG","text":""},{"location":"Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse.html#background","title":"Background","text":"<p>Here, I'll show you two things:</p> <ol> <li>How to connect Fabric or Azure Data Factory (ADF) to your local file system using an on-premises gateway.</li> <li>How to use a simple, no-code method to automatically copy JSON files into a Delta Lake table.</li> </ol>"},{"location":"Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse.html#lets-start","title":"Let's start","text":""},{"location":"Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse.html#prepare-the-local-files-and-on-premises-gateway","title":"Prepare the Local Files and On-Premises Gateway","text":"<ol> <li>Download the sample JSON files from here and place them in a local folder on your computer.</li> <li>Install the on-premises gateway. It\u2019s straightforward and easy to install. You can find detailed instructions here.</li> </ol> <p>Note: Ensure that an admin account has all permissions in the security tab of the local folder. While not always practical, admin access simplifies the setup and reduces complications.</p> <p></p>"},{"location":"Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse.html#create-a-delta-lake-table-to-store-the-json-data","title":"Create a Delta Lake Table to Store the JSON Data","text":"<p>Create a notebook and run the following SparkSQL code:</p> <pre><code>%%sql\n\nCreate table jsonDelta (\n  id string,\n  name string,\n  email string,\n  age int,\n  country string\n) using delta\n</code></pre> <p>Remember: using delta at the end.</p> <p></p>"},{"location":"Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse.html#set-up-the-copy-data-process","title":"Set Up the Copy Data Process","text":"<ul> <li>Source Setup: Follow the diagram to set up the source. Ensure the admin account has the right permissions for the source folder.</li> </ul> <ul> <li>Destination Setup: Follow the diagram for the destination setup.</li> </ul> <ul> <li>Mapping Setup: Follow the diagram to set up the mapping.</li> </ul>"},{"location":"Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse.html#run-the-pipeline","title":"Run the Pipeline","text":"<p>Click \"Run.\" The pipeline will process all the JSON files and add the data to the Delta Lake table. That\u2019s it!</p> <p></p>"},{"location":"Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse.html#check-the-delta-lake-table-data","title":"Check the Delta Lake Table Data","text":"<p>Go to the lakehouse, expand the table, and you will see that all the JSON data has been loaded into the Delta Lake table.</p> <p></p>"},{"location":"Microsoft-Fabric/ETL-OPG-Copydata-JSON-Lakehouse.html#conclusion","title":"Conclusion","text":"<p>I hope I was able to show how nearly no-code, simple, and straightforward it is to load data from your local system into a Delta Lake in Fabric.</p>"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse.html","title":"ETL with PySpark","text":""},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse.html#background","title":"Background","text":"<p>Here, I'll show you how to use a PySpark Notebook to build a complete ETL solution. We'll import parquet files from external sources into a Fabric Lakehouse folder, clean the data, and create Delta tables\u2014all using the PySpark Notebook.</p>"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse.html#ways-to-ingest-data-into-lakehouse","title":"Ways to Ingest Data into Lakehouse","text":"<p>Apart from using Pyspark in Notebooks there are other methods to Copy data into Lakehouse. Based on the the situation you will have to choose a method. </p> <ol> <li>ADF Data Pipelines: You can both ingest and transoform using ADF pipeline. Use the Copy data activity for ingestion(no transformation) and a Notebook activity or Dataflow activity for transformation. If there is no transformation, blindly choose Copy data activity.   </li> </ol> <p>{: .highlight } {: .fw-400 }   The Copy data activity: Best performance, fastest, most-direct when copying data from large datasets or migrating data from one system to another. But, this activity can't do any transformation.</p> <ol> <li> <p>Power BI Dataflow: Power BI Dataflows can handle both ingestion and transformation. They support ingestion from thousands of sources and use Power Query for transformation. Note: Fabric uses the same Power BI Dataflow.    </p> </li> <li> <p>Manual Upload: You can always manually upload your files into a folder. Then you can use a Noteook or Dataflow for the transformation and Delta Lake Table creation :-)   </p> </li> <li> <p>Additionally, there's an important T-SQL command called COPY INTO. This command copies data into tables and supports Parquet and CSV formats from Azure Data Lake Storage Gen2/Azure Blob. However, it only copies data into tables and not into Lakehouse folders from external systems.</p> </li> </ol>"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse.html#when-to-choose-which-method-microsofts-recommendation","title":"When to choose which method? Microsoft's recommendation.","text":"<p>Scenario 1: You have a Fabric tenant that contains a lakehouse named Lakehouse1. You need to ingest data into Lakehouse1 from a large Azure SQL Database table that contains more than 500 million records. The data must be ingested without applying any additional transformations. The solution must minimize costs and administrative effort.</p> <p>What should you use to ingest the data?</p> <ul> <li>a pipeline with the Copy data activity</li> <li>a SQL stored procedure</li> <li>Dataflow Gen2</li> <li>notebooks</li> </ul> <p>Answer: When ingesting a large data source without applying transformations, the recommended method is to use the Copy data activity in pipelines. Notebooks are recommended for complex data transformations, whereas Dataflow Gen2 is suitable for smaller data and/or specific connectors.</p> <p>Scenario 2:  You have a Fabric tenant that contains a lakehouse.On a local computer, you have a CSV file that contains a static list of company office locations. You need to recommend a method to perform a one-time copy to ingest the CSV file into the lakehouse. The solution must minimize administrative effort. Which method should you recommend?   - a Dataflow Gen2 query   - a local file upload by using Lakehouse explorer   - a pipeline with the Copy data activity   - a Spark notebook</p> <p>Answer: For a one-time copy of small local files into a lakehouse, using Lakehouse explorer and a local file upload is recommended.</p> <p>Scenario 3:  You need to ensure that the pipeline activity supports parameterization. Which two activities support parameterization in the data pipeline UI?   - Dataflow Gen2   - KQL activity   - notebooks   - SQL stored procedures   - user-defined functions</p> <p>Answer: Only notebooks and SQL stored procedures provide a possibility to define parameters in the data pipeline UI. Dataflow Gen2 and KQL activity only require connection details, but no parameters can be supplied. User-defined functions cannot be added as an activity to a pipeline.</p> <p>Scenario 4: You have an external Snowflake database that contains a table with 200 million rows. You need to use a data pipeline to migrate the database to Lakehouse1. What is the most performant (fastest) method for ingesting data this large (200 million rows) by using a data pipeline?</p> <ul> <li>Data Pipeline (Copy data)</li> <li>Data Pipeline (Dataflow Gen2)</li> <li>Data Pipeline (Lookup)</li> <li>Data Pipeline Spark (Notebook)</li> </ul> <p>Answer: Copy data is the fastest and most direct method for migrating data from one system to another, with no transformations applied.</p>"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse.html#the-entire-project-in-just-8-pyspark-lines","title":"The entire project in just 8 pyspark lines","text":""},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse.html#the-code","title":"THE code","text":"<p>Here is the core code for the project. I've intentionally kept it short to highlight the key concept.</p> <pre><code># Enable V-Order for Parquet files to improve data skipping and query performance.\n# V-Order helps in reducing the amount of data read during queries by organizing the data for better compression and faster access.\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")  \n\n# Enable automatic Delta optimized write to enhance write performance.\n# This setting allows Delta Lake to optimize the way data is written, improving speed and efficiency.\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n\n# wasbs path = cont_name@act_name.blob.core.windows.net/folder_path\n# Read parquet data from Azure Blob Storage path\ndf = spark.read.parquet(f'wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow')\n\n# Right click ...(RawData) folder -&gt; Copy ABFS Path.\n# ABFS_Path/yellow_taxi(New sub folder name)\n# Write the first 1000 rows as a Parquet file\ndf.limit(1000).write.mode(\"overwrite\").parquet(f\"abfss://WorkSpaceA@onelake.dfs.fabric.microsoft.com/LakeHouseBhutu.Lakehouse/Files/RawData/yellow_taxi\")\n\n# Now read back from the folder where we copied the parquets files\nraw_df = spark.read.parquet(fabric_put_path)   \n\n# Filter rows where column 'trip_distance' is greater than 0 and column 'fare_amount' is greater than 0\ncleaned_df = raw_df.filter(raw_df.tripDistance &gt; 0)\n# Now write the cleaned df into Delta Table in Lakehouse\ncleaned_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"delta_yellow_taxi\")\n\n# Display results\ndisplay(cleaned_df.limit(10))\n</code></pre>"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse.html#the-explanation","title":"The explanation","text":"<p>I have included screenshots of the working code in the notebook and added some comments to help you understand it.</p> <p></p>"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse.html#appendix","title":"Appendix","text":"<p>The code above is quite short. In the real world, it wouldn't be this simple. To better understand the concepts, review the following sections that provide more detailed explanations.</p>"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse.html#connect-to-azure-blob-storage-with-spark-from-fabric","title":"Connect to Azure blob storage with Spark from Fabric","text":"<pre><code># Azure Blob Storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"nyctlc\"\nblob_relative_path = \"yellow\"\nblob_sas_token = \"sv=2022-11-02&amp;ss=bfqt&amp;srt=c&amp;sp=rwdlacupiytfx&amp;se=2023-09-08T23:50:02Z&amp;st=2023-09-08T15:50:02Z&amp;spr=https&amp;sig=abcdefg123456\" \n\n# Construct the path for connection\nwasbs_path = f'wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net/{blob_relative_path}?{blob_sas_token}'\n\n# Read parquet data from Azure Blob Storage path\nblob_df = spark.read.parquet(wasbs_path)\n\n# Show the Azure Blob DataFrame\nblob_df.show()\n</code></pre>"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse.html#connect-to-azure-sql-database-with-a-service-principal","title":"Connect to Azure SQL Database with a Service Principal","text":"<pre><code># Placeholders for Azure SQL Database connection info\nserver_name = \"your_server_name.database.windows.net\"\nport_number = 1433  # Default port number for SQL Server\ndatabase_name = \"your_database_name\"\ntable_name = \"YourTableName\" # Database table\nclient_id = \"YOUR_CLIENT_ID\"  # Service principal client ID\nclient_secret = \"YOUR_CLIENT_SECRET\"  # Service principal client secret\ntenant_id = \"YOUR_TENANT_ID\"  # Azure Active Directory tenant ID\n\n\n# Build the Azure SQL Database JDBC URL with Service Principal (Active Directory Integrated)\njdbc_url = f\"jdbc:sqlserver://{server_name}:{port_number};database={database_name};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;Authentication=ActiveDirectoryIntegrated\"\n\n# Properties for the JDBC connection\nproperties = {\n    \"user\": client_id, \n    \"password\": client_secret,  \n    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n    \"tenantId\": tenant_id  \n}\n\n# Read entire table from Azure SQL Database using AAD Integrated authentication\nsql_df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=properties)\n\n# Show the Azure SQL DataFrame\nsql_df.show()\n</code></pre>"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse.html#write-data-into-a-lakehouse-file","title":"Write data into a Lakehouse File","text":"<pre><code># Write DataFrame to Parquet file format\nparquet_output_path = \"dbfs:/FileStore/your_folder/your_file_name\"\ndf.write.mode(\"overwrite\").parquet(parquet_output_path)\nprint(f\"DataFrame has been written to Parquet file: {parquet_output_path}\")\n\n# Write DataFrame to Delta table\ndelta_table_name = \"your_delta_table_name\"\ndf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(delta_table_name)\nprint(f\"DataFrame has been written to Delta table: {delta_table_name}\")\n</code></pre>"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse.html#write-data-into-a-lakehouse-delta-table","title":"Write data into a Lakehouse Delta Table","text":"<pre><code># Use format and save to load as a Delta table\ntable_name = \"nyctaxi_raw\"\nfiltered_df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n\n# Confirm load as Delta table\nprint(f\"Spark DataFrame saved to Delta table: {table_name}\")\n</code></pre>"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse.html#optimizefewer-files-v-order-optimizewrite","title":"Optimize[Fewer files] - V-Order &amp; optimizeWrite","text":"<p>V-Order and OptimizeWrite sorts data and creates fewer, larger, more efficient Parquet files. Hence, the deta table is optimized. V-Order is enabled by default in Microsoft Fabric and in Apache Spark.</p> <p>Here is how you can configure them in Pyspark:</p> <pre><code># Enable V-Order \nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n\n# Enable automatic Delta optimized write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n</code></pre>"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse.html#knowledge-check","title":"Knowledge check","text":"<ul> <li>What are the four data ingestion options available in Microsoft Fabric for loading data into a data warehouse?</li> </ul> <p>Answer: COPY (Transact-SQL) statement, data pipelines, dataflows, and cross-warehouse are the four data ingestion options available in Microsoft Fabric for loading data into a data warehouse.</p> <ul> <li>What are the supported data sources and file formats for the COPY (Transact-SQL) statement in Warehouse? </li> </ul> <p>Answer: The COPY (Transact-SQL) statement currently supports the PARQUET and CSV file formats, and Azure Data Lake Storage (ADLS) Gen2 and Azure Blob Storage as data sources.</p> <ul> <li>What is the recommended minimum file size when working with external data on files in Microsoft Fabric?</li> </ul> <p>Answer: When working with external data on files, we recommend that files are at least 4 MB in size.</p>"},{"location":"Microsoft-Fabric/ETL-Pyspark-Notebook-Lakehouse.html#summary","title":"Summary","text":"<ul> <li>Pyspark notebook alone can completely create end-to-end robust ETL workflows</li> <li>ADF Pipeline with Copy Data + Notebook or Dataflow can do the same job</li> <li>Copy data activity in ADF pipeline can't do transformation. It is used for data ingestion only.</li> <li> <p>Read, write and saveAsTable are the three important pyspark commands to learn.</p> </li> <li> <p><code>spark.read.parquet(\"path of external parquets\")</code></p> </li> <li><code>df.limit(1000).write.mode(\"overwrite\").parquet(\"path of lakehouse folder\")</code></li> <li><code>cleaned_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"theDeltatableName\")</code></li> </ul>"},{"location":"Microsoft-Fabric/FabricAdministration.html","title":"Administration","text":""},{"location":"Microsoft-Fabric/FabricAdministration.html#open-fabric-admin-portal","title":"Open Fabric Admin Portal","text":""},{"location":"Microsoft-Fabric/FabricAdministration.html#delete-a-fabric-workspace","title":"Delete a Fabric Workspace","text":"<p>In the Workspace settings pane, select Other &gt; Remove this workspace.</p> <p></p>"},{"location":"Microsoft-Fabric/FabricAdministration.html#find-your-fabric-home-region","title":"Find your Fabric home region","text":"<p>To find your Fabric home region, follow these steps:</p> <p>Sign in to Fabric.</p> <p>Open the Help pane and choose About Microsoft Fabric.</p> <p>Look for the value next to Your data is stored in. The location shown is the default region where your data is stored. You may also be using capacities in different regions for your workspaces.</p>"},{"location":"Microsoft-Fabric/FabricAdministration.html#configure-spark","title":"Configure Spark","text":"<p>In Microsoft Fabric, each workspace is assigned a Spark cluster. An administrator can manage settings for the Spark cluster in the Data Engineering/Science section of the workspace settings.</p> <p></p>"},{"location":"Microsoft-Fabric/FabricQ%26A.html","title":"Q&A","text":"<ul> <li>\ud83d\udccc Which of the following is a key benefit of using Microsoft Fabric in data projects?</li> <li>A. It allows data professionals to work on data projects independently, without the need for collaboration.</li> <li>B. It requires duplication of data across different systems and teams to ensure data availability.</li> <li>C. It provides a single, integrated environment for data professionals and the business to collaborate on data projects.</li> <li> <p>Answer:  C. Fabric's OneLake provides a single, integrated environment for data professionals and the business to collaborate on data projects.</p> </li> <li> <p>\ud83d\udccc What is the default storage format for Fabric's OneLake?</p> </li> <li>A. Delta</li> <li>B. JSON</li> <li>C. CSV</li> <li> <p>Answer: A. The default storage format for OneLake is Delta Parquet, an open-source storage layer that brings reliability to data lakes. <li> <p>\ud83d\udccc Which of the following Fabric workloads is used to move and transform data?</p> </li> <li>A. Data Science</li> <li>B. Data Warehousing</li> <li>C. Data Factory</li> <p>Answer: C. The Data Factory workload combines Power Query with the scale of Azure Data Factory to move and transform data. <ul> <li>\ud83d\udccc What is a Microsoft Fabric lakehouse?</li> <li>A. A relational database based on the Microsoft SQL Server database engine.</li> <li>B. A hierarchy of folders and files in Azure Data Lake Store Gen2.</li> <li>C. An analytical store that combines the file storage flexibility of a data lake with the SQL-based query capabilities of a data warehouse.</li> <li> <p>Answer: C. Lakehouses combine data lake and data warehouse features. <li> <p>\ud83d\udccc You want to include data in an external Azure Data Lake Store Gen2 location in your lakehouse, without the requirement to copy the data. What should you do?</p> </li> <li>A. Create a Data pipeline that uses a Copy Data activity to load the external data into a file.</li> <li>B. Create a shortcut.</li> <li>C. Create a Dataflow (Gen2) that extracts the data and loads it into a table.</li> <li> <p>Answer: B. A shortcut enables you to include external data in the lakehouse without copying the data. <li> <p>\ud83d\udccc You want to use Apache Spark to interactively explore data in a file in the lakehouse. What should you do?</p> </li> <li>A. Create a notebook.</li> <li>B. Switch to the SQL analytics endpoint mode.</li> <li>C. Create a Dataflow (Gen2).</li> <li> <p>Answer: A. A notebook enables interactive Spark coding. <li> <p>\ud83d\udcccWhich of the following descriptions best fits Delta Lake?</p> </li> <li> <p>A. A Spark API for exporting data from a relational database into CSV files.</p> </li> <li>B. A relational storage layer for Spark that supports tables based on Parquet files.</li> <li> <p>C. A synchronization solution that replicates data between SQL Server and Spark.</p> </li> <li> <p>Answer: B. Delta Lake provides a relational storage layer in which you can create tables based on Parquet files in a data lake. <li> <p>\ud83d\udccc You have a managed table based on a folder that contains data files in delta format. If you drop the table, what happens?</p> </li> <li> <p>A. The table metadata and data files are deleted.</p> </li> <li>B. The table definition is removed from the metastore, but the data files remain intact.</li> <li> <p>C. The table definition remains in the metastore, but the data files are deleted.</p> </li> <li> <p>Answer: A. The life-cycle of the metadata and data for a managed table are the same. <li> <p>\ud83d\udccc What is a data pipeline?</p> </li> <li>A. A special folder in OneLake storage where data can be exported from a lakehouse</li> <li>B. A sequence of activities to orchestrate a data ingestion or transformation process</li> <li>C. A saved Power Query</li> <li> <p>Answer: B. A pipeline consists of activities to ingest and transform data. <li> <p>\ud83d\udccc You want to use a pipeline to copy data to a folder with a specified name for each run. What should you do?</p> </li> <li>A. Create multiple pipelines - one for each folder name</li> <li>B. Use a Dataflow (Gen2)</li> <li>C. Add a parameter to the pipeline and use it to specify the folder name for each run</li> <li> <p>Answer: C. Using a parameter enables greater flexibility for your pipeline. <li> <p>\ud83d\udccc You have previously run a pipeline containing multiple activities. What's the best way to check how long each individual activity took to complete?</p> </li> <li>A. Rerun the pipeline and observe the output, timing each activity.</li> <li>B. View the run details in the run history.</li> <li>C. View the Refreshed value for your lakehouse's default dataset</li> <li> <p>Answer: B. The run history details show the time taken for each activity - optionally as a Gantt chart. <li> <p>\ud83d\udccc What is a Dataflow Gen2?</p> </li> <li>A. A hybrid database that supports ACID transactions.</li> <li>B. A way to export data to Power BI Desktop.</li> <li>C. A way to import and transform data with Power Query Online.</li> <li> <p>Answer: C. Dataflow Gen2 allows you to get and transform data, then optionally ingest to a lakehouse. <li> <p>\ud83d\udccc Which workload experience lets you create a Dataflow Gen2?</p> </li> <li>A. Real-time analytics.</li> <li>B. Data warehouse.</li> <li>C. Data Factory.</li> <li> <p>Answer: C. Data Factory and Power BI workloads allow Dataflow Gen2 creation. <li> <p>\ud83d\udccc You need to connect to and transform data to be loaded into a Fabric lakehouse. You aren't comfortable using Spark notebooks, so decide to use Dataflows Gen2. How would you complete this task?</p> </li> <li>A. Connect to Data Factory workload &gt; Create a Dataflow Gen2 to transform data &gt; add your lakehouse as the data destination.</li> <li>B. Connect to Real-time Analytics workload &gt; Create Pipeline to copy data &gt; Transform data with an Eventstream.</li> <li>C. Connect to Data Factory workload &gt; Create Pipeline to copy data and load to lakehouse &gt; Transform directly in the lakehouse.</li> <li> <p>Answer: A. Connect to Data Factory workload &gt; Create a Dataflow Gen2 to transform data &gt; add your lakehouse as the data destination. <li> <p>\ud83d\udccc Which type of table should an insurance company use to store supplier attribute details for aggregating claims?</p> </li> <li>A. Fact table.</li> <li>B. Dimension table.</li> <li>C. Staging table.</li> <li> <p>Answer: B. A dimension table stores attributes used to group numeric measures. <li> <p>\ud83d\udccc What is a semantic model in the data warehouse experience?</p> </li> <li>A. A semantic model is a business-oriented data model that provides a consistent and reusable representation of data across the organization.</li> <li>B. A semantic model is a physical data model that describes the structure of the data stored in the data warehouse.</li> <li>C. A semantic model is a machine learning model that is used to make predictions based on data in the data warehouse.</li> <li> <p>Answer: A. A semantic model in the data warehouse experience provides a way to organize and structure data in a way that is meaningful to business users, enabling them to easily access and analyze data. <li> <p>\ud83d\udccc What is the purpose of item permissions in a workspace?</p> </li> <li>A. To grant access to all items within a workspace.</li> <li>B. To grant access to specific columns within a table.</li> <li>C. To grant access to individual warehouses for downstream consumption.</li> <li>Answer: C. By granting access to a single data warehouse using item permissions, you can enable downstream consumption of data. Sure, here's the revised version: <p>\ud83d\udccc You have access to a historical dataset that contains the monthly expenses of the marketing department. You want to generate predictions of the expenses for the coming month. Which type of machine learning model is needed? - A. Classification   - Incorrect. Classification is used when you want to predict a categorical value. - B. Regression   - Incorrect. Regression is used when you want to predict numerical values. - C. Forecasting   - Correct. Forecasting is used when you want to predict future numerical values based on time-series data. <ul> <li>Answer: Correct. Forecasting is used when you want to predict future numerical values based on time-series data.</li> </ul> <p>\ud83d\udccc Which feature in Microsoft Fabric should you use to review the results of MLflow's tracking through a user interface? - A. Notebooks   - Incorrect. You can use MLflow in a notebook to review tracked metrics. However, when working with notebooks you need to write code and don't use a user interface. - B. Experiments   - Correct. Microsoft Fabric's experiments offer a visual user interface to explore the metrics. - C. Models   - Incorrect. Metrics are only shown when a model is saved. <ul> <li>Answer: Correct. Microsoft Fabric's experiments offer a visual user interface to explore the metrics.</li> </ul> <p>\ud83d\udccc Which feature in Microsoft Fabric should you use to accelerate data exploration and cleansing? - A. Dataflows - B. Data Wrangler - C. Lakehouse - Answer: Correct. Use Data Wrangler to visualize and clean your data.</p> <p>\ud83d\udccc Which of the following statements best describes the concept of capacity in Fabric?</p> <ul> <li> <p>Capacity refers to a dedicated space for organizations to create, store, and manage Fabric items.</p> </li> <li> <p>Capacity defines the ability of a resource to perform an activity or to produce output.</p> </li> <li> <p>Capacity is a collection of items that are logically grouped together.</p> </li> <li> <p>Answer: Capacity defines the ability of a resource to perform an activity or to produce output.</p> </li> </ul> <p>\ud83d\udccc Which of the following statements is true about the difference between promotion and certification in Fabric?</p> <ul> <li> <p>Promotion and certification both allow any workspace member to endorse content.</p> </li> <li> <p>Promotion requires a higher level of permissions than certification.</p> </li> <li> <p>Certification must be enabled in the tenant by the admin, while promotion can be done by a workspace member.</p> </li> <li> <p>Answer: Certification must be enabled in the tenant by the admin, and only designated certifiers can perform the endorsement. In contrast, promotion can be done by any workspace member who has been granted the necessary permissions.</p> </li> </ul> <p>\ud83d\udccc Which of the following sets of layers are typically associated with the Medallion Architecture for data management?</p> <ul> <li> <p>Raw, Polished, Refined</p> </li> <li> <p>Bronze, Silver, Gold</p> </li> <li> <p>Initial, Intermediate, Final</p> </li> <li> <p>Answer: Bronze, silver, gold is the correct sequence of layers typically used in the medallion architecture. Data flows from the raw and unrefined state (bronze) to a curated and validated state (silver), and finally to an enriched and well-structured presentation state (gold).</p> </li> </ul> <p>\ud83d\udccc Which tool is best suited for data transformation in Fabric when dealing with large-scale data that will continue to grow?</p> <ul> <li> <p>Dataflows (Gen2)</p> </li> <li> <p>Pipelines</p> </li> <li> <p>Notebooks</p> </li> <li> <p>Answer: Notebooks are a more suitable tool for data transformation with big data in Fabric.</p> </li> </ul> <p>\ud83d\udccc What is the benefit of storing different layers of your lakehouse in separate workspaces?</p> <ul> <li> <p>It can enhance security, manage capacity use, and optimize cost-effectiveness.</p> </li> <li> <p>It makes it easier to share data with colleagues.</p> </li> <li> <p>There's no benefit of storing different layers of your lakehouse in separate workspaces.</p> </li> <li> <p>Answer: Storing different layers of your lakehouse in separate workspaces enhances security and optimizes cost-effectiveness.</p> </li> </ul> <p>\ud83d\udccc What is the benefit of using Fabric notebooks over manual uploads for data ingestion?</p> <ul> <li> <p>Notebooks provide an automated approach to ingestion and transformation.</p> </li> <li> <p>Notebooks can orchestrate the Copy Data activity and transformations.</p> </li> <li> <p>Notebooks offer a user-friendly, low-code experience for large semantic models.</p> </li> <li> <p>Answer: Notebooks provide an automated approach to ingestion and transformation.</p> </li> </ul> <p>\ud83d\udccc What is the purpose of V-Order and Optimize Write in Delta tables?</p> <ul> <li> <p>V-Order and Optimize Write sorts the Delta table when queried with PySpark in a Fabric Notebook.</p> </li> <li> <p>V-Order and Optimize Write enhance Delta tables by sorting data and creating fewer, larger Parquet files.</p> </li> <li> <p>V-Order and Optimize Write create many small csv files.</p> </li> <li> <p>Answer: V-Order and Optimize Write enhance Delta tables by sorting data and creating fewer, larger Parquet files.</p> </li> </ul> <p>\ud83d\udccc Why consider basic data cleansing when loading data into Fabric lakehouse?</p> <ul> <li> <p>To reduce data load size and processing time.</p> </li> <li> <p>To ensure data quality and consistency.</p> </li> <li> <p>To enforce data privacy and security measures.</p> </li> <li> <p>Answer: Basic cleaning is done to ensure data quality and consistency before moving on to transformation and modeling steps.</p> </li> </ul> <p>\ud83d\udccc What is the purpose of creating a Reflex in Data Activator?</p> <ul> <li> <p>To connect to data sources, monitor conditions, and initiate actions.</p> </li> <li> <p>To customize your Fabric experience to Data Activator.</p> </li> <li> <p>To navigate between data mode and design mode.</p> </li> <li> <p>Answer: A Reflex item contains all the necessary details to connect to data sources, monitor conditions, and initiate actions for each business segment or process being monitored.</p> </li> </ul> <p>\ud83d\udccc What is Data Activator's capability in real-time data analysis?</p> <ul> <li> <p>Data Activator can only analyze data in batches.</p> </li> <li> <p>Data Activator can quickly respond to and analyze data in real-time.</p> </li> <li> <p>Data Activator can only analyze data from a single source.</p> </li> <li> <p>Answer: Data Activator is tailored to handle real-time data streams and can distinguish itself through its capability to quickly respond to and analyze data in real-time.</p> </li> </ul> <p>\ud83d\udccc What is one of Data Activator's strengths in terms of interoperability with other Fabric experiences?</p> <ul> <li> <p>Data Activator can ingest data from EventStreams and Power BI reports.</p> </li> <li> <p>Data Activator can't ingest data from other Fabric experiences.</p> </li> <li> <p>Data Activator can only ingest data from Power BI reports.</p> </li> <li> <p>Answer: One of Data Activator's strengths is its integration capabilities with other Fabric experiences, such as ingesting data from EventStreams and Power BI reports.</p> </li> </ul> <p>\ud83d\udccc You are developing a Microsoft Power BI semantic model. Two tables in the data model are not connected in a physical relationship. You need to establish a virtual relationship between the tables. Which DAX function should you use?</p> <ul> <li> <p>CROSSFILTER()</p> </li> <li> <p>PATH()</p> </li> <li> <p>TREATAS()</p> </li> <li> <p>USERELATIONSHIP()</p> </li> <li> <p>Answer: TREATAS() applies the result of a table expression as filters to columns from an unrelated table. USERELATIONSHIP() activates different physical relationships between tables during a query execution. CROSSFILTER() defines the cross filtering direction of a physical relationship. PATH() returns a string of all the members in the column hierarchy.</p> </li> </ul> <p>\ud83d\udccc You have a Fabric workspace that contains a lakehouse named Lakehouse1. A user named User1 plans to use Lakehouse explorer to read Lakehouse1 data. You need to assign a workspace role to User1. The solution must follow the principle of least privilege. Which workspace role should you assign to User1?</p> <ul> <li> <p>Admin</p> </li> <li> <p>Contributor</p> </li> <li> <p>Member</p> </li> <li> <p>Viewer</p> </li> <li> <p>Answer: To read the data from a Fabric lakehouse by using Lakehouse explorer, users must be assigned roles of either Admin, Member, or Contributor. However, respecting the least privileged principle, a user must be assigned the Contributor role. The viewer role does not provide permission to read the lakehouse data through Lakehouse explorer.</p> </li> </ul> <p>\ud83d\udccc You have a Fabric tenant that contains a workspace named Workspace1. Workspace1 contains a lakehouse, a data pipeline, a notebook, and several Microsoft Power BI reports. A user named User1 plans to use SQL to access the lakehouse to analyze data. User1 must have the following access: User1 must have read-only access to the lakehouse. User1 must NOT be able to access the rest of the items in Workspace1. User1 must NOT be able to use Spark to query the underlying files in the lakehouse. You need to configure access for User1. What should you do?</p> <ul> <li> <p>Add User1 to the workspace as a member, share the lakehouse with User1, and select Read all SQL Endpoint data.</p> </li> <li> <p>Add User1 to the workspace as a viewer, share the lakehouse with User1, and select Read all SQL Endpoint data.</p> </li> <li> <p>Share the lakehouse with User1 directly and select Build reports on the default dataset.</p> </li> <li> <p>Share the lakehouse with User1 directly and select Read all SQL Endpoint data.</p> </li> <li> <p>Answer: Since the user only needs access to the lakehouse and not the other items in the workspace, you should share the lakehouse directly and select Read all SQL Endpoint data. The user should not be added as a member of the workspace. All members of the workspace, even viewers, will be able to open all Power BI reports in the workspace. The SQL analytics endpoint itself cannot be shared directly; the Share options only show for the lakehouse.</p> </li> </ul> <p>\ud83d\udccc You use Microsoft Power BI Desktop to create a Power BI semantic model. You need to recommend a solution to collaborate with another Power BI modeler. The solution must ensure that you can both work on different parts of the model simultaneously. The solution must provide the most efficient and productive way to collaborate on the same model. What should you recommend?</p> <ul> <li> <p>Save your work as a PBIX file and email the file to the other modeler.</p> </li> <li> <p>Save your work as a PBIX file and publish the file to a Fabric workspace. Add the other modeler as member to the workspace.</p> </li> <li> <p>Save your work as a PBIX file to Microsoft OneDrive and share the file with the other modeler.</p> </li> <li> <p>Save your work as a Power BI Project (PBIP). Initialize a Git repository with version control.</p> </li> <li> <p>Answer: Saving your Power BI work as a PBIP enables you to save the work as individual plain text files in a simple, intuitive folder structure, which can be checked into a source control system such as Git. This will enable multiple developers to work on different parts of the model simultaneously. Emailing a Power BI model back and forth is not efficient for collaboration. Saving a Power BI model as a PBIX file to OneDrive eases developers access, but only one developer can have the file open at time. Publishing a PBIX file to a shared workspace does not allow multiple developers to work on the model simultaneously.</p> </li> </ul> <p>\ud83d\udccc **You have a semantic model that pulls data from an Azure SQL database and is synced via Fabric deployment pipelines to three workspaces named Development, Test, and Production. You need to reduce the</p> <p>size of the dataset. Which DAX function should you use to remove unused columns?**</p> <ul> <li> <p>SELECTCOLUMNS()</p> </li> <li> <p>KEEPFILTERS()</p> </li> <li> <p>ADDCOLUMNS()</p> </li> <li> <p>REMOVECOLUMNS()</p> </li> <li> <p>Answer: Use the SELECTCOLUMNS() function to select columns from a table while preserving the table structure. REMOVECOLUMNS() also removes columns from a table, but should be used only for columns that are not referenced anywhere else in the semantic model.</p> </li> </ul> <p>\ud83d\udccc What is the most cost-effective approach to move data from Azure Data Lake Storage to a lakehouse in Fabric?</p> <ul> <li> <p>Use the built-in Data Factory in Fabric to move the data.</p> </li> <li> <p>Use a Dataflow Gen2 to move the data.</p> </li> <li> <p>Use a Power BI Dataflow to move the data.</p> </li> <li> <p>Use a Spark Notebook to move the data.</p> </li> <li> <p>Answer: Using a built-in Data Factory in Fabric can be a cost-effective approach to move the data, providing an integrated and seamless way to handle data transfer within the same platform.</p> </li> </ul> <p>You are planning a Fabric analytics solution.</p> <p>You need to recommend a licensing strategy to support 10 Microsoft Power BI report authors and 600 report consumers. The solution must use Dataflow Gen2 for data ingestion and minimize costs. Which Fabric license type should you recommend? Select only one answer.</p> <p>F16 F32 F64 Premium Per User (PPU)</p>"},{"location":"Microsoft-Fabric/FabricQ%26A.html#answer-while-f32-and-f16-license-types-will-provide-all-the-necessary-set-of-features-these-licenses-are-not-cost-efficient-because-report-consumers-require-a-pro-or-ppu-license-starting-with-the-f64-license-report-consumers-can-use-a-free-per-user-license-ppu-is-incorrect-because-you-cannot-create-non-power-bi-items-in-this-case-dataflow-gen2-with-ppu","title":"Answer: While F32 and F16 license types will provide all the necessary set of features, these licenses are not cost-efficient because report consumers require a Pro or PPU license. Starting with the F64 license, report consumers can use a free per-user license. PPU is incorrect, because you cannot create non-Power BI items (in this case Dataflow Gen2) with PPU.","text":"<p>You are planning a Fabric analytics solution for the following users:</p> <p>2,000 Microsoft Power BI consumers without an individual Power BI Pro license. 32 Power BI modelers with an individual Power BI Pro license. 16 data scientists You need to recommend a Fabric capacity SKU. The solution must minimize costs.</p> <p>What should you recommend?</p> <p>Select only one answer.</p> <p>F2 F2048 F32 F64</p>"},{"location":"Microsoft-Fabric/FabricQ%26A.html#answer-f64-is-the-smallest-fabric-capacity-equivalent-to-a-p1-power-bi-premium-capacity-that-supports-premium-fabric-workspaces-and-does-not-require-power-bi-report-consumers-to-have-individual-power-bi-pro-licenses-f2-and-f32-are-incorrect-since-they-require-that-the-2000-employees-have-individual-power-bi-pro-licenses-to-consume-power-bi-content-f2048-is-incorrect-since-it-is-not-the-smallest-capacity-that-meets-the-stated-requirements","title":"Answer: F64 is the smallest Fabric capacity (equivalent to a P1 Power BI Premium capacity) that supports premium Fabric workspaces and does not require Power BI report consumers to have individual Power BI Pro licenses. F2 and F32 are incorrect since they require that the 2,000 employees have individual Power BI Pro licenses to consume Power BI content. F2048 is incorrect since it is not the smallest capacity that meets the stated requirements.","text":"<p>You are planning the configuration of a new Fabric tenant.</p> <p>You need to recommend a solution to ensure that reports meet the following requirements:</p> <p>Require authentication for embedded reports. Allow only read-only (live) connections against Fabric capacity cloud semantic models. Which two actions should you recommend performing from the Fabric admin portal? Each correct answer presents part of the solution.</p> <p>Select all answers that apply.</p> <p>From Capacity settings, set XMLA Endpoint to Read Write.</p> <p>From Embed Codes, delete all existing codes.</p> <p>From Premium Per User, set XMLA Endpoint to Off.</p> <p>From Tenant settings, disable Allow XMLA endpoints and Analyze in Excel with on-premises semantic models.</p> <p>From Tenant settings, disable Publish to web.</p> <p>Answer: Disabling Publish to web disables the ability to publish any unsecured (no login required) reports to any embedded location. Disabling XMLA Endpoints ensures that semantic models can be connected to, but not edited directly in, workspaces.</p> <p>You have a new Fabric tenant. You need to recommend a workspace architecture to meet best practices for content distribution and data governance. Which two actions should you recommend? Each correct answer presents part of the solution. Select all answers that apply.</p> <p>Create a copy of each semantic model in each workspace. Create direct query semantic models in each workspace. Place semantic models and reports in separate workspaces. Place semantic models and reports in the same workspace. Reuse shared semantic models for multiple reports.</p> <p>Answer: Using shared semantic models for multiple reports enables the reusability of items, while placing semantic models and reports in separate workspaces ensures that the data governance recommended practices are in place.</p> <p>You use Microsoft Power BI Desktop to create a Power BI semantic model.</p> <p>You need to recommend a solution to collaborate with another Power BI modeler. The solution must ensure that you can both work on different parts of the model simultaneously. The solution must provide the most efficient and productive way to collaborate on the same model.</p> <p>What should you recommend?</p> <p>Save your work as a PBIX file and email the file to the other modeler.</p> <p>Save your work as a PBIX file and publish the file to a Fabric workspace. Add the other modeler as member to the workspace.</p> <p>Save your work as a PBIX file to Microsoft OneDrive and share the file with the other modeler.</p> <p>Save your work as a Power BI Project (PBIP). Initialize a Git repository with version control.</p> <p>Answer: Saving your Power BI work as a PBIP enables you to save the work as individual plain text files in a simple, intuitive folder structure, which can be checked into a source control system such as Git. This will enable multiple developers to work on different parts of the model simultaneously.</p> <p>Emailing a Power BI model back and forth is not efficient for collaboration. Saving a Power BI model as a PBIX file to OneDrive eases developers access, but only one developer can have the file open at time. Publishing a PBIX file to a shared workspace does not allow multiple developers to work on the model simultaneously.</p> <p>You have a Fabric tenant that has XMLA Endpoint set to Read Write.</p> <p>You need to use the XMLA endpoint to deploy changes to only one table from the data model.</p> <p>What is the main limitation of using XMLA endpoints for the Microsoft Power BI deployment process?</p> <p>Select only one answer.</p> <p>A PBIX file cannot be downloaded from the Power BI service. Only the user that deployed the report can make changes. Table partitioning is impossible. You cannot use parameters for incremental refresh.</p> <p>Answer:  Whenever the semantic model is deployed/changed by using XMLA endpoints, there is no possibility to download the PBIX file from the Power BI service. This means that no one can download the PBIX file (even the user who deployed the report). Table partitioning, as well as using parameters, is still supported, thus doesn\u2019t represent a limitation.</p> <p>You have an Azure SQL database that contains a customer dimension table. The table contains two columns named CustomerID and CustomerCompositeKey.</p> <p>You have a Fabric workspace that contains a Dataflow Gen2 query that connects to the database.</p> <p>You need to use Dataflows Query Editor to identify which of the two columns contains non-duplicate values per customer.</p> <p>Which option should you use?</p> <p>Select only one answer.</p> <p>Column distribution \u2013 distinct values Column distribution \u2013 unique values Column profile \u2013 values count Column quality \u2013 valid values Answer: Only the distinct values displayed under Column distribution will show the true number of rows of values that are distinct (one row per value). The count of unique only shows the number of distinct values that are in the first 1,000 rows, and the other two options do not review uniqueness</p> <p>You have a Fabric workspace that contains a lakehouse named Lakehouse1.</p> <p>A user named User1 plans to use Lakehouse explorer to read Lakehouse1 data.</p> <p>You need to assign a workspace role to User1. The solution must follow the principle of least privilege.</p> <p>Which workspace role should you assign to User1?</p> <p>Select only one answer.</p> <p>Admin</p> <p>Contributor</p> <p>Member</p> <p>Viewer To read the data from a Fabric lakehouse by using Lakehouse explorer, users must be assigned roles of either Admin, Member, or Contributor. However, respecting the least privileged principle, a user must be assigned the Contributor role. The viewer role does not provide permission to read the lakehouse data through Lakehouse explorer.</p> <p>Question:</p> <p>You are developing a large semantic model.</p> <p>You have a fact table that contains 500 million rows. Most analytic queries will target aggregated data, but some users must still be able to view data on a detailed level.</p> <p>You plan to create a composite model and implement user-defined aggregations.</p> <p>Which three storage modes should you use for each type of table? Each correct answer presents part of the solution.</p> <p>Select all answers that apply. - Aggregated tables should use Dual mode. - Aggregated tables should use Import mode. - The detailed fact table should use DirectQuery mode. - The detailed fact table should use Import mode. - Dimension tables should use DirectQuery mode. - Dimension tables should use Dual mode.</p> <p>Answer: - Aggregated tables should use Import mode. - The detailed fact table should use DirectQuery mode. - Dimension tables should use Dual mode.</p> <p>Explanation:  When using user-defined aggregations, the detailed fact table must be in DirectQuery mode. It is recommended to set the storage mode to Import for aggregated tables because of the performance, while dimension tables should be set to Dual mode to avoid the limitations of limited relationships</p> <p>Question:</p> <p>You have a Microsoft Power BI report that contains a bar chart visual.</p> <p>You need to ensure that users can change the y-axis category of the bar chart by using a slicer selection.</p> <p>Which Power BI feature should you add?</p> <ul> <li>calculation groups</li> <li>drillthrough</li> <li>field parameters</li> <li>WhatIf parameters</li> </ul> <p>Answer: field parameters</p> <p>Explanation: Field parameters allow users to change between columns that can be used on the categorical axis of visuals. All other options do not grant this ability</p>"},{"location":"Microsoft-Fabric/FabricSparkStreaming.html","title":"Spark Streaming","text":""},{"location":"Microsoft-Fabric/FabricSparkStreaming.html#microsoft-fabric-delta-tables-spark-streaming","title":"Microsoft Fabric - Delta Tables - Spark Streaming","text":"<p>Delta Lake supports streaming data, allowing Delta tables to serve as both sinks and sources. In this example, I will demonstrate how to use a Delta Lake table in Microsoft Fabric for Spark streaming.</p>"},{"location":"Microsoft-Fabric/FabricSparkStreaming.html#steps","title":"Steps:","text":"<ol> <li>Make a few copies of the sample source JSON file.</li> <li> <p>Place one file inside the <code>Files</code> folder. Create a subfolder for aesthetics, such as <code>Files/FolderA/</code>. Place the JSON file inside this subfolder.</p> </li> <li> <p>In a notebook, run the following code:</p> </li> </ol> <pre><code>from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\n# Define the schema for the JSON data\njsonSchema = StructType([\n    StructField(\"device\", StringType(), False),\n    StructField(\"status\", StringType(), False)\n])\n\n# Create a stream that reads data from JSON files in the folder\niotstream = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(\"Files/FolderA/SSF\")\n\nprint(\"Source stream created...\")\n\n# Write the stream to a Delta table. The table will be created automatically\ndeltastream = iotstream.writeStream.format(\"delta\").option(\"checkpointLocation\", \"Files/FolderA/ChkPT\").start(\"Tables/TableDLTS\")\n\nprint(\"Streaming to delta sink...\")\n\n# Keep the stream running. Note: This will need to be stopped manually as it will continue to add new rows.\ndeltastream.awaitTermination()\n</code></pre> <p></p> <ol> <li>Now, add more json files and see the Delta table grow!</li> </ol>"},{"location":"Microsoft-Fabric/FabricSparkStreaming.html#how-it-works","title":"How It Works","text":"<p>The source code continuously runs, first creating rows in the Delta Lake table from the JSON files present in the source folder. As new files are added, the code continues to append new rows to the Delta table.</p>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html","title":"Getting Started with Microsoft Fabric","text":""},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#what-is-microsoft-fabric","title":"What is Microsoft Fabric?","text":"<p>Microsoft Fabric is a one-stop, low-to-no-code analytics platform that brings together various data tools under a single environment.</p> <p>Core Components</p> <ul> <li>OneLake - Central data storage</li> <li>Data Engineering - Apache Spark &amp; notebook experiences</li> <li>Data Factory - Data pipeline experiences</li> <li>Data Science - Machine learning experiences</li> <li>Real-time Analytics - Streaming &amp; real-time analytics</li> <li>Power BI - Business intelligence &amp; reporting</li> </ul>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#understanding-onelake","title":"Understanding OneLake","text":"<p>OneLake is the foundation of Microsoft Fabric's data storage system.</p> OneLake Feature Description Default allocation Each fabric tenant receives one OneLake instance by default Storage concept Similar to OneDrive but for data (built on Azure Data Lake Storage) Data handling Import data directly or create shortcuts to external data Default format Delta Lake format"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#lakehouse-fundamentals","title":"Lakehouse Fundamentals","text":"<p>Lakehouse combines the best of Data Lakes and Data Warehouses.</p> <p></p> <p>A Lakehouse provides:</p> <ul> <li>Flexibility of storing raw data like a data lake</li> <li>Structure and performance of a data warehouse</li> <li>SQL querying capabilities on lake data</li> </ul>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#pre-requisites-for-a-lakehouse","title":"Pre-requisites for a Lakehouse","text":"<p>Before creating a lakehouse, you need to create a workspace in the Microsoft Fabric platform.</p> <p>When you create a lakehouse in the Data Engineering workload, three items are produced:</p> <ol> <li>Lakehouse: Storage and metadata where you interact with files, folders, and table data</li> <li>Semantic model: Automatically created data model based on tables</li> <li>SQL Endpoint: Read-only endpoint for Transact-SQL queries</li> </ol> <p></p> <p>You can interact with the data in two modes:</p> <ol> <li>Lakehouse mode: Add and interact with tables, files, and folders</li> <li>SQL analytics endpoint: Query tables using SQL and manage data models</li> </ol> <p></p>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#shortcuts-in-lakehouse","title":"Shortcuts in Lakehouse","text":"<p>Shortcuts let you integrate external data into your lakehouse without moving it.</p> <p>About Shortcuts</p> <ul> <li>Appear as folders in the lake</li> <li>Available in both lakehouses and KQL databases</li> <li>Accessible via Spark, SQL, Real-Time Analytics, and Analysis Services</li> <li>Source data permissions and credentials managed by OneLake</li> </ul>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#data-ingestion-methods","title":"Data Ingestion Methods","text":"<p>There are multiple ways to get data into your lakehouse:</p> UploadDataflows (Gen2)NotebooksData Factory pipelines <p>Upload local files or folders and load results into tables</p> <p>Import and transform data from various sources using Power Query Online</p> <p>Use notebooks to ingest, transform, and load data</p> <p>Copy data and orchestrate processing activities</p>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#data-transformation-methods","title":"Data Transformation Methods","text":"<p>Choose from various transformation methods:</p> <ul> <li>Apache Spark: PySpark, SparkSQL, Notebooks, Spark job definitions</li> <li>SQL analytic endpoint: Transact SQL</li> <li>Dataflows (Gen2): Power Query</li> <li>Data pipelines: Orchestrated data flows</li> </ul>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#visualization","title":"Visualization","text":"<p>Create end-to-end solutions using Power BI and Fabric Lakehouse.</p>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#creating-a-fabric-lakehouse","title":"Creating a Fabric Lakehouse","text":""},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#create-a-lakehouse-workspace","title":"Create a Lakehouse Workspace","text":"<ol> <li>Go to https://app.fabric.microsoft.com</li> <li>Select Synapse Data Engineering</li> </ol>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#create-a-lakehouse","title":"Create a Lakehouse","text":"<ol> <li>Click on Create then Lakehouse</li> <li>Give your Lakehouse a name</li> </ol> <ol> <li>Fabric will create everything automatically</li> </ol>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#upload-a-sample-file","title":"Upload a Sample File","text":"<ol> <li>Download the sample CSV file from https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/sales.csv</li> </ol> <ol> <li>Go to Explorer, create a Data subfolder under Files, then upload the CSV</li> </ol>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#load-data-into-a-table","title":"Load Data into a Table","text":"<p>To use SQL with your data, import it into a table:</p> <ol> <li>Click on the ellipsis (...) next to the CSV file</li> <li>Select Load to Tables</li> </ol> <p></p> <p>Once loaded, you can see your table in tabular format:</p> <p></p>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#query-data-using-sql","title":"Query Data Using SQL","text":""},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#standard-sql-query","title":"Standard SQL Query","text":""},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#visual-query","title":"Visual Query","text":""},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#create-a-power-bi-report","title":"Create a Power BI Report","text":"<ol> <li>At the bottom of the SQL Endpoint page, click the Model tab</li> <li>Navigate to the Reporting tab and select New report</li> </ol> <ol> <li>In the Data pane, expand the sales table and select Item and Quantity</li> <li>Hide the Data and Filters panes to create more space</li> <li>Change the visualization to a Clustered bar chart and resize it</li> <li>Save the report as Item Sales Report</li> </ol> <ol> <li>Verify that your workspace contains:</li> <li>Your lakehouse</li> <li>SQL analytics endpoint</li> <li>Default semantic model</li> <li>Item Sales Report</li> </ol>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#connect-external-data-using-shortcuts","title":"Connect External Data Using Shortcuts","text":"<p>If you want to leave data external but access it from Fabric:</p> <ol> <li>Create shortcuts to external data sources like Dataverse</li> <li>The data appears as a folder in your Lakehouse</li> </ol> <p>Warning</p> <p>The region of Dataverse and Fabric should be the same.</p> <p></p>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#apache-spark-in-microsoft-fabric","title":"Apache Spark in Microsoft Fabric","text":"<p>Spark divides and conquers large data processing jobs across multiple computers.</p> <p>Key Points</p> <ul> <li>Each Fabric workspace gets one Spark cluster</li> <li>PySpark and SparkSQL are the most commonly used languages</li> <li>SparkContext handles the job splitting and distribution</li> </ul>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#running-a-spark-notebook","title":"Running a Spark Notebook","text":""},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#running-pyspark-code-in-a-notebook","title":"Running PySpark Code in a Notebook","text":"<p>Spark sessions are pre-created in Fabric notebooks. Just create dataframes and start coding!</p> <p></p>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#creating-a-spark-job-definition","title":"Creating a Spark Job Definition","text":"<ol> <li>Access the Spark Job Definition page</li> </ol> <ol> <li>Create a PySpark job definition</li> </ol> <p>Develop a main definition file named <code>anyname.py</code>:</p> <pre><code>from pyspark.sql import SparkSession\n\n# This code executes only when the .py file is run directly.\nif __name__ == \"__main__\":\n    # Initialize a Spark session specifically for this job.\n    spark = SparkSession.builder.appName(\"Sales Aggregation\").getOrCreate()\n    # Read data from a CSV file into a DataFrame.\n    df = spark.read.csv('Files/data/sales.csv', header=True, inferSchema=True)\n    # Write the DataFrame to a Delta table, overwriting existing data.\n    df.write.mode('overwrite').format('delta').save('Files/data/delta')\n</code></pre> <ol> <li>Upload and schedule the file</li> </ol> <p></p>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#delta-lake-in-microsoft-fabric","title":"Delta Lake in Microsoft Fabric","text":"<p>Delta Lake provides a SQL interface over data lakes. In Fabric, any table imported from CSV/Excel automatically becomes a Delta Lake table.</p> <p></p> <p>For these tables, you'll find: * <code>.parquet</code> files * <code>_delta_log</code> folders</p> <p></p> <p>This automatic conversion to Delta format is a significant advantage - there's no need to convert files separately.</p>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#delta-lake-tables","title":"Delta Lake Tables","text":""},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#using-dataframe-write-methods","title":"Using DataFrame Write Methods","text":""},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#managed-table","title":"Managed Table","text":"<pre><code>df.write.format(\"delta\").saveAsTable(\"tableName\")\n</code></pre>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#external-table","title":"External Table","text":"<pre><code>df.write.format(\"delta\").saveAsTable(\"tableName\", path=\"Files/folderX\")\n</code></pre>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#using-deltatablebuilder-api","title":"Using DeltaTableBuilder API","text":"<pre><code>from delta.tables import *\n\nDeltaTable.create(spark) \\\n  .tableName(\"Planet\") \\\n  .addColumn(\"Size\", \"INT\") \\\n  .addColumn(\"Name\", \"STRING\") \\\n  .execute()\n</code></pre>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#using-spark-sql","title":"Using Spark SQL","text":""},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#create-managed-table","title":"Create Managed Table","text":"<pre><code>CREATE TABLE salesorders\n(\n    Orderid INT NOT NULL,\n    OrderDate TIMESTAMP NOT NULL,\n    CustomerName STRING,\n    SalesTotal FLOAT NOT NULL\n)\nUSING DELTA;\n</code></pre>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#create-external-table","title":"Create External Table","text":"<pre><code>CREATE TABLE MyExternalTable\nUSING DELTA\nLOCATION 'Files/mydata';\n</code></pre>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#insert-rows","title":"Insert Rows","text":"<pre><code>spark.sql(\"INSERT INTO products VALUES (1, 'Widget', 'Accessories', 2.99)\")\n</code></pre>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#using-sql-magic","title":"Using SQL Magic","text":"<pre><code>%%sql\n\nUPDATE products\nSET Price = 2.6 WHERE ProductId = 1;\n</code></pre>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#working-with-delta-files","title":"Working with Delta Files","text":"<p>You can save dataframes directly as Delta format without creating a table:</p> <pre><code>df.write.format(\"delta\").mode(\"overwrite\").save(\"Folder/Path\")\n</code></pre> <p>After running this code, you'll see: 1. Parquet files 2. <code>_delta_log</code> subfolder</p> <p>Later, you can create a DeltaTable from the folder:</p> <pre><code>from delta.tables import *\nfrom pyspark.sql.functions import *\n\n# Create a DeltaTable object\ndelta_path = \"Files/mytable\"\ndeltaTable = DeltaTable.forPath(spark, delta_path)\n\n# Update the table\ndeltaTable.update(\n    condition = \"Category == 'Accessories'\",\n    set = { \"Price\": \"Price * 0.9\" })\n</code></pre>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#time-travel","title":"Time Travel","text":"<p>View all transactions to a table:</p> <pre><code>%%sql\nDESCRIBE HISTORY products\n</code></pre> <p>Access a specific version:</p> <pre><code>df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path)\n</code></pre> <p>Access data as of a specific date:</p> <pre><code>df = spark.read.format(\"delta\").option(\"timestampAsOf\", '2022-01-01').load(delta_path)\n</code></pre>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#delta-lake-spark-streaming","title":"Delta Lake - Spark Streaming","text":""},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#using-a-delta-table-as-a-streaming-source","title":"Using a Delta Table as a Streaming Source","text":"<pre><code>from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\n# Load a streaming dataframe from the Delta Table\nstream_df = spark.readStream.format(\"delta\") \\\n    .option(\"ignoreChanges\", \"true\") \\\n    .load(\"Files/delta/internetorders\")\n\n# Process the streaming data\nstream_df.show()\n</code></pre>"},{"location":"Microsoft-Fabric/HelloMicrosoftFabric.html#using-a-delta-table-as-a-streaming-sink","title":"Using a Delta Table as a Streaming Sink","text":"<pre><code>from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\n# Create a stream that reads JSON data from a folder\ninputPath = 'Files/streamingdata/'\njsonSchema = StructType([\n    StructField(\"device\", StringType(), False),\n    StructField(\"status\", StringType(), False)\n])\nstream_df = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath)\n\n# Write the stream to a delta table\ntable_path = 'Files/delta/devicetable'\ncheckpoint_path = 'Files/delta/checkpoint'\ndelta_stream = stream_df.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpoint_path).start(table_path)\n</code></pre> <p>Stop the stream when finished:</p> <pre><code>delta_stream.stop()\n</code></pre>"},{"location":"Microsoft-Fabric/InspectingDataframes.html","title":"DataFrame Inspection","text":""},{"location":"Microsoft-Fabric/InspectingDataframes.html#whats-in-your-df-find-out-quick","title":"Whats in your df? Find out quick.","text":"<p>When working with a dataframe, you often need to understand its structure, statistics, and null values - essentially, you want to 'profile' the dataset. Here's how you can do it quickly in Fabric notebook.</p>"},{"location":"Microsoft-Fabric/InspectingDataframes.html#firs-step-use-the-display-function","title":"Firs step - Use the <code>display()</code> Function <p>The first step is to use the <code>display()</code> function. This will produce a rich table and chart view, displaying it directly in the output.</p> <p><pre><code>display(df)\n</code></pre> Once you have displayed your df using df(). You can use the Table view Chart view and Inspect(Table view) option. Lets explore them one by one.</p>","text":""},{"location":"Microsoft-Fabric/InspectingDataframes.html#table-view","title":"Table View <p>The table view allows you to view the data in a tabular format. Follow these steps to inspect your dataframe:</p> <ol> <li>View the Data: The data is displayed in a table format.</li> <li>Click on the <code>Inspect</code> Button: This is a powerful tool for a detailed inspection of your dataframe.</li> </ol> <p></p> <ol> <li>Explore Further: Use the <code>Inspect</code> button to delve deeper into your dataframe.</li> </ol> <p></p>","text":""},{"location":"Microsoft-Fabric/InspectingDataframes.html#chart-view","title":"Chart View <p>The chart view provides a visual representation of your data. Here's how to get started:</p> <ol> <li>Switch to Chart View: Click on the chart view icon.</li> <li>Automatic Key-Value Pair: Fabric will automatically generate a key-value pair to get you started.</li> <li>Aggregate Data: You can then aggregate and analyze the data as needed.</li> </ol> <p></p>","text":""},{"location":"Microsoft-Fabric/InspectingDataframes.html#creating-a-power-bi-report","title":"Creating a Power BI Report <p>You can also create a real-time Power BI report using your dataframe with the following code:</p> <pre><code># Create a Power BI report object from spark dataframe\nfrom powerbiclient import QuickVisualize, get_dataset_config\nPBI_visualize = QuickVisualize(get_dataset_config(df))\n\n# Render new report\nPBI_visualize\n</code></pre> <p></p>","text":""},{"location":"Microsoft-Fabric/InspectingDataframes.html#microsoft-fabric-dp-600-question","title":"Microsoft Fabric DP-600 question <p>In your fabric Notebook you have a dataframe. You want to profile the dataframe - want to find the the columns, which columsn are empty etc etc.</p> <p>What would the easiest way to do it?</p> <p>Options:</p> <ul> <li>Create a pandas DataFrame first, and then visualize the data in an embedded Microsoft Power BI report by running QuickVisualize method on the pandas DataFrame.</li> <li>Display the DataFrame by running display(df), and then clicking the Inspect button.</li> <li>Display the DataDrame by running display(df), and then switching to chart view.</li> <li>Visualize the data in an embedded Microsoft Power BI report by running the QuickVisualize method on the DataFrame.</li> </ul> <p>Answer: Use the Inspect(Table view) option</p> <p></p>","text":""},{"location":"Microsoft-Fabric/InspectingDataframes.html#reference","title":"Reference <p>Microsoft Reference</p>","text":""},{"location":"Microsoft-Fabric/KQL.html","title":"KQL","text":""},{"location":"Microsoft-Fabric/KQL.html#what-is-kql","title":"What is KQL?","text":"<p>A KQL (Kusto Query Language) Database handles large volumes of structured, semi-structured, and unstructured data for real-time analytics and ad-hoc querying. It is part of the Azure Data Explorer service. The data in a KQL database is stored in Azure Data Explorer. It uses a columnar storage format, for high-performance.</p> <p></p>"},{"location":"Microsoft-Fabric/KQL.html#how-to-run-kql-query-in-fabric","title":"How to run KQL query in Fabric?","text":"<p>There is no magic command like %%KQL</p>"},{"location":"Microsoft-Fabric/KQL.html#kql-vs-sql-databases","title":"KQL vs SQL Databases","text":"Feature KQL Database Standard SQL Database Query Language Kusto Query Language (KQL) Structured Query Language (SQL) Storage Format Columnar Row-based Optimized For Real-time analytics, log and time-series data Transactional data, relational data Data Structure Tables, columns, materialized views, functions Tables, columns, views, stored procedures Scalability Highly scalable and distributed Varies by implementation (SQL Server, MySQL, etc.) Indexing Automatically indexed for fast query performance Manual and automatic indexing Data Ingestion Supports batch and streaming ingestion Primarily batch ingestion Use Cases Log analytics, telemetry data, IoT data OLTP, data warehousing, reporting Storage Location Azure Data Explorer service in the cloud Varies (on-premises, cloud-based) Performance Optimized for read-heavy and analytical workloads Balanced for read and write operations Schema Flexible schema with support for semi-structured data Rigid schema with well-defined data types"},{"location":"Microsoft-Fabric/KQL.html#kql-vs-sql-query","title":"KQL Vs SQL Query","text":""},{"location":"Microsoft-Fabric/KQL.html#kql-vs-sql-dql","title":"KQL vs SQL - DQL","text":"Operation SQL KQL Select and Count <code>SELECT Name, Age, COUNT(*) FROM Employees WHERE Age &gt; 30 GROUP BY Name, Age;</code> <code>Employees \\| where Age &gt; 30 \\| summarize count() by Name, Age</code> Group By and Order By <code>SELECT Department, AVG(Salary) AS AverageSalary FROM Employees GROUP BY Department ORDER BY AverageSalary DESC;</code> <code>Employees \\| summarize AverageSalary=avg(Salary) by Department \\| sort by AverageSalary desc</code> Join <code>SELECT e.Name, d.DepartmentName FROM Employees e JOIN Departments d ON e.DepartmentID = d.ID;</code> <code>Employees \\| join kind=inner (Departments) on $left.DepartmentID == $right.ID \\| project Name, DepartmentName</code> Subquery and Limit <code>SELECT Name FROM (SELECT * FROM Employees WHERE Age &gt; 30) AS SubQuery WHERE DepartmentID = 5 LIMIT 10;</code> <code>let SubQuery = Employees \\| where Age &gt; 30; SubQuery \\| where DepartmentID == 5 \\| project Name \\| take 10</code> String Functions <code>SELECT Name FROM Employees WHERE UPPER(FirstName) = 'JOHN';</code> <code>Employees \\| where tolower(FirstName) == 'john' \\| project Name</code> Date Functions <code>SELECT Name FROM Employees WHERE YEAR(HireDate) = 2020;</code> <code>Employees \\| where datetime_part('year', HireDate) == 2020 \\| project Name</code> Between <code>SELECT * FROM Employees WHERE Age BETWEEN 25 AND 35;</code> <code>Employees \\| where Age between (25 .. 35)</code> Date Range <code>SELECT * FROM Sales WHERE SaleDate BETWEEN '2021-01-01' AND '2021-12-31';</code> <code>Sales \\| where SaleDate between (datetime(2021-01-01) .. datetime(2021-12-31))</code> Distinct <code>SELECT DISTINCT Department FROM Employees;</code> <code>Employees \\| summarize by Department</code> Top N <code>SELECT TOP 5 Name, Salary FROM Employees ORDER BY Salary DESC;</code> <code>Employees \\| top 5 by Salary desc \\| project Name, Salary</code> Aggregation with Conditions <code>SELECT Department, COUNT(*) FROM Employees WHERE Age &gt; 30 GROUP BY Department;</code> <code>Employees \\| where Age &gt; 30 \\| summarize count() by Department</code>"},{"location":"Microsoft-Fabric/KQL.html#kql-vs-sql-ddls-dmls-dqls","title":"KQL Vs SQL - DDLs, DMLs &amp; DQLs","text":"Description Example Category Tables Create a new table <code>.create table MyTable (Column1: string, Column2: int)</code> DDL Show the schema of a table <code>.show table MyTable schema</code> DQL Ingest data into a table <code>.ingest into table MyTable &lt;DataSource&gt;</code> DML Rename a table <code>.rename table OldTableName to NewTableName</code> DDL Drop a table <code>.drop table TableName</code> DDL List all tables <code>.show tables</code> DQL Columns Add a column <code>.alter table TableName add column ColumnName: DataType</code> DDL Drop a column <code>.alter table TableName drop column ColumnName</code> DDL Rename a column <code>.rename column OldColumnName to NewColumnName in table TableName</code> DDL Functions Create a new function <code>.create function with (docstring = \"Description\", folder = \"FolderName\") MyFunction () { &lt;KQLQuery&gt; }</code> DDL Show available functions <code>.show functions</code> DQL Materialized Views Create a new materialized view <code>.create materialized-view MyView on table MyTable { &lt;KQLQuery&gt; }</code> DDL Show available materialized views <code>.show materialized-views</code> DQL Indexes Create an index <code>.create index IndexName on TableName (ColumnName)</code> DDL Drop an index <code>.drop index IndexName on TableName</code> DDL Show indexes <code>.show indexes</code> DQL Ingest Ingest data into a table <code>.ingest into table MyTable &lt;DataSource&gt;</code> DML Ingest data from JSON <code>.ingest into table TableName h@\"https://path/to/file.json\"</code> DML Database Operations Create a database <code>.create database DatabaseName</code> DDL Drop a database <code>.drop database DatabaseName</code> DDL List all databases <code>.show databases</code> DQL Permissions Grant table permissions <code>.grant select on table TableName to UserName</code> DDL Revoke table permissions <code>.revoke select on table TableName from UserName</code> DDL Show permissions <code>.show table TableName policy access</code> DQL Views Create a view <code>.create view ViewName as &lt;KQLQuery&gt;</code> DDL Drop a view <code>.drop view ViewName</code> DDL Show views <code>.show views</code> DQL Diagnostics Show cluster diagnostics <code>.show cluster diagnostics</code> DQL Show table statistics <code>.show table TableName stats</code> DQL Data Export Export data to JSON <code>.export to json at &lt;FilePath&gt; &lt;KQLQuery&gt;</code> DML"},{"location":"Microsoft-Fabric/KQL.html#kql-qa","title":"KQL Q&amp;A","text":"<p>Highlight the answers to reveal it!</p>"},{"location":"Microsoft-Fabric/KQL.html#kql-questions-keywords","title":"KQL Questions - Keywords","text":"<ol> <li>Which KQL keyword is used to limit the results of a query to a specified number of rows?</li> <li>A. select</li> <li>B. take</li> <li> <p>C. project</p> <p>Answer: Take limits the results to a specified number of rows.</p> </li> <li> <p>Which KQL keyword is used to group and aggregate data?</p> </li> <li>A. group_by</li> <li>B. aggregate</li> <li>C. summarize</li> </ol> <p>Answer: Use the summarize keyword to group and aggregate data.</p> <ol> <li>Which KQL keyword is used to filter rows based on a condition?</li> <li>A. where</li> <li>B. filter</li> <li>C. select</li> </ol> <p>Answer: where is used to filter rows based on a condition.</p> <ol> <li>Which KQL keyword is used to create a new column or modify an existing column?</li> <li>A. create</li> <li>B. extend</li> <li>C. modify</li> </ol> <p>Answer: extend is used to create a new column or modify an existing column.</p> <ol> <li>Which KQL keyword is used to sort the results of a query?</li> <li>A. order</li> <li>B. arrange</li> <li>C. sort</li> </ol> <p>Answer: sort is used to order the results of a query.</p> <ol> <li>Which KQL keyword is used to rename a column in the results?</li> <li>A. rename</li> <li>B. project-rename</li> <li>C. alias</li> </ol> <p>Answer: project-rename is used to rename a column in the results.</p> <ol> <li>Which KQL keyword is used to join two tables on a common column?</li> <li>A. merge</li> <li>B. union</li> <li>C. join</li> </ol> <p>Answer: join is used to combine two tables on a common column.</p> <ol> <li>Which KQL keyword is used to calculate the total number of rows in the results?</li> <li>A. count</li> <li>B. total</li> <li>C. sum</li> </ol> <p>Answer: count is used to calculate the total number of rows in the results.</p> <ol> <li>Which KQL keyword is used to remove duplicates from the results?</li> <li>A. distinct</li> <li>B. unique</li> <li>C. remove-duplicates</li> </ol> <p>Answer: distinct is used to remove duplicate rows from the results.</p> <ol> <li> <p>Which KQL keyword is used to extract a substring from a string column?</p> <ul> <li>A. substring</li> <li>B. extract</li> <li>C. substr</li> </ul> <p>Answer: substring is used to extract a part of a string column.</p> </li> <li> <p>Which KQL keyword is used to combine the results of two or more queries?</p> <ul> <li>A. combine</li> <li>B. union</li> <li>C. join</li> </ul> <p>Answer: union is used to combine the results of two or more queries.</p> </li> <li> <p>Which KQL keyword is used to convert a column to a different data type?</p> <ul> <li>A. convert</li> <li>B. cast</li> <li>C. toType</li> </ul> <p>Answer: cast is used to convert a column to a different data type.</p> </li> <li> <p>Which KQL keyword is used to filter rows with null values?</p> <ul> <li>A. isnull</li> <li>B. isnotnull</li> <li>C. isnonempty</li> </ul> <p>Answer: isnotnull is used to filter rows with null values.</p> </li> <li> <p>Which KQL keyword is used to calculate the average of a numeric column?</p> <ul> <li>A. average</li> <li>B. mean</li> <li>C. avg</li> </ul> <p>Answer: avg is used to calculate the average of a numeric column.</p> </li> <li> <p>Which KQL keyword is used to create a time series chart?</p> <ul> <li>A. timeseries</li> <li>B. render</li> <li>C. chart</li> </ul> <p>Answer: render is used to create a time series chart.</p> </li> <li> <p>Which KQL keyword is used to specify the columns to include in the results?</p> <ul> <li>A. include</li> <li>B. select</li> <li>C. project</li> </ul> <p>Answer: project is used to specify the columns to include in the results.</p> </li> <li> <p>Which KQL keyword is used to calculate the maximum value of a numeric column?</p> <ul> <li>A. max</li> <li>B. maximum</li> <li>C. highest</li> </ul> <p>Answer: max is used to calculate the maximum value of a numeric column.</p> </li> <li> <p>Which KQL keyword is used to calculate the minimum value of a numeric column?</p> <ul> <li>A. min</li> <li>B. minimum</li> <li>C. lowest</li> </ul> <p>Answer: min is used to calculate the minimum value of a numeric column.</p> </li> <li> <p>Which KQL keyword is used to convert a datetime column to a specific format?</p> <ul> <li>A. format</li> <li>B. convert</li> <li>C. format_datetime</li> </ul> <p>Answer: format_datetime is used to convert a datetime column to a specific format.</p> </li> <li> <p>Which KQL keyword is used to calculate the difference between two datetime columns?</p> <ul> <li>A. datetime_diff</li> <li>B. date_diff</li> <li>C. time_diff</li> </ul> <p>Answer: date_diff is used to calculate the difference between two datetime columns.</p> </li> <li> <p>Which KQL keyword is used to filter rows based on a regular expression?</p> <ul> <li>A. regex_match</li> <li>B. matches_regex</li> <li>C. search</li> </ul> <p>Answer: matches_regex is used to filter rows based on a regular expression.</p> </li> <li> <p>Which KQL keyword is used to calculate the sum of a numeric column?</p> <ul> <li>A. sum</li> <li>B. total</li> <li>C. aggregate_sum</li> </ul> <p>Answer: sum is used to calculate the sum of a numeric column.</p> </li> <li> <p>Which KQL keyword is used to create a new table with the results of a query?</p> <ul> <li>A. create_table</li> <li>B. into</li> <li>C. output</li> </ul> <p>Answer: into is used to create a new table with the results of a query.</p> </li> <li> <p>Which KQL keyword is used to parse a string into multiple columns?</p> <ul> <li>A. split</li> <li>B. parse</li> <li>C. dissect Answer: parse is used to parse a string into multiple columns.</li> </ul> </li> <li> <p>Which KQL keyword is used to join two tables and keep only the rows with matching keys?</p> <ul> <li>A. inner join</li> <li>B. equijoin</li> <li>C. join</li> </ul> <p>Answer: join is used to join two tables and keep only the rows with matching keys.</p> </li> <li> <p>Which KQL keyword is used to create an alias for a column in the results?</p> <ul> <li>A. alias</li> <li>B. as</li> <li>C. rename</li> </ul> <p>Answer: as is used to create an alias for a column in the results.</p> </li> <li> <p>Which KQL keyword is used to filter rows based on a range of values?</p> <ul> <li>A. between</li> <li>B. in_range</li> <li>C. within</li> </ul> <p>Answer: between is used to filter rows based on a range of values.</p> </li> <li> <p>Which KQL keyword is used to concatenate two or more strings?</p> <ul> <li>A. concat</li> <li>B. strcat</li> <li>C. joinstr</li> </ul> <p>Answer: strcat is used to concatenate two or more strings.</p> </li> <li> <p>Which KQL keyword is used to extract a portion of a datetime value?</p> <ul> <li>A. extract</li> <li>B. datetime_part</li> <li>C. datetime_extract</li> </ul> <p>Answer: extract is used to extract a portion of a datetime value.</p> </li> <li> <p>Which KQL keyword is used to find the median of a numeric column?</p> <ul> <li>A. median</li> <li>B. percentile</li> <li>C. mid</li> </ul> <p>Answer: percentile is used to find the median of a numeric column (percentile 50).</p> </li> <li> <p>Which KQL keyword is used to return a specified number of rows from the start of the results?</p> <ul> <li>A. top</li> <li>B. limit</li> <li>C. head</li> </ul> <p>Answer: head is used to return a specified number of rows from the start of the results.</p> </li> <li> <p>Which KQL keyword is used to combine multiple conditions in a query?</p> <ul> <li>A. combine</li> <li>B. and</li> <li>C. both</li> </ul> <p>Answer: and is used to combine multiple conditions in a query.</p> </li> <li> <p>Which KQL keyword is used to calculate the standard deviation of a numeric column?</p> <ul> <li>A. stddev</li> <li>B. stdev</li> <li>C. sd</li> </ul> <p>Answer: stdev is used to calculate the standard deviation of a numeric column.</p> </li> <li> <p>Which KQL keyword is used to return rows where a column value is within a list of values?</p> <ul> <li>A. in</li> <li>B. within</li> <li>C. includes</li> </ul> <p>Answer: in is used to return rows where a column value is within a list of values.</p> </li> <li> <p>Which KQL keyword is used to calculate the variance of a numeric column?</p> <ul> <li>A. variance</li> <li>B. var</li> <li>C. varp</li> </ul> <p>Answer: var is used to calculate the variance of a numeric column.</p> </li> <li> <p>Which KQL keyword is used to format a string column?</p> <ul> <li>A. format</li> <li>B. str_format</li> <li>C. tostring</li> </ul> <p>Answer: tostring is used to format a string column.</p> </li> <li> <p>Which KQL keyword is used to pivot a table?</p> <ul> <li>A. pivot</li> <li>B. transform</li> <li>C. make-series</li> </ul> <p>Answer: make-series is used to pivot a table.</p> </li> <li> <p>Which KQL keyword is used to calculate the cumulative sum of a numeric column?</p> <ul> <li>A. cumulative_sum</li> <li>B. sum</li> <li>C. running_sum</li> </ul> <p>Answer: cumulative_sum is used to calculate the cumulative sum of a numeric column.</p> </li> <li> <p>Which KQL keyword is used to create a histogram of a numeric column?</p> <ul> <li>A. histogram</li> <li>B. bin</li> <li>C. bucket</li> </ul> <p>Answer: bin is used to create a histogram of a numeric column.</p> </li> <li> <p>Which KQL keyword is used to evaluate a condition and return one of two values?</p> <ul> <li>A. if</li> <li>B. case</li> <li>C. switch</li> </ul> <p>Answer: if is used to evaluate a condition and return one of two values.</p> </li> </ol>"},{"location":"Microsoft-Fabric/KQL.html#kql-quesitons-general","title":"KQL Quesitons - General","text":"<ol> <li>You have a table <code>Sales</code> with columns <code>ProductID</code>, <code>Quantity</code>, and <code>Price</code>. How would you calculate the total revenue for each product?</li> <li>A. <code>Sales | summarize TotalRevenue = sum(Quantity * Price) by ProductID</code></li> <li>B. <code>Sales | summarize TotalRevenue = avg(Quantity * Price) by ProductID</code></li> <li>C. <code>Sales | extend TotalRevenue = Quantity * Price | summarize Total = sum(TotalRevenue) by ProductID</code></li> </ol> <p>Answer: <code>Sales | summarize TotalRevenue = sum(Quantity *&lt;/span&gt; Price) by ProductID</code> <ol> <li>How would you find all records in the <code>Logs</code> table where the <code>Message</code> column contains the word \"error\"?</li> <li>A. <code>Logs | where Message contains \"error\"</code></li> <li>B. <code>Logs | where Message == \"error\"</code></li> <li> <p>C. <code>Logs | search \"error\"</code> Answer: <code>Logs | where Message contains \"error\"</code></p> </li> <li> <p>What function would you use to calculate the moving average of a column in KQL?</p> </li> <li>A. <code>moving_avg()</code></li> <li>B. <code>series_fir()</code></li> <li>C. <code>avg()</code></li> </ol> <p>Answer: <code>series_fir()</code> is used to calculate the moving average.</p> <ol> <li>How do you join two tables <code>Table1</code> and <code>Table2</code> on the <code>ID</code> column, keeping all records from <code>Table1</code>?</li> <li>A. <code>Table1 | innerjoin (Table2) on ID</code></li> <li>B. <code>Table1 | join kind=inner (Table2) on ID</code></li> <li>C. <code>Table1 | join kind=leftouter (Table2) on ID</code></li> </ol> <p>Answer: <code>Table1 | join kind=leftouter (Table2) on ID</code></p> <ol> <li>How can you create a histogram of the <code>Age</code> column in the <code>Users</code> table?</li> <li>A. <code>Users | histogram Age by 10</code></li> <li>B. <code>Users | summarize count() by Age</code></li> <li>C. <code>Users | summarize count() by bin(Age, 10)</code></li> </ol> <p>Answer: <code>Users | summarize count() by bin(Age, 10)</code></p> <ol> <li>You need to extract the year from a <code>datetime</code> column called <code>Timestamp</code> in the <code>Events</code> table. Which function would you use?</li> <li>A. <code>year(Timestamp)</code></li> <li>B. <code>extract_year(Timestamp)</code></li> <li>C. <code>datetime_part('year', Timestamp)</code></li> </ol> <p>Answer: <code>year(Timestamp)</code></p> <ol> <li>How would you filter rows in the <code>Sales</code> table to only include those where the <code>Date</code> is within the last 30 days?</li> <li>A. <code>Sales | where Date &gt; ago(30d)</code></li> <li>B. <code>Sales | where Date between (now() - 30d) and now()</code></li> <li>C. <code>Sales | where Date &gt; datetime(30 days ago)</code></li> </ol> <p>Answer: <code>Sales | where Date &gt; ago(30d)</code></p> <ol> <li>How do you rename the column <code>OldName</code> to <code>NewName</code> in a KQL query?</li> <li>A. <code>| project-rename NewName = OldName</code></li> <li>B. <code>| rename OldName to NewName</code></li> <li>C. <code>| project OldName as NewName</code></li> </ol> <p>Answer: <code>| project OldName as NewName</code></p> <ol> <li>Which KQL function would you use to concatenate the values of two columns <code>FirstName</code> and <code>LastName</code> in the <code>Employees</code> table?</li> <li>A. <code>concat(FirstName, LastName)</code></li> <li>B. <code>strcat(FirstName, \" \", LastName)</code></li> <li>C. <code>combine(FirstName, LastName)</code></li> </ol> <p>Answer: <code>strcat(FirstName, \" \", LastName)</code></p> <ol> <li> <p>In the <code>Orders</code> table, how would you calculate the average order value?</p> <ul> <li>A. <code>Orders | summarize avg(OrderValue)</code></li> <li>B. <code>Orders | summarize AverageOrder = mean(OrderValue)</code></li> <li>C. <code>Orders | summarize AverageOrder = avg(OrderValue)</code></li> </ul> <p>Answer: <code>Orders | summarize AverageOrder = avg(OrderValue)</code></p> </li> <li> <p>How can you list the unique values of the <code>Country</code> column from the <code>Customers</code> table?</p> <ul> <li>A. <code>Customers | distinct Country</code></li> <li>B. <code>Customers | summarize by Country</code></li> <li>C. <code>Customers | unique Country</code></li> </ul> <p>Answer: <code>Customers | distinct Country</code></p> </li> <li> <p>What is the correct way to calculate the total number of orders in the <code>Orders</code> table?</p> <ul> <li>A. <code>Orders | summarize count()</code></li> <li>B. <code>Orders | count()</code></li> <li>C. <code>Orders | summarize total_orders = count()</code></li> </ul> <p>Answer: <code>Orders | summarize total_orders = count()</code></p> </li> <li> <p>How would you convert the <code>Price</code> column in the <code>Products</code> table from a string to a real number?</p> <ul> <li>A. <code>Products | project Price = toreal(Price)</code></li> <li>B. <code>Products | extend Price = todouble(Price)</code></li> <li>C. <code>Products | cast(Price as real)</code></li> </ul> <p>Answer: <code>Products | project Price = toreal(Price)</code></p> </li> <li> <p>You want to visualize the <code>Sales</code> table's total revenue over time using a line chart. Which render statement should you use?</p> <ul> <li>A. <code>| render linechart</code></li> <li>B. <code>| render timechart</code></li> <li>C. <code>| render barchart</code></li> </ul> <p>Answer: <code>| render linechart</code></p> </li> <li> <p>How can you combine the results of two queries in KQL?</p> <ul> <li>A. <code>combine</code></li> <li>B. <code>union</code></li> <li>C. <code>join</code></li> </ul> <p>Answer: <code>union</code></p> </li> <li> <p>In KQL, how do you create a new column that shows the length of the <code>Description</code> column in the <code>Products</code> table?</p> <ul> <li>A. <code>Products | extend Length = len(Description)</code></li> <li>B. <code>Products | project Length = strlen(Description)</code></li> <li>C. <code>Products | project Length = length(Description)</code></li> </ul> <p>Answer: <code>Products | project Length = strlen(Description)</code></p> </li> <li> <p>How would you find the earliest <code>OrderDate</code> in the <code>Orders</code> table?</p> <ul> <li>A. <code>Orders | summarize EarliestDate = min(OrderDate)</code></li> <li>B. <code>Orders | summarize EarliestDate = earliest(OrderDate)</code></li> <li>C. <code>Orders | summarize EarliestDate = first(OrderDate)</code></li> </ul> <p>Answer: <code>Orders | summarize EarliestDate = min(OrderDate)</code></p> </li> <li> <p>Which KQL keyword is used to combine two tables side by side, based on a common column?</p> <ul> <li>A. <code>merge</code></li> <li>B. <code>union</code></li> <li>C. <code>join</code></li> </ul> <p>Answer: <code>join</code></p> </li> <li> <p>How would you calculate the median value of the <code>Income</code> column in the <code>Employees</code> table?</p> <ul> <li>A. <code>Employees | summarize median(Income)</code></li> <li>B. <code>Employees | summarize Percentile_50 = percentile(Income, 50)</code></li> <li>C. <code>Employees | summarize MedianIncome = median(Income)</code></li> </ul> <p>Answer: <code>Employees | summarize Percentile_50 = percentile(Income, 50)</code></p> </li> <li> <p>How can you filter the <code>Events</code> table to show only records where the <code>Status</code> column is either \"Active\" or \"Pending\"?</p> <ul> <li>A. <code>Events | where Status in (\"Active\", \"Pending\")</code></li> <li>B. <code>Events | where Status == \"Active\" or Status == \"Pending\"</code></li> <li>C. <code>Events | where Status matches (\"Active\", \"Pending\")</code></li> </ul> <p>Answer: <code>Events | where Status in (\"Active\", \"Pending\")</code></p> </li> <li> <p>How would you list the top 5 products by total sales in the <code>Sales</code> table?</p> <ul> <li>A. <code>Sales | top 5 by sum(TotalSales)</code></li> <li>B. <code>Sales | summarize TotalSales = sum(SalesAmount) by ProductID | top 5 by TotalSales</code></li> <li>C. <code>Sales | summarize TotalSales = sum(SalesAmount) by ProductID | limit 5 by TotalSales</code></li> </ul> <p>Answer: <code>Sales | summarize TotalSales = sum(SalesAmount) by ProductID | top 5 by TotalSales</code></p> </li> <li> <p>Which function in KQL would you use to format a datetime value as a string?</p> <ul> <li>A. <code>format_datetime()</code></li> <li>B. <code>datetime_to_string()</code></li> <li>C. <code>tostring()</code></li> </ul> <p>Answer: <code>format_datetime()</code></p> </li> <li> <p>How can you find the number of unique customers in the <code>Sales</code> table?</p> <ul> <li>A. <code>Sales | summarize UniqueCustomers = dcount(CustomerID)</code></li> <li>B. <code>Sales | summarize UniqueCustomers = countdistinct(CustomerID)</code></li> <li>C. <code>Sales | summarize UniqueCustomers = unique(CustomerID)</code></li> </ul> <p>Answer: <code>Sales | summarize UniqueCustomers = dcount(CustomerID)</code></p> </li> <li> <p>Which KQL function would you use to convert a string to a datetime value?</p> <ul> <li>A. <code>todatetime()</code></li> <li>B. <code>string_to_datetime()</code></li> <li>C. <code>datetime()</code></li> </ul> <p>Answer: <code>todatetime()</code></p> </li> <li> <p>How can you calculate the standard deviation of the <code>Price</code> column in the <code>Products</code> table?</p> <ul> <li>A. <code>Products | summarize StdDevPrice = stdev(Price)</code></li> <li>B. <code>Products | summarize StdDevPrice = stddev(Price)</code></li> <li>C. <code>Products | summarize StdDevPrice = variance(Price)</code></li> </ul> <p>Answer: <code>Products | summarize StdDevPrice = stddev(Price)</code></p> </li> <li> <p>Which KQL function is used to round a numeric value to the nearest integer?</p> <ul> <li>A. <code>round()</code></li> <li>B. <code>truncate()</code></li> <li>C. <code>ceil()</code></li> </ul> <p>Answer: <code>round()</code></p> </li> <li> <p>How would you extract the day of the week from a datetime column <code>OrderDate</code> in the <code>Orders</code> table?</p> <ul> <li>A. <code>Orders | extend DayOfWeek = dayofweek(OrderDate)</code></li> <li>B. <code>Orders | extend DayOfWeek = extract('dow', OrderDate)</code></li> <li>C. <code>Orders | extend DayOfWeek = day(OrderDate)</code></li> </ul> <p>Answer: <code>Orders | extend DayOfWeek = dayofweek(OrderDate)</code></p> </li> <li> <p>Which function in KQL can be used to split a string into an array based on a delimiter?</p> <ul> <li>A. <code>split()</code></li> <li>B. <code>string_split()</code></li> <li>C. <code>explode()</code></li> </ul> <p>Answer: <code>split()</code></p> </li> <li> <p>How would you calculate the cumulative sum of the <code>SalesAmount</code> column in the <code>Sales</code> table?</p> <ul> <li>A. <code>Sales | extend CumulativeSales = sum(SalesAmount)</code></li> <li>B. <code>Sales | extend CumulativeSales = running_sum(SalesAmount)</code></li> <li>C. <code>Sales | extend CumulativeSales = cumulative_sum(SalesAmount</code></li> </ul> <p>Answer: <code>Sales | extend CumulativeSales = cumulative_sum(SalesAmount)</code></p> </li> <li> <p>How do you find all records in the <code>Logs</code> table where the <code>Severity</code> column is either \"Error\" or \"Warning\"?</p> <ul> <li>A. <code>Logs | where Severity == \"Error\" or Severity == \"Warning\"</code></li> <li>B. <code>Logs | where Severity in (\"Error\", \"Warning\")</code></li> <li>C. <code>Logs | where Severity matches (\"Error\", \"Warning\")</code></li> </ul> <p>Answer: <code>Logs | where Severity in (\"Error\", \"Warning\")</code></p> </li> <li> <p>Which KQL function would you use to get the current date and time?</p> <ul> <li>A. <code>now()</code></li> <li>B. <code>current_datetime()</code></li> <li>C. <code>getdate()</code></li> </ul> <p>Answer: <code>now()</code></p> </li> <li> <p>How would you calculate the number of days between two datetime columns <code>StartDate</code> and <code>EndDate</code> in the <code>Projects</code> table?</p> <ul> <li>A. <code>Projects | extend DaysBetween = date_diff('day', EndDate, StartDate)</code></li> <li>B. <code>Projects | extend DaysBetween = datetime_diff('day', EndDate, StartDate)</code></li> <li>C. <code>Projects | extend DaysBetween = day_diff(EndDate, StartDate</code></li> </ul> <p>Answer: <code>Projects | extend DaysBetween = date_diff('day', EndDate, StartDate)</code></p> </li> <li> <p>Which function would you use to get the maximum value of a column <code>Price</code> in the <code>Products</code> table?</p> <ul> <li>A. <code>Products | summarize MaxPrice = max(Price)</code></li> <li>B. <code>Products | summarize MaxPrice = maximum(Price)</code></li> <li>C. <code>Products | summarize MaxPrice = greatest(Price)</code></li> </ul> <p>Answer: <code>Products | summarize MaxPrice = max(Price)</code></p> </li> <li> <p>How can you filter the <code>Events</code> table to show records where the <code>EventDate</code> is in the current year?</p> <ul> <li>A. <code>Events | where year(EventDate) == year(now())</code></li> <li>B. <code>Events | where EventDate &gt;= startofyear(now())</code></li> <li>C. <code>Events | where EventDate between (startofyear(now()) .. endofyear(now()))</code></li> </ul> <p>Answer: <code>Events | where year(EventDate) == year(now())</code></p> </li> <li> <p>How would you rename the <code>OldColumn</code> to <code>NewColumn</code> in the <code>Data</code> table?</p> <ul> <li>A. <code>Data | project NewColumn = OldColumn</code></li> <li>B. <code>Data | rename OldColumn as NewColumn</code></li> <li>C. <code>Data | project-rename NewColumn = OldColumn</code></li> </ul> <p>Answer: <code>Data | project-rename NewColumn = OldColumn</code></p> </li> <li> <p>Which function in KQL is used to get the number of elements in an array?</p> <ul> <li>A. <code>count()</code></li> <li>B. <code>length()</code></li> <li>C. <code>array_length()</code></li> </ul> <p>Answer: <code>length()</code></p> </li> <li> <p>How would you calculate the average <code>Salary</code> for each department in the <code>Employees</code> table?</p> <ul> <li>A. <code>Employees | summarize AvgSalary = avg(Salary) by Department</code></li> <li>B. <code>Employees | summarize AvgSalary = mean(Salary) by Department</code></li> <li>C. <code>Employees | summarize AvgSalary = average(Salary) by Department</code></li> </ul> <p>Answer: <code>Employees | summarize AvgSalary = avg(Salary) by Department</code></p> </li> <li> <p>How can you convert the <code>Price</code> column in the <code>Products</code> table from a string to a real number?</p> <ul> <li>A. <code>Products | project Price = toreal(Price)</code></li> <li>B. <code>Products | extend Price = todouble(Price)</code></li> <li>C. <code>Products | cast(Price as real)</code></li> </ul> <p>Answer: <code>Products | project Price = toreal(Price)</code></p> </li> <li> <p>Which function would you use to extract the hour from a datetime column <code>EventTime</code> in the <code>Events</code> table?</p> <ul> <li>A. <code>Events | extend Hour = hour(EventTime)</code></li> <li>B. <code>Events | extend Hour = extract('hour', EventTime)</code></li> <li>C. <code>Events | extend Hour = gethour(EventTime)</code></li> </ul> <p>Answer: <code>Events | extend Hour = hour(EventTime)</code></p> </li> <li> <p>How can you create a column <code>FullName</code> by concatenating <code>FirstName</code> and <code>LastName</code> in the <code>Employees</code> table?</p> <ul> <li>A. <code>Employees | extend FullName = FirstName + \" \" + LastName</code></li> <li>B. <code>Employees | extend FullName = strcat(FirstName, \" \", LastName)</code></li> <li>C. <code>Employees | extend FullName = concat(FirstName, \" \", LastName)</code></li> </ul> <p>Answer: <code>Employees | extend FullName = strcat(FirstName, \" \", LastName)</code></p> </li> <li> <p>How would you calculate the 90th percentile of the <code>ResponseTime</code> column in the <code>Requests</code> table?</p> <ul> <li>A. <code>Requests | summarize Percentile_90 = percentile(ResponseTime, 90)</code></li> <li>B. <code>Requests | summarize Percentile_90 = p90(ResponseTime)</code></li> <li>C. <code>Requests | summarize Percentile_90 = percentile_approx(ResponseTime, 0.90)</code></li> </ul> <p>Answer: <code>Requests | summarize Percentile_90 = percentile(ResponseTime, 90)</code></p> </li> <li> <p>Which function in KQL would you use to replace all occurrences of a substring in a string column?</p> <ul> <li>A. <code>replace()</code></li> <li>B. <code>str_replace()</code></li> <li>C. <code>substitute()</code></li> </ul> <p>Answer: <code>replace()</code></p> </li> <li> <p>How can you find the maximum value of the <code>Score</code> column in the <code>Results</code> table?</p> <ul> <li>A. <code>Results | summarize MaxScore = max(Score)</code></li> <li>B. <code>Results | summarize MaxScore = maximum(Score)</code></li> <li>C. <code>Results | summarize MaxScore = highest(Score)</code></li> </ul> <p>Answer: <code>Results | summarize MaxScore = max(Score)</code></p> </li> <li> <p>How would you create a new column <code>Month</code> by extracting the month from the <code>OrderDate</code> column in the <code>Sales</code> table?</p> <ul> <li>A. <code>Sales | extend Month = extract('month', OrderDate)</code></li> <li>B. <code>Sales | extend Month = month(OrderDate)</code></li> <li>C. <code>Sales | extend Month = getmonth(OrderDate)</code></li> </ul> <p>Answer: <code>Sales | extend Month = month(OrderDate)</code></p> </li> <li> <p>How do you calculate the variance of the <code>Duration</code> column in the <code>Sessions</code> table?</p> <ul> <li>A. <code>Sessions | summarize VarDuration = variance(Duration)</code></li> <li>B. <code>Sessions | summarize VarDuration = var(Duration)</code></li> <li>C. <code>Sessions | summarize VarDuration = varp(Duration)</code></li> </ul> <p>Answer: <code>Sessions | summarize VarDuration = var(Duration)</code></p> </li> <li> <p>Which KQL function would you use to count the number of non-null values in a column?</p> <ul> <li>A. <code>count()</code></li> <li>B. <code>countif()</code></li> <li>C. <code>count_not_null()</code></li> </ul> <p>Answer: <code>countif()</code></p> </li> <li> <p>How can you filter the <code>Orders</code> table to show only records where the <code>TotalAmount</code> is greater than 100?</p> <ul> <li>A. <code>Orders | where TotalAmount &gt; 100</code></li> <li>B. <code>Orders | filter TotalAmount &gt; 100</code></li> <li>C. <code>Orders | find TotalAmount &gt; 100</code></li> </ul> <p>Answer: <code>Orders | where TotalAmount &gt; 100</code></p> </li> <li> <p>How would you create a new column <code>Year</code> by extracting the year from the <code>PurchaseDate</code> column in the <code>Purchases</code> table?</p> <ul> <li>A. <code>Purchases | extend Year = year(PurchaseDate)</code></li> <li>B. <code>Purchases | extend Year = extract('year', PurchaseDate)</code></li> <li>C. <code>Purchases | extend Year = getyear(PurchaseDate)</code></li> </ul> <p>Answer: <code>Purchases | extend Year = year(PurchaseDate)</code></p> </li> <li> <p>Which function in KQL would you use to get the number of elements in an array?</p> <ul> <li>A. <code>count()</code></li> <li>B. <code>length()</code></li> <li>C. <code>array_length()</code></li> </ul> <p>Answer: <code>length()</code></p> </li> <li> <p>How do you calculate the sum of the <code>SalesAmount</code> column in the <code>Sales</code> table?</p> <ul> <li>A. <code>Sales | summarize TotalSales = sum(SalesAmount)</code></li> <li>B. <code>Sales | summarize TotalSales = adds(SalesAmount)</code></li> <li>C. <code>Sales | summarize TotalSales = cumulative_sum(SalesAmount)</code></li> </ul> <p>Answer: <code>Sales | summarize TotalSales = sum(SalesAmount)</code></p> </li> <li> <p>How can you find the earliest <code>StartDate</code> in the <code>Projects</code> table?</p> <ul> <li>A. <code>Projects | summarize EarliestStart = earliest(StartDate)</code></li> <li>B. <code>Projects | summarize EarliestStart = min(StartDate)</code></li> <li>C. <code>Projects | summarize EarliestStart = first(StartDate)</code></li> </ul> <p>Answer: <code>Projects | summarize EarliestStart = min(StartDate)</code></p> </li> <li> <p>Which KQL function is used to get the current date and time?</p> <ul> <li>A. <code>now()</code></li> <li>B. <code>current_datetime()</code></li> <li>C. <code>getdate()</code></li> </ul> <p>Answer: <code>now()</code></p> </li> <li> <p>How would you calculate the difference in months between two datetime columns <code>StartDate</code> and <code>EndDate</code> in the <code>Tasks</code> table?</p> <ul> <li>A. <code>Tasks | extend MonthsBetween = date_diff('month', EndDate, StartDate)</code></li> <li>B. <code>Tasks | extend MonthsBetween = datetime_diff('month', EndDate, StartDate)</code></li> <li>C. <code>Tasks | extend MonthsBetween = month_diff(EndDate, StartDate)</code></li> </ul> <p>Answer: <code>Tasks | extend MonthsBetween = date_diff('month', EndDate, StartDate)</code></p> </li> <li> <p>Which function in KQL would you use to get the sum of a column <code>Amount</code> in the <code>Transactions</code> table?</p> <ul> <li>A. <code>Transactions | summarize TotalAmount = sum(Amount)</code></li> <li>B. <code>Transactions | summarize TotalAmount = sum_amount(Amount)</code></li> <li>C. <code>Transactions | summarize TotalAmount = sum(Amount)</code></li> </ul> <p>Answer: <code>Transactions | summarize TotalAmount = sum(Amount)</code></p> </li> <li> <p>How can you filter the <code>Logs</code> table to show only records where the <code>Level</code> column is \"Error\"?</p> <ul> <li>A. <code>Logs | where Level == \"Error\"</code></li> <li>B. <code>Logs | where Level equals \"Error\"</code></li> <li>C. <code>Logs | filter Level == \"Error\"</code></li> </ul> <p>Answer: <code>Logs | where Level == \"Error\"</code></p> </li> <li> <p>How would you rename the <code>OldColumn</code> to <code>NewColumn</code> in the <code>Data</code> table?</p> <ul> <li>A. <code>Data | project NewColumn = OldColumn</code></li> <li>B. <code>Data | rename OldColumn as NewColumn</code></li> <li>C. <code>Data | project-rename NewColumn = OldColumn</code></li> </ul> <p>Answer: <code>Data | project-rename NewColumn = OldColumn</code></p> </li> <li> <p>Which function in KQL is used to count the number of elements in an array?</p> <ul> <li>A. <code>count()</code></li> <li>B. <code>length()</code></li> <li>C. <code>array_length()</code></li> </ul> <p>Answer: <code>length()</code></p> </li> <li> <p>How would you calculate the average <code>Price</code> for each product in the <code>Products</code> table?</p> <ul> <li>A. <code>Products | summarize AvgPrice = avg(Price) by ProductID</code></li> <li>B. <code>Products | summarize AvgPrice = mean(Price) by ProductID</code></li> <li>C. <code>Products | summarize AvgPrice = average(Price) by ProductID</code></li> </ul> <p>Answer: <code>Products | summarize AvgPrice = avg(Price) by ProductID</code></p> </li> <li> <p>How can you convert the <code>Revenue</code> column in the <code>Sales</code> table from a string to a real number?</p> <ul> <li>A. <code>Sales | project Revenue = toreal(Revenue)</code></li> <li>B. <code>Sales | extend Revenue = todouble(Revenue)</code></li> <li>C. <code>Sales | cast(Revenue as real)</code></li> </ul> <p>Answer: <code>Sales | project Revenue = toreal(Revenue)</code></p> </li> <li> <p>Which function would you use to extract the minute from a datetime column <code>EventTime</code> in the <code>Events</code> table?</p> <ul> <li>A. <code>Events | extend Minute = minute(EventTime)</code></li> <li>B. <code>Events | extend Minute = extract('minute', EventTime)</code></li> <li>C. <code>Events | extend Minute = getminute(EventTime)</code></li> </ul> <p>Answer: <code>Events | extend Minute = minute(EventTime)</code></p> </li> <li> <p>How can you create a column <code>FullAddress</code> by concatenating <code>Street</code>, <code>City</code>, and <code>ZipCode</code> in the <code>Addresses</code> table?</p> <ul> <li>A. <code>Addresses | extend FullAddress = strcat(Street, \", \", City, \", \", ZipCode)</code></li> <li>B. <code>Addresses | extend FullAddress = concat(Street, \", \", City, \", \", ZipCode)</code></li> <li>C. <code>Addresses | extend FullAddress = Street + \", \" + City + \", \" + ZipCode</code></li> </ul> <p>Answer: <code>Addresses | extend FullAddress = strcat(Street, \", \", City, \", \", ZipCode)</code></p> </li> <li> <p>How would you calculate the 95th percentile of the <code>LoadTime</code> column in the <code>WebRequests</code> table?</p> <ul> <li>A. <code>WebRequests | summarize Percentile_95 = percentile(LoadTime, 95)</code></li> <li>B. <code>WebRequests | summarize Percentile_95 = p95(LoadTime)</code></li> <li>C. <code>WebRequests | summarize Percentile_95 = percentile_approx(LoadTime, 0.95)</code></li> </ul> <p>Answer: <code>WebRequests | summarize Percentile_95 = percentile(LoadTime, 95)</code></p> </li> <li> <p>Which function in KQL would you use to replace all occurrences of a substring in a string column?</p> <ul> <li>A. <code>replace()</code></li> <li>B. <code>str_replace()</code></li> <li>C. <code>substitute()</code></li> </ul> <p>Answer: <code>replace()</code></p> </li> <li> <p>How can you find the maximum value of the <code>Salary</code> column in the <code>Employees</code> table?</p> <ul> <li>A. <code>Employees | summarize MaxSalary = max(Salary)</code></li> <li>B. <code>Employees | summarize MaxSalary = maximum(Salary)</code></li> <li>C. <code>Employees | summarize MaxSalary = highest(Salary)</code></li> </ul> <p>Answer: <code>Employees | summarize MaxSalary = max(Salary)</code></p> </li> <li> <p>How would you create a new column <code>Quarter</code> by extracting the quarter from the <code>OrderDate</code> column in the <code>Orders</code> table?</p> <ul> <li>A. <code>Orders | extend Quarter = extract('quarter', OrderDate)</code></li> <li>B. <code>Orders | extend Quarter = quarter(OrderDate)</code></li> <li>C. <code>Orders | extend Quarter = getquarter(OrderDate)</code></li> </ul> <p>Answer: <code>Orders | extend Quarter = quarter(OrderDate)</code></p> </li> <li> <p>How do you calculate the variance of the <code>ProcessingTime</code> column in the <code>Transactions</code> table?</p> <ul> <li>A. <code>Transactions | summarize VarProcessingTime = variance(ProcessingTime)</code></li> <li>B. <code>Transactions | summarize VarProcessingTime = var(ProcessingTime)</code></li> <li>C. <code>Transactions | summarize VarProcessingTime = varp(ProcessingTime)</code></li> </ul> <p>Answer: <code>Transactions | summarize VarProcessingTime = var(ProcessingTime)</code></p> </li> <li> <p>Which KQL function would you use to count the number of non-null values in a column?</p> <ul> <li>A. <code>count()</code></li> <li>B. <code>countif()</code></li> <li>C. <code>count_not_null()</code></li> </ul> <p>Answer: <code>countif()</code></p> </li> <li> <p>How can you filter the <code>Invoices</code> table to show only records where the <code>Total</code> is greater than 500?</p> <ul> <li>A. <code>Invoices | where Total &gt; 500</code></li> <li>B. <code>Invoices | filter Total &gt; 500</code></li> <li>C. <code>Invoices | find Total &gt; 500</code></li> </ul> <p>Answer: <code>Invoices | where Total &gt; 500</code></p> </li> <li> <p>How would you create a new column <code>Year</code> by extracting the year from the <code>Date</code> column in the <code>Events</code> table?</p> <ul> <li>A. <code>Events | extend Year = year(Date)</code></li> <li>B. <code>Events | extend Year = extract('year', Date)</code></li> <li>C. <code>Events | extend Year = getyear(Date)</code></li> </ul> <p>Answer: <code>Events | extend Year = year(Date)</code></p> </li> <li> <p>Which function in KQL would you use to get the sum of a column <code>Amount</code> in the <code>Payments</code> table?</p> <ul> <li>A. <code>Payments | summarize TotalAmount = sum(Amount)</code></li> <li>B. <code>Payments | summarize TotalAmount = sum_amount(Amount)</code></li> <li>C. <code>Payments | summarize TotalAmount = adds(Amount)</code></li> </ul> <p>Answer: <code>Payments | summarize TotalAmount = sum(Amount)</code></p> </li> <li> <p>How can you find the earliest <code>HireDate</code> in the <code>Employees</code> table?</p> <ul> <li>A. <code>Employees | summarize EarliestHire = earliest(HireDate)</code></li> <li>B. <code>Employees | summarize EarliestHire = min(HireDate)</code></li> <li>C. <code>Employees | summarize EarliestHire = first(HireDate)</code></li> </ul> <p>Answer: <code>Employees | summarize EarliestHire = min(HireDate)</code></p> </li> <li> <p>How would you calculate the difference in days between two datetime columns <code>StartDate</code> and <code>EndDate</code> in the <code>Projects</code> table?</p> <ul> <li>A. <code>Projects | extend DaysBetween = date_diff('day', EndDate, StartDate)</code></li> <li>B. <code>Projects | extend DaysBetween = datetime_diff('day', EndDate, StartDate)</code></li> <li>C. <code>Projects | extend DaysBetween = day_diff(EndDate, StartDate</code></li> </ul> <p>Answer: <code>Projects | extend DaysBetween = date_diff('day', EndDate, StartDate)</code></p> </li> <li> <p>How can you find the latest <code>EndDate</code> in the <code>Tasks</code> table?</p> <ul> <li>A. <code>Tasks | summarize LatestEnd = latest(EndDate)</code></li> <li>B. <code>Tasks | summarize LatestEnd = max(EndDate)</code></li> <li>C. <code>Tasks | summarize LatestEnd = last(EndDate)</code></li> </ul> <p>Answer: <code>Tasks | summarize LatestEnd = max(EndDate)</code></p> </li> <li> <p>Which function in KQL would you use to concatenate multiple string columns in the <code>Products</code> table?</p> <ul> <li>A. <code>Products | extend FullDescription = concat(Description, \" - \", Category)</code></li> <li>B. <code>Products | extend FullDescription = strcat(Description, \" - \", Category)</code></li> <li>C. <code>Products | extend FullDescription = joinstr(Description, \" - \", Category)</code></li> </ul> <p>Answer: <code>Products | extend FullDescription = strcat(Description, \" - \", Category)</code></p> </li> <li> <p>How can you calculate the total number of orders in the <code>Orders</code> table?</p> <ul> <li>A. <code>Orders | summarize TotalOrders = count()</code></li> <li>B. <code>Orders | count()</code></li> <li>C. <code>Orders | summarize TotalOrders = count(OrderID)</code></li> </ul> <p>Answer: <code>Orders | summarize TotalOrders = count()</code></p> </li> <li> <p>How would you find all records in the <code>Errors</code> table where the <code>Message</code> column contains the word \"timeout\"?</p> <ul> <li>A. <code>Errors | where Message contains \"timeout\"</code></li> <li>B. <code>Errors | where Message == \"timeout\"</code></li> <li>C. <code>Errors | search \"timeout\"</code></li> </ul> <p>Answer: <code>Errors | where Message contains \"timeout\"</code></p> </li> <li> <p>How would you list the unique values of the <code>Status</code> column from the <code>Tasks</code> table?</p> <ul> <li>A. <code>Tasks | distinct Status</code></li> <li>B. <code>Tasks | summarize by Status</code></li> <li>C. <code>Tasks | unique Status</code></li> </ul> <p>Answer: <code>Tasks | distinct Status</code></p> </li> <li> <p>How can you calculate the average <code>ResponseTime</code> for each URL in the <code>WebRequests</code> table?</p> <ul> <li>A. <code>WebRequests | summarize AvgResponseTime = avg(ResponseTime) by URL</code></li> <li>B. <code>WebRequests | summarize AvgResponseTime = mean(ResponseTime) by URL</code></li> <li>C. <code>WebRequests | summarize AvgResponseTime = average(ResponseTime) by URL</code></li> </ul> <p>Answer: <code>WebRequests | summarize AvgResponseTime = avg(ResponseTime) by URL</code></p> </li> <li> <p>How would you calculate the cumulative sum of the <code>Revenue</code> column in the <code>Sales</code> table?</p> <ul> <li>A. <code>Sales | extend CumulativeRevenue = sum(Revenue)</code></li> <li>B. <code>Sales | extend CumulativeRevenue = running_sum(Revenue)</code></li> <li>C. <code>Sales | extend CumulativeRevenue = cumulative_sum(Revenue)</code></li> </ul> <p>Answer: <code>Sales | extend CumulativeRevenue = cumulative_sum(Revenue)</code></p> </li> <li> <p>Which function in KQL would you use to get the maximum value of a column <code>Value</code> in the <code>Metrics</code> table?</p> <ul> <li>A. <code>Metrics | summarize MaxValue = max(Value)</code></li> <li>B. <code>Metrics | summarize MaxValue = maximum(Value)</code></li> <li>C. <code>Metrics | summarize MaxValue = greatest(Value)</code></li> </ul> <p>Answer: <code>Metrics | summarize MaxValue = max(Value)</code></p> </li> <li> <p>How can you filter the <code>Activities</code> table to show only records where the <code>ActivityDate</code> is in the current month?</p> <ul> <li>A. <code>Activities | where month(ActivityDate) == month(now())</code></li> <li>B. <code>Activities | where ActivityDate &gt;= startofmonth(now())</code></li> <li>C. <code>Activities | where ActivityDate between (startofmonth(now()) .. endofmonth(now()))</code></li> </ul> <p>Answer: <code>Activities | where month(ActivityDate) == month(now())</code></p> </li> <li> <p>Which function in KQL is used to split a string into an array based on a delimiter?</p> <ul> <li>A. <code>split()</code></li> <li>B. <code>string_split()</code></li> <li>C. <code>explode()</code></li> </ul> <p>Answer: <code>split()</code></p> </li> <li> <p>How would you calculate the 75th percentile of the <code>ProcessingTime</code> column in the <code>Operations</code> table?</p> <ul> <li>A. <code>Operations | summarize Percentile_75 = percentile(ProcessingTime, 75)</code></li> <li>B. <code>Operations | summarize Percentile_75 = p75(ProcessingTime)</code></li> <li>C. <code>Operations | summarize Percentile_75 = percentile_approx(ProcessingTime, 0.75)</code></li> </ul> <p>Answer: <code>Operations | summarize Percentile_75 = percentile(ProcessingTime, 75)</code></p> </li> <li> <p>How would you calculate the median value of the <code>Income</code> column in the <code>Employees</code> table?</p> <ul> <li>A. <code>Employees | summarize MedianIncome = median(Income)</code></li> <li>B. <code>Employees | summarize MedianIncome = percentile(Income, 50)</code></li> <li>C. <code>Employees | summarize MedianIncome = avg(Income)</code></li> </ul> <p>Answer: <code>Employees | summarize MedianIncome = percentile(Income, 50)</code></p> </li> <li> <p>How can you list the unique values of the <code>Category</code> column from the <code>Products</code> table?</p> <ul> <li>A. <code>Products | distinct Category</code></li> <li>B. <code>Products | summarize by Category</code></li> <li>C. <code>Products | unique Category</code></li> </ul> <p>Answer: <code>Products | distinct Category</code></p> </li> <li> <p>Which function in KQL is used to format a datetime value as a string?</p> <ul> <li>A. <code>format_datetime()</code></li> <li>B. <code>datetime_to_string()</code></li> <li>C. <code>tostring()</code></li> </ul> <p>Answer: <code>format_datetime()</code></p> </li> <li> <p>How can you create a new column <code>Month</code> by extracting the month from the <code>Timestamp</code> column in the <code>Events</code> table?</p> <ul> <li>A. <code>Events | extend Month = extract('month', Timestamp)</code></li> <li>B. <code>Events | extend Month = month(Timestamp)</code></li> <li>C. <code>Events | extend Month = getmonth(Timestamp)</code></li> </ul> <p>Answer: <code>Events | extend Month = month(Timestamp)</code></p> </li> <li> <p>How do you filter the <code>Orders</code> table to include only those where the <code>OrderDate</code> is within the last 7 days?</p> <ul> <li>A. <code>Orders | where OrderDate &gt; ago(7d)</code></li> <li>B. <code>Orders | where OrderDate &gt;= startofweek(now())</code></li> <li>C. <code>Orders | where OrderDate between (now() - 7d) and now()</code></li> </ul> <p>Answer: <code>Orders | where OrderDate &gt; ago(7d)</code></p> </li> <li> <p>How would you extract the year from a <code>datetime</code> column called <code>Timestamp</code> in the <code>Logs</code> table?</p> <ul> <li>A. <code>Logs | extend Year = year(Timestamp)</code></li> <li>B. <code>Logs | extend Year = extract('year', Timestamp)</code></li> <li>C. <code>Logs | extend Year = getyear(Timestamp)</code></li> </ul> <p>Answer: <code>Logs | extend Year = year(Timestamp)</code></p> </li> <li> <p>How would you rename the column <code>OldName</code> to <code>NewName</code> in a KQL query?</p> <ul> <li>A. <code>| project-rename NewName = OldName</code></li> <li>B. <code>| rename OldName to NewName</code></li> <li>C. <code>| project OldName as NewName</code></li> </ul> <p>Answer: <code>| project OldName as NewName</code></p> </li> <li> <p>Which function in KQL would you use to get the current date and time?</p> <ul> <li>A. <code>now()</code></li> <li>B. <code>current_datetime()</code></li> <li>C. <code>getdate()</code></li> </ul> <p>Answer: <code>now()</code></p> </li> <li> <p>How can you create a column <code>FullName</code> by concatenating <code>FirstName</code> and <code>LastName</code> in the <code>Employees</code> table?</p> <ul> <li>A. <code>Employees | extend FullName = FirstName + \" \" + LastName</code></li> <li>B. <code>Employees | extend FullName = strcat(FirstName, \" \", LastName)</code></li> <li>C. <code>Employees | extend FullName = concat(FirstName, \" \", LastName)</code></li> </ul> <p>Answer: <code>Employees | extend FullName = strcat(FirstName, \" \", LastName)</code></p> </li> <li> <p>How would you find all records in the <code>Logs</code> table where the <code>Message</code> column contains the word \"error\"?</p> <ul> <li>A. <code>Logs | where Message contains \"error\"</code></li> <li>B. <code>Logs | where Message == \"error\"</code></li> <li>C. <code>Logs | search \"error\"</code></li> </ul> <p>Answer: <code>Logs | where Message contains \"error\"</code></p> </li> <li> <p>Which KQL function would you use to convert a string to a datetime value?</p> <ul> <li>A. <code>todatetime()</code></li> <li>B. <code>string_to_datetime()</code></li> <li>C. <code>datetime()</code></li> </ul> <p>Answer: <code>todatetime()</code></p> </li> <li> <p>How would you filter rows in the <code>Sales</code> table to only include those where the <code>Date</code> is within the last 30 days?</p> <ul> <li>A. <code>Sales | where Date &gt; ago(30d)</code></li> <li>B. <code>Sales | where Date between (now() - 30d) and now()</code></li> <li>C. <code>Sales | where Date &gt; datetime(30 days ago)</code></li> </ul> <p>Answer: <code>Sales | where Date &gt; ago(30d)</code></p> </li> <li> <p>How do you join two tables <code>Table1</code> and <code>Table2</code> on the <code>ID</code> column, keeping all records from <code>Table1</code>?</p> <ul> <li>A. <code>Table1 | innerjoin (Table2) on ID</code></li> <li>B. <code>Table1 | join kind=inner (Table2) on ID</code></li> <li>C. <code>Table1 | join kind=leftouter (Table2) on ID</code> Answer: <code>Table1 | join kind=leftouter (Table2) on ID</code></li> </ul> </li> <li> <p>How would you list the top 10 products by total sales in the <code>Sales</code> table?</p> <ul> <li>A. <code>Sales | top 10 by sum(TotalSales)</code></li> <li>B. <code>Sales | summarize TotalSales = sum(SalesAmount) by ProductID | top 10 by TotalSales</code></li> <li>C. <code>Sales | summarize TotalSales = sum(SalesAmount) by ProductID | limit 10 by TotalSales</code></li> </ul> <p>Answer: <code>Sales | summarize TotalSales = sum(SalesAmount) by ProductID | top 10 by TotalSales</code></p> </li> <li> <p>How would you create a time series chart for the <code>Sales</code> table's total revenue over time?</p> <ul> <li>A. <code>Sales | summarize TotalRevenue = sum(Revenue) by bin(Time, 1h) | render timechart</code></li> <li>B. <code>Sales | summarize TotalRevenue = sum(Revenue) by bin(Time, 1h) | render linechart</code></li> <li>C. <code>Sales | summarize TotalRevenue = sum(Revenue) by bin(Time, 1h) | render barchart</code></li> </ul> <p>Answer: <code>Sales | summarize TotalRevenue = sum(Revenue) by bin(Time, 1h) | render linechart</code></p> </li> <li> <p>How can you calculate the total number of distinct customers in the <code>Sales</code> table?</p> <ul> <li>A. <code>Sales | summarize DistinctCustomers = dcount(CustomerID)</code></li> <li>B. <code>Sales | summarize DistinctCustomers = countdistinct(CustomerID)</code></li> <li>C. <code>Sales | summarize DistinctCustomers = unique(CustomerID)</code></li> </ul> <p>Answer: <code>Sales | summarize DistinctCustomers = dcount(CustomerID)</code></p> </li> <li> <p>How can you concatenate the values of two columns <code>FirstName</code> and <code>LastName</code> in the <code>Contacts</code> table?</p> <ul> <li>A. <code>Contacts | extend FullName = strcat(FirstName, \" \", LastName)</code></li> <li>B. <code>Contacts | extend FullName = concat(FirstName, \" \", LastName)</code></li> <li>C. <code>Contacts | extend FullName = joinstr(FirstName, \" \", LastName)</code></li> </ul> <p>Answer: <code>Contacts | extend FullName = strcat(FirstName, \" \", LastName)</code></p> </li> </ol>"},{"location":"Microsoft-Fabric/PandasVsSparkDf.html","title":"Pandas vs Spark","text":"<p>Here is a question from Microsoft Fabric Certificaiton exam:</p> <p>You have a Parquet file named Customers.parquet uploaded to the Files section of a Fabric lakehouse.</p> <p>You plan to use Data Wrangler to view basic summary statistics for the data before you load it to a Delta table.</p> <p>You open a notebook in the lakehouse.</p> <p>You need to load the data to a pandas DataFrame.</p> <p>Which PySpark code should you run to complete the task?</p> <p>Select only one answer.</p> <p>df = pandas.read_parquet(\"/lakehouse/default/Files/Customers.parquet\")</p> <p>df = pandas.read_parquet(\"/lakehouse/Files/Customers.parquet\") This answer is incorrect.</p> <p>import pandas as pd df = pd.read_parquet(\"/lakehouse/default/Files/Customers.parquet\") This answer is correct.</p> <p>import pandas as pd df = pd.read_parquet(\"/lakehouse/Files/Customers.parquet\") To load data to a pandas DataFrame, you must first import the pandas library by running import pandas as pd. Pandas DataFrames use the File API Path vs. the File relative path that Spark uses. The File API Path has the format of lakehouse/default/Files/Customers.parquet.</p> <p></p>"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL.html","title":"PySpark & SQL","text":""},{"location":"Microsoft-Fabric/Pyspark_SparkSQL.html#creating-a-sparksession","title":"Creating a SparkSession","text":"<p><pre><code>from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"xxx\").getOrCreate()\ndf = spark.read.csv(\"abc.csv\", header=True, inferSchema=True)\n</code></pre> </p>"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL.html#creating-a-dataframe","title":"Creating a DataFrame","text":""},{"location":"Microsoft-Fabric/Pyspark_SparkSQL.html#showing-dataframe-rows-describing-tables","title":"Showing dataframe, rows describing tables","text":""},{"location":"Microsoft-Fabric/Pyspark_SparkSQL.html#dfshow","title":"df.show()","text":"<p><pre><code>df.show()\ndf.show(n=10, truncate=False)\n</code></pre> </p>"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL.html#displaydf","title":"display(df)","text":"<p><pre><code>display(df.limit(3))\n</code></pre> </p>"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL.html#dfdescribe","title":"df.describe()","text":"<p>describe is used to generate descriptive statistics of the DataFrame. For numeric data, results include COUNT, MEAN, STD, MIN, and MAX, while for object data it will also include TOP, UNIQUE, and FREQ.</p> <p><pre><code>df.describe()\ndf.describe().show()\n</code></pre> </p> <pre><code>df.printSchema()\ndf.columns\n</code></pre>"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL.html#describe-formatted-tablename","title":"DESCRIBE FORMATTED tableName","text":"<p><pre><code>spark.sql(\"DESCRIBE FORMATTED tableName\")\n</code></pre> </p>"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL.html#sql","title":"SQL","text":"<p>CREATE OR REPLACE VIEW  doesn't work in Fabric/AzureSynapse/ADF etc. Instead use this: <pre><code>If Exists (Select * From sys.sysobjects where name = 'apple')\n    DROP TABLE dbo.apple;\nGO\n</code></pre>"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL.html#dropping-a-table","title":"Dropping a table","text":"<p>Small dataframe</p> <p>df.limit(100)</p> <ol> <li> <p>Selecting Columns <pre><code>df.select(\"column1\", \"column2\").show()\n</code></pre></p> </li> <li> <p>Filtering Data <pre><code>df.filter(df[\"column\"] &gt; value).show()\ndf.filter(df[\"column\"] == \"value\").show()\n</code></pre></p> </li> <li> <p>Adding Columns <pre><code>df.withColumn(\"new_column\", df[\"existing_column\"] * 2).show()\n</code></pre></p> </li> <li> <p>Renaming Columns <pre><code>df.withColumnRenamed(\"old_name\", \"new_name\")\n</code></pre> </p> </li> <li> <p>Dropping Columns <pre><code>df.drop(\"column_name\")\n</code></pre></p> </li> <li> <p>Grouping and Aggregating <pre><code>df.groupBy(\"column\").count().show()\ndf.groupBy(\"column\").agg({\"column2\": \"avg\", \"column3\": \"sum\"}).show()\n</code></pre> </p> </li> <li>Sorting Data <pre><code>df.orderBy(\"column\").show()\ndf.orderBy(df[\"column\"].desc()).show()\n</code></pre></li> </ol>"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL.html#rdd-operations","title":"RDD Operations","text":"<ol> <li> <p>Creating an RDD <pre><code>rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n</code></pre></p> </li> <li> <p>Transformations <pre><code>rdd2 = rdd.map(lambda x: x * 2)\nrdd3 = rdd.filter(lambda x: x % 2 == 0)\n</code></pre></p> </li> <li> <p>Actions <pre><code>rdd.collect()\nrdd.count()\nrdd.first()\nrdd.take(3)\n</code></pre></p> </li> </ol>"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL.html#sql-operations","title":"SQL Operations","text":"<ol> <li> <p>Creating Temp View <pre><code>df.createOrReplaceTempView(\"table_name\")\n</code></pre></p> </li> <li> <p>Running SQL Queries <pre><code>spark.sql(\"SELECT * FROM table_name\").show()\n</code></pre></p> </li> </ol>"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL.html#saving-data","title":"Saving Data","text":"<ol> <li> <p>Saving as CSV <pre><code>df.write.csv(\"path/to/save.csv\")\n</code></pre></p> </li> <li> <p>Saving as Parquet <pre><code>df.write.parquet(\"path/to/save.parquet\")\n</code></pre></p> </li> <li> <p>Saving to Hive <pre><code>df.write.saveAsTable(\"table_name\")\n</code></pre> </p> </li> </ol>"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL.html#miscellaneous","title":"Miscellaneous","text":"<ol> <li> <p>Caching and Unpersisting DataFrames <pre><code>df.cache()\ndf.unpersist()\n</code></pre></p> </li> <li> <p>Explain Plan <pre><code>df.explain()\n</code></pre></p> </li> <li> <p>Repartitioning Data <pre><code>df.repartition(10)\ndf.coalesce(5)\n</code></pre></p> </li> </ol>"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL.html#pyspark-whenconditionotherwisedefault","title":"Pyspark when(condition).otherwise(default)","text":"<pre><code>from pyspark.sql.functions import col, when\n\nresult = when(col(\"Age\") &gt; 16, True).otherwise(False)\n</code></pre>"},{"location":"Microsoft-Fabric/Pyspark_SparkSQL.html#remember","title":"Remember","text":"<p>The GroupBY columns must match the columns used in the SELECT statement.</p> <p>DENSE_RANK() function returns the rank of each row within the result set partition, with no gaps in the ranking values. The RANK() function includes gaps in the ranking.</p>"},{"location":"Microsoft-Fabric/RealTimeAnalytics.html","title":"Real-time Intelligence - Microsoft Fabric","text":""},{"location":"Microsoft-Fabric/RealTimeAnalytics.html#core-elements-of-real-time-intelligence-in-microsoft-fabric","title":"Core elements of Real-Time Intelligence in Microsoft Fabric","text":""},{"location":"Microsoft-Fabric/RealTimeAnalytics.html#eventhouse","title":"Eventhouse","text":"<ul> <li>Central workspace/hub - has multiple KQL databases</li> <li>Use an Eventhouse for event-based scenarios</li> <li>Automatically index and partition data based on ingestion time</li> <li>When you create an EventHouse, it initializes a KQL database with the same name inside it</li> <li>KQL databses can be standalone or part of an EventHouse</li> <li>Can ingest data from multiple sources</li> </ul>"},{"location":"Microsoft-Fabric/RealTimeAnalytics.html#kql-database","title":"KQL Database","text":"<p>A KQL (Kusto Query Language) Database handles large volumes of structured, semi-structured, and unstructured data for real-time analytics and ad-hoc querying. It is part of the Azure Data Explorer service. The data in a KQL database is stored in Azure Data Explorer. It uses a columnar storage format, for high-performance.</p>"},{"location":"Microsoft-Fabric/RealTimeAnalytics.html#kql-queryset","title":"KQL Queryset","text":"<p>It is a just a query written in KQL. Let's not make it more complex than that!</p>"},{"location":"Microsoft-Fabric/RealTimeAnalytics.html#real-time-dashboards","title":"Real-Time Dashboards","text":"<ul> <li>Customizable control panels for displaying specific data.</li> <li>Tiles for different data views, organized on various pages.</li> <li>Export KQL queries into visual tiles.</li> <li>Enhances data exploration and visualization.</li> </ul>"},{"location":"Microsoft-Fabric/RealTimeAnalytics.html#eventstream","title":"Eventstream","text":"<ul> <li>Handles live data without coding.</li> <li>Automates data collection, transformation, and distribution.</li> <li>Processes real-time data for immediate insights.</li> </ul>"},{"location":"Microsoft-Fabric/RealTimeAnalytics.html#lets-get-started","title":"Let's Get Started","text":"<ul> <li>Open Real-Time Intelligence: Select it from the bottom left-hand corner of the screen:</li> </ul> <ul> <li>Start Screen: Here is how it looks:</li> </ul> <ul> <li> <p>Create an Eventhouse: Eventhouses are groups of databases. When you create an Eventhouse, Fabric creates a KQL Database with the same name inside it</p> </li> <li> <p>KQL Database Structure: This is the standard structure:</p> </li> </ul> <p></p> <p>Data Ingestion: Importing data from files like .csv is a childs play using the GUI, which creates a fully structured database with the correct columns.</p> <p></p>"},{"location":"Microsoft-Fabric/RealTimeAnalytics.html#create-a-power-bi-report-from-kql-queryset","title":"Create a Power BI Report from KQL Queryset","text":"<ol> <li>Click the three dots next to the table.</li> <li>Select \"Show any 100 records\" to open the KQL editor.</li> <li>Create your custom KQL query.</li> <li>Save the query or create the Power BI report directly from the editor.</li> </ol>"},{"location":"Misc/10_Fact_vs_Dimension_tables.html","title":"Fact vs Dimension","text":""},{"location":"Misc/10_Fact_vs_Dimension_tables.html#fact-table-vs-dimension-table-simple-explanation","title":"Fact Table vs. Dimension Table (Simple Explanation)","text":"<p>In simple terms, a fact table is where you store numbers or quantities, and a dimension table is where you store descriptions.</p> <p>Just remember these: </p> <ul> <li>Fact tables = verbs(e.g. Orders). Dimension tables = nowns(products, customers).</li> <li>Fact  tables are heavy. Dimension tables are light.</li> <li>The fact table has the measurements (numbers) and has the primary keys to link to the descriptive information in dimension tables.</li> </ul>"},{"location":"Misc/1_GoogleCloudSeeUsage.html","title":"GCP Usage","text":""},{"location":"Misc/1_GoogleCloudSeeUsage.html#background","title":"Background","text":"<p>Worried about what resources are running and causing costs? You can check them directly from the cloud console. Here are some of the useful commands to check it.</p>"},{"location":"Misc/1_GoogleCloudSeeUsage.html#single-command-to-see-it-all","title":"Single command to see it all","text":"<p>Copy and paste this command in your cloud console to get an overview.</p> <pre><code>gcloud compute instances list &amp;&amp; \\\ngcloud container clusters list &amp;&amp; \\\ngcloud storage buckets list &amp;&amp; \\\ngcloud sql instances list &amp;&amp; \\\ngcloud functions list &amp;&amp; \\\ngcloud app services list\n</code></pre> <p></p>"},{"location":"Misc/1_GoogleCloudSeeUsage.html#separate-commands","title":"Separate commands","text":"<p>You can run the following commands in your cloud console to see each service if they are running or not.</p> Task Command Description List All Compute Engine VM Instances <code>gcloud compute instances list</code> Shows all the virtual machines running in your project, with details like zone, machine type, and status. List Kubernetes Clusters <code>gcloud container clusters list</code> Shows all the Kubernetes Engine clusters in your project. List Cloud Storage Buckets <code>gcloud storage buckets list</code> Lists all the storage buckets in your project. List Cloud SQL Instances <code>gcloud sql instances list</code> Shows all the Cloud SQL instances in your project. List App Engine Services <code>gcloud app services list</code> Shows all the services you have deployed in App Engine. List Cloud Functions <code>gcloud functions list</code> Lists all the Cloud Functions running in your project. List BigQuery Datasets <code>bq ls</code> Lists all the BigQuery datasets in your project. List All Resources in Your Project <code>gcloud projects describe $(gcloud config get-value project)</code> Gives an overview of your project, but doesn\u2019t list every resource. Use specific commands for detailed info. Check Billing Information <code>gcloud alpha billing accounts list</code> Shows billing accounts, but might not give a detailed cost breakdown. List Active Firewall Rules (Networking) <code>gcloud compute firewall-rules list</code> Lists all the firewall rules, helping you see network settings that might be in use. List All Active Services <code>gcloud services list --enabled</code> Shows all the enabled APIs and services in your Google Cloud project."},{"location":"Misc/2_InstallVMWareFree.html","title":"How to Download and Install VMware Workstation Pro from Broadcom And Use for Free!","text":"<ol> <li>Visit Broadcom's Website:    Go to broadcom.com.</li> </ol> <ol> <li>Access the Support Portal:    In the top right corner, click on 'Support Portal'.  </li> <li>If you have an account, click 'Go To Portal' to log in.</li> <li> <p>If not, click 'Register' to create a basic Broadcom account.</p> </li> <li> <p>Go to VMware Cloud Foundation:    After logging in, if you're not automatically redirected, visit support.broadcom.com.    From the dropdown, select the 'VMware Cloud Foundation' division.</p> </li> </ol> <p></p> <ol> <li> <p>Find Your Downloads:    On the left side, click on 'My Downloads'.    Search for 'Fusion' or <code>Workstation</code>.</p> </li> <li> <p>Download VMware Workstation Pro:    Click on 'VMware Workstation Pro for Windows'.  </p> </li> <li>You\u2019ll see a dropdown for the Personal Use edition (same as the Commercial version).</li> <li>Select the version you want (17.5.2 or 13.5.2).</li> </ol> <p></p> <ol> <li>Install and Set Up:  </li> <li>Download and install the software.  </li> <li>When asked for a key, just skip it.  </li> <li>After installation, open the software, select 'personal use', and you're all set to create VM images!</li> </ol> <p></p>"},{"location":"Misc/2_InstallVMWareFree.html#install-and-use-linux-os","title":"Install and use Linux OS","text":"<p>Download Ubuntu Linux. Its free on their website. It will be dowloaded as <code>ubuntu-24.04-desktop-amd64.iso</code> or some other version.</p> <p>Then Use the VMWare workstation to create the image</p> <p></p> <p></p> <p></p>"},{"location":"Misc/3_Markdown.html","title":"Fontsh1 in LovelexieHandwritten Architects Daughter h1 in Gunny Rewritten h1 in Permanent Marker h1 Comic Sans MS   # Escape PIPE character  In markdown using | will create confusion. Use |   # HTML Colors  This guide helps you quickly find and use HTML colors. You can style your text by using the following format: **\\The text\\**. This changes the color of \"The text\" to blue.    Basic Colors Extended Colors Named Colors   Red Aqua AliceBlue   Green Fuchsia AntiqueWhite   Blue Lime Aquamarine   Yellow Maroon Azure   Cyan Navy Beige   Magenta Olive Bisque   Black Purple BlanchedAlmond   White Silver BlueViolet   Gray Teal Brown     BurlyWood     CadetBlue     Chartreuse     Chocolate     Coral     CornflowerBlue     Cornsilk     Crimson     DarkBlue     DarkCyan     DarkGoldenRod     ## Almost all colors  | Color Name | Hex Code | Example | |------------|----------|---------| | Black      | #000000  | This is a sample text\u25a0 | | White      | #FFFFFF  | \u25a0 | | Red        | #FF0000  | This is a sample text\u25a0 | | Lime       | #00FF00  | This is a sample text\u25a0 | | Blue       | #0000FF  | This is a sample text\u25a0 | | Yellow     | #FFFF00  | This is a sample text\u25a0 | | Cyan       | #00FFFF  | This is a sample text\u25a0 | | Magenta    | #FF00FF  | This is a sample text\u25a0 | | Silver     | #C0C0C0  | This is a sample text\u25a0 | | Gray       | #808080  | This is a sample text\u25a0 | | Maroon     | #800000  | This is a sample text\u25a0 | | Olive      | #808000  | This is a sample text\u25a0 | | Green      | #008000  | This is a sample text\u25a0 | | Purple     | #800080  | This is a sample text\u25a0 | | Teal       | #008080  | This is a sample text\u25a0 | | Navy       | #000080  | This is a sample text\u25a0 | | Orange     | #FFA500  | This is a sample text\u25a0 | | AliceBlue  | #F0F8FF  | This is a sample text\u25a0 | | AntiqueWhite | #FAEBD7 | This is a sample text\u25a0 | | Aqua       | #00FFFF  | This is a sample text\u25a0 | | Aquamarine | #7FFFD4  | This is a sample text\u25a0 | | Azure      | #F0FFFF  | This is a sample text\u25a0 | | Beige      | #F5F5DC  | This is a sample text\u25a0 | | Bisque     | #FFE4C4  | This is a sample text\u25a0 | | BlanchedAlmond | #FFEBCD | This is a sample text\u25a0 | | BlueViolet | #8A2BE2  | This is a sample text\u25a0 | | Brown      | #A52A2A  | This is a sample text\u25a0 | | BurlyWood  | #DEB887  | This is a sample text\u25a0 | | CadetBlue  | #5F9EA0  | This is a sample text\u25a0 | | Chartreuse | #7FFF00  | This is a sample text\u25a0 | | Chocolate  | #D2691E  | This is a sample text\u25a0 | | Coral      | #FF7F50  | This is a sample text\u25a0 | | CornflowerBlue | #6495ED | This is a sample text\u25a0 | | Cornsilk   | #FFF8DC  | This is a sample text\u25a0 | | Crimson    | #DC143C  | This is a sample text\u25a0 | | DarkBlue   | #00008B  | This is a sample text\u25a0 | | DarkCyan   | #008B8B  | This is a sample text\u25a0 | | DarkGoldenRod | #B8860B | This is a sample text\u25a0 | | DarkGray   | #A9A9A9  | This is a sample text\u25a0 | | DarkGreen  | #006400  | This is a sample text\u25a0 | | DarkKhaki  | #BDB76B  | This is a sample text\u25a0 | | DarkMagenta | #8B008B | This is a sample text\u25a0 | | DarkOliveGreen | #556B2F | This is a sample text\u25a0 | | Darkorange | #FF8C00  | This is a sample text\u25a0 | | DarkOrchid | #9932CC  | This is a sample text\u25a0 | | DarkRed    | #8B0000  | This is a sample text\u25a0 | | DarkSalmon | #E9967A  | This is a sample text\u25a0 | | DarkSeaGreen | #8FBC8F | This is a sample text\u25a0 | | DarkSlateBlue | #483D8B | This is a sample text\u25a0 | | DarkSlateGray | #2F4F4F | This is a sample text\u25a0 | | DarkTurquoise | #00CED1 | This is a sample text\u25a0 | | DarkViolet | #9400D3  | This is a sample text\u25a0 | | DeepPink   | #FF1493  | This is a sample text\u25a0 | | DeepSkyBlue | #00BFFF  | This is a sample text\u25a0 | | DimGray    | #696969  | This is a sample text\u25a0 | | DodgerBlue | #1E90FF  | This is a sample text\u25a0 | | FireBrick  | #B22222  | This is a sample text\u25a0 | | FloralWhite | #FFFAF0 | This is a sample text\u25a0 | | ForestGreen | #228B22 | This is a sample text\u25a0 | | Fuchsia    | #FF00FF  | This is a sample text\u25a0 | | Gainsboro  | #DCDCDC  | This is a sample text\u25a0 | | GhostWhite | #F8F8FF  | This is a sample text\u25a0 | | Gold       | #FFD700  | This is a sample text\u25a0 | | GoldenRod  | #DAA520  | This is a sample text\u25a0 | | GreenYellow | #ADFF2F | This is a sample text\u25a0 | | HoneyDew   | #F0FFF0  | This is a sample text\u25a0 | | HotPink    | #FF69B4  | This is a sample text\u25a0 | | IndianRed  | #CD5C5C  | This is a sample text\u25a0 | | Indigo     | #4B0082  | This is a sample text\u25a0 | | Ivory      | #FFFFF0  | This is a sample text\u25a0 | | Khaki      | #F0E68C  | This is a sample text\u25a0 | | Lavender   | #E6E6FA  | This is a sample text\u25a0 | | LavenderBlush | #FFF0F5 | This is a sample text\u25a0 | | LawnGreen  | #7CFC00  | This is a sample text\u25a0 | | LemonChiffon | #FFFACD | This is a sample text\u25a0 | | LightBlue  | #ADD8E6  | This is a sample text\u25a0 | | LightCoral | #F08080  | This is a sample text\u25a0 | | LightCyan  | #E0FFFF  | This is a sample text\u25a0 | | LightGoldenRodYellow | #FAFAD2 | This is a sample text\u25a0 | | LightGray  | #D3D3D3  | This is a sample text\u25a0 | | LightGreen | #90EE90  | This is a sample text\u25a0 | | LightPink  | #FFB6C1  | This is a sample text\u25a0 | | LightSalmon | #FFA07A | This is a sample text\u25a0 | | LightSeaGreen | #20B2AA | This is a sample text\u25a0 | | LightSkyBlue | #87CEFA | This is a sample text\u25a0 | | LightSlateGray | #778899 | This is a sample text\u25a0 | | LightSteelBlue | #B0C4DE | This is a sample text\u25a0 | | LightYellow | #FFFFE0 | This is a sample text\u25a0 | | LimeGreen  | #32CD32  | This is a sample text\u25a0 | | Linen      | #FAF0E6  | This is a sample text\u25a0 | | MediumAquaMarine | #66CDAA | This is a sample text\u25a0 | | MediumBlue | #0000CD  | This is a sample text\u25a0 | | MediumOrchid | #BA55D3 | This is a sample text\u25a0 | | MediumPurple | #9370DB | This is a sample text\u25a0 | | MediumSeaGreen | #3CB371 | This is a sample text\u25a0 | | MediumSlateBlue | #7B68EE | This is a sample text\u25a0 | | MediumSpringGreen | #00FA9A | This is a sample text\u25a0 | | MediumTurquoise | #48D1CC | This is a sample text\u25a0 | | MediumVioletRed | #C71585 | This is a sample text\u25a0 | | MidnightBlue | #191970 | This is a sample text\u25a0 | | MintCream  | #F5FFFA  | This is a sample text\u25a0 | | MistyRose  | #FFE4E1  | This is a sample text\u25a0 | | Moccasin   | #FFE4B5  | This is a sample text\u25a0 | | NavajoWhite | #FFDEAD | This is a sample text\u25a0 | | OldLace    | #FDF5E6  | This is a sample text\u25a0 | | OliveDrab  | #6B8E23  | This is a sample text\u25a0 | | OrangeRed  | #FF4500  | This is a sample text\u25a0 | | Orchid     | #DA70D6  | This is a sample text\u25a0 | | PaleGoldenRod | #EEE8AA | This is a sample text\u25a0 | | PaleGreen  | #98FB98  | This is a sample text\u25a0 | | PaleTurquoise | #AFEEEE | This is a sample text\u25a0 | | PaleVioletRed | #DB7093 | This is a sample text\u25a0 | | PapayaWhip | #FFEFD5  | This is a sample text\u25a0 | | PeachPuff  | #FFDAB9  | This is a sample text\u25a0 | | Peru       | #CD853F  | This is a sample text\u25a0 | | Pink       | #FFC0CB  | This is a sample text\u25a0 | | Plum       | #DDA0DD  | This is a sample text\u25a0 | | PowderBlue | #B0E0E6  | This is a sample text\u25a0 | | RosyBrown  | #BC8F8F  | This is a sample text\u25a0 | | RoyalBlue  | #4169E1  | This is a sample text\u25a0 | | SaddleBrown | #8B4513 | This is a sample text\u25a0 | | Salmon     | #FA8072  | This is a sample text\u25a0 | | SandyBrown | #F4A460  | This is a sample text\u25a0 | | SeaGreen   | #2E8B57  | This is a sample text\u25a0 | | SeaShell   | #FFF5EE  | This is a sample text\u25a0 | | Sienna     | #A0522D  | This is a sample text\u25a0 | | SkyBlue    | #87CEEB  | This is a sample text\u25a0 | | SlateBlue  | #6A5ACD  | This is a sample text\u25a0 | | SlateGray  | #708090  | This is a sample text\u25a0 | | Snow       | #FFFAFA  | This is a sample text\u25a0 | | SpringGreen | #00FF7F | This is a sample text\u25a0 | | SteelBlue  | #4682B4  | This is a sample text\u25a0 | | Tan        | #D2B48C  | This is a sample text\u25a0 | | Thistle    | #D8BFD8  | This is a sample text\u25a0 | | Tomato     | #FF6347  | This is a sample text\u25a0 | | Turquoise  | #40E0D0  | This is a sample text\u25a0 | | Violet     | #EE82EE  | This is a sample text\u25a0 | | Wheat      | #F5DEB3  | This is a sample text\u25a0 | | WhiteSmoke | #F5F5F5 | This is a sample text\u25a0 | | YellowGreen | #9ACD32 | This is a sample text\u25a0 |   # Sample Div  This is how I create divs. I create the block once, and reuse it.   Note: <p>Changeable items (also called mutable) are items that can be modified after they are created. For example:</p> <ul> <li>Lists: You can add, remove, or change elements.</li> <li>Dictionaries: You can add, remove, or change key-value pairs.</li> </ul> <p>These items cannot be added to a set because sets need items that do not change.</p>  <p>Unchangeable items (also called immutable) are items that cannot be modified after they are created. For example:</p> <ul> <li>Numbers: Once created, their values cannot be changed.</li> <li>Strings: Any modification creates a new string.</li> <li>Tuples: Their elements cannot be changed once created.</li> </ul> <p>These items can be added to a set because their values stay the same.</p>                       Note 1                                 Note 2                                 Note 3                                   Note 4                                 Note 5                                 Note 6                   Dashboard      Sidebar <ul> <li>Home</li> <li>Profile</li> <li>Settings</li> <li>Logout</li> </ul>      Content Block 1 <p>This is some content inside the first content block.</p>     Content Block 2 <p>This is some content inside the second content block.</p>     Content Block 3 <p>This is some content inside the third content block.</p>       <p>\u00a9 2024 Dass creation. All rights reserved.</p>     range(start, stop, step)    Start optional     Stop Must     Step Optional       Stop Must     print(*(i for i in range(5)))    0   1   2   3   4       This text will wrap itself. In Synapse or Azure Data Factory (ADF), a pipeline is simply the end-to-end workflow.     This text will wrap itself. print(*(fruits[i] for i in range(len(fruits))))    This will not wrap.    print(*(fruits[i] for i in range(len(fruits))))      print(             *(             fruits[i]              for i in range(             len(fruits))))            <p>     Creating connection strings is pretty easy with the icons in Synapse Linked Service. </p> <pre><code># &lt;span style=\"color: blueviolet;Font-family: Comic Sans MS, sans-serif;\"&gt;Synapse Analytics Core Concepts&lt;/span&gt;\n\n# &lt;span style=\"color: DarkCyan;Font-family: Comic Sans MS, sans-serif;\"&gt;Synapse Workspace&lt;/span&gt;\n\n# &lt;span style=\"color: DodgerBlue\"&gt;Serverless SQL Pool&lt;/span&gt;\n\n# &lt;span style=\"color: FireBrick\"&gt;Synapse Spark Pool&lt;/span&gt;\n\n# &lt;span style=\"color: CadetBlue\"&gt;What is Integration Runtime?&lt;/span&gt;\n\n# &lt;span style=\"color: BlueViolet\"&gt;1. Azure Integration Runtime:&lt;/span&gt;\n\n# &lt;span style=\"color: PaleVioletRed;Font-family: Segoe UI, sans-serif;\"&gt;Background&lt;/span&gt;\n\n# &lt;span style=\"color: blueviolet;Font-family: Segoe UI, sans-serif;\"&gt;Azure Synapse analytics&lt;/span&gt;\n\n# &lt;span style=\"color: DarkSeaGreen; font-family: 'Comic Sans MS', sans-serif;\"&gt;Accessing Items in a Tuple: Indexing&lt;/span&gt;\n\n\n &lt;p style=\"color: #804000; font-family: 'Trebuchet MS', Helvetica, sans-serif; background-color: BurlyWood; padding: 15px; border-left: 5px solid #b35900;\"&gt;\nA service to to run SQL Server Integration Services (SSIS) packages in the Azure cloud.\n&lt;/p&gt;\n\n\n&lt;p style=\"color: #006600; font-family: 'Trebuchet MS', Helvetica, sans-serif; background-color: #e6ffe6; padding: 15px; border-left: 5px solid #00cc66;\"&gt;\nA cloud-based compute resource for running data integration and transformation tasks in Azure Data Factory and Azure Synapse Analytics.\n&lt;/p&gt;\n\n &lt;p style=\"color: #804000; font-family: 'Trebuchet MS', Helvetica, sans-serif; background-color: #fff5e6; padding: 15px; border-left: 5px solid #b35900;\"&gt;\nA tool for connecting and moving data between on-premises sources and Azure cloud services.\n&lt;/p&gt;\n\n&lt;div style=\"display: flex; justify-content: center; align-items: center; margin: 5px;\"&gt;\n    &lt;div style=\"padding: 5px; border: 1px solid #ddd; box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.3); border-radius: 5px; background-color: #000000; margin: 3px;font-family: 'Comic Sans MS', sans-serif;\"&gt;\n        &lt;span style=\"font-size: 1.2em;color: #fff; font-weight: bold;font-family: 'Courier New', Courier, monospace;\"&gt;print(*(fruits[i] for i in range(len(fruits))))&lt;/span&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n\n&lt;div style=\"display: flex; justify-content: center; align-items: center; margin: 5px;\"&gt;\n    &lt;div style=\"padding: 5px; border: 1px solid #ddd; box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.3); border-radius: 5px; background-color: #ffeb3b; margin: 3px;font-family: 'Comic Sans MS', sans-serif;\"&gt;\n        &lt;span style=\"font-size: 1.2em;color: #000; font-weight: bold;font-family: 'Courier New', Courier, monospace;\"&gt;This text will wrap itself. In Synapse or Azure Data Factory (ADF), a pipeline is simply the end-to-end workflow.&lt;/span&gt;\n    &lt;/div&gt;\n&lt;/div&gt; \n</code></pre>  # My progressively darkening color themes for many colors---  ### **1. BlueViolet to Darker Shades:** <pre><code># &lt;span style=\"color: BlueViolet; font-family: Comic Sans MS, sans-serif;\"&gt;\"BlueViolet; Font-family: Comic Sans MS, sans-serif;\"&lt;/span&gt;\n## &lt;span style=\"color: #7A3DAA; font-family: Comic Sans MS, sans-serif;\"&gt;\"#7A3DAA; Font-family: Comic Sans MS, sans-serif;\"&lt;/span&gt;\n### &lt;span style=\"color: #653090; font-family: Comic Sans MS, sans-serif;\"&gt;\"#653090; Font-family: Comic Sans MS, sans-serif;\"&lt;/span&gt;\n#### &lt;span style=\"color: #502276; font-family: Comic Sans MS, sans-serif;\"&gt;\"#502276; Font-family: Comic Sans MS, sans-serif;\"&lt;/span&gt;\n##### &lt;span style=\"color: #3B165C; font-family: Comic Sans MS, sans-serif;\"&gt;\"#3B165C; Font-family: Comic Sans MS, sans-serif;\"&lt;/span&gt;\n###### &lt;span style=\"color: #260A42; font-family: Comic Sans MS, sans-serif;\"&gt;\"#260A42; Font-family: Comic Sans MS, sans-serif;\"&lt;/span&gt;\n</code></pre>  ### **2. MediumOrchid to Darker Shades:** <pre><code># &lt;span style=\"color: MediumOrchid; font-family: Segoe UI, sans-serif;\"&gt;\"MediumOrchid; Font-family: Segoe UI, sans-serif;\"&lt;/span&gt;\n## &lt;span style=\"color: #AD49B3; font-family: Segoe UI, sans-serif;\"&gt;\"#AD49B3; Font-family: Segoe UI, sans-serif;\"&lt;/span&gt;\n### &lt;span style=\"color: #963F9C; font-family: Segoe UI, sans-serif;\"&gt;\"#963F9C; Font-family: Segoe UI, sans-serif;\"&lt;/span&gt;\n#### &lt;span style=\"color: #7F3585; font-family: Segoe UI, sans-serif;\"&gt;\"#7F3585; Font-family: Segoe UI, sans-serif;\"&lt;/span&gt;\n##### &lt;span style=\"color: #682A6E; font-family: Segoe UI, sans-serif;\"&gt;\"#682A6E; Font-family: Segoe UI, sans-serif;\"&lt;/span&gt;\n###### &lt;span style=\"color: #511F57; font-family: Segoe UI, sans-serif;\"&gt;\"#511F57; Font-family: Segoe UI, sans-serif;\"&lt;/span&gt;\n</code></pre>  ### **3. PaleVioletRed to Darker Shades:** <pre><code># &lt;span style=\"color: PaleVioletRed; font-family: Segoe UI, sans-serif;\"&gt;\"PaleVioletRed; Font-family: Segoe UI, sans-serif;\"&lt;/span&gt;\n## &lt;span style=\"color: #D86487; font-family: Segoe UI, sans-serif;\"&gt;\"#D86487; Font-family: Segoe UI, sans-serif;\"&lt;/span&gt;\n### &lt;span style=\"color: #C05075; font-family: Segoe UI, sans-serif;\"&gt;\"#C05075; Font-family: Segoe UI, sans-serif;\"&lt;/span&gt;\n#### &lt;span style=\"color: #A93C63; font-family: Segoe UI, sans-serif;\"&gt;\"#A93C63; Font-family: Segoe UI, sans-serif;\"&lt;/span&gt;\n##### &lt;span style=\"color: #932851; font-family: Segoe UI, sans-serif;\"&gt;\"#932851; Font-family: Segoe UI, sans-serif;\"&lt;/span&gt;\n###### &lt;span style=\"color: #7C143F; font-family: Segoe UI, sans-serif;\"&gt;\"#7C143F; Font-family: Segoe UI, sans-serif;\"&lt;/span&gt;\n</code></pre>  ### **4. FireBrick to Darker Shades:** <pre><code># &lt;span style=\"color: FireBrick;\"&gt;\"FireBrick;\"&lt;/span&gt;\n## &lt;span style=\"color: #C22E2E;\"&gt;\"#C22E2E;\"&lt;/span&gt;\n### &lt;span style=\"color: #A12828;\"&gt;\"#A12828;\"&lt;/span&gt;\n#### &lt;span style=\"color: #801F1F;\"&gt;\"#801F1F;\"&lt;/span&gt;\n##### &lt;span style=\"color: #601515;\"&gt;\"#601515;\"&lt;/span&gt;\n###### &lt;span style=\"color: #400B0B;\"&gt;\"#400B0B;\"&lt;/span&gt;\n</code></pre>  ### **5. MediumSlateBlue to Darker Shades:** <pre><code># &lt;span style=\"color: MediumSlateBlue;\"&gt;\"MediumSlateBlue;\"&lt;/span&gt;\n## &lt;span style=\"color: #695ED6;\"&gt;\"#695ED6;\"&lt;/span&gt;\n### &lt;span style=\"color: #574BB3;\"&gt;\"#574BB3;\"&lt;/span&gt;\n#### &lt;span style=\"color: #463890;\"&gt;\"#463890;\"&lt;/span&gt;\n##### &lt;span style=\"color: #35256D;\"&gt;\"#35256D;\"&lt;/span&gt;\n###### &lt;span style=\"color: #24124A;\"&gt;\"#24124A;\"&lt;/span&gt;\n</code></pre>  ### **6. DodgerBlue to Darker Shades:** <pre><code># &lt;span style=\"color: DodgerBlue;\"&gt;\"DodgerBlue;\"&lt;/span&gt;\n## &lt;span style=\"color: #1E7FCC;\"&gt;\"#1E7FCC;\"&lt;/span&gt;\n### &lt;span style=\"color: #1B6FB2;\"&gt;\"#1B6FB2;\"&lt;/span&gt;\n#### &lt;span style=\"color: #185F99;\"&gt;\"#185F99;\"&lt;/span&gt;\n##### &lt;span style=\"color: #144F7F;\"&gt;\"#144F7F;\"&lt;/span&gt;\n###### &lt;span style=\"color: #103F66;\"&gt;\"#103F66;\"&lt;/span&gt;\n</code></pre>  ### **7. CadetBlue to Darker Shades:** <pre><code># &lt;span style=\"color: CadetBlue;\"&gt;\"CadetBlue;\"&lt;/span&gt;\n## &lt;span style=\"color: #4D8C8C;\"&gt;\"#4D8C8C;\"&lt;/span&gt;\n### &lt;span style=\"color: #437A7A;\"&gt;\"#437A7A;\"&lt;/span&gt;\n#### &lt;span style=\"color: #396868;\"&gt;\"#396868;\"&lt;/span&gt;\n##### &lt;span style=\"color: #2F5656;\"&gt;\"#2F5656;\"&lt;/span&gt;\n###### &lt;span style=\"color: #253B3B;\"&gt;\"#253B3B;\"&lt;/span&gt;\n</code></pre>  ### **8. DarkSeaGreen to Darker Shades:** <pre><code># &lt;span style=\"color: DarkSeaGreen; font-family: 'Comic Sans MS', sans-serif;\"&gt;\"DarkSeaGreen; Font-family: 'Comic Sans MS', sans-serif;\"&lt;/span&gt;\n## &lt;span style=\"color: #89A989; font-family: 'Comic Sans MS', sans-serif;\"&gt;\"#89A989; Font-family: 'Comic Sans MS', sans-serif;\"&lt;/span&gt;\n### &lt;span style=\"color: #768E76; font-family: 'Comic Sans MS', sans-serif;\"&gt;\"#768E76; Font-family: 'Comic Sans MS', sans-serif;\"&lt;/span&gt;\n#### &lt;span style=\"color: #627362; font-family: 'Comic Sans MS', sans-serif;\"&gt;\"#627362; Font-family: 'Comic Sans MS', sans-serif;\"&lt;/span&gt;\n##### &lt;span style=\"color: #4F584F; font-family: 'Comic Sans MS', sans-serif;\"&gt;\"#4F584F; Font-family: 'Comic Sans MS', sans-serif;\"&lt;/span&gt;\n###### &lt;span style=\"color: #3B3D3B; font-family: 'Comic Sans MS', sans-serif;\"&gt;\"#3B3D3B; Font-family: 'Comic Sans MS', sans-serif;\"&lt;/span&gt;\n</code></pre>  ### **9. DarkCyan to Darker Shades:** <pre><code># &lt;span style=\"color: DarkCyan; font-family: Comic Sans MS, sans-serif;\"&gt;\"DarkCyan; Font-family: Comic Sans MS, sans-serif;\"&lt;/span&gt;\n## &lt;span style=\"color: #007A7A; font-family: Comic Sans MS, sans-serif;\"&gt;\"#007A7A; Font-family: Comic Sans MS, sans-serif;\"&lt;/span&gt;\n### &lt;span style=\"color: #006767; font-family: Comic Sans MS, sans-serif;\"&gt;\"#006767; Font-family: Comic Sans MS, sans-serif;\"&lt;/span&gt;\n#### &lt;span style=\"color: #005454; font-family: Comic Sans MS, sans-serif;\"&gt;\"#005454; Font-family: Comic Sans MS, sans-serif;\"&lt;/span&gt;\n##### &lt;span style=\"color: #004141; font-family: Comic Sans MS, sans-serif;\"&gt;\"#004141; Font-family: Comic Sans MS, sans-serif;\"&lt;/span&gt;\n###### &lt;span style=\"color: #002E2E; font-family: Comic Sans MS, sans-serif;\"&gt;\"#002E2E; Font-family: Comic Sans MS, sans-serif;\"&lt;/span&gt;\n</code></pre>  # How to use headers in markdown and still apply some style  ### Teal Style <pre><code>&lt;h2 style=\"text-align: center; background-color: #008080; color: white; font-family: 'Segoe UI', sans-serif; padding: 15px 20px; border-radius: 10px; box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.3); border: 2px solid #005F5F; letter-spacing: 1px;\"&gt;\n    Single-Node Bitnami Spark With Master and Worker\n&lt;/h2&gt;\n</code></pre>      Some text here bla bla bla    ### Light Blue Style <pre><code>&lt;h2 style=\"text-align: center; background-color: #4682B4; color: white; font-family: 'Segoe UI', sans-serif; padding: 15px 20px; border-radius: 10px; box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.3); border: 2px solid #365f82; letter-spacing: 1px;\"&gt;\n    Single-Node Bitnami Spark With Master and Worker\n&lt;/h2&gt;\n</code></pre> - **Background**: Steel Blue (`#4682B4`) - **Border**: Dark Steel Blue (`#365f82`) - **Text**: White  ### Green Style <pre><code>&lt;h2 style=\"text-align: center; background-color: #228B22; color: white; font-family: 'Segoe UI', sans-serif; padding: 15px 20px; border-radius: 10px; box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.3); border: 2px solid #176317; letter-spacing: 1px;\"&gt;\n    Single-Node Bitnami Spark With Master and Worker\n&lt;/h2&gt;\n</code></pre> - **Background**: Forest Green (`#228B22`) - **Border**: Darker Forest Green (`#176317`) - **Text**: White  ### Navy Blue Style <pre><code>&lt;h2 style=\"text-align: center; background-color: #000080; color: white; font-family: 'Segoe UI', sans-serif; padding: 15px 20px; border-radius: 10px; box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.3); border: 2px solid #00004f; letter-spacing: 1px;\"&gt;\n    Single-Node Bitnami Spark With Master and Worker\n&lt;/h2&gt;\n</code></pre> - **Background**: Navy (`#000080`) - **Border**: Dark Navy (`#00004f`) - **Text**: White  ### Olive Green Style <pre><code>&lt;h2 style=\"text-align: center; background-color: #808000; color: white; font-family: 'Segoe UI', sans-serif; padding: 15px 20px; border-radius: 10px; box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.3); border: 2px solid #5f5f00; letter-spacing: 1px;\"&gt;\n    Single-Node Bitnami Spark With Master and Worker\n&lt;/h2&gt;\n</code></pre> - **Background**: Olive (`#808000`) - **Border**: Dark Olive (`#5f5f00`) - **Text**: White  ### Cyan Style <pre><code>&lt;h2 style=\"text-align: center; background-color: #00CED1; color: white; font-family: 'Segoe UI', sans-serif; padding: 15px 20px; border-radius: 10px; box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.3); border: 2px solid #009ca0; letter-spacing: 1px;\"&gt;\n    Single-Node Bitnami Spark With Master and Worker\n&lt;/h2&gt;\n</code></pre> - **Background**: Dark Turquoise (`#00CED1`) - **Border**: Darker Cyan (`#009ca0`) - **Text**: White","text":"<p>These fonts should be already present in your system. You can download it from internet and install them.</p>"},{"location":"Misc/4_VSTrics.html","title":"VS Code Tips","text":"<ol> <li>Replace all .png files with images.png</li> </ol> <p>![([^]]*)](([^\\/]+.png))</p> <p></p> <p></p> <ol> <li> <p>Replace constructs like</p> </li> <li> <p>Correct Answer: B. extend is used to create a new column or modify an existing column.</p> </li> </ol> <p>With</p> <p>extend is used to create a new column or modify an existing column.</p> <p></p> <p>Search: - **Correct Answer:** [A-Z]. (.+) Replace: $1</p> <p>Quick datasets:</p> <p>df = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/titanic.csv\") display(df)</p>"},{"location":"Misc/4_VSTrics.html#intelisence-not-working-in-vs","title":"Intelisence not working in VS","text":""},{"location":"Misc/5_WhichDatastsToUse.html","title":"Datasets","text":""},{"location":"Misc/5_WhichDatastsToUse.html#overview","title":"Overview","text":"<p>If you're new to data modeling and want to learn about how to create fact and dimension tables, it's best to start with some sample databases that are designed for practice. Here are some good options:</p>"},{"location":"Misc/5_WhichDatastsToUse.html#1-adventureworks-microsoft-sql-server-sample-database","title":"1. AdventureWorks (Microsoft SQL Server Sample Database)","text":"<ul> <li>What It Is: AdventureWorks is a sample database provided by Microsoft. It contains data for a fictional company that sells bicycles and related products.</li> <li>Where to Get It: Download AdventureWorks</li> </ul>"},{"location":"Misc/5_WhichDatastsToUse.html#2-northwind-traders-microsoft-access-database","title":"2. Northwind Traders (Microsoft Access Database)","text":"<ul> <li>What It Is: Northwind is another sample database that simulates a trading company. It includes data about customers, products, orders, and suppliers.</li> <li>Where to Get It: Download Northwind Database</li> </ul>"},{"location":"Misc/5_WhichDatastsToUse.html#3-tpc-h-benchmark-dataset","title":"3. TPC-H Benchmark Dataset","text":"<ul> <li>What It Is: TPC-H is a dataset used for testing database performance. It contains data for a wholesale supplier, including orders, customers, parts, and suppliers.</li> <li>Where to Get It: TPC-H Benchmark Dataset</li> </ul>"},{"location":"Misc/5_WhichDatastsToUse.html#4-imdb-movie-dataset","title":"4. IMDB Movie Dataset","text":"<ul> <li>What It Is: The IMDb dataset has information about movies, actors, directors, and genres.</li> <li>Where to Get It: IMDb Datasets</li> </ul>"},{"location":"Misc/5_WhichDatastsToUse.html#5-retail-transaction-data-from-kaggle","title":"5. Retail Transaction Data (from Kaggle)","text":"<ul> <li>What It Is: There are several retail transaction datasets on Kaggle, like the \"Online Retail Dataset\" or \"Instacart Market Basket Analysis.\"</li> <li>Where to Get It: <ul> <li>UCI link. Kaggle Link</li> <li>Instacart Market Basket Analysis</li> </ul> </li> </ul>"},{"location":"Misc/5_WhichDatastsToUse.html#6-airbnb-listings-dataset","title":"6. Airbnb Listings Dataset","text":"<ul> <li>What It Is: This dataset contains details about Airbnb properties, like prices, locations, host details, and reviews.</li> <li>Where to Get It: Airbnb Listings Dataset</li> </ul>"},{"location":"Misc/5_WhichDatastsToUse.html#7-chinook-database","title":"7. Chinook Database","text":"<ul> <li>What It Is: Chinook is a sample database that simulates a digital media store. It includes data about artists, albums, media tracks, customers, and sales.</li> <li>Where to Get It: Download Chinook Database</li> </ul>"},{"location":"Misc/5_WhichDatastsToUse.html#summary","title":"Summary","text":"<p>Start with AdventureWorks or Northwind if you want something easy to get started with. These databases are ready to use and perfect for beginners. If you\u2019re looking for more real-world examples, Kaggle\u2019s retail or Airbnb datasets are also good options.</p>"},{"location":"Misc/6_RunningAppsInBg.html","title":"Background Apps","text":""},{"location":"Misc/6_RunningAppsInBg.html#how-to-run-a-service-in-the-background-on-linux-and-check-if-its-running","title":"How to Run a Service in the Background on Linux and Check if It\u2019s Running","text":"<p>In this article, I will show you how to run a service in the background. I've used Apache Airflow as an example, but you can apply these methods to other services as well.</p>"},{"location":"Misc/6_RunningAppsInBg.html#running-a-service-in-the-background","title":"Running a Service in the Background","text":"<ol> <li>Using <code>nohup</code> (Linux/MacOS):</li> <li>The <code>nohup</code> command allows you to run the Airflow scheduler in the background and ensures it continues running even after you close the terminal.</li> </ol> <pre><code>nohup airflow scheduler &gt; scheduler.log 2&gt;&amp;1 &amp;\n</code></pre> <ul> <li> <p>Here, <code>nohup</code> keeps the process running, <code>&gt; scheduler.log 2&gt;&amp;1</code> saves the output to a log file, and <code>&amp;</code> runs it in the background.</p> </li> <li> <p>Using <code>screen</code> (Linux/MacOS):</p> </li> <li><code>screen</code> is a terminal multiplexer that lets you start a terminal session that you can detach from and reattach to later.</li> </ul> <pre><code>screen -S airflow-scheduler\nairflow scheduler\n</code></pre> <ul> <li>After starting the scheduler, detach the screen session by pressing <code>Ctrl + A</code>, then <code>D</code>. To reattach to the session, use:</li> </ul> <pre><code>screen -r airflow-scheduler\n</code></pre> <ol> <li>Using <code>tmux</code> (Linux/MacOS):</li> <li><code>tmux</code> is similar to <code>screen</code>, providing another way to manage terminal sessions that you can detach and reattach to.</li> </ol> <pre><code>tmux new -s airflow-scheduler\nairflow scheduler\n</code></pre> <ul> <li>Detach from the session with <code>Ctrl + B</code>, then <code>D</code>. Reattach with:</li> </ul> <pre><code>tmux attach -t airflow-scheduler\n</code></pre> <ol> <li>Using <code>systemd</code> (Linux):</li> <li>To manage the Airflow scheduler as a service, you can create a <code>systemd</code> service file. This method is more permanent and reliable.</li> </ol> <p>Create a file at <code>/etc/systemd/system/airflow-scheduler.service</code> with the following content:</p> <pre><code>[Unit]\nDescription=Airflow Scheduler\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/airflow scheduler\nRestart=always\nUser=airflow\nGroup=airflow\nStandardOutput=syslog\nStandardError=syslog\nSyslogIdentifier=airflow-scheduler\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <ul> <li>Then enable and start the service:</li> </ul> <pre><code>sudo systemctl enable airflow-scheduler\nsudo systemctl start airflow-scheduler\n</code></pre> <ol> <li>Using <code>&amp;</code> (Linux/MacOS):</li> <li>For a quick and simple way to run the Airflow scheduler in the background, just append <code>&amp;</code> to the command:</li> </ol> <pre><code>airflow scheduler &amp;\n</code></pre> <ol> <li>Using Docker (If Airflow is running inside a container):</li> <li>If you're using Docker, you can run the Airflow scheduler in detached mode using the following command:</li> </ol> <pre><code>docker run -d --name airflow-scheduler apache/airflow:latest scheduler\n</code></pre> <ul> <li>This command will start the scheduler in the background inside the Docker container.</li> </ul>"},{"location":"Misc/6_RunningAppsInBg.html#checking-if-a-service-is-running","title":"Checking if a Service is Running","text":"<ol> <li>Checking the Airflow Scheduler:</li> <li>To see if the Airflow scheduler is running, use the <code>ps</code> command:</li> </ol> <pre><code>ps aux | grep 'airflow scheduler'\n</code></pre> <ul> <li> <p>This command will list the running processes, including the Airflow scheduler if it's active.</p> </li> <li> <p>Checking Apache Web Server:</p> </li> <li>For Apache, which is a widely used web server, you can check its status with:</li> </ul> <pre><code>sudo systemctl status apache2\n</code></pre> <ul> <li>If Apache is running, you'll see \"active (running)\" in the status output.</li> </ul>"},{"location":"Misc/6_RunningAppsInBg.html#conclusion","title":"Conclusion","text":"<p>So, the popular commands to run services in the background in linux like os are:<code>nohup</code>, <code>screen</code>, <code>tmux</code> and <code>systemctl</code></p>"},{"location":"Misc/7_Azure_Budget.html","title":"How to set a Azure Budget","text":""},{"location":"Misc/9_WhyUbuntuIsGood.html","title":"Ubuntu Benefits","text":""},{"location":"Misc/9_WhyUbuntuIsGood.html#why-ubuntu-is-a-valuable-os-for-microsoft-ecosystem","title":"Why Ubuntu is a Valuable OS for Microsoft Ecosystem","text":"<p>Ubuntu is an important operating system for Microsoft ecosystem, particularly Azure. Here\u2019s why:</p> <ol> <li> <p>Cloud Popularity: Ubuntu is the most widely used Linux distribution in cloud environments, including Azure. Many Azure Virtual Machines (VMs), containers, and Databricks clusters operate on Ubuntu, making it essential to know for cloud-related tasks.</p> </li> <li> <p>Seamless Azure Integration: Ubuntu is fully supported by Azure services. Tools like Azure CLI and DevOps agents are optimized for Ubuntu, allowing for smooth integration and efficient management.</p> </li> <li> <p>Databricks Support: Azure Databricks relies on Ubuntu for its virtual machines, including both worker and driver nodes. Knowing Ubuntu helps you manage and optimize these clusters effectively.</p> </li> <li> <p>Command-Line Skills: Azure frequently uses command-line interfaces. Ubuntu\u2019s native bash shell is ideal for scripting and managing Azure resources, boosting your productivity and effectiveness.</p> </li> <li> <p>Open-Source Benefits: Ubuntu is free and open-source, which means you can learn and experiment without worrying about licensing costs. This aligns well with Azure\u2019s support for open-source technologies.</p> </li> <li> <p>Practical Usage: Many enterprises deploy Ubuntu on Azure for their production environments. Learning Ubuntu gives you practical skills that are directly applicable in the workplace.</p> </li> </ol>"},{"location":"Misc/Catalogue_Vs_TableFormat.html","title":"Catalogue, Metastore, and Table Format \u2014 So confusing!","text":"<p>When people first hear the words catalogue, metastore, and table format, it almost sounds like three different versions of the same thing. And if you read random articles online, it becomes even more confusing. Some say the catalogue stores metadata. Some say the table format stores metadata. And then someone else jumps in and says Hive Metastore is still needed. Most of these explanations mix everything together, so let\u2019s clean it up and keep it very simple.</p> <p>I will explain this in the exact way I would want someone to explain it to me: minimal jargon, no unnecessary sections, and only the pieces that matter.</p> <p>A Table Format (like Iceberg, Delta Lake, or Hudi) is basically the \"internal brain\" of a table. It decides how the table lives inside storage. All the important mechanics\u2014snapshots, schema evolution, partitions, ACID, file layout\u2014these are rules defined by the table format.</p> <p>If you open an Iceberg table on MinIO, you will literally see its brain laid out:</p> <pre><code>mytable/\n  data/\n  metadata/\n    v1.metadata.json\n    v2.metadata.json\n    snapshots/\n    manifests/\n</code></pre> <p>Those <code>metadata.json</code> files are the table\u2019s life history. If someone says \u201cIceberg supports time travel,\u201d it\u2019s because these files exist. If someone says \u201cDelta supports ACID,\u201d it\u2019s because the format defines how commits work. So the job of the table format is simple: decide how a single table behaves internally.</p> <p>It does not keep track of all tables in your system. It only cares about itself.</p> <p>A Catalogue / Metastore is the \"index of all tables.\" That\u2019s it. It does not store your data. It does not store snapshots. It does not store Parquet files. It only stores very small information:</p> <ul> <li>what tables exist</li> <li>which database they belong to</li> <li>where the table lives in MinIO</li> <li>what the latest metadata file is</li> </ul> <p>That\u2019s why I call it \"the library index.\" A library does not store books inside the index drawer. It only tells you where the books are kept. The catalogue does exactly that.</p> <p>For Iceberg, the modern recommended catalogue is the Iceberg REST Catalogue, a lightweight service that Spark, Trino, and Flink talk to. Older systems like Hive Metastore (HMS) also work, but they were designed in a very different era, when everything was tied to Hadoop. Today, REST-based catalogues are simpler, cleaner, and much easier to fit inside container-based setups. </p>"},{"location":"Misc/MarkdownColor/Color.html","title":"Color Palettes","text":"<p>I found this from the internet. This could be very useful for technical writers who want  a handy color rereence guid.</p>"},{"location":"Misc/MarkdownColor/Color.html#1-burnt-sienna-orange-bedazzled-blue-color-palette","title":"1. Burnt Sienna Orange + Bedazzled Blue Color Palette","text":"<p> Hex Codes: <code>#3d5a80</code>, <code>#98c1d9</code>, <code>#e0fbfc</code>, <code>#ee6c4d</code>, <code>#293241</code></p>"},{"location":"Misc/MarkdownColor/Color.html#2-imperial-red-space-cadet-blue-color-palette","title":"2. Imperial Red + Space Cadet Blue Color Palette","text":"<p> Hex Codes: <code>#2b2d42</code>, <code>#8d99ae</code>, <code>#edf2f4</code>, <code>#ef233c</code>, <code>#d90429</code></p>"},{"location":"Misc/MarkdownColor/Color.html#3-orange-honey-yellow-prussian-blue-color-palette","title":"3. Orange + Honey Yellow + Prussian Blue Color Palette","text":"<p> Hex Codes: <code>#8ecae6</code>, <code>#219ebc</code>, <code>#023047</code>, <code>#ffb703</code>, <code>#fb8500</code></p>"},{"location":"Misc/MarkdownColor/Color.html#4-candy-pink-rose-desert-y-in-mn-blue-color-palette","title":"4. Candy Pink + Rose Desert + Y in Mn Blue Color Palette","text":"<p> Hex Codes: <code>#355070</code>, <code>#6d597a</code>, <code>#b56576</code>, <code>#e56b6f</code>, <code>#eaac8b</code></p>"},{"location":"Misc/MarkdownColor/Color.html#5-paradise-pink-caribbean-green-ncs-blue-color-palette","title":"5. Paradise Pink + Caribbean Green + NCS Blue Color Palette","text":"<p> Hex Codes: <code>#ef476f</code>, <code>#ffd166</code>, <code>#06d6a0</code>, <code>#118ab2</code>, <code>#073b4c</code></p>"},{"location":"Misc/MarkdownColor/Color.html#6-lemon-meringue-prussian-blue-color-palette","title":"6. Lemon Meringue + Prussian Blue Color Palette","text":"<p> Hex Codes: <code>#003049</code>, <code>#d62828</code>, <code>#f77f00</code>, <code>#fcbf49</code>, <code>#eae2b7</code></p>"},{"location":"Misc/MarkdownColor/Color.html#7-orange-web-oxford-blue-color-palette","title":"7. Orange Web + Oxford Blue Color Palette","text":"<p> Hex Codes: <code>#000000</code>, <code>#14213d</code>, <code>#fca311</code>, <code>#e5e5e5</code>, <code>#ffffff</code></p>"},{"location":"Misc/MarkdownColor/Color.html#8-carolina-blue-cg-blue-color-palette","title":"8. Carolina Blue + CG Blue Color Palette","text":"<p> Hex Codes: <code>#ffffff</code>, <code>#00171f</code>, <code>#003459</code>, <code>#007ea7</code>, <code>#00a8e8</code></p>"},{"location":"Misc/MarkdownColor/Color.html#9-cyber-yellow-royal-dark-blue-color-palette","title":"9. Cyber Yellow + Royal Dark Blue Color Palette","text":"<p> Hex Codes: <code>#00296b</code>, <code>#003f88</code>, <code>#00509d</code>, <code>#fdc500</code>, <code>#ffd500</code></p>"},{"location":"Misc/MarkdownColor/Color.html#10-shades-of-blue-color-palette","title":"10. Shades of Blue Color Palette","text":"<p> Hex Codes: <code>#03045e</code>, <code>#0077b6</code>, <code>#00b4d8</code>, <code>#90e0ef</code>, <code>#caf0f8</code></p>"},{"location":"Misc/MarkdownColor/Color.html#11-midnight-eagle-green-metallic-seaweed-blue-color-palette","title":"11. Midnight Eagle Green + Metallic Seaweed Blue Color Palette","text":"<p> Hex Codes: <code>#177e89</code>, <code>#084c61</code>, <code>#db3a34</code>, <code>#ffc857</code>, <code>#323031</code></p>"},{"location":"Misc/MarkdownColor/Color.html#12-sage-ming-indigo-dye-blue-color-palette","title":"12. Sage + Ming + Indigo Dye Blue Color Palette","text":"<p> Hex Codes: <code>#033f63</code>, <code>#28666e</code>, <code>#7c9885</code>, <code>#b5b682</code>, <code>#fedc97</code></p>"},{"location":"Misc/MarkdownColor/Color.html#13-almond-purple-navy-oxford-blue-color-palette","title":"13. Almond + Purple Navy + Oxford Blue Color Palette","text":"<p> Hex Codes: <code>#f1dac4</code>, <code>#a69cac</code>, <code>#474973</code>, <code>#161b33</code>, <code>#0d0c1d</code></p>"},{"location":"Misc/MarkdownColor/Color.html#14-ruby-bright-yellow-crayola-sky-blue-crayola-color-palette","title":"14. Ruby + Bright Yellow Crayola + Sky Blue Crayola Color Palette","text":"<p> Hex Codes: <code>#d81159</code>, <code>#8f2d56</code>, <code>#218380</code>, <code>#fbb13c</code>, <code>#73d2de</code></p>"},{"location":"Misc/MarkdownColor/Color.html#15-atomic-tangerine-pacific-blue-yale-blue-color-palette","title":"15. Atomic Tangerine + Pacific Blue + Yale Blue Color Palette","text":"<p> Hex Codes: <code>#f79256</code>, <code>#fbd1a2</code>, <code>#7dcfb6</code>, <code>#00b2ca</code>, <code>#1d4e89</code></p>"},{"location":"Misc/MarkdownColor/Color.html#16-burnt-sienna-cadet-blue-columbia-blue-color-palette","title":"16. Burnt Sienna + Cadet Blue + Columbia Blue Color Palette","text":"<p> Hex Codes: <code>#dd6e42</code>, <code>#e8dab2</code>, <code>#4f6d7a</code>, <code>#c0d6df</code>, <code>#eaeaea</code></p>"},{"location":"Misc/MarkdownColor/Color.html#17-jet-ming-indigo-dye-blue-color-palette","title":"17. Jet + Ming + Indigo Dye Blue Color Palette","text":"<p> Hex Codes: <code>#353535</code>, <code>#3c6e71</code>, <code>#ffffff</code>, <code>#d9d9d9</code>, <code>#284b63</code></p>"},{"location":"Misc/MarkdownColor/Color.html#18-sunglow-sizzling-red-crayola-blue-color-palette","title":"18. Sunglow + Sizzling Red + Crayola Blue Color Palette","text":"<p> Hex Codes: <code>#ff595e</code>, <code>#ffca3a</code>, <code>#8ac926</code>, <code>#1982c4</code>, <code>#6a4c93</code></p>"},{"location":"Misc/MarkdownColor/Color.html#19-light-salmon-french-pink-baby-blue-color-palette","title":"19. Light Salmon + French Pink + Baby Blue Color Palette","text":"<p> Hex Codes: <code>#70d6ff</code>, <code>#ff70a6</code>, <code>#ff9770</code>, <code>#ffd670</code>, <code>#e9ff70</code></p>"},{"location":"Misc/MarkdownColor/Color.html#20-black-coffee-duke-blue-true-blue-color-palette","title":"20. Black Coffee + Duke Blue + True Blue Color Palette","text":"<p> Hex Codes: <code>#3c3744</code>, <code>#090c9b</code>, <code>#3066be</code>, <code>#b4c5e4</code>, <code>#fbfff1</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html","title":"Color Palettes","text":""},{"location":"Misc/MarkdownColor/images/Color.html#1-burnt-sienna-orange-bedazzled-blue-color-palette","title":"1. Burnt Sienna Orange + Bedazzled Blue Color Palette","text":"<p> Hex Codes: <code>#3d5a80</code>, <code>#98c1d9</code>, <code>#e0fbfc</code>, <code>#ee6c4d</code>, <code>#293241</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html#2-imperial-red-space-cadet-blue-color-palette","title":"2. Imperial Red + Space Cadet Blue Color Palette","text":"<p> Hex Codes: <code>#2b2d42</code>, <code>#8d99ae</code>, <code>#edf2f4</code>, <code>#ef233c</code>, <code>#d90429</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html#3-orange-honey-yellow-prussian-blue-color-palette","title":"3. Orange + Honey Yellow + Prussian Blue Color Palette","text":"<p> Hex Codes: <code>#8ecae6</code>, <code>#219ebc</code>, <code>#023047</code>, <code>#ffb703</code>, <code>#fb8500</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html#4-candy-pink-rose-desert-y-in-mn-blue-color-palette","title":"4. Candy Pink + Rose Desert + Y in Mn Blue Color Palette","text":"<p> Hex Codes: <code>#355070</code>, <code>#6d597a</code>, <code>#b56576</code>, <code>#e56b6f</code>, <code>#eaac8b</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html#5-paradise-pink-caribbean-green-ncs-blue-color-palette","title":"5. Paradise Pink + Caribbean Green + NCS Blue Color Palette","text":"<p> Hex Codes: <code>#ef476f</code>, <code>#ffd166</code>, <code>#06d6a0</code>, <code>#118ab2</code>, <code>#073b4c</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html#6-lemon-meringue-prussian-blue-color-palette","title":"6. Lemon Meringue + Prussian Blue Color Palette","text":"<p> Hex Codes: <code>#003049</code>, <code>#d62828</code>, <code>#f77f00</code>, <code>#fcbf49</code>, <code>#eae2b7</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html#7-orange-web-oxford-blue-color-palette","title":"7. Orange Web + Oxford Blue Color Palette","text":"<p> Hex Codes: <code>#000000</code>, <code>#14213d</code>, <code>#fca311</code>, <code>#e5e5e5</code>, <code>#ffffff</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html#8-carolina-blue-cg-blue-color-palette","title":"8. Carolina Blue + CG Blue Color Palette","text":"<p> Hex Codes: <code>#ffffff</code>, <code>#00171f</code>, <code>#003459</code>, <code>#007ea7</code>, <code>#00a8e8</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html#9-cyber-yellow-royal-dark-blue-color-palette","title":"9. Cyber Yellow + Royal Dark Blue Color Palette","text":"<p> Hex Codes: <code>#00296b</code>, <code>#003f88</code>, <code>#00509d</code>, <code>#fdc500</code>, <code>#ffd500</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html#10-shades-of-blue-color-palette","title":"10. Shades of Blue Color Palette","text":"<p> Hex Codes: <code>#03045e</code>, <code>#0077b6</code>, <code>#00b4d8</code>, <code>#90e0ef</code>, <code>#caf0f8</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html#11-midnight-eagle-green-metallic-seaweed-blue-color-palette","title":"11. Midnight Eagle Green + Metallic Seaweed Blue Color Palette","text":"<p> Hex Codes: <code>#177e89</code>, <code>#084c61</code>, <code>#db3a34</code>, <code>#ffc857</code>, <code>#323031</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html#12-sage-ming-indigo-dye-blue-color-palette","title":"12. Sage + Ming + Indigo Dye Blue Color Palette","text":"<p> Hex Codes: <code>#033f63</code>, <code>#28666e</code>, <code>#7c9885</code>, <code>#b5b682</code>, <code>#fedc97</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html#13-almond-purple-navy-oxford-blue-color-palette","title":"13. Almond + Purple Navy + Oxford Blue Color Palette","text":"<p> Hex Codes: <code>#f1dac4</code>, <code>#a69cac</code>, <code>#474973</code>, <code>#161b33</code>, <code>#0d0c1d</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html#14-ruby-bright-yellow-crayola-sky-blue-crayola-color-palette","title":"14. Ruby + Bright Yellow Crayola + Sky Blue Crayola Color Palette","text":"<p> Hex Codes: <code>#d81159</code>, <code>#8f2d56</code>, <code>#218380</code>, <code>#fbb13c</code>, <code>#73d2de</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html#15-atomic-tangerine-pacific-blue-yale-blue-color-palette","title":"15. Atomic Tangerine + Pacific Blue + Yale Blue Color Palette","text":"<p> Hex Codes: <code>#f79256</code>, <code>#fbd1a2</code>, <code>#7dcfb6</code>, <code>#00b2ca</code>, <code>#1d4e89</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html#16-burnt-sienna-cadet-blue-columbia-blue-color-palette","title":"16. Burnt Sienna + Cadet Blue + Columbia Blue Color Palette","text":"<p> Hex Codes: <code>#dd6e42</code>, <code>#e8dab2</code>, <code>#4f6d7a</code>, <code>#c0d6df</code>, <code>#eaeaea</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html#17-jet-ming-indigo-dye-blue-color-palette","title":"17. Jet + Ming + Indigo Dye Blue Color Palette","text":"<p> Hex Codes: <code>#353535</code>, <code>#3c6e71</code>, <code>#ffffff</code>, <code>#d9d9d9</code>, <code>#284b63</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html#18-sunglow-sizzling-red-crayola-blue-color-palette","title":"18. Sunglow + Sizzling Red + Crayola Blue Color Palette","text":"<p> Hex Codes: <code>#ff595e</code>, <code>#ffca3a</code>, <code>#8ac926</code>, <code>#1982c4</code>, <code>#6a4c93</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html#19-light-salmon-french-pink-baby-blue-color-palette","title":"19. Light Salmon + French Pink + Baby Blue Color Palette","text":"<p> Hex Codes: <code>#70d6ff</code>, <code>#ff70a6</code>, <code>#ff9770</code>, <code>#ffd670</code>, <code>#e9ff70</code></p>"},{"location":"Misc/MarkdownColor/images/Color.html#20-black-coffee-duke-blue-true-blue-color-palette","title":"20. Black Coffee + Duke Blue + True Blue Color Palette","text":"<p> Hex Codes: <code>#3c3744</code>, <code>#090c9b</code>, <code>#3066be</code>, <code>#b4c5e4</code>, <code>#fbfff1</code></p>"},{"location":"MongoDB/HowMongoDBStoresFiles.html","title":"File Storage","text":""},{"location":"MongoDB/HowMongoDBStoresFiles.html#overview","title":"Overview","text":"<p>MongoDB is a document database. Document database? You mean, it's good for storing PDFs/Word files? No. The reason it is called a Document database is not because it stores PDFs/Word docs well(it does). But, because whatever data it stores\u2014whether it's files or SQL table data\u2014it stores as JSON files.</p> <p>What? Yes, even for storing something simple like Name and Roll Number, it will be saved like this: <code>{Name: \"Donald\", Roll Number: 100}</code>. And for storing a PDF, the 0s and 1s (binary data of the PDF) are stored as entries in JSON files.</p> <p> So, whatever data you store, it\u2019s stored in a JSON document. Hence, it's called a document database. </p> <p>Let's see below how the data, structured(sql tables) and unstructured(pdf documents) are stored in MongoDB.</p>"},{"location":"MongoDB/HowMongoDBStoresFiles.html#how-mongodb-would-store-your-normal-sql-datasimple-text","title":"How MongoDB would store your normal SQL Data(simple text)?","text":"<p>Suppose, we have a simple MSSQL table but, we want to store the data now in MongoDB. How would MongoDB store it. When you save this data in MongoDB, each row becomes a JSON file. These documents are stored inside a collection (like a table) called <code>countries</code> inside a database called <code>Earth</code>.</p> <p></p> <p>Now, let's do some practical to see it in reality. Follow the steps below.</p> <p>To run MongoDB commands, you can install MongoDB Shell from here.</p> <ul> <li> <p>Open CMD and key in <code>mongosh</code>. It will log in and start a <code>test</code> database:</p> <p></p> </li> <li> <p>Create the <code>Earth</code> Database. Just enter <code>Earth</code> and it will create the database. No need to create it separately.</p> </li> <li> <p>Now, insert the Data into the <code>countries</code> Collection:</p> </li> </ul> <p>This will create the collection. No need to create it beforehand.</p> <pre><code>```javascript\ndb.countries.insertMany([\n  { \"Country\": \"USA\", \"Population\": 300, \"Language\": \"English\" },\n  { \"Country\": \"Cuba\", \"Population\": 100, \"Language\": \"Spanish\" }\n]);\n```\n&lt;img src=\"images/custom-image-2024-07-20-02-22-46.png\" alt=\"Description of the image\" style=\"max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\"&gt;\n</code></pre> <ul> <li> <p>You can see the data in MongoDB Compass</p> <p></p> </li> <li> <p>To make sure the data is inserted properly. you can query the collection using this command: <code>db.countries.find().pretty();</code>. This command will display the documents stored in the <code>countries</code> collection in a readable format.</p> </li> </ul>"},{"location":"MongoDB/HowMongoDBStoresFiles.html#storing-unstructured-data-like-files-in-mongodb","title":"Storing unstructured data like files in MongoDB","text":"<p>Let's now see how MongoDB stores a PDF file. Say we have a 1 MB PDF file named <code>Resume.pdf</code>. How would MongoDB store it? MongoDB uses a specification called GridFS to store large files. GridFS divides the file into smaller chunks and stores each chunk as a separate document.</p>"},{"location":"MongoDB/HowMongoDBStoresFiles.html#how-gridfs-splits-files","title":"How GridFS Splits Files","text":"<p>When you save a file in MongoDB using GridFS, the file is split into smaller pieces (around 255 KB). For a 1 MB PDF file, it will be split into 4 smaller files.</p> <p>Each file-piece is stored in the <code>fs.chunks</code> collection as a JSON file. Yes, your Adobe PDF's binary data is split into pieces, and the 01$#$#$ is put inside the JSON file.</p> <p>There will be one metadata file for the entire PDF stored as a JSON file in the <code>fs.files</code> collection. JSON, JSON everywhere.</p>"},{"location":"MongoDB/HowMongoDBStoresFiles.html#storing-a-sample-file-resumepdf","title":"Storing a sample file <code>Resume.pdf</code>","text":"<p>When you save the <code>Resume.pdf</code> file in MongoDB, it is broken into chunks and stored across two collections: <code>fs.files</code> and <code>fs.chunks</code>.</p> <ul> <li> <p>File Metadata - <code>fs.files</code>:    Each file has a metadata document in the <code>fs.files</code> collection.</p> <pre><code>{\n  \"_id\": ObjectId(\"...\"),\n  \"filename\": \"Resume.pdf\",\n  \"length\": 1048576, // Size in bytes (1 MB)\n  \"chunkSize\": 261120, // Default chunk size (255 KB)\n  \"uploadDate\": ISODate(\"2024-07-20T12:34:56Z\"),\n  \"md5\": \"...\"\n}\n</code></pre> <p></p> </li> <li> <p>File Chunks - <code>fs.chunks</code>:    The file data is stored in chunks in the <code>fs.chunks</code> collection. Each chunk is linked to the file via the <code>files_id</code>.</p> <p><pre><code>{\n  \"_id\": ObjectId(\"...\"),\n  \"files_id\": ObjectId(\"...\"), // Reference to fs.files document\n  \"n\": 0, // Chunk number\n  \"data\": &lt;binary data $###$WhenYouOpenAdobePDFinNotepad01010101&gt;\n}\n{\n  \"_id\": ObjectId(\"...\"),\n  \"files_id\": ObjectId(\"...\"), // Reference to fs.files document\n  \"n\": 1, // Chunk number\n  \"data\": &lt;binary data $###$WhenYouOpenAdobePDFinNotepad01010101&gt;\n}\n// More chunks follow...\n</code></pre> </p> </li> </ul>"},{"location":"MongoDB/HowMongoDBStoresFiles.html#lets-see-this-in-practice","title":"Let's See This in Practice","text":"<p>To run MongoDB commands, you can install MongoDB Shell from here. Now, let's try to create a Python code to upload a sample file into MongoDB.</p> <ul> <li>Install Required Packages:    Make sure you have <code>pymongo</code> installed. You can install it using pip:</li> </ul> <pre><code>pip install pymongo gridfs\n</code></pre> <ul> <li> <p>Python Code to Store <code>Resume.pdf</code>:</p> <pre><code>from pymongo import MongoClient\nimport gridfs\n\n# Connect to MongoDB\nclient = MongoClient('mongodb://localhost:27017')\ndb = client['Earth']\n\n# Create a GridFS bucket\nfs = gridfs.GridFS(db)\n\n# Open the PDF file and store it in MongoDB\nwith open('Resume.pdf', 'rb') as f:\n    file_id = fs.put(f, filename='Resume.pdf')\n\nprint(f'File stored with id: {file_id}')\n</code></pre> </li> <li> <p>Verify the File Storage:</p> </li> </ul> <p>You can verify that the file has been stored correctly by querying the <code>fs.files</code> collection:</p> <pre><code>```python\n# Verify file storage\nstored_file = db.fs.files.find_one({'filename': 'Resume.pdf'})\nprint(stored_file)\n```\n</code></pre>"},{"location":"MongoDB/HowMongoDBStoresFiles.html#lets-recap-the-splitting-process","title":"Let's recap the splitting process","text":"<p>When you upload Resume.pdf using GridFS: - The file is split into pieces. The default chunk size is 255 KB. - For a 1 MB file, it would be split into around 4 pieces (1 MB / 255 KB \u2248 4). - Each piece is stored as a separate JSON file in the <code>fs.chunks</code> collection. Yes, the Adobe PDF's binary data is stuffed inside the JSON file. - There will be one JSON file in the <code>fs.files</code> collection to store metadata about the real file, including its length, chunk size, and upload date. - Each chunk document in <code>fs.chunks</code> contains a reference to the file's metadata document in <code>fs.files</code>.</p>"},{"location":"MongoDB/HowMongoDBStoresFiles.html#now-lets-see-the-actual-details","title":"Now, let's see the actual details","text":"<p>In actual fact, each row in a SQL table when saved in MongoDB becomes a document. Documents are not exactly .JSON files. Rather, they are stored in BSON (Binary JSON) format within MongoDB, which is an internal storage format optimized for performance and space efficiency.</p>"},{"location":"MongoDB/HowMongoDBStoresFiles.html#detailed-explanation","title":"Detailed Explanation","text":"<ul> <li>SQL to MongoDB: When migrating data from a SQL table to MongoDB, each row from the SQL table becomes a document in a MongoDB collection.</li> <li>Documents: These documents are similar to JSON objects but are stored internally as BSON. BSON allows MongoDB to efficiently store and retrieve data.</li> <li>Storage in MongoDB: </li> <li>MongoDB does not store each document as a separate file on the filesystem. Instead, all documents within a collection are stored together in large database files managed by MongoDB\u2019s storage engine.</li> <li>These database files typically reside in the <code>dbpath</code> directory of your MongoDB installation and have extensions like <code>.wt</code> for WiredTiger.</li> <li>Accessing Data: When you query MongoDB, it retrieves the BSON documents from these large files and converts them to JSON-like structures for ease of use in your application.</li> </ul>"},{"location":"MongoDB/HowMongoDBStoresFiles.html#summary","title":"Summary","text":"<p>For conceptual understanding, you can think of each document as a JSON object. However, in reality, these documents are stored in a more efficient binary format (BSON) within MongoDB's internal database files.</p>"},{"location":"MongoDB/MongoDBCommands.html","title":"MongoDB Command Cheatsheet","text":""},{"location":"MongoDB/MongoDBCommands.html#using-the-mongodb-shell","title":"Using the MongoDB Shell","text":""},{"location":"MongoDB/MongoDBCommands.html#connecting-to-mongodb","title":"Connecting to MongoDB","text":"Description Command Connect to a MongoDB instance <code>mongo</code> Connect to a specific database <code>mongo &lt;database_name&gt;</code> Connect to a remote MongoDB server <code>mongo &lt;hostname&gt;:&lt;port&gt;/&lt;database_name&gt;</code> Connect via mongosh <code>mongosh</code> Connect to specific host and port via mongosh <code>mongosh --host &lt;host&gt; --port &lt;port&gt; --authenticationDatabase admin -u &lt;user&gt; -p &lt;pwd&gt;</code> Connect via mongosh with connection string <code>mongosh \"mongodb://&lt;user&gt;:&lt;password&gt;@192.168.1.1:27017\"</code> Connect to MongoDB Atlas via mongosh <code>mongosh \"mongodb+srv://cluster-name.abcde.mongodb.net/&lt;dbname&gt;\" --apiVersion 1 --username &lt;username&gt;</code>"},{"location":"MongoDB/MongoDBCommands.html#database-operations","title":"Database Operations","text":"Description Command Show all databases <code>show dbs</code> Use a specific database <code>use &lt;database_name&gt;</code> Get the current database <code>db</code> Drop the current database <code>db.dropDatabase()</code>"},{"location":"MongoDB/MongoDBCommands.html#collection-operations","title":"Collection Operations","text":"Description Command Show all collections in the current database <code>show collections</code> Create a collection <code>db.createCollection(\"&lt;collection_name&gt;\")</code> Drop a collection <code>db.&lt;collection_name&gt;.drop()</code>"},{"location":"MongoDB/MongoDBCommands.html#crud-operations","title":"CRUD Operations","text":""},{"location":"MongoDB/MongoDBCommands.html#create","title":"Create","text":"Description Command Insert a document <code>db.coll.insertOne({name: \"Max\"})</code> Insert multiple documents (ordered) <code>db.coll.insertMany([{name: \"Max\"}, {name: \"Alex\"}])</code> Insert multiple documents (unordered) <code>db.coll.insertMany([{name: \"Max\"}, {name: \"Alex\"}], {ordered: false})</code> Insert document with current date <code>db.coll.insertOne({date: ISODate()})</code> Insert document with write concern <code>db.coll.insertOne({name: \"Max\"}, {\"writeConcern\": {\"w\": \"majority\", \"wtimeout\": 5000}})</code>"},{"location":"MongoDB/MongoDBCommands.html#read","title":"Read","text":"Description Command Find one document <code>db.coll.findOne()</code> Find all documents <code>db.coll.find()</code> Find all documents (pretty print) <code>db.coll.find().pretty()</code> Find documents with a query <code>db.coll.find({name: \"Max\", age: 32})</code> Find documents with date <code>db.coll.find({date: ISODate(\"2020-09-25T13:57:17.180Z\")})</code> Explain query execution stats <code>db.coll.find({name: \"Max\", age: 32}).explain(\"executionStats\")</code> Get distinct values of a field <code>db.coll.distinct(\"name\")</code> Count documents with a query <code>db.coll.countDocuments({age: 32})</code> Get estimated document count <code>db.coll.estimatedDocumentCount()</code>"},{"location":"MongoDB/MongoDBCommands.html#update","title":"Update","text":"Description Command Update a document (set fields) <code>db.coll.updateOne({\"_id\": 1}, {$set: {\"year\": 2016, name: \"Max\"}})</code> Update a document (unset fields) <code>db.coll.updateOne({\"_id\": 1}, {$unset: {\"year\": 1}})</code> Rename a field <code>db.coll.updateOne({\"_id\": 1}, {$rename: {\"year\": \"date\"}})</code> Increment a field <code>db.coll.updateOne({\"_id\": 1}, {$inc: {\"year\": 5}})</code> Multiply a field <code>db.coll.updateOne({\"_id\": 1}, {$mul: {price: NumberDecimal(\"1.25\"), qty: 2}})</code> Set minimum value of a field <code>db.coll.updateOne({\"_id\": 1}, {$min: {\"imdb\": 5}})</code> Set maximum value of a field <code>db.coll.updateOne({\"_id\": 1}, {$max: {\"imdb\": 8}})</code> Set current date <code>db.coll.updateOne({\"_id\": 1}, {$currentDate: {\"lastModified\": true}})</code> Set current date as timestamp <code>db.coll.updateOne({\"_id\": 1}, {$currentDate: {\"lastModified\": {$type: \"timestamp\"}}})</code> Array updates (push) <code>db.coll.updateOne({\"_id\": 1}, {$push: {\"array\": 1}})</code> Array updates (pull) <code>db.coll.updateOne({\"_id\": 1}, {$pull: {\"array\": 1}})</code> Array updates (addToSet) <code>db.coll.updateOne({\"_id\": 1}, {$addToSet: {\"array\": 2}})</code> Array updates (pop last element) <code>db.coll.updateOne({\"_id\": 1}, {$pop: {\"array\": 1}})</code> Array updates (pop first element) <code>db.coll.updateOne({\"_id\": 1}, {$pop: {\"array\": -1}})</code> Array updates (pullAll) <code>db.coll.updateOne({\"_id\": 1}, {$pullAll: {\"array\": [3, 4, 5]}})</code> Array updates (push with each) <code>db.coll.updateOne({\"_id\": 1}, {$push: {\"scores\": {$each: [90, 92]}}})</code> Array updates (push with sort) <code>db.coll.updateOne({\"_id\": 2}, {$push: {\"scores\": {$each: [40, 60], $sort: 1}}})</code> Update array element <code>db.coll.updateOne({\"_id\": 1, \"grades\": 80}, {$set: {\"grades.$\": 82}})</code> Update all array elements <code>db.coll.updateMany({}, {$inc: {\"grades.$[]\": 10}})</code> Update array elements with filter <code>db.coll.updateMany({}, {$set: {\"grades.$[element]\": 100}}, {multi: true, arrayFilters: [{\"element\": {$gte: 100}}]})</code> Find and update <code>db.coll.findOneAndUpdate({\"name\": \"Max\"}, {$inc: {\"points\": 5}}, {returnNewDocument: true})</code> Upsert (update or insert) <code>db.coll.updateOne({\"_id\": 1}, {$set: {item: \"apple\"}, $setOnInsert: {defaultQty: 100}}, {upsert: true})</code> Replace a document <code>db.coll.replaceOne({\"name\": \"Max\"}, {\"firstname\": \"Maxime\", \"surname\": \"Beugnet\"})</code> Update with write concern <code>db.coll.updateMany({}, {$set: {\"x\": 1}}, {\"writeConcern\": {\"w\": \"majority\", \"wtimeout\": 5000}})</code>"},{"location":"MongoDB/MongoDBCommands.html#delete","title":"Delete","text":"Description Command Delete one document <code>db.coll.deleteOne({name: \"Max\"})</code> Delete multiple documents <code>db.coll.deleteMany({name: \"Max\"}, {\"writeConcern\": {\"w\": \"majority\", \"wtimeout\": 5000}})</code> Delete all documents <code>db.coll.deleteMany({})</code> Find and delete <code>db.coll.findOneAndDelete({\"name\": \"Max\"})</code>"},{"location":"MongoDB/MongoDBCommands.html#indexing","title":"Indexing","text":"Description Command Create an index on a field <code>db.&lt;collection_name&gt;.createIndex({&lt;field&gt;: 1})</code> Create a unique index <code>db.&lt;collection_name&gt;.createIndex({&lt;field&gt;: 1}, {unique: true})</code> List all indexes on a collection <code>db.&lt;collection_name&gt;.getIndexes()</code> Drop an index <code>db.&lt;collection_name&gt;.dropIndex(\"&lt;index_name&gt;\")</code> Hide an index <code>db.coll.hideIndex(\"name_1\")</code> Unhide an index <code>db.coll.unhideIndex(\"name_1\")</code>"},{"location":"MongoDB/MongoDBCommands.html#aggregation","title":"Aggregation","text":"Description Command Aggregation framework <code>db.&lt;collection_name&gt;.aggregate([ { $match: { &lt;field&gt;: &lt;value&gt; } }, { $group: { _id: \"$&lt;group_field&gt;\", total: { $sum: \"$&lt;sum_field&gt;\" } } }, { $sort: {total: -1} } ])</code>"},{"location":"MongoDB/MongoDBCommands.html#backup-and-restore","title":"Backup and Restore","text":"Description Command Backup a database <code>mongodump --db &lt;database_name&gt; --out &lt;backup_directory&gt;</code> Restore a database <code>mongorestore --db &lt;database_name&gt; &lt;backup_directory&gt;/&lt;database_name&gt;</code>"},{"location":"MongoDB/MongoDBCommands.html#user-management","title":"User Management","text":"Description Command Create a new user <code>db.createUser({ user: \"&lt;username&gt;\", pwd: \"&lt;password&gt;\", roles: [ { role: \"&lt;role&gt;\", db: \"&lt;database&gt;\" } ] })</code> Show users <code>show users</code> Drop a user <code>db.dropUser(\"&lt;username&gt;\")</code>"},{"location":"MongoDB/MongoDBCommands.html#server-administration","title":"Server Administration","text":"Description Command Server status <code>db.serverStatus()</code> Database statistics <code>db.stats()</code> Collection statistics <code>db.&lt;collection_name&gt;.stats()</code> Current operations <code>db.currentOp()</code> Kill an operation <code>db.killOp(&lt;operation_id&gt;)</code> Lock the database <code>db.fsyncLock()</code> Unlock the database <code>db.fsyncUnlock()</code> Get collection names <code>db.getCollectionNames()</code> Get collection info <code>db.getCollectionInfos()</code> Print collection stats <code>db.printCollectionStats()</code> Replication info <code>db.getReplicationInfo()</code> Print replication info <code>db.printReplicationInfo()</code> Server info <code>db.hello()</code> Host info <code>db.hostInfo()</code> Shutdown server <code>db.shutdownServer()</code> Profiling status <code>db.getProfilingStatus()</code> Set profiling level <code>db.setProfilingLevel(1, 200)</code> Enable free monitoring <code>db.enableFreeMonitoring()</code> Disable free monitoring <code>db.disableFreeMonitoring()</code> Get free monitoring status <code>db.getFreeMonitoringStatus()</code>"},{"location":"MongoDB/MongoDBCommands.html#handy-commands","title":"Handy Commands","text":"Description Command Use admin database <code>use admin</code> Create root user <code>db.createUser({\"user\": \"root\", \"pwd\": passwordPrompt(), \"roles\": [\"root\"]})</code> Drop root user <code>db.dropUser(\"root\")</code> Authenticate user <code>db.auth( \"user\", passwordPrompt() )</code> Switch to test database <code>use test</code> Get sibling database <code>db.getSiblingDB(\"dbname\")</code> Get current operations <code>db.currentOp()</code> Kill operation <code>db.killOp(123)</code> Get collection stats <code>db.printCollectionStats()</code> Get server status <code>db.serverStatus()</code> Create a view <code>db.createView(\"viewName\", \"sourceColl\", [{$project:{department: 1}}])</code>"},{"location":"MongoDB/MongoDBCommands.html#change-streams","title":"Change Streams","text":"Description Command Watch for changes <code>watchCursor = db.coll.watch([ { $match : {\"operationType\" : \"insert\" } } ])</code> Iterate change stream <code>while (!watchCursor.isExhausted()){ if (watchCursor.hasNext()){ print(tojson(watchCursor.next())); } }</code>"},{"location":"MongoDB/MongoDBCommands.html#replica-set","title":"Replica Set","text":"Description Command Replica set status <code>rs.status()</code> Initiate replica set <code>rs.initiate({\"_id\": \"RS1\", members: [ { _id: 0, host: \"mongodb1.net:27017\" }, { _id: 1, host: \"mongodb2.net:27017\" }, { _id: 2, host: \"mongodb3.net:27017\" }]})</code> Add a member <code>rs.add(\"mongodb4.net:27017\")</code> Add an arbiter <code>rs.addArb(\"mongodb5.net:27017\")</code> Remove a member <code>rs.remove(\"mongodb1.net:27017\")</code> Get replica set config <code>rs.conf()</code> Replica set hello <code>rs.hello()</code> Print replication info <code>rs.printReplicationInfo()</code> Print secondary replication info <code>rs.printSecondaryReplicationInfo()</code> Reconfigure replica set <code>rs.reconfig(config)</code> Set read preference <code>db.getMongo().setReadPref('secondaryPreferred')</code> Step down primary <code>rs.stepDown(20, 5)</code>"},{"location":"MongoDB/MongoDBCommands.html#sharded-cluster","title":"Sharded Cluster","text":"Description Command Print sharding status <code>db.printShardingStatus()</code> Sharding status <code>sh.status()</code> Add a shard <code>sh.addShard(\"rs1/mongodb1.example.net:27017\")</code> Shard a collection <code>sh.shardCollection(\"mydb.coll\", {zipcode: 1})</code> Move a chunk <code>sh.moveChunk(\"mydb.coll\", { zipcode: \"53187\" }, \"shard0019\")</code> Split a chunk at a key <code>sh.splitAt(\"mydb.coll\", {x: 70})</code> Split a chunk by find query <code>sh.splitFind(\"mydb.coll\", {x: 70})</code> Start balancer <code>sh.startBalancer()</code> Stop balancer <code>sh.stopBalancer()</code> Disable balancing <code>sh.disableBalancing(\"mydb.coll\")</code> Enable balancing <code>sh.enableBalancing(\"mydb.coll\")</code> Get balancer state <code>sh.getBalancerState()</code> Set balancer state <code>sh.setBalancerState(true/false)</code> Is balancer running <code>sh.isBalancerRunning()</code> Start auto merger <code>sh.startAutoMerger()</code> Stop auto merger <code>sh.stopAutoMerger()</code> Enable auto merger <code>sh.enableAutoMerger()</code> Disable auto merger <code>sh.disableAutoMerger()</code> Update zone key range <code>sh.updateZoneKeyRange(\"mydb.coll\", {state: \"NY\", zip: MinKey }, { state: \"NY\", zip: MaxKey }, \"NY\")</code> Remove range from zone <code>sh.removeRangeFromZone(\"mydb.coll\", {state: \"NY\", zip: MinKey }, { state: \"NY\", zip: MaxKey })</code> Add shard to zone <code>sh.addShardToZone(\"shard0000\", \"NYC\")</code> Remove shard from zone <code>sh.removeShardFromZone(\"shard0000\", \"NYC\")</code>"},{"location":"PowerPlatform/CustomConnectors.html","title":"Custom Connectors","text":""},{"location":"PowerPlatform/CustomConnectors.html#background","title":"Background","text":"<p>Power Platform is great in automation, but its true potential unlocks when you can connect it to non-standard data sources. This is where custom connectors come in, acting as bridges between Power Platform and external APIs. </p> <p>In this guide I will show you how to create custom connectors:</p>"},{"location":"PowerPlatform/CustomConnectors.html#steps-to-creating-custom-connectors-for-power-platform","title":"Steps to creating Custom Connectors for Power Platform","text":"<p>Building the Connector in Power Automate:</p> <ol> <li> <p>Launch Power Automate: Sign in and navigate to the \"Solutions\" section. Create a new solution or open an existing one.</p> </li> <li> <p>Add a Custom Connector: Within the solution, expand the \"New\" menu and select \"Automation\" &gt; \"Custom connector.\"</p> </li> <li> <p>Define Connection Details:  Choose \"Create from blank\" and provide a descriptive name for your connector.</p> </li> <li> <p>Import API Definition (Optional): If the API offers an OpenAPI (Swagger) definition file (.json or .yaml), you can import it to automatically populate actions and data models. Otherwise, you'll need to define them manually.</p> </li> <li> <p>Define Actions:  Here's where the magic happens! Create actions representing the functionalities you want to expose in Power Apps. Each action should have a clear name, a concise description, and well-defined parameters (inputs) and outputs. </p> </li> <li> <p>Parameters: Specify the data types (text, number, etc.) required for each action.</p> </li> <li>Outputs: Define the structure of the data returned by the action (often matching the API's response format).</li> </ol>"},{"location":"PowerPlatform/CustomConnectors.html#example-action","title":"Example Action","text":"<p>Let's say you're building a connector for a weather API. You might create an action named \"GetWeather\" that accepts a \"city\" parameter (text) and returns an output containing \"temperature,\" \"humidity,\" and \"weather description\" (all text).</p> <ol> <li> <p>Configure Authentication:  Specify the authentication method your chosen API requires. This could involve API keys, OAuth, or other methods.</p> </li> <li> <p>Test and Validate:  The \"Test\" tab allows you to send test requests with sample data to ensure your connector functions as expected. Refine your definitions until the tests pass successfully.</p> </li> <li> <p>Save and Publish:  Once everything is working smoothly, save your connector and then publish it to make it available in your environment.</p> </li> </ol>"},{"location":"PowerPlatform/CustomConnectors.html#using-the-custom-connector-in-power-apps","title":"Using the Custom Connector in Power Apps","text":"<ol> <li> <p>Create a Power App:  Build a new Power App or open an existing one.</p> </li> <li> <p>Connect to the Custom Connector:  Navigate to the \"Data\" pane and select \"Add data.\" Choose the \"Connectors\" tab and locate your custom connector by name. Click on it to establish a connection.</p> </li> <li> <p>Utilize the Connector Actions:  Within your Power App formulas and expressions, you can now leverage the actions you defined in your custom connector.</p> </li> </ol>"},{"location":"PowerPlatform/CustomConnectors.html#example-usage","title":"Example Usage","text":"<p>In your weather app, you could use the \"GetWeather\" action within a formula to display the current temperature and weather description for a user-entered city.</p>"},{"location":"PowerPlatform/ECMCaptureFlow.html","title":"ECM Capture","text":""},{"location":"PowerPlatform/ECMCaptureFlow.html#overview","title":"Overview","text":"<p>Image capture and extraction workflows are heavily used in the Banking and Insurance sectors. Almost all institutions use products like OpenText inteligent capture, Kofax Capture, Datacap  etc to Capture documents from multiple channels and store them in backend ECM systems like Opentext content server, SharePoint server etc.</p>"},{"location":"PowerPlatform/ECMCaptureFlow.html#traditional-method-of-document-capture","title":"Traditional method of document capture","text":"<p>In the banking and insurance sectors, the usual way to handle document capture is shown in the diagram below:</p> <p></p>"},{"location":"PowerPlatform/ECMCaptureFlow.html#alternative-method-using-power-platform","title":"Alternative method - using Power Platform","text":"<p>I'll demonstrate how you can use Power Platform to achieve some of these functions easily. I'll guide you through a simplified solution to get started with this approach.</p>"},{"location":"PowerPlatform/ECMCaptureFlow.html#lets-get-started","title":"Let's get started","text":""},{"location":"PowerPlatform/ECMCaptureFlow.html#our-workflow","title":"Our Workflow","text":"<p>This guide outlines how to capture and extract data from invoices and store them in SharePoint using a simple workflow. Invoices are placed in a dedicated folder on OneDrive and processed automatically when a new file is detected by the Power Automate OneDrive trigger.</p> <p></p>"},{"location":"PowerPlatform/ECMCaptureFlow.html#how-to-create-the-workflow","title":"How to Create the Workflow","text":"<p>Follow these steps to set up the automated workflow:</p> <ol> <li>In Power Automate, click + Create &gt; Automated cloud flow. Provide a flow name, enter When a file is created in the search box, and then select When a file is created (OneDrive for Business).</li> </ol> <p></p> <ol> <li>Click on Next Step, then + New step. To specify the folder to monitor, click on the folder icon under Parameters and select the OneDrive folder that will contain the invoices.</li> </ol> <p></p> <ol> <li>Click + Add an action, search for Extract information from invoice, and select AI Builder &gt; Extract information from invoices.</li> </ol> <p></p> <ol> <li>Configure the Extract information from invoices step by clicking on it and then selecting the Parameters tab. For Invoice File, click the lightning bolt icon , then select File content.</li> </ol> <p></p> <ol> <li>Add another action by selecting SharePoint &gt; Create file.</li> </ol> <p></p> <ol> <li>Configure the Create file action with the following details:</li> <li>Site Address: Enter the SharePoint site URL where the document library is located.</li> <li>Folder Path: Choose the document library.</li> <li>File Name: Select When a file is created &gt; File name.</li> <li>File Content: Choose When a file is created &gt; File content.</li> </ol> <p></p> <ol> <li>Add an action for SharePoint &gt; Update file properties. Configure it by providing:</li> <li>The site address hosting the document library.</li> <li>The document library from the dropdown menu.</li> <li>Id: Select Create file &gt; body &gt; ItemId.</li> <li>Map the properties of the document with values extracted from the invoice in the previous steps.</li> </ol> <p></p> <ol> <li>To test the flow, click Test, opt for a manual test, and place a file in the monitored OneDrive folder. After a short period, the flow will trigger, and the SharePoint library will be updated with the document and all extracted metadata.</li> </ol> <p></p> <p>Below is how the file will appear in the document library, showcasing the extracted metadata:</p> <p></p>"},{"location":"PowerPlatform/ECMCaptureFlow.html#conclusion","title":"Conclusion","text":"<p>Choosing between AI-based methods and traditional document capture software like Kofax Capture or Captiva Capture involves a careful evaluation of various factors:</p> <ul> <li> <p>Licensing Costs: Traditional software often has licensing fees based on usage volume. For instance, a Kofax Capture license for processing 100,000 pages annually may cost approximately $16,526.00 Kofax Capture License Cost.</p> </li> <li> <p>Functionality and Modules: OpenText Intelligent Capture (formerly Captiva) includes a suite of modules designed for different aspects of document handling, such as Standard Import, Standard Export, and Completion (for manual verification), as well as eInput (for browser-based capture). These features may integrate seamlessly into the workflow of financial or insurance environments, offering out-of-the-box solutions.</p> </li> <li> <p>Integration and Customization: Microsoft Power Automate allows for the creation of customized document capture workflows, which can be advantageous for those already utilizing the Microsoft 365 platform. It offers a way to avoid substantial licensing fees while still achieving a high level of functionality.</p> </li> <li> <p>Scan-to-network-folder: A very typical document capture is to scan to a network folder and use Captiv standard import to crate batches. I have seen that this workflow can be easily created using Power Autoamte. All you have to do is configure your scanner to scan to the oneDrive folder.</p> </li> </ul> <p>Long story short: The decision hinges on your company's specific needs, existing infrastructure, and budget. For future reference, should this subject arise, consider conducting a POC to show the power of Power Automate to your management team and let them take a call!</p>"},{"location":"PowerPlatform/ECMCaptureFlow.html#further-reading","title":"Further Reading","text":"<p>Use the invoice processing prebuilt model in Power Automate.</p>"},{"location":"PowerPlatform/EnableMicrosoftSyntex.html","title":"Overview","text":"<p>Microsoft Syntex uses content and AI to enable end-to-end intelligent document processing solutions.</p> <p>With Microsoft Syntex you can: - Quickly process and extract information from common business documents like contracts, invoices, and receipts. - Identify field values in structured documents (forms, invoices) and extract information from unstructured documents (letters, contracts). Image Tagging, Taxonomy Tagging, and Translation are also available.</p>"},{"location":"PowerPlatform/EnableMicrosoftSyntex.html#steps-to-activate-microsoft-syntex","title":"Steps to activate Microsoft Syntex","text":"<ol> <li>Access the Microsoft 365 Admin Center:</li> <li> <p>Log in to your Microsoft 365 admin center with an account that has Global admin or SharePoint admin permissions.</p> </li> <li> <p>Navigate to Files and Content Settings:</p> </li> <li>In the admin center, go to Setup.       </li> <li> <p>Under the Files and content section, select Use content AI with Microsoft Syntex.       </p> </li> <li> <p>Manage Microsoft Syntex:</p> </li> <li> <p>On the Use content AI with Microsoft Syntex page, click Manage Microsoft Syntex.</p> </li> <li> <p>Choose the Service to Set Up:</p> </li> <li> <p>Select the specific Microsoft Syntex service that you want to set up. You can choose from various services like prebuilt document processing, structured and freeform document processing, unstructured document processing, content assembly, image tagging, taxonomy tagging, translation, Syntex eSignature, optical character recognition, Microsoft 365 Archive (Preview), and Microsoft 365 Backup (Preview).</p> </li> <li> <p>Configure Options:</p> </li> <li>Customize the options based on your requirements.</li> <li>Click Save to apply the settings.</li> </ol> <p>Here are direct links to setup instructions for each service: - Set up prebuilt document processing - Set up structured and freeform document processing - Set up unstructured document processing - Set up content assembly - Set up image tagging - Set up taxonomy tagging - Set up document translation - Set up Syntex eSignature - Set up optical character recognition - Set up Microsoft 365 Archive (Preview) - Set up Microsoft 365 Backup (Preview)</p>"},{"location":"PowerPlatform/EnableSyntexOnYourDocumentLibrary.html","title":"Create a model on a local SharePoint site with Microsoft Syntex","text":""},{"location":"PowerPlatform/EnableSyntexOnYourDocumentLibrary.html#create-a-model-on-a-local-site","title":"Create a model on a local site","text":"<ol> <li> <p>From a SharePoint document library, select the files you want to analyze, and then select Classify and extract.</p> <p></p> </li> <li> <p>The first time you use this feature, you're activating Syntex on your site. You'll see the following message.</p> <p></p> <p>Note</p> <p>You must have the Manage Web Site permission to perform administration tasks and manage content for the site. This would be a site owner. Once the feature is activated, anyone with the Manage Lists permission will be able to create and manage models.</p> </li> <li> <p>Select Activate to continue. You'll see the following message.</p> <p></p> </li> <li> <p>Select Create a model.</p> </li> <li> <p>On the Create a model panel, type the name of the model, add a description, and then select Create.</p> <p></p> </li> <li> <p>Proceed to train your custom model or to configure your trained model using the files that you selected.</p> </li> <li> <p>When done, the Add to library panel opens.</p> <p></p> </li> <li> <p>On the Add to library panel, you'll see the name of your SharePoint site and the document library that the model will be applied to. If you want to apply the model to a different library, select Back to libraries, and choose the library you want to use. Then select Add.</p> </li> <li> <p>On the model home page, in the Where the model is applied on this site section, you can see the libraries that have the model applied. To apply the model to other libraries on the site, select Apply model.</p> <p></p> </li> </ol>"},{"location":"PowerPlatform/GoogleProviderPowerPages.html","title":"Power Pages","text":""},{"location":"PowerPlatform/GoogleProviderPowerPages.html#overview","title":"Overview","text":"<p>Power Apps portals &amp; Dynamics 365 portals are now called Power Pages. Power Pages websites can be open-to-all or you can have register/sign-in using Google, LinkedIn, Twitter, and Facebook.</p> <p></p> <p>Here, I will show you how to use Google sign-in on your Power Pages websites. The protocol behind this is OAuth2.0. Using this Google allow users to access your site using their google accounts without exposing their userid/passwords.</p>"},{"location":"PowerPlatform/GoogleProviderPowerPages.html#steps-to-follow","title":"Steps to follow","text":""},{"location":"PowerPlatform/GoogleProviderPowerPages.html#set-up-google-in-power-pages","title":"Set up Google in Power Pages","text":"<ol> <li>In your Power Pages site, select Set up &gt; Identity providers. </li> <li> <p>Google &gt; More Commands (\u2026) &gt; Configure. </p> </li> <li> <p>Select Next.</p> </li> <li>Under Reply URL, select Copy.</li> <li>Select Open Google.</li> </ol>"},{"location":"PowerPlatform/GoogleProviderPowerPages.html#register-an-app-in-google","title":"Register an App in Google","text":""},{"location":"PowerPlatform/GoogleProviderPowerPages.html#add-the-api","title":"Add the API","text":"<ol> <li> <p>Open the Google Developers Console. And create an API project. </p> </li> <li> <p>In the left side panel, select APIs &amp; Services.</p> </li> <li> <p>Select + Enable APIs and Services. </p> </li> <li> <p>Search for and enable Google People API. </p> </li> </ol>"},{"location":"PowerPlatform/GoogleProviderPowerPages.html#set-up-your-consent-screen","title":"Set up your consent screen","text":"<ol> <li> <p>In the left side panel, select Credentials &gt; Configure consent screen. </p> </li> <li> <p>Select the External user type and click Create.</p> </li> <li>Enter the name of the application and select your organization's user support email address.</li> <li>Upload a logo image file if necessary.</li> <li>Enter the URLs of your site's home page, privacy policy, and terms of service, if applicable.</li> <li>Enter an email address where Google can send you developer notifications. </li> </ol>"},{"location":"PowerPlatform/GoogleProviderPowerPages.html#enter-your-top-level-domain","title":"Enter your top-level domain","text":"<ol> <li>Under Authorized domains, select + Add Domain.</li> <li>Enter your site's top-level domain; for example, <code>powerappsportals.com</code>.</li> <li>Select Save and Continue.</li> </ol>"},{"location":"PowerPlatform/GoogleProviderPowerPages.html#add-credentials","title":"Add credentials","text":"<ol> <li>In the left side panel, select Credentials.</li> <li> <p>Select Create credentials &gt; OAuth client ID. </p> </li> <li> <p>Select Web application as the application type.</p> </li> <li>Enter any name e.g. <code>Web sign-in</code>. This is internal. Not shown.</li> <li>Under Authorized JavaScript origins, select + Add URI.</li> <li>Enter your site's URL; for example, <code>https://mySite.powerappsportals.com</code>.</li> <li>Under Authorized redirect URIs, select + Add URI.</li> <li> <p>Enter your site's URL followed by <code>/signin</code>; for example, <code>https://mySite.powerappsportals.com/signin</code>. </p> </li> <li> <p>Select Create.</p> </li> <li> <p>In the OAuth client created window, select the copy icons to copy the Client ID and Client secret. </p> </li> <li> <p>Click CLOSE.</p> </li> </ol>"},{"location":"PowerPlatform/GoogleProviderPowerPages.html#enter-site-settings-in-power-pages","title":"Enter site settings in Power Pages","text":"<ol> <li>Return to the Power Pages Configure identity provider page you left earlier.</li> <li>Under Configure site settings, paste the following values:<ul> <li>Client ID\u200b: Paste the Client ID you copied.</li> <li>Client secret: Paste the Client secret you copied. </li> </ul> </li> <li>Click Continue then Close </li> </ol>"},{"location":"PowerPlatform/GoogleProviderPowerPages.html#appendix","title":"Appendix","text":"<p>How to open Power Page Design Studio</p> <ul> <li>Go to Power Pages(https://make.powerpages.microsoft.com/) </li> <li>Select the Microsoft Dataverse environment </li> <li>Locate your site in the Active sites list. </li> </ul>"},{"location":"PowerPlatform/HealthClinicDataverseSecurity.html","title":"Security","text":""},{"location":"PowerPlatform/HealthClinicDataverseSecurity.html#health-clinic-management-system","title":"Health Clinic Management System","text":""},{"location":"PowerPlatform/HealthClinicDataverseSecurity.html#background","title":"Background","text":"<p>A health clinic employs Microsoft's Power Platform, built on Microsoft Dataverse, to manage its patient records, appointments, and internal communications efficiently and securely. The system includes:</p> <ul> <li>Patient Records Management (Model-driven App)</li> <li>Appointment Scheduling (Canvas App)</li> <li>Internal Communications (Microsoft Teams integration with automated workflows)</li> </ul>"},{"location":"PowerPlatform/HealthClinicDataverseSecurity.html#security-requirements-and-implementation","title":"Security Requirements and Implementation","text":"<p>1. Licensing - All staff members are provided with Power Apps licenses, determining their base level of access to the Power Platform ecosystem.</p> <p>2. Environment and Business Unit Setup - The clinic establishes a single Dataverse environment as the primary container for its applications and data. - Within this environment, departments such as Pediatrics and Orthopedics are set up as distinct Business Units, reflecting the clinic's hierarchical and matrix organizational structure.</p> <p>3. Hierarchical and Matrix Data Access Structure - Senior medical staff are granted broad access across business units due to their role, allowing them to oversee patient care across departments. - Specialized medical staff, such as those with cross-departmental expertise, are given matrix access to multiple business units, enabling them to access patient records in both Pediatrics and Orthopedics.</p> <p>4. Security Roles, Teams, and Ownership - Custom security roles are created and assigned to staff based on their department and job function. These roles define access at the business unit level, table/record level, and even field level for sensitive information. - Teams are formed to group staff by their function and department. These teams facilitate efficient permission management and reflect the clinic's organizational structure within Dataverse.   - Medical Staff Teams: Include doctors and nurses, with access permissions to patient records based on their specific roles and the hierarchical structure.   - Receptionist Team: Has access to the Appointment Scheduling app but limited access to patient records, ensuring they can manage appointments without viewing sensitive health information.   - Group Teams (Microsoft 365 groups) are used for internal communications, integrating seamlessly with Microsoft Teams for efficient collaboration without compromising patient data security.</p> <p>5. Table/Record Ownership and Access Controls - Patient records are owned by the clinic but are accessible only to authorized medical personnel based on their security roles and team memberships. - Appointment data is managed by receptionists, who are the primary owners of this data, ensuring that only they can make changes to the schedule.</p>"},{"location":"PowerPlatform/HealthClinicDataverseSecurity.html#advanced-security-measures","title":"Advanced Security Measures","text":"<ul> <li>Row-Level Security is implemented to ensure that only medical staff involved in a patient's care can access their records.</li> <li>Field-Level Security restricts access to highly sensitive patient information, such as medical history, to authorized senior medical staff only.</li> <li>Custom connectors to external services (e.g., SMS notifications for appointments) are secured and managed based on the credentials of the service, ensuring data is shared securely and in compliance with health data protection regulations.</li> </ul>"},{"location":"PowerPlatform/HealthClinicDataverseSecurity.html#conclusion","title":"Conclusion","text":"<p>Hope this gave you good overview of how Dataverse security concepts are implemented.</p>"},{"location":"PowerPlatform/HelloDataverse.html","title":"Hello Dataverse","text":""},{"location":"PowerPlatform/HelloDataverse.html#what-is-dataverse","title":"What is Dataverse?","text":"<p>Dataverse is Power Platform's Database - for simplicty.</p>"},{"location":"PowerPlatform/HelloDataverse.html#what-are-dataverse-verions","title":"What are Dataverse verions?","text":"<ul> <li>Microsoft Dataverse: Premium edition. Requires PowerApps subscription.</li> <li>Dataverse for Teams: Free edition.</li> </ul>"},{"location":"PowerPlatform/HelloDataverse.html#dataverse-storage-behind-the-scenes","title":"Dataverse Storage - behind the scenes","text":""},{"location":"PowerPlatform/HelloDataverse.html#dataverse-vs-mssql-tables-a-quick-comparison","title":"Dataverse vs. MSSQL Tables: A Quick Comparison","text":"<p>Hello readers! Hope you had a wonderful day. Today, let's explore one of the important aspects of Power Platform: Dataverse. How does it differ from other options like MSSQL? What's its importance? Let's find out!</p>"},{"location":"PowerPlatform/HelloDataverse.html#long-story-short","title":"Long story short","text":"<p>Dataverse and MSSQL, while both store data in tables within the Microsoft ecosystem, there are many differences between them. Here is a quick breakdown to help you understand the key differences:</p> Feature Dataverse MSSQL Schema Flexible Fixed Data Types Various (complex included) Similar (granular control) Relationships Built-in Foreign Keys Data Manipulation User-friendly interface (low-code) T-SQL (code), Programming languages Security RBAC (roles), Auditing Manual setup, Auditing (configuration needed) Table Types Standard, Virtual, Custom, Elastic (limited) Base, Temporary, Views, Table-Valued Functions Views Limited (virtual tables) Traditional views (complex) Business Logic (Table Level) Power Automate workflows Stored Procedures, Triggers Business Logic (Column Level) Validation Rules, Workflows Constraints, Triggers Ideal for User-friendly app development, Built-in security, Power Platform Integration Complex data management, Granular control, Complex data models <p>Table Types: Also, here are some differences between the types of tables allowed in both.</p> Feature Dataverse MSSQL Standard Tables \u2713 \u2713 Virtual Tables (simplified data views) \u2713 Views (complex data views) Custom Tables (more control) \u2713 Elastic Tables (large datasets) \u2713 (limited)"},{"location":"PowerPlatform/HelloDataverse.html#how-to-choose","title":"How to choose?","text":"<ul> <li>For Power Platform users, Dataverse stands out as the go-to choice. User-friendly interface and low-code approach make it ideal for simple data models. Built-in security and lots of OOTB tables for various applications will give you a head start. These pre-built tables can be used immediately and further customized for specific needs.</li> <li>Alternatively, choose MSSQL if you have a complex data model, want to build things from scratch and want to store a huge volume of data.</li> </ul>"},{"location":"PowerPlatform/HelloDynamics365.html","title":"Hello Dynamics","text":""},{"location":"PowerPlatform/HelloDynamics365.html#list-of-dynamics-365-products","title":"List of Dynamics 365 products","text":"<p>Here\u2019s a table showcasing various products within the Dynamics 365 ecosystem. Keep in mind that new products may be added or existing ones may change names over time, so this table might not always reflect the latest updates:</p> Product Description Link Dynamics 365 Sales Focuses on sales processes, lead management, and pipeline tracking. Enhance revenue generation. Sales Dynamics 365 Marketing Personalize customer journeys, create targeted campaigns, and analyze marketing performance. Marketing Dynamics 365 Finance Manage financial operations, track expenses, and optimize financial processes. Finance Dynamics 365 Supply Chain Mgmt Improve supply chain visibility, efficiency, and collaboration across the supply network. Supply Chain Dynamics 365 Field Service Streamline field service operations, manage work orders, and empower field technicians. Field Service Dynamics 365 Commerce Manage e-commerce, point-of-sale, and store operations in the retail industry. Commerce Dynamics 365 Human Resources Focuses on workforce management, employee engagement, and talent acquisition. Human Resources Dynamics 365 Business Central Integrated solution for small and medium-sized businesses, covering finance, sales, service, and ops. Business Central Dynamics 365 Customer Insights Unify customer data, gain actionable insights, and understand customer behavior and preferences. Customer Insights Dynamics 365 Project Operations Optimize project management, resource allocation, and project financials for project-based organizations. Project Operations Dynamics 365 Customer Service Enhance agent productivity with a browser-like, tabbed experience. Agents can work on multiple cases and conversations simultaneously. It uses AI tools like Smart Assist to identify similar cases and relevant articles, streamlining customer interactions. Customer Service"},{"location":"PowerPlatform/HelloPowerPlatform.html","title":"Hello Platform","text":""},{"location":"PowerPlatform/HelloPowerPlatform.html#microsoft-power-platform","title":"Microsoft Power Platform","text":""},{"location":"PowerPlatform/HelloPowerPlatform.html#key-products","title":"Key Products","text":"<ul> <li>Power Apps - For web and mobile apps. PowerApps is a low-code/no-code platform to create apps quickly. Its data can live in SharePoint, Dynamics, M365, or anywhere. Its language is PowerFX (like Excel formulas). Power apps are canvas apps (any data) and model-driven apps (Dataverse only).</li> <li>Power Automate - Workflows to automate work tasks.</li> <li>Power BI - Data dashboards and reports.</li> <li>Power Pages - For websites.</li> </ul>"},{"location":"PowerPlatform/HelloPowerPlatform.html#supporting-tools","title":"Supporting Tools","text":"<ul> <li>Copilot Studio [Power Virtual Agents] - Tool to automate the automation.</li> <li>Connectors - Connect to Dropbox, Twitter, etc. Approx 900.</li> <li>AI Builder - Add AI functionality to Power Automate and Power Apps.</li> <li>Dataverse - Backend data for Power Platform.</li> <li>Power FX - Programming language for Power Platform.</li> </ul>"},{"location":"PowerPlatform/HelloPowerPlatform.html#a-typical-power-platform-project","title":"A typical power platform project","text":"<p>Receive invoice emails from vendors (Office 365 Outlook) -&gt;  Store attachments in SharePoint (Microsoft SharePoint) -&gt;  Send for approval in Teams (Microsoft Teams) -&gt;  Enter approved invoices in ERP (Oracle) -&gt;  Send confirmation email to vendors (Office 365 Outlook)</p>"},{"location":"PowerPlatform/HelloPowerPlatform.html#dataverse","title":"Dataverse","text":"<ul> <li>Dataverse ~ Power Platform DB.</li> <li>Tables: Standard (OOTB), managed (part of a solution, read-only), custom.</li> <li>4TB storage limit.</li> <li>You can apply business logic to tables! E.g., If country is US, then postal code mandatory.</li> </ul>"},{"location":"PowerPlatform/HelloPowerPlatform.html#copilot","title":"Copilot","text":"<p>Copilot was earlier called Power Virtual Agents. This is like GPT-4/Bing. Here, you just say in plain English what you want. CoPilot will create the power automate/app for you!</p> <p></p>"},{"location":"PowerPlatform/HelloPowerPlatform.html#power-fx","title":"Power FX","text":"<p>This is the language of Power Platform. It's like Excel formulas. It's used in PowerApps, Dataverse, and Copilot Studio.</p> <p></p>"},{"location":"PowerPlatform/HelloPowerPlatform.html#power-platform-and-microsoft-teams","title":"Power Platform and Microsoft Teams","text":"<p>All Power Platform components can be used from within MS Teams.</p> <p> </p>"},{"location":"PowerPlatform/HelloPowerPlatform.html#power-platform-dynamics-365","title":"Power Platform &amp; Dynamics 365","text":"<ul> <li>Power Apps: All Dynamics 365 customer engagement apps are model-driven apps (built in PowerApps). The data for such apps is in Dataverse.</li> <li>Power BI: Can create reports from Dynamics 365.</li> <li>Power Automate: Business process flows are created using Power Automate.</li> <li>Copilot Studio: Call/SMS/Facebook Msgs -&gt; Dynamics 365 Customer Service App -&gt; Copilot -&gt; Live agent.</li> <li>Power Pages: Self-support websites.</li> </ul>"},{"location":"PowerPlatform/HelloPowerPlatform.html#power-platform-and-azure","title":"Power Platform and Azure","text":"<p>Power Platform and Azure offer countless ways to create end-to-end solutions. For instance, an airline project where:</p> <ul> <li>Azure API Management hosts a custom API for airline system communication.</li> <li>A coordinator handles notifications, assigns flights to Teams channels, and sends them to Power Apps.</li> <li>Azure Functions process Graph API calls from a storage queue, sending notifications to Teams and streaming events to Azure Event Hubs.</li> <li>Azure Bot Service powers a custom bot messaging service for flight updates in Teams.</li> <li>Azure Data Lake stores event data from Event Hubs for long-term retention and analytics with Power BI.</li> </ul> <p></p>"},{"location":"PowerPlatform/HelloPowerPlatform.html#dataverse-ootb-features","title":"Dataverse OOTB features","text":"<p>Here is a picture showing OOTB features of Dataverse.</p> <p></p>"},{"location":"PowerPlatform/HelloPowerPlatform.html#dataflows","title":"Dataflows","text":"<ul> <li>ETL tool for Power Platform Ecosystem.</li> <li>Uses very little code.</li> <li>Uses Power Query (like Excel).</li> <li>No infra required, fully on cloud.</li> <li>No separate license, use Power BI / Power Apps license.</li> </ul>"},{"location":"PowerPlatform/HelloPowerPlatform.html#common-data-model","title":"Common Data Model","text":"<p>Common Data Model is a ready-made collection of tables. For healthcare, you have a patient table and an admission table. With this, you don't have to design tables and choose columns. Microsoft has partnered with industries like healthcare, automobile, and banking to create CDM.</p> <p>Long story short: Use Common Data Model. Save yourself from complex data modeling tasks.</p>"},{"location":"PowerPlatform/HelloPowerPlatform.html#data-sources-triggers-actions-connector-types","title":"Data Sources | Triggers &amp; Actions | Connector Types","text":"<p>Data Sources:  - Tabular: SharePoint, SQL Server, Dataverse - Function Based: 365 Users, Azure Blobs</p> <p>Connectors:  - Simply put, they link data sources to the Power Platform. - Types: standard, premium, custom</p> <p></p> <p>Triggers and Actions: - Triggers: Start the flow in Power Automate, like a spark! - Actions: Perform tasks or operations.</p> <p></p>"},{"location":"PowerPlatform/HelloPowerPlatform.html#dataverse-table-creation-aka-dataverse-model","title":"Dataverse Table Creation AKA Dataverse Model","text":"<p>\"Does 'Build a Dataverse model' sound intimidating? Don't worry, it's simply about creating tables in the Dataverse database.\"</p> <p></p>"},{"location":"PowerPlatform/HelloPowerPlatform.html#dataverse-vs-mssql","title":"Dataverse vs. MSSQL","text":"Feature Dataverse MSSQL Focus Business Applications Data Management Schema Flexible Fixed Data Types Limited Complete Relationships Built-in Foreign Keys Data Manipulation User-friendly interface, low-code T-SQL, Programming Languages Security Role-Based Access Control Manual Setup Table Types Standard, Custom, Virtual, Elastic Base, Temporary, Views, Table-Valued Functions Views Limited (Virtual Tables) Traditional Views Business Logic (Table Level) Power Automate Workflows Stored Procedures, Triggers Business Logic (Column Level) Validation Rules, Workflows Constraints, Triggers"},{"location":"PowerPlatform/ModelDrivenApps.html","title":"Model Apps","text":"<ul> <li>What are model-driven apps?</li> </ul>"},{"location":"PowerPlatform/ModelDrivenApps.html#what-are-model-driven-apps","title":"What are model-driven apps?","text":"<p>In canvas apps you have complete control over everything. In model-driven apps the layout done already  on the components you choose.</p> <p>Canvas apps are built screen-by-screen. Model-driven apps are created with few simple steps.</p> <p>Remember: Canvas apps can have many data sources. Model-driven apps must be on dataverse tables.</p> <p>Model-driven apps start with a page:</p> <p></p> <p>Different types of pages are Dataverse table, Dashboard, URL, Web resource, and Custom</p> <p></p> <p>Remember: Two pages for each Dataverse table</p> <p></p>"},{"location":"PowerPlatform/OnPremiseGateway.html","title":"On-Premise to Cloud ETL Using DataFlows &amp; On-Premise Gateway","text":""},{"location":"PowerPlatform/OnPremiseGateway.html#background","title":"Background","text":"<p>Case Study: We have a large number of xml files in a local folder. We want to move it to Dataverse. In this article I will show you how you can do it using on-premise gateway and Dataflow in Power platform. This ETL is particularly  useful in bank settings where xml files from many sources like right-fax servers are recieved and they have to be moved to backend database.Even though its not a full-solution as rightfax servers also send documents along with control files. But, the document part can be handled seprately handled.</p>"},{"location":"PowerPlatform/OnPremiseGateway.html#what-is-an-on-premise-gateway","title":"What is an On-Premise Gateway?","text":"<p>An on-premise gateway is software installed on a local system that enables access to local files and databases from Power BI, Power Apps, Power Automate, Azure Analysis Services, and Azure Logic Apps. There are two modes available: Personal, which allows use only with Power BI, and Standard, which supports all mentioned applications. Installation requirements include Windows 10 (64-bit) or Windows Server 2019 and .NET Framework 4.8. Note that these requirements may change in the future.</p> <p></p>"},{"location":"PowerPlatform/OnPremiseGateway.html#installing-the-gateway","title":"Installing the Gateway","text":"<ul> <li>Download the standard gateway.</li> </ul> <ul> <li>In the gateway installer, maintain the default installation path, accept the terms of use, and then select \"Install.\"</li> </ul> <ul> <li>Enter the email address associated with your Office 365 organizational account and select \"Sign in.\"</li> </ul> <ul> <li>Choose \"Register a new gateway on this computer\" and click \"Next.\"</li> </ul> <ul> <li>Enter a unique name for the gateway and a recovery key. This key is crucial for recovering or relocating your gateway in the future. Click \"Configure.\"</li> </ul> <ul> <li>Review the information in the final window. Since the same account is used for Power BI, Power Apps, and Power Automate, the gateway will be accessible for all three services. Select \"Close.\"</li> </ul>"},{"location":"PowerPlatform/OnPremiseGateway.html#use-gateway-import-files-from-local-folder","title":"Use Gateway - Import files from local folder","text":"<ul> <li>Open PowerApps -&gt; Dataflows -&gt; New Dataflow -&gt; Start from Blank -&gt; Provide a name -&gt; Click \"Create.\"</li> </ul> <ul> <li>On the \"Get Data\" page, click Folder.</li> </ul> <ul> <li> <p>Provide credentials and choose the source folder on the Connect to Data Source page. </p> </li> <li> <p>Ensure the account has read/write privileges by checking the folder's properties under the security tab. </p> </li> <li> <p>If all settings are correct, files will be loaded and displayed on the preview page.</p> </li> </ul> <p></p> <ul> <li>Click Combine or Transform Data. In this example, I used Combine.</li> </ul> <p>One common issue during XML conversion is that DataFlow assigns its own locale, which is often overlooked during testing but causes the flow to fail during actual execution. To resolve this, go to \"Options,\" select \"Regional Settings,\" choose the appropriate locale (e.g., English (United States)), and click OK</p> <p></p> <ul> <li>Then click \"Next.\" This will open the mapping page where you can load data into an existing table or create a new table with automatic mapping. Once you are finished, click \"Next.\"</li> </ul> <p></p> <ul> <li>Now, decide how to run the flow. You can run it ad hoc manually or at a predetermined interval. Once selected, click \"Publish\" to publish the flow.</li> </ul> <p></p> <ul> <li>Once published successfully, you can view your dataflow in the \"My DataFlow\" tab.</li> </ul> <p></p> <ul> <li>Since the XML data was exported into Dataverse, you can view the table and its data from the \"Tables\" page.</li> </ul> <p></p> <p>Voila! Your data is now in Dataverse and can be accessed using a wide range of M365 apps.</p>"},{"location":"PowerPlatform/OnPremiseGateway.html#using-the-gateway-in-microsoft-fabric","title":"Using the Gateway in Microsoft Fabric","text":"<p>You can reuse the gateway! For example, if you want to collaborate:</p> <p></p>"},{"location":"PowerPlatform/OnPremiseGateway.html#troubleshooting","title":"Troubleshooting","text":"<p>If the connection fails, ensure that your user has the appropriate permissions for the folder:</p> <p></p>"},{"location":"PowerPlatform/PowerAutomateIsWorkflowTeams.html","title":"Workflow = Power Automate(Teams) = make.powerautomate.com","text":"<p>The Workflow app within Microsoft Teams seamlessly integrates Power Automate directly into your Teams environment. It is the same old Power Automate app for Teams, but renamed as Workflow.</p>"},{"location":"PowerPlatform/PowerAutomateIsWorkflowTeams.html#key-features-and-benefits","title":"Key Features and Benefits","text":"<ul> <li>Previously known as \"Power Automate,\" the app was rebranded as \"Workflow\" around November 2023.</li> </ul> <ul> <li>The Workflow app allows you to create Power Automate workflows from scratch or utilize pre-built templates.</li> </ul> <ul> <li>Access these templates directly within Teams, making it convenient to get started with automation.</li> <li>All workflows created within the Workflow app are powered by Power Automate.</li> </ul> <ul> <li>The app is completely identical to make.powerautomate.com.</li> </ul>"},{"location":"PowerPlatform/PowerAutomateIsWorkflowTeams.html#getting-started","title":"Getting Started","text":"<ol> <li>Install the Workflow App:</li> <li>Head to the Teams store and search for \"Workflow.\" Install the app to get started.</li> <li> <p>Once installed, you'll find it in your Teams app bar.</p> </li> <li> <p>Create Your First Workflow:</p> </li> <li>Click on the Workflow app icon in Teams.</li> <li>Choose to create a new workflow from scratch or explore the available templates.</li> <li>Customize your workflow by adding actions, conditions, and triggers.</li> </ol>"},{"location":"PowerPlatform/PowerAutomateIsWorkflowTeams.html#conclusion","title":"Conclusion","text":"<p>Workflow = Power Automate(Teams) = make.powerautomate.com</p>"},{"location":"PowerPlatform/PowerPlatformAdminCentral.html","title":"Admin Central","text":"<p>TBD</p> <p>Power Platform Admin Central(PPAC) - Managing Users, Roles and Teams.</p> <p>PPAC provides a centralized admin experience to manage users/security  roles and Teams.</p>"},{"location":"PowerPlatform/PowerPlatformAdminCentral.html#background","title":"Background","text":"<p>In this article I will show you what are application users. Whey they are used. How we can create them. I will use a simple case-study so that you can related to it easily.</p>"},{"location":"PowerPlatform/PowerPlatformAdminCentral.html#application-users-case-study","title":"Application users - case study","text":"<p>Say you have a power automate workflow(leave application) which adds new records in dataverse when a form is submitted by users. Now we have so many employees, we can't give all their ids access to the backend database. And its not feasible as well. They just want to submit the form. What is the soluton: </p> <p>Create an application user for the leave application workflow. Give the application user read/write access to the tables.</p>"},{"location":"PowerPlatform/PowerPlatformAdminCentral.html#add-an-application-user","title":"Add an Application user","text":"<p>Application users system accounts for background  tasks. You shouldn't update them or change their security roles. They don't consume any license.  The application users are given access to environment's data on behalf of the user who's using the application.</p> <ul> <li> <p>Go to PPAC https://admin.powerplatform.microsoft.com/ -&gt; Environments -&gt; Select the environment </p> </li> <li> <p>Click Settings </p> </li> <li> <p>Click Users + permissions -&gt; Application users.   </p> </li> <li>Click + New app user.</li> </ul> <p>Certainly! Let's delve into managing application users in the Power Platform admin center. This process is crucial for ensuring smooth access and security within your environment. Below, I'll outline the steps involved:</p>"},{"location":"PowerPlatform/PowerPlatformAdminCentral.html#manage-application-users-in-the-power-platform-admin-center","title":"Manage Application Users in the Power Platform Admin Center","text":""},{"location":"PowerPlatform/PowerPlatformAdminCentral.html#1-view-application-users-in-an-environment","title":"1. View Application Users in an Environment","text":"<ul> <li>Sign in to the Power Platform admin center as a System Administrator.</li> <li>Select Environments, and then choose an environment from the list.</li> <li>Navigate to Settings and select Users + permissions.</li> <li>Click on Application users to view and manage application users.</li> </ul>"},{"location":"PowerPlatform/PowerPlatformAdminCentral.html#2-create-an-application-user","title":"2. Create an Application User","text":"<ul> <li>You can create an unlicensed application user in your environment. This user will have access to your environment's data on behalf of the application.</li> <li>In an environment, you can only have one application user for each Microsoft Entra\u2013registered application.</li> <li>Here's how:<ul> <li>Sign in to the Power Platform admin center as a System Administrator.</li> <li>Select Environments, choose an environment, and go to Settings.</li> <li>Click on Users + permissions, then select Application users.</li> <li>Click + New app user to open the Create a new app user page.</li> <li>Choose the registered Microsoft Entra application created for the user.</li> <li>Assign a business unit and select security roles for the new application user.</li> <li>Save your changes and create the user.</li> </ul> </li> </ul>"},{"location":"PowerPlatform/PowerPlatformAdminCentral.html#3-view-or-edit-application-user-details","title":"3. View or Edit Application User Details","text":"<ul> <li>Sign in to the Power Platform admin center as a System Administrator.</li> <li>Select Environments, choose an environment, and go to Settings.</li> <li>Click on Users + permissions, then select Application users.</li> <li>You can view or edit the details of an application user here.</li> </ul> <p>Remember that deleting an application user is currently not supported. For more detailed information, you can refer to the official Microsoft Learn article.</p>"},{"location":"PowerPlatform/PowerPlatformQ%26A.html","title":"Q&A","text":"<p>Note: Highlight the answers to reveal them.</p> <p>1. What are the benefits of using Common Data Model? - It eliminates the need for data modeling. - It enables interoperability between different systems and applications. - It provides a standard user interface for data management.  </p> <p>Answer: Common Data Model enables interoperability between different systems and applications.</p> <p>2. Someone adds an item in SharePoint, which prompts a flow to run in Power Automate. What type of operation was used to start your flow? - Trigger. - Action. - Function-based.  </p> <p>Answer: A trigger is an operation that tells a workflow to begin or prompts some type of action.</p> <p>3. A client likes the idea of implementing a Microsoft Power Platform solution but is concerned about the ability to interact with a custom API. How should you respond? - Microsoft Power Platform has over 900 connectors to use in these situations. - Microsoft Power Platform offers the ability to create custom connectors, which allow you to connect to Power Apps and Power Automate. - Microsoft Power Platform uses connectors that hold a series of functions available for developers.  </p> <p>Answer: You can build out a custom connector to bridge your app or workflow to the API.</p> <p>4. Someone asks you to describe a connector. How would you respond? - Connectors hold a series of functions available for developers. - Connectors connect your data source to your app, workflow, or dashboard. - Connectors are a cloud-based service that makes it practical and simple for line-of-business users to build workflows that automate time-consuming business tasks and processes across applications and services.  </p> <p>Answer: Connectors allow functions and information to pass from your data source to your app or workflow.</p> <p>4. In Microsoft Dataverse, where is your data being stored?</p> <ul> <li>Tables</li> <li>Forms</li> <li>SharePoint</li> <li>Access</li> </ul> <p>Answer: In Dataverse, data is stored in tables.</p> <p>5. What is the benefit of building a model-driven app?</p> <ul> <li>Logic-focused design, no code required, and different UIs across multiple devices</li> <li>Complex responsiveness with different UIs across multiple devices</li> <li>Apps can be distributed by using complex coding and different UIs across mobile devices</li> <li>Component-focused design, no code required, complex responsiveness with similar UI across multiple devices, and apps can be distributed as solutions</li> </ul> <p>Answer: The benefits of building a model-driven app are component-focused design, no code required, complex responsiveness with similar UI across multiple devices, and apps can be distributed as solutions.</p> <p>6. What are the three important areas to focus on when creating model-driven apps?</p> <ul> <li>Licenses, business data, and composing the app</li> <li>App design pattern, business data, and defining business process</li> <li>Modeling business data, defining business process, and composing the app</li> <li>Model licensing design, defining business process, and composing the app</li> </ul> <p>Answer: When creating a model-driven app, it's important to focus on modeling business data to see where data is read from and saved, the business process including who the users are, and composing the app to make it easily accessible.</p>"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch.html","title":"Integrate AI Search and Azure AI Document Intelligence","text":""},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch.html#overview","title":"Overview","text":"<p>In this module, you'll learn how to create a AI Search custom skill that calls a model in Azure AI Document Intelligence to improve indexing documents.</p> <p>Learning objectives Describe how a custom skill can enrich content passed through an Azure AI Search pipeline. Build a custom skill that calls an Azure AI Document Intelligence solution to index data from forms.</p>"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch.html#case-study","title":"Case Study","text":"<p>You have thousands of marriage certificates stored in a data lake and wish to search them using key-value pairs (e.g., \"Girlfriend_Name = 'Rita'\"). You are evaluating two options:</p>"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch.html#solution-1-adls-files-azure-cognitive-search","title":"Solution 1 - ADLS Files -&gt; Azure Cognitive Search","text":"<p>You implement Azure Cognitive Search, which will automatically process the files by opening them, performing OCR, and indexing the extracted data. It offers many built-in features, including sentiment analysis. However, the cornerstone of effective search capabilities is the quality of the OCR output. Since the OCR provided by Azure Cognitive Search is basic, any search functionality relying on this OCR output may yield suboptimal results.</p>"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch.html#solution-2-adls-files-azure-ai-document-intelligence-azure-cognitive-search","title":"Solution 2 - ADLS Files -&gt; Azure AI Document Intelligence -&gt; Azure Cognitive Search","text":"<p>By creating a custom extraction model tailored for your marriage certificates, Azure AI Document Intelligence can provide highly accurate OCR results. After processing the certificates with this custom model, you then index the extracted data using Azure Cognitive Search. This approach enables robust search capabilities that significantly surpass the effectiveness of using Azure Cognitive Search's basic OCR alone.</p>"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch.html#how-to-implement-solution-2","title":"How to implement Solution 2","text":"<p>To integrate Azure AI Document Intelligence into the AI Search indexing process:</p> <p>Use AAIDI document model(prebuilt/custom) to extract data from the marriage certificates. Write a web service that can integrate custom skill with Azure AI Document Intelligence resource. In this module, you'll use an Azure Function to host this service. Add a custom web API skill, with the correct configuration to the AI Search skillset. This skill should be configured to send requests to the web service.</p>"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch.html#what-is-a-skill-in-azure-cognitive-search","title":"What is a skill in Azure Cognitive Search","text":"<p>Skills are like identifying the language of documents, recognizing logos, or detecting names of places. Skills can pre-built or custom-built. During the indexing process, these skills are applied one after another, to make the ocred data more searchable.</p> <p>They add more Indexing data to make Azure cognitive search better.</p>"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch.html#types-of-custom-skill","title":"Types of custom skill","text":""},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch.html#azure-machine-learning-aml-custom-skills","title":"Azure Machine Learning (AML) Custom Skills","text":"<p>You can use a model from Azure Machine Learning to create a custom skill.</p>"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch.html#custom-web-api-skills","title":"Custom Web API Skills","text":"<p>These are simply custom skills provided as web api. The connect to services like Azure Document Intellingece etc.</p>"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch.html#build-an-azure-ai-document-intelligence-custom-skill","title":"Build an Azure AI Document Intelligence custom skill","text":""},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch.html#appendix","title":"Appendix","text":""},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_AzureCognitiveSearch.html#stages-of-indexing-process","title":"Stages of Indexing process","text":"<p>There are five stages to the indexing process:</p> <p>Document Cracking. In document cracking, the indexer opens the content files and extracts their content. Field Mappings. Fields such as titles, names, dates, and more are extracted from the content. You can use field mappings to control how they're stored in the index. Skillset Execution. In the optional skillset execution stage, custom AI processing is done on the content to enrich the final index. Output field mappings. If you're using a custom skillset, its output is mapped to index fields in this stage. Push to index. The results of the indexing process are stored in the index in Azure AI Search.</p>"},{"location":"PowerPlatform/DocumentIntelligence/AAIDI_Q%26A.html","title":"Q&A","text":"<ol> <li>You have a composed model that consists of three custom models. You're writing code that sends forms to the composed model and you need to check which of the custom models was used to analyze each form. Which property should you use from the returned JSON? </li> </ol> <p>modelId. status. docType.</p> <p>Ans: docType. Explanation: The docType property includes the model ID of the custom model that was used to analyze the document. </p> <ol> <li>You're trying to create a composed model but you're receiving an error. Which of the following should you check? </li> </ol> <p>That the custom models were trained with labels. That the custom models all have the same model ID. That the custom models all have the same list of fields.</p> <p>Ans: That the custom models were trained with labels. Explanation: Only custom models that have been trained with labeled example forms can be added to a composed model</p> <ol> <li>You have a large set of documents with varying structures that contain customer name and address information. You want to extract entities for each customer. Which prebuilt model should you use? </li> </ol> <p>Read model. General document model. ID document model.</p> <p>Answer: General document model. Explanation: The general document model is the only one that supports entity extraction.</p> <ol> <li>You are using the prebuilt layout model to analyze a document with many checkboxes. You want to find out whether each box is checked or empty. What object should you use in the returned JSON code? </li> </ol> <p>Selection marks. Bounding boxes. Confidence indicators.</p> <p>Answer: Selection marks.</p> <p>Explanation: Selection marks record checkboxes and radio buttons and include whether they're selected or not.</p> <ol> <li>You submit a Word document to the Azure AI Document Intelligence general document model for analysis but you receive an error. The file is A4 size, contains 1 MB of data, and is not password-protected. How should you resolve the error? </li> </ol> <p>Change from the free tier to the standard tier. Submit the document one page at a time. Convert the document to PDF format.</p> <p>Answer: Convert the document to PDF format. Explanation: Word documents are not supported by Azure AI Document Intelligence but PDF documents are supported. Azure AI Document Intelligence is designed to analyze scanned and photographed paper documents, not documents that are already in a digital format so you should consider using another technology to extract the data in Word documents.</p> <ol> <li>A person plans to use an Azure Document Intelligence prebuilt invoice model. To extract document data using the model, what are two calls they need to make to the API? </li> </ol> <p>Train Model and Get Model Labels Analyze Invoice and Get Analyze Invoice Result Create Azure Document Intelligence and Get Analyze Invoice Result</p> <p>Answer: Analyze Invoice and Get Analyze Invoice Result Explanation: The Analyze Invoice function starts the form analysis and returns a result ID, which they can pass in a subsequent call to the Get Analyze Invoice Result function to retrieve the results.</p> <ol> <li>A person needs to build an application that submits expense claims and extracts the merchant, date, and total from scanned receipts. What's the best way to do this? </li> </ol> <p>Use the Read API of the Computer Vision service. Use Azure Document Intelligence's Layout service Use Azure Document Intelligence's prebuilt receipts model</p> <p>Answer: Use Azure Document Intelligence's prebuilt receipts model Explanation: Use the Azure Document Intelligence's prebuilt receipts model. It can intelligently extract the required fields even if the scanned receipts have different names in them.</p> <ol> <li>A person is building a custom model with Azure Document Intelligence services. What is required to train a model? </li> </ol> <p>Along with the form to analyze, JSON files need to be provided. Training must be done through language specific SDKs. Nothing else is required.</p> <p>Answer: Along with the form to analyze, JSON files need to be provided. Explanation: The labels needed in training are referenced in the ocr.json files, labels.json files, and single fields.json file.</p>"},{"location":"PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence.html","title":"Azure AI Document Intelligence","text":""},{"location":"PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence.html#what-is-azure-ai-document-intelligence","title":"What is Azure AI Document Intelligence?","text":"<p>Azure AI Document Intelligence (Form Recognizer) is a machine-learning AI service that extracts text from documents(JPG, PNG, BMP, PDF, or TIFF). It offers REST APIs client library SDKs(Python, C#, Java, and JavaScript) for programmers.</p> <p>Azure Document Intelligence uses Optical Character Recognition (OCR) capabilities and deep learning models to extract text, key-value pairs, selection marks, and tables from documents.</p> <p>The service offers two types of models - Prebuilt models are ready-to-use models for common  documents like receipts and credit cards. Custom models are used when a document has unusual and unique format.</p> <p>The service is accessible via SDKs in Python, C#, Java, and JavaScript.</p> <p></p>"},{"location":"PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence.html#models-in-azure-ai-document-intelligence","title":"Models in Azure AI Document Intelligence","text":"<p>Azure AI Document Intelligence (formerly Form Recognizer) offers various models for using this service in your application. Here are the models:</p> <ol> <li> <p>Prebuilt Models:</p> <ul> <li>These are pre-trained, ready-to-use models which you can use rightaway without any customization:<ul> <li>General-Document-Read: Extracts text and key-value pairs from documents.</li> <li>General-Document-Layout: Analyzes the layout and structure of documents.</li> <li>Contract: Extracts details from contracts.</li> <li>Health Insurance Card: Extracts information from health insurance cards.</li> <li>ID Document: Extracts data from identification documents.</li> <li>Invoice: Extracts relevant details from invoices.</li> <li>Receipt: Extracts transaction information from receipts.</li> <li>US Tax Forms (e.g., 1040, 1098, W2): Extracts data from various US tax forms.</li> <li>Marriage Certificate: Extracts details from marriage certificates.</li> <li>Credit Card: Extracts information from credit cards.</li> <li>Business Card (deprecated): Previously used for business card extraction.</li> </ul> </li> </ul> </li> <li> <p>Custom Models:</p> <ul> <li> <p>When you have forms with unusual or unique formats, you can create and train your own custom models in Azure AI Document Intelligence. A custom model can provide field extraction for the data that is unique to your form and generate data targeted to your unique business application.</p> <ul> <li>Custom Classification Model: Allows you to create custom classifiers.</li> <li>Neural Model: Use custom neural models for inconsistent, semi-structured or unstructured forms.</li> <li>Template Model: Use custom template models when your forms have a consistent visual template. The formatting and layout should be consistent across all completed examples of the form.</li> <li>Composed Model: You can build hundreds of custom models within one Azure AI Document Intelligence resource and combine them into a composed model using Azure AI Document Intelligence Studio's GUI or the StartCreateComposedModelAsync() method in code.</li> </ul> </li> </ul> </li> <li> <p>Add-On Capabilities:</p> <ul> <li>These enhance existing models:<ul> <li>Font Property Extraction: Extracts font-related information.</li> </ul> </li> </ul> </li> </ol>"},{"location":"PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence.html#azure-ai-document-intelligence-vs-azure-ai-vision","title":"Azure AI Document Intelligence Vs Azure AI Vision","text":"<p>Azure AI Document Intelligence is based on Azure AI Vision services. For basic text extraction from images, Azure AI Vision OCR is appropriate. For more detailed document analysis, such as identifying key/value pairs, tables, and contextual information, Azure AI Document Intelligence is more suitable.</p>"},{"location":"PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence.html#tools-for-aaidi","title":"Tools for AAIDI","text":"<p>Azure AI Document Intelligence tools offer Document Intelligence Studio(https://formrecognizer.appliedai.azure.com/) or (https://documentintelligence.ai.azure.com/studio) to use it without much coding. Here, you can find a range off pre-built models and option to create custom models.</p> <p></p> <p>To embed AADI in your applications, coding is required. For instance, users can scan receipts through a mobile app, which uses this service to extract detailed information for integration into a CRM database.  AADI provides APIs for each pre-built model. You can use C#/.NET, Java, Python, and JavaScript to access them. Also, you can access the service via its RESTful web service for other languages.</p>"},{"location":"PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence.html#azure-document-intelligence-studio","title":"Azure Document Intelligence Studio","text":"<p>Azure Document Intelligence services are accessible through both SDKs and APIs, but if you prefer a more visual approach, you can use the Azure Document Intelligence Studio. This web application lets you manage your projects more interactively. In the Studio, you can set up various types of projects like document analysis models, which include capabilities for extracting text, tables, and structure from documents and images. You can also work with prebuilt models for specific document types or even train your own custom models to fit your specific needs. It's a flexible tool that caters to a variety of document processing requirements.</p>"},{"location":"PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence.html#use-prebuilt-document-model","title":"Use prebuilt document model","text":"<p>Create an Azure AI Document Intelligence resource Before you can call the Azure AI Document Intelligence service, you must create a resource to host that service in Azure:</p> <p>In a browser tab, open the Azure portal at https://portal.azure.com, signing in with the Microsoft account associated with your Azure subscription. On the Azure portal home page, navigate to the top search box and type Document Intelligence and then press Enter. On the Document Intelligence page, select Create. On the Create Document Intelligence page, use the following to configure your resource: Subscription: Your Azure subscription. Resource group: Select or create a resource group with a unique name such as DocIntelligenceResources. Region: select a region near you. Name: Enter a globally unique name. Pricing tier: select Free F0 (if you don't have a Free tier available, select Standard S0). Then select Review + create, and Create. Wait while Azure creates the Azure AI Document Intelligence resource. When the deployment is complete, select Go to resource. Keep this page open for the rest of this exercise. Use the Read model Let's start by using the Azure AI Document Intelligence Studio and the Read model to analyze a document with multiple languages. You'll connect Azure AI Document Intelligence Studio to the resource you just created to perform the analysis:</p> <p>Open a new browser tab and go to the Azure AI Document Intelligence Studio at https://documentintelligence.ai.azure.com/studio.</p> <p>Under Document Analysis, select the Read tile.</p> <p></p> <p>If you are asked to sign into your account, use your Azure credentials.</p> <p>If you are asked which Azure AI Document Intelligence resource to use, select the subscription and resource name you used when you created the Azure AI Document Intelligence resource.</p> <p></p> <p>In the list of documents on the left, select read-german.png.</p> <p></p> <p>When the analysis is complete, the text extracted from the image is shown on the right in the Content tab. Review this text and compare it to the text in the original image for accuracy.</p> <p>Select the Result tab. This tab displays the extracted JSON code.</p> <p></p> <p>Scroll to the bottom of the JSON code in the Result tab. Notice that the read model has detected the language of each span. Most spans are in German (language code de) but the last span is in English (language code en).</p>"},{"location":"PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence.html#use-composed-model","title":"Use composed model","text":"<p>Once you've created a set of custom models, you must assemble them into a composed model. You can do this in a Graphical User Interface (GUI) by using Azure AI Document Intelligence Studio, or by using the StartCreateComposedModelAsync() method in custom code.</p> <p>In the results from the composed model, you can determine which custom model has been used for the analysis by checking the docType field.</p> <p>The maximum number of custom models that can be added to a single composed model is 100.</p>"},{"location":"PowerPlatform/DocumentIntelligence/AzureAIDocumentIntelligence.html#file-type-requirement","title":"File type  requirement","text":"<p>Note: AAIDI can't extract data from word documents or excel. It only supports, photographed documents like JPG, PNG, BMP, TIFF and PDF. This is also  true for traditional document capture tools like OpenText Intelligent Capture(Captiva InputAccel) whre you can use word document but an intermediate image conversion module converts it to image beore ocring.</p> <p>Here are the technical reuiqrment for files:</p> <p>Format must be JPG, PNG, BMP, PDF (text or scanned), or TIFF. The file size must be less than 500 MB for paid (S0) tier and 4 MB for free (F0) tier. Image dimensions must be between 50 x 50 pixels and 10000 x 10000 pixels. The total size of the training data set must be 500 pages or less.</p>"},{"location":"PowerPlatform/DocumentIntelligence/DocumentAutomation.html","title":"Background","text":"<p>This article may be beneficial for ECM and Data Capture specialists exploring alternative solutions offered by Power Platform to traditional Document Capture tools such as Captiva InputAccel (OpenText Intelligent Capture), Kofax Capture, and IBM Datacap.</p>"},{"location":"PowerPlatform/DocumentIntelligence/DocumentAutomation.html#how-its-done-traditionally","title":"How its done traditionally","text":"<p>In most major banks, insurance companies, and pharmaceutical organizations, document capture tools like OpenText Intelligent Capture or Kofax are utilized to capture documents from scanners, emails, and other sources. These tools extract metadata and then forward both the documents and metadata to backend ECM systems such as OpenText Documentum, IBM FileNet, or SharePoint.</p> Multi-channel capture using traditional products <p>A key feature of these Document Capture products is their capability to automatically classify and extract information, allowing users to validate this information.</p> Captiva Document Processing Modules <p>OpenText facilitates this process through modules like ScanPlus for capture, Classification for automatic classification, Extraction for metadata extraction, and IndexPlus/Completion for manual validation. Over the past two decades, EMC Captiva, Kofax, and now OpenText Intelligent Capture have dominated the market. These products have been widely adopted across large banks, insurance corporations, and the pharmaceutical sector.</p> Captiva Capture provides a centralized development tool called Captiva Designer for creating, configuring, deploying, and testing the capture system end-to-end.."},{"location":"PowerPlatform/DocumentIntelligence/DocumentAutomation.html#enter-power-platforms-ai-builder","title":"Enter Power Platform's AI Builder","text":"<p>In recent years, Microsoft has made notable strides in the Document Capture market. I've demonstrated how straightforward it is to develop a workflow using Power Automate and AI Builder that extracts data from fixed-template documents like passports and driving licenses.</p> <p>AI Builder also provides ready-to-use models for processing standard documents such as invoices and receipts, which can be utilized as they are or customized with ease. </p> <p>For instance, if you need to extract information from Singapore NRIC or Indian PAN Cards, you only need about 5 sample images in PNG or JPEG format. Simply create the fields and outline the areas you want AI Builder to recognize and extract. It takes less than 30 minutes to create a custom model if your documents adhere to a very standard fixed template. </p> <p>While this capability is powerful, it does not include an operator-based validation feature, which means the extracted data might not always be 100% accurate. However, you can deisgn a solution for that too. You can create an end-to-end solution that combines a Power Automate workflow for data extraction using AI Builder, Power Apps for manual validation, and Dataverse or SharePoint to manage storage and indexing queues. Yet, building such a solution from scratch can be complex.</p>"},{"location":"PowerPlatform/DocumentIntelligence/DocumentAutomation.html#introduing-document-automation-base-kit","title":"Introduing Document automation base kit","text":"<p>Microsoft now offers a Document Automation Base\u2014a ready-to-use, complete solution that includes AI Builder, Power Automate, and Power Apps for document capture, extraction, and validation.</p> <p></p> <p>Document automation base solution has these components:</p> <p>Email Importer Flow:  Documents received by email are imported for processing. If you receive documents differently, you can adjust this to collect documents from places like SharePoint or cloud storage.</p> <p>Document Processor Flow:  This step uses AI to extract data from the documents. It triggers when a document reaches the 'Extracting data' state in the queue. The data extracted by the AI is then stored in Dataverse, a storage system.</p> <p>Validator Flow:  This flow adds business rules to the process. It starts after the data has been extracted and stored. If the data meets certain criteria (like confidence levels or specific data thresholds), it might be approved automatically; otherwise, it goes for manual review. By default, this step does not have preset rules and sends all documents for manual review.</p> <p></p> <p>Automation Application:  This is where manual review and approval of documents take place. Users can see all documents in the pipeline, open them to view the data, make necessary edits, and approve them. This application also allows the process owner to configure the AI model and track documents through various states from receiving and extracting to validating and final approval.</p> <p></p>"},{"location":"PowerPlatform/DocumentIntelligence/DocumentAutomation.html#conclusion","title":"Conclusion","text":"<p>The key question is whether the Power Platform's AI-Builder and related solutions can effectively handle real-world document processing in sectors like banking.</p> <p>From my observations, the Power Platform may not be suitable for large-scale scanning where industrial scanners are linked to desktops and data validation is performed by dedicated operators, typical of setups outsourced to third-party operators like Iron Mountain. Power Automate lacks support for scanner-connected setups and may struggle with high-volume scanning due to its web-based nature. However, it could work for processes involving already digitized documents needing OCR processing and ECM integration. While solutions like OpenText Intelligent Capture\u2019s web-based Captiva Capture Web Client might be partly replaceable by Power Automate, the durability of AI Builder's OCR engines compared to established ones like Nuance OCR from Captiva is less certain.</p> <p>Below, you'll find a table comparing traditional scanning products with the Power Platform's solutions.</p> Feature / Module Microsoft Power Platform Document Automation Base Kit OpenText Intelligent Capture Document Import Email import, SharePoint and cloud storage integration. Modules like ScanPlus and Standard Import handle various sources including emails, MFP, Fax, browser-connected scanners etc.. AI-Powered Data Extraction AI Builder for configurable data extraction. Uses modules like NuanceOCR, Classification, Recognition and IndexPlus for OCR, classification, and data extraction from various document types. OCR Engine Built-in OCR capabilities within AI Builder. NuanceOCR for high-accuracy OCR, capable of handling both machine-printed and handwritten texts. Data Storage and Management Uses Dataverse for data management and storage. Focuses on processing and exporting data; does not typically store documents long-term. Process Customization Customizable through Power Apps for various workflows. Extensive customization options with C#/VB.NET scripting, advanced recognition settings, and use of RESTful services. Validation and Review Validator flow for manual and automated data validation. Modules like IndexPlus, Completion for manual review and validation, with robust tools for automated data validation. Business Rule Application Validator flow for applying business rules. Advanced scripting capabilities for applying complex business logic through modules like IndexPlus. Integration with Other Systems Limitless integration capability with M365 and Azure ecosystem Ready-made modules like Documentum Export, SharePoint Export, XML Export, SAP Export, makes it very easy to integrate with ECM, ERP, BPM systems. User Interface and Experience Power Apps-based centralized application. Mostly windows desktop applications also browser based application like Captiva Web  Client. Specific Modules Import, AI Extraction, Validation, and Review modules. Comprehensive suite including ScanPlus, IndexPlus, Completion, Web Services Input/Output, and more. Scalability and Distribution Auto-scalling. ScaleServer for load distribution and clustering for HA and DR."},{"location":"Python/1.0.0_Hello_Python.html","title":"Getting started","text":"<p>Here, I will assume you are a programmer who has used other languages like Java or C#. So, what makes python different?</p> <p>But, that being said, you need to remember this important command which let's you check python version:</p> <p>python -V</p> <p>To check python version use run in terminal/CMD</p> <p></p> <p>Remember: V in CAPS</p> <p></p>"},{"location":"Python/1.0.1_Variables_DataTypes.html","title":"Chapter 1: Variables and Data Types - Your First Real Conversation with Python","text":"<p>Look, if you've programmed before, you know what a variable is. But Python? Python treats variables like that friend who never judges you for changing your mind.</p>"},{"location":"Python/1.0.1_Variables_DataTypes.html#variables-no-declarations-no-drama","title":"Variables: No Declarations, No Drama","text":"<p>In most languages, you declare before you use. You tell the compiler, \"Hey, this is going to be an integer, get ready.\" Python doesn't care about that ceremony.</p> <pre><code>x = 5\nx = \"now I'm a string\"\nx = [1, 2, 3]  # oh look, a list now\n</code></pre> <p>This just works. No declaration, no type annotation (unless you want them for clarity), no questions asked. Python figures it out. This is called dynamic typing, and it's both your best friend and occasionally, your worst enemy. More on that later.</p>"},{"location":"Python/1.0.1_Variables_DataTypes.html#the-underscore-pythons-little-secret","title":"The Underscore: Python's Little Secret","text":"<p>Variables can't start with numbers. <code>2fast</code> is illegal. <code>fast2</code> is fine. But here's what Python does differently - the underscore isn't just allowed, it's actually meaningful:</p> <pre><code>_private = \"I'm internal, don't touch\"\n__very_private = \"I'm REALLY internal\"\n_ = \"I'm the result you just ignored\"\n</code></pre> <p>That last one? In the Python REPL, <code>_</code> holds the last result. Handy when you forgot to save something.</p>"},{"location":"Python/1.0.1_Variables_DataTypes.html#data-types-the-usual-suspects-with-a-twist","title":"Data Types: The Usual Suspects with a Twist","text":"<p>You know integers, floats, strings, booleans. Python has them all. But let me tell you what's different.</p>"},{"location":"Python/1.0.1_Variables_DataTypes.html#integers-unlimited-size","title":"Integers: Unlimited Size","text":"<p>In C, your int overflows. In Java, your int overflows. In Python? Your integer just... grows.</p> <pre><code>x = 99999999999999999999999999999999999999\ny = x * x  # still works, no overflow\n</code></pre> <p>Python doesn't cap integers at 32 or 64 bits. It'll use as much memory as needed. Performance hit? Sure. Brain damage from overflow bugs? None.</p>"},{"location":"Python/1.0.1_Variables_DataTypes.html#strings-immutable-but-flexible","title":"Strings: Immutable but Flexible","text":"<p>Strings are immutable. You can't change them in place. But Python gives you three ways to write them:</p> <pre><code>s1 = 'single quotes'\ns2 = \"double quotes\"\ns3 = '''triple quotes\ncan span\nmultiple lines'''\n</code></pre> <p>All three are strings. Use single or double for short ones (your choice, be consistent). Use triple for multi-line strings or docstrings.</p> <p>Here's the thing about immutability:</p> <pre><code>name = \"John\"\nname[0] = \"D\"  # ERROR! Can't modify strings\nname = \"D\" + name[1:]  # This works - you're creating a NEW string\n</code></pre> <p>Every string operation creates a new string. Remember this when you're concatenating in loops. It matters.</p>"},{"location":"Python/1.0.1_Variables_DataTypes.html#none-pythons-way-of-saying-nothing-here","title":"None: Python's Way of Saying \"Nothing Here\"","text":"<p>Other languages have <code>null</code>, <code>nil</code>, <code>undefined</code>. Python has <code>None</code>. It's an object, it's a singleton, and it's the default return value of functions that don't explicitly return anything.</p> <pre><code>result = print(\"hello\")  # prints \"hello\"\nprint(result)  # prints None\n</code></pre> <p>Test for it with <code>is</code>, not <code>==</code>:</p> <pre><code>if x is None:  # Good\nif x == None:  # Works, but not Pythonic\n</code></pre> <p>Why? Because <code>None</code> is a singleton. There's only one <code>None</code> object in memory. <code>is</code> checks identity, <code>==</code> checks value. For <code>None</code>, identity is what you want.</p>"},{"location":"Python/1.0.1_Variables_DataTypes.html#type-conversion-explicit-is-better-than-implicit","title":"Type Conversion: Explicit is Better than Implicit","text":"<p>Python won't automatically convert types for you in most cases. This is intentional.</p> <pre><code>x = \"5\"\ny = 3\nz = x + y  # ERROR! Can't add string and int\n</code></pre> <p>You have to be explicit:</p> <pre><code>z = int(x) + y  # 8\nz = x + str(y)  # \"53\"\n</code></pre> <p>This seems annoying until it saves you from a bug where you accidentally concatenated instead of adding, or vice versa. Python makes you say what you mean.</p>"},{"location":"Python/1.0.1_Variables_DataTypes.html#quick-type-checking","title":"Quick Type Checking","text":"<p>Want to know what type something is?</p> <pre><code>type(5)        # &lt;class 'int'&gt;\ntype(\"hello\")  # &lt;class 'str'&gt;\ntype(None)     # &lt;class 'NoneType'&gt;\n\nisinstance(5, int)  # True - better for checking types\n</code></pre> <p>Use <code>isinstance()</code> over <code>type()</code> for checking. It respects inheritance, which matters when you get into classes.</p> <p>What You Just Learned: - Variables need no declaration - Integers don't overflow - Strings are immutable - <code>None</code> is Python's null, check it with <code>is</code> - Type conversions are explicit - Underscores in variable names have meaning</p> <p>Next up: we'll talk about making decisions with <code>if</code>, <code>elif</code>, and <code>else</code>. Because knowing how to store data means nothing if you can't decide what to do with it.</p>"},{"location":"Python/1.0.2_Control_Flow.html","title":"Chapter 2: Control Flow - Making Decisions Like a Pythonista","text":"<p>You know how to make decisions in code. <code>if</code> this, <code>else</code> that. But Python does it differently, and once you see why, you'll appreciate the elegance.</p>"},{"location":"Python/1.0.2_Control_Flow.html#indentation-its-not-optional-its-the-syntax","title":"Indentation: It's Not Optional, It's the Syntax","text":"<p>First things first. Python doesn't use curly braces or <code>end</code> keywords. Your indentation is your code structure.</p> <pre><code>if x &gt; 5:\n    print(\"x is big\")\n    print(\"really big\")\nprint(\"this runs regardless\")\n</code></pre> <p>That indentation? It's not for humans. It's the actual syntax. Mix tabs and spaces? Python will yell at you. Inconsistent indentation? Error. This seems strict until you realize you'll never have a debate about where to put braces or whether to use them for single-line blocks.</p> <p>Use 4 spaces. Not tabs, not 2 spaces. Four spaces is the Python way.</p>"},{"location":"Python/1.0.2_Control_Flow.html#the-if-elif-else-dance","title":"The if-elif-else Dance","text":"<p>You've written this a million times, but here's the Python version:</p> <pre><code>age = 25\n\nif age &lt; 13:\n    print(\"child\")\nelif age &lt; 20:\n    print(\"teenager\")\nelif age &lt; 60:\n    print(\"adult\")\nelse:\n    print(\"senior\")\n</code></pre> <p>Notice <code>elif</code>, not <code>else if</code>. Python likes brevity when it doesn't hurt clarity. No parentheses around conditions either (you can add them, but why?).</p>"},{"location":"Python/1.0.2_Control_Flow.html#truthiness-what-counts-as-true","title":"Truthiness: What Counts as True?","text":"<p>Here's where Python gets interesting. Everything has a boolean value in a conditional context:</p> <pre><code>if \"hello\":  # non-empty string? True\nif \"\":       # empty string? False\nif [1, 2]:   # non-empty list? True\nif []:       # empty list? False\nif 42:       # non-zero number? True\nif 0:        # zero? False\nif None:     # None? Always False\n</code></pre> <p>This is called \"truthiness.\" Empty containers are falsy. Zero is falsy. <code>None</code> is falsy. Everything else is usually truthy.</p> <p>This lets you write clean code:</p> <pre><code># Instead of this\nif len(my_list) &gt; 0:\n    process(my_list)\n\n# Write this\nif my_list:\n    process(my_list)\n</code></pre> <p>Shorter, cleaner, more Pythonic. But be careful - this can bite you when <code>0</code> is a valid value:</p> <pre><code>user_input = 0\nif user_input:  # Oops! 0 is falsy, this block won't run\n    print(f\"You entered: {user_input}\")\n</code></pre> <p>When zero matters, be explicit: <code>if user_input is not None:</code>.</p>"},{"location":"Python/1.0.2_Control_Flow.html#the-ternary-operator-one-liner-if-else","title":"The Ternary Operator: One-Liner if-else","text":"<p>Python has a ternary operator, but it reads differently than C-style languages:</p> <pre><code># Other languages: condition ? value_if_true : value_if_false\n# Python: value_if_true if condition else value_if_false\n\nstatus = \"even\" if x % 2 == 0 else \"odd\"\n</code></pre> <p>It reads almost like English: \"status is 'even' if x is divisible by 2, else 'odd'.\" Use it for simple assignments. Don't nest them - that way lies madness.</p>"},{"location":"Python/1.0.2_Control_Flow.html#match-case-pythons-modern-switch","title":"Match-Case: Python's Modern Switch","text":"<p>From Python 3.10 onwards, you've got <code>match-case</code>. It's not just a switch statement - it's pattern matching:</p> <pre><code>def describe_number(n):\n    match n:\n        case 0:\n            return \"zero\"\n        case 1 | 2 | 3:  # multiple values with |\n            return \"small\"\n        case n if n &lt; 0:  # with guard condition\n            return \"negative\"\n        case _:  # default case\n            return \"other\"\n</code></pre> <p>The <code>_</code> is the wildcard - catches everything else. But here's where it gets powerful:</p> <pre><code>point = (0, 5)\n\nmatch point:\n    case (0, 0):\n        print(\"origin\")\n    case (0, y):  # matches any point on y-axis, captures y value\n        print(f\"on y-axis at {y}\")\n    case (x, 0):\n        print(f\"on x-axis at {x}\")\n    case (x, y):\n        print(f\"somewhere at ({x}, {y})\")\n</code></pre> <p>Pattern matching with destructuring. If you're on Python 3.10+, use it. If not, stick with <code>if-elif-else</code>.</p>"},{"location":"Python/1.0.2_Control_Flow.html#loops-the-for-and-while-story","title":"Loops: The for and while Story","text":""},{"location":"Python/1.0.2_Control_Flow.html#the-for-loop-its-always-a-for-each","title":"The for Loop: It's Always a For-Each","text":"<p>Python's <code>for</code> loop is different. There's no C-style <code>for(i=0; i&lt;10; i++)</code>. Every <code>for</code> loop iterates over a collection:</p> <pre><code># Looping over a list\nfor item in [1, 2, 3]:\n    print(item)\n\n# Looping over a string\nfor char in \"hello\":\n    print(char)\n\n# The classic counter pattern\nfor i in range(5):  # 0, 1, 2, 3, 4\n    print(i)\n</code></pre> <p><code>range()</code> gives you a sequence of numbers. <code>range(5)</code> is 0 to 4. <code>range(1, 10)</code> is 1 to 9. <code>range(0, 10, 2)</code> is 0, 2, 4, 6, 8.</p> <p>Want both index and value? Use <code>enumerate()</code>:</p> <pre><code>fruits = [\"apple\", \"banana\", \"cherry\"]\nfor i, fruit in enumerate(fruits):\n    print(f\"{i}: {fruit}\")\n</code></pre> <p>This is cleaner than maintaining a counter manually.</p>"},{"location":"Python/1.0.2_Control_Flow.html#the-while-loop-same-old-friend","title":"The while Loop: Same Old Friend","text":"<p>While loops work like everywhere else:</p> <pre><code>count = 0\nwhile count &lt; 5:\n    print(count)\n    count += 1\n</code></pre> <p>Nothing fancy. When you need to loop until a condition changes, use <code>while</code>. When you're iterating over something, use <code>for</code>.</p>"},{"location":"Python/1.0.2_Control_Flow.html#break-continue-and-the-else-clause-nobody-uses","title":"Break, Continue, and the Else Clause Nobody Uses","text":"<p><code>break</code> exits the loop. <code>continue</code> skips to the next iteration. You know this.</p> <p>But here's Python's quirk - loops can have an <code>else</code> clause:</p> <pre><code>for i in range(10):\n    if i == 5:\n        break\nelse:\n    print(\"loop completed without breaking\")\n</code></pre> <p>The <code>else</code> runs only if the loop completes normally (no <code>break</code>). It's useful for search operations:</p> <pre><code>for user in users:\n    if user.name == search_name:\n        print(f\"Found: {user}\")\n        break\nelse:\n    print(\"User not found\")\n</code></pre> <p>Most Python developers don't use this often. But when you need it, it's elegant.</p>"},{"location":"Python/1.0.2_Control_Flow.html#pass-the-art-of-doing-nothing","title":"Pass: The Art of Doing Nothing","text":"<p>Sometimes you need a placeholder. Python doesn't allow empty blocks, so you use <code>pass</code>:</p> <pre><code>if condition:\n    pass  # TODO: implement this later\nelse:\n    do_something()\n</code></pre> <p>It's literally a no-op. The code equivalent of \"noted\" in a conversation.</p> <p>What You Just Learned: - Indentation is syntax, not style - Everything has truthiness (empty = false, non-empty = true) - <code>elif</code>, not <code>else if</code> - Ternary reads like English - <code>match-case</code> for pattern matching (Python 3.10+) - <code>for</code> loops always iterate over collections - Loops can have <code>else</code> clauses - <code>pass</code> when you need to do nothing</p> <p>Next: we'll dive into lists, tuples, and the art of storing multiple things without losing your mind.</p>"},{"location":"Python/1.0.3_List_Tuples_Collections.html","title":"Chapter 3: Lists, Tuples &amp; Collections - Python's Data Containers Unleashed","text":"<p>Imagine you're organizing a party. Lists are your flexible guest list - people can arrive, leave, swap positions. Tuples? That's your venue address - it doesn't change once set. Dictionaries are your seating chart - each guest has their assigned spot. Sets? The actual people who showed up - no duplicates, no order, just unique individuals.</p> <p>Let's throw this party.</p>"},{"location":"Python/1.0.3_List_Tuples_Collections.html#lists-your-shape-shifting-array","title":"Lists: Your Shape-Shifting Array","text":"<p>Lists in Python are what arrays wish they could be in other languages. Dynamic sizing? Check. Mixed types? Sure, why not. Negative indexing? You bet.</p> <pre><code>guests = [\"Alice\", \"Bob\", \"Charlie\"]\nchaos = [1, \"hello\", 3.14, True, None]  # perfectly legal\n</code></pre>"},{"location":"Python/1.0.3_List_Tuples_Collections.html#indexing-forward-backward-and-everywhere","title":"Indexing: Forward, Backward, and Everywhere","text":"<p>You can access from the front or the back:</p> <pre><code>fruits = [\"apple\", \"banana\", \"cherry\", \"date\"]\nprint(fruits[0])   # \"apple\" - the beginning\nprint(fruits[-1])  # \"date\" - the end\nprint(fruits[-2])  # \"cherry\" - second from end\n</code></pre> <p>Negative indexing is brilliant. No more <code>fruits[len(fruits)-1]</code> nonsense. Just grab from the back.</p>"},{"location":"Python/1.0.3_List_Tuples_Collections.html#slicing-the-swiss-army-knife","title":"Slicing: The Swiss Army Knife","text":"<p>Here's where Python shows off:</p> <pre><code>numbers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nnumbers[2:5]     # [2, 3, 4] - from index 2 to 5 (exclusive)\nnumbers[:3]      # [0, 1, 2] - first three\nnumbers[7:]      # [7, 8, 9] - from 7 to end\nnumbers[-3:]     # [7, 8, 9] - last three\nnumbers[::2]     # [0, 2, 4, 6, 8] - every second element\nnumbers[::-1]    # [9, 8, 7, 6, 5, 4, 3, 2, 1, 0] - REVERSED!\n</code></pre> <p>That last one? <code>[::-1]</code> to reverse? That's Python flexing. The syntax is <code>[start:stop:step]</code>. Negative step means go backwards.</p> <p>Want to copy a list? <code>new_list = old_list[:]</code>. Want every third element starting from index 1? <code>list[1::3]</code>. Slicing is poetry.</p>"},{"location":"Python/1.0.3_List_Tuples_Collections.html#list-methods-add-remove-rearrange","title":"List Methods: Add, Remove, Rearrange","text":"<pre><code>items = [1, 2, 3]\n\nitems.append(4)        # [1, 2, 3, 4] - add to end\nitems.insert(0, 0)     # [0, 1, 2, 3, 4] - insert at index\nitems.extend([5, 6])   # [0, 1, 2, 3, 4, 5, 6] - add multiple\nitems.remove(3)        # removes first occurrence of 3\nitems.pop()            # removes and returns last item\nitems.pop(0)           # removes and returns item at index 0\n</code></pre> <p>Important distinction: <code>append()</code> adds one item. <code>extend()</code> adds multiple items from an iterable. Don't do <code>items.append([5, 6])</code> unless you want a nested list.</p> <pre><code>items.sort()           # sorts in place\nitems.reverse()        # reverses in place\nlen(items)             # how many items?\nitems.count(2)         # how many times does 2 appear?\nitems.index(4)         # what's the index of 4?\n</code></pre> <p><code>sort()</code> and <code>reverse()</code> modify the list directly. They return <code>None</code>, not a new list. If you want a new sorted list, use <code>sorted(items)</code>.</p>"},{"location":"Python/1.0.3_List_Tuples_Collections.html#list-comprehensions-the-game-changer","title":"List Comprehensions: The Game Changer","text":"<p>This is where Python goes from \"nice language\" to \"I'm never going back.\"</p> <p>Instead of: <pre><code>squares = []\nfor i in range(10):\n    squares.append(i ** 2)\n</code></pre></p> <p>Write this: <pre><code>squares = [i ** 2 for i in range(10)]\n</code></pre></p> <p>One line. Clear. Readable. Fast.</p> <p>You can add conditions: <pre><code>evens = [x for x in range(20) if x % 2 == 0]\n# [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n</code></pre></p> <p>Transform and filter: <pre><code>words = [\"hello\", \"world\", \"python\", \"is\", \"amazing\"]\nlong_upper = [w.upper() for w in words if len(w) &gt; 5]\n# [\"PYTHON\", \"AMAZING\"]\n</code></pre></p> <p>Nested loops? Yes: <pre><code>pairs = [(x, y) for x in range(3) for y in range(3)]\n# [(0,0), (0,1), (0,2), (1,0), (1,1), (1,2), (2,0), (2,1), (2,2)]\n</code></pre></p> <p>List comprehensions are faster than loops and more Pythonic. Use them. Master them. Love them.</p>"},{"location":"Python/1.0.3_List_Tuples_Collections.html#tuples-the-immutable-twin","title":"Tuples: The Immutable Twin","text":"<p>Tuples look like lists but use parentheses. The real difference? You can't change them.</p> <pre><code>coordinates = (10, 20)\nrgb = (255, 128, 0)\n\ncoordinates[0] = 15  # ERROR! Tuples are immutable\n</code></pre> <p>Why would you want immutability? </p> <p>Reason 1: It's safer. You can't accidentally modify data that shouldn't change.</p> <p>Reason 2: Tuples can be dictionary keys (lists can't).</p> <p>Reason 3: They're slightly faster and use less memory.</p>"},{"location":"Python/1.0.3_List_Tuples_Collections.html#tuple-packing-and-unpacking","title":"Tuple Packing and Unpacking","text":"<p>This is where tuples shine:</p> <pre><code># Packing\npoint = 10, 20, 30  # parentheses optional\n\n# Unpacking\nx, y, z = point\nprint(x)  # 10\n\n# Swap variables (no temp variable needed!)\na, b = 5, 10\na, b = b, a  # a is now 10, b is now 5\n</code></pre> <p>Functions returning multiple values? They're returning tuples:</p> <pre><code>def get_dimensions():\n    return 1920, 1080  # tuple\n\nwidth, height = get_dimensions()\n</code></pre> <p>This unpacking works with lists too, but tuples are the idiomatic choice for fixed-size collections.</p>"},{"location":"Python/1.0.3_List_Tuples_Collections.html#single-element-tuples-the-comma-matters","title":"Single Element Tuples: The Comma Matters","text":"<pre><code>not_a_tuple = (5)      # this is just 5 with parentheses\nactual_tuple = (5,)    # this is a tuple with one element\n</code></pre> <p>That trailing comma? It's what makes it a tuple. Weird, but that's the syntax.</p>"},{"location":"Python/1.0.3_List_Tuples_Collections.html#sets-the-unique-only-club","title":"Sets: The Unique-Only Club","text":"<p>Sets store unique values. No duplicates. No guaranteed order.</p> <pre><code>unique_numbers = {1, 2, 3, 2, 1}  # {1, 2, 3}\n</code></pre>"},{"location":"Python/1.0.3_List_Tuples_Collections.html#why-use-sets","title":"Why Use Sets?","text":"<p>Removing duplicates: <pre><code>items = [1, 2, 2, 3, 4, 4, 5]\nunique = list(set(items))  # [1, 2, 3, 4, 5]\n</code></pre></p> <p>Fast membership testing: <pre><code>allowed_users = {\"alice\", \"bob\", \"charlie\"}\n\nif \"alice\" in allowed_users:  # O(1) - instant lookup\n    grant_access()\n</code></pre></p> <p>Checking <code>if item in list</code> is slow (checks every element). Checking <code>if item in set</code> is instant.</p>"},{"location":"Python/1.0.3_List_Tuples_Collections.html#set-operations-math-made-practical","title":"Set Operations: Math Made Practical","text":"<pre><code>a = {1, 2, 3, 4}\nb = {3, 4, 5, 6}\n\na | b  # union: {1, 2, 3, 4, 5, 6}\na &amp; b  # intersection: {3, 4}\na - b  # difference: {1, 2}\na ^ b  # symmetric difference: {1, 2, 5, 6}\n</code></pre> <p>These operations are incredibly useful for finding common elements, differences, or combinations.</p>"},{"location":"Python/1.0.3_List_Tuples_Collections.html#dictionaries-the-real-mvp","title":"Dictionaries: The Real MVP","text":"<p>We'll give dictionaries their own chapter because they deserve it. But here's the teaser: if lists are Python's bread and butter, dictionaries are the entire meal.</p> <p>What You Just Learned: - Lists are flexible, mutable, and slice-able masterpieces - <code>[::-1]</code> reverses anything - List comprehensions replace loops with elegance - Tuples are immutable and perfect for unpacking - Sets eliminate duplicates and enable lightning-fast lookups - Slicing syntax: <code>[start:stop:step]</code></p> <p>Coming up: Dictionaries - where Python becomes a superpower. You thought lists were cool? Wait until you see what key-value pairs can do.</p> <p>Your Challenge: Before moving on, write a list comprehension that takes a string and returns a list of vowels found in it. One line. Go.</p> <pre><code># Answer: \n# vowels = [char for char in \"hello world\" if char in \"aeiou\"]\n</code></pre> <p>Did you get it? If yes, you're ready for dictionaries. If not, read this chapter again. List comprehensions are that important.</p>"},{"location":"Python/1.0_Sets.html","title":"Sets","text":""},{"location":"Python/1.0_Sets.html#python-sets-for-busy-people","title":"Python Sets for busy people","text":"<ul> <li>Set items don't have a serial number (index)</li> <li>No Index. No Order. You can't refer to an item like <code>mySet[2]</code></li> <li>No duplicate items</li> <li>No Lists/Dictionaries</li> <li>Can add new items: </li> <li><code>mySet.add(4)</code>: Adds a single element to the set.</li> <li><code>mySet.update([4, 5])</code>: Adds multiple elements to the set.</li> <li>Can remove items:</li> <li><code>mySet.remove(element)</code>: Removes a specific element. Raises a <code>KeyError</code> if not found.</li> <li><code>mySet.discard(element)</code>: Removes a specific element. No issues if not found.</li> <li><code>mySet.pop()</code>: Removes and returns a random element. <code>KeyError</code> if set empty.</li> <li> <p><code>mySet.clear()</code>: Removes all elements. Slate clean.</p> </li> <li> <p>Check if an element exists: </p> </li> <li><code>element in mySet</code>: <code>True</code> if  item present, else <code>False</code>.</li> <li>Get the number of elements: </li> <li><code>len(mySet)</code>: Total no of items in set.</li> <li>Copy a set: </li> <li><code>mySet.copy()</code>: Creates a shallow copy of the set.</li> <li>Union of sets: </li> <li><code>mySet.union(other_set)</code> or <code>mySet | other_set</code>: Combines all elements from both sets, without duplicates.</li> <li>Intersection of sets: </li> <li><code>mySet.intersection(other_set)</code> or <code>mySet &amp; other_set</code>: Returns elements common to both sets.</li> <li>Difference of sets: </li> <li><code>mySet.difference(other_set)</code> or <code>mySet - other_set</code>: Returns elements in the first set but not in the second.</li> <li>Symmetric difference of sets: </li> <li><code>mySet.symmetric_difference(other_set)</code> or <code>mySet ^ other_set</code>: Returns elements in either set, but not in both.</li> </ul> Note: <p>Changeable items (also called mutable) are items that can be modified after they are created. For example:</p> <ul> <li>Lists: You can add, remove, or change elements.</li> <li>Dictionaries: You can add, remove, or change key-value pairs.</li> </ul> <p>These items cannot be added to a set because sets need items that do not change.</p> <p>Unchangeable items (also called immutable) are items that cannot be modified after they are created. For example:</p> <ul> <li>Numbers: Once created, their values cannot be changed.</li> <li>Strings: Any modification creates a new string.</li> <li>Tuples: Their elements cannot be changed once created.</li> </ul> <p>These items can be added to a set because their values stay the same.</p> <p></p>"},{"location":"Python/1.0_Sets.html#python-sets","title":"Python Sets","text":"<p>  A Python set is like your travel kit. Collection of unique items. There can be different items. But, they should be unique.  Set items don't have serial numbers (Index).      Without a serial number, you can't do something like <code>mySet[2]=\"Guava\"</code>.      All items in a set must be different. Otherwise, how would you tell them apart?      If your set has two apples, which one is which?      But, you can remove items from a set. You can take out an apple and add a guava.      Don't think about removing an apple and adding another apple.      Sets can't contain a <code>list</code> or a <code>dictionary</code>. Period.      They can contain tuples, but these tuples can't have lists or dictionaries inside them.      (It won't cause an error, but it can make the code unstable.) </p>"},{"location":"Python/1.0_Sets.html#python-sets-properties","title":"Python Sets Properties","text":"<p>So, here are some properties of Python Sets:</p>"},{"location":"Python/1.0_Sets.html#items-have-no-index","title":"Items have No Index:","text":"<p>  Python stores Set items but does not keep track of their order. This means there is no first item, second item, etc. For example, if you input `apple`, `orange`, `banana`, you might get `banana`, `apple`, `orange` as the output.  </p> <p></p> <pre><code>```python\nmySet = {1, 2, 3}\nprint(mySet)  # Output could be {1, 2, 3} or {3, 1, 2} or any permutation\n\nmySet[0] # THIS IS AN ERROR. No one is sitting at 0. There is no order, no index.\n```\n</code></pre>"},{"location":"Python/1.0_Sets.html#no-duplicates","title":"No Duplicates:","text":"<p> Since items in a set do not have serial numbers, duplicates are not allowed. If you try to add two apples, how would you distinguish between them? Therefore, when you add duplicates to a set, Python automatically removes the duplicates.</p> <p></p> <pre><code>mySet = {1, 2, 2, 3}\nprint(mySet)  # Output: {1, 2, 3}\n</code></pre>"},{"location":"Python/1.0_Sets.html#no-in-place-replace-addremove-instead","title":"No In-Place Replace. Add/remove instead.","text":"<p> You can add/remove items, but can't change an item's value directly. Can't in-place replace items. First, remove the old one and add the new one.</p> <p></p> <pre><code>```python\nmySet = {1, 2, 3}\n\nmySet.remove(2) # OK\nmySet.add(4) # OK\nmySet[0] = 5 # ERROR\n```\n</code></pre>"},{"location":"Python/1.0_Sets.html#no-listsdictionaries-tuples-are-ok","title":"No Lists/Dictionaries, Tuples Are OK.","text":"<p> Sets use hashing, so you can't store lists or dictionaries in them. However, you can store tuples. Just make sure these tuples don't contain lists or dictionaries inside them.</p> <p></p> <pre><code>```python\n# Valid elements\nmySet = {1, \"hello\", (1, 2)} # TUPLES OK\n\n# Invalid elements\nmySet = {[1, 2], {\"key\": \"value\"}} # ERROR, NO LISTS, NO DICTS\n```\n</code></pre>"},{"location":"Python/1.0_Sets.html#when-to-use-sets","title":"When to use sets","text":"<p> Sets for Python are very useful when you need keep unique items and do quick membership checks.  Here are some scenarios where sets are frequently used:</p>"},{"location":"Python/1.0_Sets.html#removing-duplicates","title":"Removing Duplicates","text":"<ul> <li>Use Case: When you need to ensure that a collection of elements contains no duplicates.</li> <li>Example: Removing duplicates from a list.   <pre><code>items = [1, 2, 2, 3, 4, 4, 5]\nunique_items = list(set(items))  # [1, 2, 3, 4, 5]\n</code></pre></li> </ul>"},{"location":"Python/1.0_Sets.html#membership-testing","title":"Membership Testing","text":"<ul> <li>Use Case: When you need to check if an element exists in a collection. Sets provide average O(1) time complexity for membership tests.</li> <li>Example: Checking if an item exists in a collection.   <pre><code>allowed_items = {\"apple\", \"banana\", \"cherry\"}\nif \"banana\" in allowed_items:\n    print(\"Banana is allowed\")\n</code></pre></li> </ul>"},{"location":"Python/1.0_Sets.html#set-operations","title":"Set Operations","text":"<ul> <li>Use Case: When you need to perform operations like union, intersection, difference, and symmetric difference between collections.</li> <li>Example: Finding common elements between two sets.   <pre><code>set1 = {1, 2, 3}\nset2 = {3, 4, 5}\ncommon_items = set1 &amp; set2  # {3}\n</code></pre></li> </ul>"},{"location":"Python/1.0_Sets.html#data-validation","title":"Data Validation","text":"<ul> <li>Use Case: When validating data to ensure uniqueness, such as checking for duplicate entries in a dataset.</li> <li>Example: Validating unique user IDs.   <pre><code>user_ids = [101, 102, 103, 101]\nunique_user_ids = set(user_ids)\nif len(user_ids) != len(unique_user_ids):\n    print(\"Duplicate user IDs found\")\n</code></pre></li> </ul>"},{"location":"Python/1.0_Sets.html#tracking-unique-elements","title":"Tracking Unique Elements","text":"<ul> <li>Use Case: When you need to keep track of unique items encountered during processing.</li> <li>Example: Tracking unique words in a text.   <pre><code>text = \"hello world hello\"\nwords = text.split()\nunique_words = set(words)  # {\"hello\", \"world\"}\n</code></pre></li> </ul>"},{"location":"Python/1.0_Sets.html#efficient-data-lookups","title":"Efficient Data Lookups","text":"<ul> <li>Use Case: When you need a data structure that allows for fast lookups, insertions, and deletions.</li> <li>Example: Keeping track of visited URLs in a web crawler.   <pre><code>visited_urls = set()\nvisited_urls.add(\"https://example.com\")\nif \"https://example.com\" in visited_urls:\n    print(\"URL already visited\")\n</code></pre></li> </ul>"},{"location":"Python/1.0_Sets.html#test-your-knowledge","title":"Test your knowledge","text":"<p>Highlight the answer section to reveal!</p>"},{"location":"Python/1.0_Sets.html#question-setupdate","title":"Question - set.update()","text":"<p> What will be the output of the following statement?</p> <pre><code>thisset = {\"apple\", \"banana\", \"cherry\", False, True, 0}\nprint(thisset)\n</code></pre> <p>Answer: {'apple', 'banana', 'cherry', False, True}</p>"},{"location":"Python/1.0_Sets.html#question-setadd","title":"Question - set.add()","text":"<p> What will be the output of the following statement?</p> <pre><code>thisset = {\"apple\", \"banana\", \"cherry\"}\nthisset.add(\"apple\")\nprint(thisset)\n</code></pre> <p>Answer: {'apple', 'banana', 'cherry'}</p>"},{"location":"Python/1.0_Sets.html#question-setdiscard","title":"Question - set.discard()","text":"<p> What will be the output of the following statement?</p> <pre><code>thisset = {1, 2, 3, 4, 5}\nthisset.discard(6)\nprint(thisset)\n</code></pre> <p>Answer: {1, 2, 3, 4, 5}</p>"},{"location":"Python/1.0_Sets.html#question-setremove","title":"Question - set.remove()","text":"<p> What will be the output of the following statement?</p> <pre><code>thisset = {1, 2, 3, 4, 5}\nthisset.remove(6)\nprint(thisset)\n</code></pre> <p>Answer: Raises a KeyError</p>"},{"location":"Python/1.0_Sets.html#question-setupdate_1","title":"Question - set.update()","text":"<p> What will be the output of the following statement?</p> <pre><code>thisset = {\"apple\", \"banana\", \"cherry\"}\nthisset.update([\"orange\", \"mango\"])\nprint(thisset)\n</code></pre> <p>Answer: {'apple', 'banana', 'cherry', 'orange', 'mango'}</p>"},{"location":"Python/1.0_Sets.html#question-setcopy","title":"Question - set.copy()","text":"<p> What will be the output of the following statement?</p> <pre><code>thisset = {\"apple\", \"banana\", \"cherry\"}\nnewset = thisset.copy()\nthisset.add(\"orange\")\nprint(newset)\n</code></pre> <p>Answer: {'apple', 'banana', 'cherry'}</p>"},{"location":"Python/1.0_Sets.html#question-set-membership","title":"Question - set membership","text":"<p> What will be the output of the following statement?</p> <pre><code>thisset = {1, 2, 3, 4, 5}\nresult = 3 in thisset\nprint(result)\n</code></pre> <p>Answer: True</p>"},{"location":"Python/1.0_Sets.html#question-set-intersection","title":"Question - set intersection","text":"<p> What will be the output of the following statement?</p> <pre><code>thisset1 = {1, 2, 3}\nthisset2 = {3, 4, 5}\nresult = thisset1 &amp; thisset2\nprint(result)\n</code></pre> <p>Answer: {3}</p>"},{"location":"Python/1.0_Sets.html#question-set-union","title":"Question - set union","text":"<p> What will be the output of the following statement?</p> <pre><code>thisset1 = {1, 2, 3}\nthisset2 = {3, 4, 5}\nresult = thisset1 | thisset2\nprint(result)\n</code></pre> <p>Answer: {1, 2, 3, 4, 5}</p>"},{"location":"Python/1.0_Sets.html#question-set-difference","title":"Question - set difference","text":"<p> What will be the output of the following statement?</p> <pre><code>thisset1 = {1, 2, 3}\nthisset2 = {3, 4, 5}\nresult = thisset1 - thisset2\nprint(result)\n</code></pre> <p>Answer: {1, 2}</p>"},{"location":"Python/1.0_Sets.html#set-operations-and-properties","title":"Set Operations and Properties","text":"Operation Syntax Description &amp; Example Union <code>x1.union(x2)</code><code>x1 &amp;#124; x2</code> Combines all elements from both sets, without duplicates.<code>x1 = {1, 2, 3}</code><code>x2 = {3, 4, 5}</code><code>x1.union(x2)</code>Output: <code>{1, 2, 3, 4, 5}</code> Intersection <code>x1.intersection(x2)</code><code>x1 &amp; x2</code> Returns elements common to both sets.<code>x1 = {1, 2, 3}</code><code>x2 = {3, 4, 5}</code><code>x1 &amp; x2</code>Output: <code>{3}</code> Difference <code>x1.difference(x2)</code><code>x1 - x2</code> Returns elements in the first set but not in the second.<code>x1 = {1, 2, 3}</code><code>x2 = {3, 4, 5}</code><code>x1 - x2</code>Output: <code>{1, 2}</code> Symmetric Difference <code>x1.symmetric_difference(x2)</code><code>x1 ^ x2</code> Elements in either set, but not both.<code>x1 = {1, 2, 3}</code><code>x2 = {3, 4, 5}</code><code>x1 ^ x2</code>Output: <code>{1, 2, 4, 5}</code> Subset <code>x1.issubset(x2)</code><code>x1 &lt;= x2</code> Checks if all elements of one set are in another.<code>x1 = {1, 2}</code><code>x2 = {1, 2, 3}</code><code>x1 &lt;= x2</code>Output: <code>True</code> Superset <code>x1.issuperset(x2)</code><code>x1 &gt;= x2</code> Checks if one set contains all elements of another.<code>x1 = {1, 2, 3}</code><code>x2 = {1, 2}</code><code>x1 &gt;= x2</code>Output: <code>True</code> Disjoint <code>x1.isdisjoint(x2)</code> Checks if two sets have no elements in common.<code>x1 = {1, 2, 3}</code><code>x2 = {4, 5, 6}</code><code>x1.isdisjoint(x2)</code>Output: <code>True</code> Add Element <code>x1.add(element)</code> Adds a single element to the set.<code>x1 = {1, 2, 3}</code><code>x1.add(4)</code>Output: <code>{1, 2, 3, 4}</code> Remove Element <code>x1.remove(element)</code> Removes a specific element from the set.<code>x1 = {1, 2, 3}</code><code>x1.remove(2)</code>Output: <code>{1, 3}</code> Discard Element <code>x1.discard(element)</code> Removes a specific element if it is present.<code>x1 = {1, 2, 3}</code><code>x1.discard(2)</code>Output: <code>{1, 3}</code> Clear Set <code>x1.clear()</code> Removes all elements from the set.<code>x1 = {1, 2, 3}</code><code>x1.clear()</code>Output: <code>set()</code> Copy Set <code>x1.copy()</code> Creates a shallow copy of the set.<code>x1 = {1, 2, 3}</code><code>x2 = x1.copy()</code>Output: <code>x2 = {1, 2, 3}</code> Update Set <code>x1.update(x2)</code> Adds elements from another set.<code>x1 = {1, 2}</code><code>x2 = {3, 4}</code><code>x1.update(x2)</code>Output: <code>{1, 2, 3, 4}</code> Intersection Update <code>x1.intersection_update(x2)</code> Updates the set, keeping only elements found in it and another set.<code>x1 = {1, 2, 3}</code><code>x2 = {2, 3, 4}</code><code>x1.intersection_update(x2)</code>Output: <code>{2, 3}</code> Difference Update <code>x1.difference_update(x2)</code> Updates the set, removing elements found in another set.<code>x1 = {1, 2, 3}</code><code>x2 = {2, 3, 4}</code><code>x1.difference_update(x2)</code>Output: <code>{1}</code> Symmetric Difference Update <code>x1.symmetric_difference_update(x2)</code> Updates the set, keeping only elements found in either set, but not both.<code>x1 = {1, 2, 3}</code><code>x2 = {2, 3, 4}</code><code>x1.symmetric_difference_update(x2)</code>Output: <code>{1, 4}</code>"},{"location":"Python/1.1.0_assert_methods.html","title":"Assert & Methods","text":""},{"location":"Python/1.1.0_assert_methods.html#understanding-assert-methods-in-python","title":"Understanding Assert Methods in Python","text":"<p>Assert methods are used in Python to check if a certain condition is true. They help in verifying that your code is working as expected. If the condition is not met, the program stops and shows an error message.</p>"},{"location":"Python/1.1.0_assert_methods.html#key-points","title":"Key Points","text":"<ol> <li>Basic Assertion:</li> <li>You use the <code>assert</code> keyword followed by a condition.</li> <li>If the condition is <code>True</code>, the program continues.</li> <li>If the condition is <code>False</code>, the program raises an <code>AssertionError</code>.</li> </ol> <p>Example:    <pre><code>x = 5\nassert x == 5  # This will not raise an error because the condition is True\n</code></pre></p> <ol> <li>Assertion with Error Message:</li> <li>You can add a message to the <code>assert</code> statement.</li> <li>This message will be shown if the assertion fails.</li> </ol> <p>Example:    <pre><code>x = 3\nassert x == 5, \"x should be 5\"  # This will raise an AssertionError with the message\n</code></pre></p> <ol> <li>Common Use Cases:</li> <li>Testing: Assertions are often used in testing to check if functions return the expected results.</li> <li> <p>Debugging: They help in finding bugs by ensuring conditions are met at different stages of the code.</p> </li> <li> <p>Using Assertions in Functions:</p> </li> <li>You can use <code>assert</code> to check input values or results within functions.</li> </ol> <p>Example:    <pre><code>def divide(a, b):\n    assert b != 0, \"Division by zero is not allowed\"\n    return a / b\n</code></pre></p> <p>Here, if <code>b</code> is zero, the assertion will fail, and an error message will be displayed.</p>"},{"location":"Python/1.1.0_assert_methods.html#takeaways","title":"Takeaways","text":"<ul> <li>Assert Methods are a simple way to verify that your code behaves as expected.</li> <li>Use Assertions to catch errors early and ensure your program runs correctly.</li> <li>Keep Messages Clear to make debugging easier when assertions fail.</li> </ul> <p>In summary, assert methods are useful for checking conditions in your code and help in debugging and testing by providing clear error messages when things go wrong.</p>"},{"location":"Python/1.1.1_decorators.html","title":"Decorators","text":""},{"location":"Python/1.1.1_decorators.html#understanding-decorators-in-python","title":"Understanding Decorators in Python","text":"<p>In Python, decorators are a handy feature that lets you modify/enhance functions/methods without changing their actual code. Decorators are likes wrappers that can change how a function behaves.</p>"},{"location":"Python/1.1.1_decorators.html#how-decorators-work","title":"How Decorators Work","text":"<ol> <li> <p>Basic Concept: A decorator is a function that takes another function and extends its behavior without modifying it. It\u2019s like adding extra toppings to a basic dish to make it more interesting.</p> </li> <li> <p>Using a Decorator: You apply a decorator using the <code>@</code> symbol followed by the decorator's name right above the function you want to modify. For example:    <pre><code>@my_decorator\ndef my_function():\n    print(\"Hello!\")\n</code></pre>    Here, <code>@my_decorator</code> is applied to <code>my_function</code>.</p> </li> <li> <p>Creating a Decorator: A decorator itself is just a function that returns another function. Here\u2019s a simple example:    <pre><code>def my_decorator(func):\n    def wrapper():\n        print(\"Something is happening before the function.\")\n        func()\n        print(\"Something is happening after the function.\")\n    return wrapper\n\n@my_decorator\ndef say_hello():\n    print(\"Hello!\")\n\nsay_hello()\n</code></pre>    When <code>say_hello()</code> is called, you\u2019ll see extra messages before and after the \"Hello!\" message.</p> </li> <li> <p>Why Use Decorators?: They help in adding common functionality (like logging, timing, etc.) to multiple functions easily, keeping your code clean and organized.</p> </li> </ol>"},{"location":"Python/1.1.1_decorators.html#key-points","title":"Key Points","text":"<ul> <li>Flexibility: Decorators let you add functionality to functions or methods without changing their core logic.</li> <li>Reusability: Once created, a decorator can be used on multiple functions, making your code DRY (Don't Repeat Yourself).</li> <li>Built-in Decorators: Python also provides some built-in decorators like <code>@staticmethod</code>, <code>@classmethod</code>, and <code>@property</code>.</li> </ul> <p>What is <code>unittest</code>?</p> <p><code>unittest</code> is a built-in Python module that helps you test your code. Think of it like a way to check if your code is doing what it's supposed to. Here\u2019s a simple breakdown:</p>"},{"location":"Python/1.1.1_decorators.html#why-use-unittest","title":"Why Use <code>unittest</code>?","text":"<p>Imagine you\u2019re building a machine that makes coffee. You'd want to test if it makes coffee properly, right? <code>unittest</code> does something similar for your code. It helps you check if your functions and classes are working as expected.</p>"},{"location":"Python/1.1.1_decorators.html#how-does-it-work","title":"How Does It Work?","text":"<ol> <li> <p>Write Test Cases: You write test cases that check if your code is correct. For example, if you have a function that adds two numbers, you\u2019d write a test to check if it actually adds them correctly.</p> </li> <li> <p>Run Tests: You run these test cases to see if your code passes all the checks. If everything works fine, you\u2019re good to go. If not, you\u2019ll see where things went wrong and can fix them.</p> </li> </ol>"},{"location":"Python/1.1.1_decorators.html#basic-example","title":"Basic Example","text":"<p>Here's how you might use <code>unittest</code>:</p> <pre><code>import unittest\n\n# Function to test\ndef add(a, b):\n    return a + b\n\n# Test case\nclass TestAddFunction(unittest.TestCase):\n\n    def test_add_positive(self):\n        self.assertEqual(add(2, 3), 5)  # Test if 2 + 3 equals 5\n\n    def test_add_negative(self):\n        self.assertEqual(add(-1, -1), -2)  # Test if -1 + -1 equals -2\n\n# Run the tests\nif __name__ == \"__main__\":\n    unittest.main()\n</code></pre> <p>In this example, <code>unittest</code> helps us test if our <code>add</code> function works with positive and negative numbers.</p>"},{"location":"Python/1.1.1_decorators.html#why-it-matters","title":"Why It Matters","text":"<p>Using <code>unittest</code> helps you catch bugs early, so you don't find out about them later when it's harder to fix. It\u2019s like checking your coffee machine before your guests arrive to make sure it works!</p> <p>So, <code>unittest</code> is a handy tool to make sure your code does what it\u2019s supposed to do, making your coding life easier and your code more reliable.</p>"},{"location":"Python/1.1.2_argv.html","title":"Arguments","text":""},{"location":"Python/1.1.2_argv.html#argv","title":"argv","text":"<p><code>argv</code> stands for \"argument vector.\" It\u2019s a feature in Python that allows you to pass command-line arguments to your script when you run it. This is useful when you want your script to handle different inputs without changing the code itself.</p>"},{"location":"Python/1.1.2_argv.html#how-it-works","title":"How It Works","text":"<p>When you run a Python script from the command line, you can provide extra information after the script name. For example:</p> <pre><code>python myscript.py arg1 arg2 arg3\n</code></pre> <p>In this case, <code>arg1</code>, <code>arg2</code>, and <code>arg3</code> are command-line arguments. <code>argv</code> helps you capture and use these arguments in your script.</p>"},{"location":"Python/1.1.2_argv.html#using-argv-in-a-script","title":"Using <code>argv</code> in a Script","text":"<p>Here\u2019s a simple example:</p> <pre><code>import sys\n\n# Print all arguments\nprint(\"All arguments:\", sys.argv)\n\n# Access individual arguments\nif len(sys.argv) &gt; 1:\n    print(\"First argument:\", sys.argv[1])\n</code></pre>"},{"location":"Python/1.1.2_argv.html#explanation","title":"Explanation","text":"<ul> <li><code>sys.argv</code> is a list that contains all command-line arguments passed to the script.</li> <li><code>sys.argv[0]</code> is always the script name (<code>myscript.py</code> in this case).</li> <li><code>sys.argv[1]</code>, <code>sys.argv[2]</code>, etc., are the actual arguments you pass to the script.</li> </ul>"},{"location":"Python/1.1.2_argv.html#example","title":"Example","text":"<p>If you run:</p> <pre><code>python myscript.py hello world\n</code></pre> <p>The output will be:</p> <pre><code>All arguments: ['myscript.py', 'hello', 'world']\nFirst argument: hello\n</code></pre>"},{"location":"Python/1.1.2_argv.html#why-its-useful","title":"Why It\u2019s Useful","text":"<p><code>argv</code> is handy for creating scripts that need to handle different inputs or configurations without hardcoding values. For example, a script that processes files can use <code>argv</code> to specify which file to process, making the script flexible and reusable.</p>"},{"location":"Python/1.1.3_Diff_And_Patch.html","title":"Diff & Patch","text":""},{"location":"Python/1.1.3_Diff_And_Patch.html#understanding-diff-and-patch","title":"Understanding <code>diff</code> and <code>patch</code>","text":"<p>When working with code, especially with a team, it's important to handle changes efficiently. Two handy tools for this are <code>diff</code> and <code>patch</code>.</p>"},{"location":"Python/1.1.3_Diff_And_Patch.html#diff","title":"<code>diff</code>","text":"<p>The <code>diff</code> command helps you compare two files line by line and shows you the differences. This makes it easy to see what changes have been made.</p> <p>Example:</p> <p>Suppose you have two versions of a Python file, <code>file1.py</code> and <code>file2.py</code>. You want to check what changes were made from <code>file1.py</code> to <code>file2.py</code>.</p> <pre><code>diff file1.py file2.py\n</code></pre> <p>The output might look like this:</p> <pre><code>3c3\n&lt; print(\"Hello, World!\")\n---\n&gt; print(\"Hello, Universe!\")\n</code></pre> <p>This output means that line 3 in <code>file1.py</code>, which prints \"Hello, World!\", has been changed to print \"Hello, Universe!\" in <code>file2.py</code>.</p>"},{"location":"Python/1.1.3_Diff_And_Patch.html#patch","title":"<code>patch</code>","text":"<p>The <code>patch</code> command lets you apply changes to a file based on a <code>diff</code> file. This is useful for sharing updates or fixes without needing to send the entire file.</p> <p>Example:</p> <p>You have a <code>diff</code> file, <code>changes.diff</code>, which contains the differences between two versions of a file. You want to apply these changes to <code>file1.py</code>.</p> <pre><code>patch file1.py &lt; changes.diff\n</code></pre> <p>This command updates <code>file1.py</code> with the changes specified in <code>changes.diff</code>.</p>"},{"location":"Python/1.1.3_error_handling.html","title":"Error Handling","text":""},{"location":"Python/1.1.3_error_handling.html#understanding-try-and-except-in-python","title":"Understanding <code>try</code> and <code>except</code> in Python","text":"<p>In Python, the <code>try</code> and <code>except</code> statements are used to handle errors and exceptions gracefully. This is very useful when you want your program to keep running even if something goes wrong.</p>"},{"location":"Python/1.1.3_error_handling.html#how-try-and-except-work","title":"How <code>try</code> and <code>except</code> Work","text":"<ol> <li><code>try</code> Block:</li> <li>The code that might cause an error is put inside the <code>try</code> block.</li> <li> <p>Python tries to execute this code.</p> </li> <li> <p><code>except</code> Block:</p> </li> <li>If an error occurs in the <code>try</code> block, the execution jumps to the <code>except</code> block.</li> <li>You can specify the type of error you want to handle.</li> </ol>"},{"location":"Python/1.1.3_error_handling.html#example-counting-character-frequency-in-a-file","title":"Example: Counting Character Frequency in a File","text":"<p>Let\u2019s look at an example that reads a file does something with it. We\u2019ll use <code>try</code> and <code>except</code> to handle any errors that might occur when opening the file.</p> <pre><code>def character_frequency(filename):\n    \"\"\"Counts the frequency of each character in the given file.\"\"\"\n    # First, try to open the file\n    try:\n        f = open(filename)\n    except OSError:\n        return None  # Return None if the file cannot be opened\n\n    # Now, process the file\n</code></pre>"},{"location":"Python/1.1.3_error_handling.html#raising-errors-in-python","title":"Raising Errors in Python","text":"<p>In Python, you can raise (or trigger) errors intentionally using the <code>raise</code> statement. This is useful when you want to raise errors if certain conditions arise. This approach can help in debugging.</p>"},{"location":"Python/1.1.3_error_handling.html#why-raise-errors","title":"Why Raise Errors?","text":"<ul> <li>Input Validation: Ensure that the input to a function is valid.</li> <li>Preventing Illegal Operations: Stop the program from performing operations that aren't allowed.</li> <li>Custom Error Messages: Provide clear messages to help understand what went wrong.</li> </ul>"},{"location":"Python/1.1.3_error_handling.html#how-to-raise-errors","title":"How to Raise Errors","text":"<p>You can raise built-in exceptions or define your own custom exceptions. Here\u2019s how:</p>"},{"location":"Python/1.1.3_error_handling.html#raising-built-in-exceptions","title":"Raising Built-in Exceptions","text":"<p>Python has several built-in exceptions like <code>ValueError</code>, <code>TypeError</code>, <code>IndexError</code>, etc. You can raise these using the <code>raise</code> statement.</p> <p>Example: Raising a ValueError</p> <pre><code>def divide(a, b):\n    if b == 0:\n        raise ValueError(\"The denominator cannot be zero.\")\n    return a / b\n\ntry:\n    result = divide(10, 0)\nexcept ValueError as e:\n    print(e)\n</code></pre> <p>In this example: - The function <code>divide</code> raises a <code>ValueError</code> if the denominator is zero. - The <code>try</code> block catches the error and prints the error message.</p>"},{"location":"Python/1.1.3_error_handling.html#custom-exceptions","title":"Custom Exceptions","text":"<p>You can define your own exceptions by creating a class that inherits from the <code>Exception</code> class.</p> <p>Example: Custom Exception</p> <pre><code>class NegativeNumberError(Exception):\n    pass\n\ndef check_positive(number):\n    if number &lt; 0:\n        raise NegativeNumberError(\"Negative numbers are not allowed.\")\n    return number\n\ntry:\n    print(check_positive(-5))\nexcept NegativeNumberError as e:\n    print(e)\n</code></pre> <p>In this example: - <code>NegativeNumberError</code> is a custom exception. - The function <code>check_positive</code> raises <code>NegativeNumberError</code> if the input is negative. - The <code>try</code> block catches the error and prints the message.</p>"},{"location":"Python/1.1.3_error_handling.html#common-use-cases","title":"Common Use Cases","text":"<ol> <li>Validating Function Arguments</li> </ol> <pre><code>def sqrt(x):\n    if x &lt; 0:\n        raise ValueError(\"Cannot compute the square root of a negative number.\")\n    return x ** 0.5\n\ntry:\n    print(sqrt(-9))\nexcept ValueError as e:\n    print(e)\n</code></pre> <ol> <li>Handling Invalid States</li> </ol> <pre><code>def withdraw(amount, balance):\n    if amount &gt; balance:\n        raise RuntimeError(\"Insufficient funds.\")\n    return balance - amount\n\ntry:\n    print(withdraw(100, 50))\nexcept RuntimeError as e:\n    print(e)\n</code></pre>"},{"location":"Python/1.1.3_error_handling.html#summary","title":"Summary","text":"<ul> <li>Raising Errors: Use the <code>raise</code> statement to trigger exceptions.</li> <li>Built-in Exceptions: Raise common errors like <code>ValueError</code> and <code>TypeError</code>.</li> <li>Custom Exceptions: Create custom error types for specific situations.</li> </ul> <p>assert statements</p>"},{"location":"Python/1.1.4_pdb.html","title":"Debugging","text":""},{"location":"Python/1.1.4_pdb.html#using-pdb-for-debugging","title":"Using <code>pdb</code> for Debugging","text":"<p>Debugging is an important part of writing code, and Python has a built-in tool called <code>pdb</code> to help with this.</p>"},{"location":"Python/1.1.4_pdb.html#what-is-pdb","title":"What is <code>pdb</code>?","text":"<p><code>pdb</code> is Python's interactive debugger. It lets you pause your code, inspect variables, and step through the code line by line to find out what's going wrong.</p>"},{"location":"Python/1.1.4_pdb.html#setting-breakpoints","title":"Setting Breakpoints","text":"<p>Breakpoints allow you to pause your code at specific points to see what's happening at that moment.</p> <p>Example:</p> <p>In your Python script, you can set a breakpoint like this:</p> <pre><code>import pdb; pdb.set_trace()\n\ndef add(a, b):\n    return a + b\n\nresult = add(2, 3)\nprint(result)\n</code></pre> <p>When you run this script, it will pause before returning the result, allowing you to inspect variables and step through the code.</p>"},{"location":"Python/1.1.4_pdb.html#stepping-through-code","title":"Stepping Through Code","text":"<p><code>pdb</code> lets you execute your code line by line to see what happens at each step.</p> <p>Example:</p> <p>Using the same script, once <code>pdb</code> pauses execution, you can step through the code.</p> <pre><code>(pdb) n  # 'n' is for next line\n(pdb) p a  # 'p' is for print\n2\n(pdb) p b\n3\n(pdb) c  # 'c' is for continue\n</code></pre>"},{"location":"Python/1.1.4_pdb.html#inspecting-and-modifying-variables","title":"Inspecting and Modifying Variables","text":"<p>You can check and change the values of variables at any point during execution.</p> <p>Example:</p> <p>While debugging, you might find the value of <code>a</code> is not as expected. You can modify it.</p> <pre><code>(pdb) p a\n2\n(pdb) a = 5\n(pdb) p a\n5\n</code></pre>"},{"location":"Python/1.1.4_pdb.html#evaluating-expressions","title":"Evaluating Expressions","text":"<p><code>pdb</code> lets you run Python expressions interactively, so you can test code snippets in real-time.</p> <p>Example:</p> <pre><code>(pdb) p a + b\n8\n</code></pre>"},{"location":"Python/1.1.4_pdb.html#why-use-pdb","title":"Why Use <code>pdb</code>?","text":"<p>Using <code>pdb</code> gives you a deeper understanding of your code's execution flow compared to just using <code>print</code> statements. Instead of scattering <code>print</code> statements throughout your code, you can use <code>pdb</code> to explore and diagnose issues interactively. This makes debugging more efficient and helps you find the root cause of problems more effectively.</p>"},{"location":"Python/1.1.5_pyformat.html","title":"Formatting","text":"# Using `%` and `.format()` in Python  ## Introduction In Python, formatting strings is a common task. Two popular methods to do this are using the `%` operator and the `.format()` method. Let's understand how to use them with simple examples.  ## The `%` Operator  The `%` operator is a traditional way to format strings. It places values in a string using placeholders.  ### Example: <pre><code>name = \"Alice\"\nage = 30\nformatted_string = \"My name is %s and I am %d years old.\" % (name, age)\nprint(formatted_string)\n</code></pre>  Output: My name is Alice and I am 30 years old.   Here, `%s` is a placeholder for a string, and `%d` is a placeholder for an integer.  ### More Placeholders: - `%s` - String - `%d` - Integer - `%f` - Floating-point number  ## The `.format()` Method  The `.format()` method is more flexible and was introduced in Python 3. It uses curly braces `{}` as placeholders in a string.  ### Example: <pre><code>name = \"Bob\"\nage = 25\nformatted_string = \"My name is {} and I am {} years old.\".format(name, age)\nprint(formatted_string)\n</code></pre>  Output: My name is Bob and I am 25 years old.   Here, `{}` are placeholders that get replaced by values passed to the `format()` method.  ### Using Index Numbers: You can specify the order of values using index numbers inside `{}`.  #### Example: <pre><code>formatted_string = \"My name is {0} and I am {1} years old. {0} loves coding.\".format(name, age)\nprint(formatted_string)\n</code></pre>  Output: My name is Bob and I am 25 years old. Bob loves coding.   ### Using Keywords: You can also use keywords to make the code more readable.  #### Example: <pre><code>formatted_string = \"My name is {name} and I am {age} years old.\".format(name=\"Charlie\", age=22)\nprint(formatted_string)\n</code></pre>  Output: My name is Charlie and I am 22 years old.   ## Conclusion  Both `%` and `.format()` are useful for formatting strings in Python. The `%` operator is simple and good for basic formatting. The `.format()` method is more powerful and flexible for more complex string formatting needs.  Happy coding!"},{"location":"Python/1.10_Func_Modl_Summary.html","title":"Functions, Methods, Modules, and Libraries in Python","text":"<p>Here, I have consolidated all the points related to Functions, Methods, Modules and Library in Python. Hope you will find it useful.</p> Type Description Examples Functions Standalone blocks of code inside a <code>def</code>. <code>math.sqrt()</code>, <code>os.path.exists()</code>, <code>json.loads()</code> Methods Functions within a class, always linked to objects. <code>list.append()</code>, <code>str.upper()</code>, <code>dict.items()</code> Modules A Python file with a <code>.py</code> extension containing functions, classes, and other code. <code>import math</code><code>from ikea import add</code> Libraries A collection of modules. Libraries contain similar modules for similar tasks. <code>import pandas as pd</code>"},{"location":"Python/1.10_Func_Modl_Summary.html#importing-modules-and-differences","title":"Importing Modules and Differences","text":"Syntax Description Example Difference <code>import module</code> Imports the full module. Use <code>moduleName.functionName</code> to access functions. <code>import math</code><code>print(math.sqrt(16))</code> Keeps the namespace clean, avoids naming conflicts. <code>from module import function</code> Imports specific functions directly, no need for the module name prefix. <code>from ikea import add</code><code>print(add(1, 2))</code> Convenient for using specific functions frequently. <code>from module import *</code> Imports all functions, classes, and variables from the module directly into the current namespace. <code>from math import *</code><code>print(sqrt(16))</code> Can cause naming conflicts, harder to track where functions/classes come from. Generally not recommended for larger codebases."},{"location":"Python/1.10_Func_Modl_Summary.html#installing-libraries","title":"Installing Libraries","text":"Command Description Example <code>pip install libraryname</code> Installs a library from the command prompt or terminal. <code>pip install pandas</code> <code>os.system('pip install libraryname')</code> Installs a library from within your Python code using the <code>os</code> module to run shell commands. <code>import os</code><code>os.system('pip install pandas')</code>"},{"location":"Python/1.10_Func_Modl_Summary.html#managing-libraries-when-sharing-python-code","title":"Managing Libraries When Sharing Python Code","text":"Step Description Example <code>pip freeze &gt; requirements.txt</code> Create a <code>requirements.txt</code> file that lists all the libraries your script needs. <code>pip freeze &gt; requirements.txt</code> <code>pip install -r requirements.txt</code> Install dependencies on another system. <code>pip install -r requirements.txt</code>"},{"location":"Python/1.10_Func_Modl_Summary.html#example-workflow","title":"Example Workflow","text":"Step Description Code Create the Python script Write your Python script and save it to a file. <code>echo \"import pandas as pd ... \" &gt; data_analysis.py</code> Install pandas if not installed Ensure that the pandas library is installed. <code>pip install pandas</code> Generate <code>requirements.txt</code> Create a <code>requirements.txt</code> file listing all required libraries. <code>pip freeze &gt; requirements.txt</code> Clone project or copy files Copy your project files to the target system. <code>scp user@development_system:/path/to/project/* /path/to/local/directory/</code> Create virtual environment (Optional) Create a virtual environment for your project. <code>python -m venv myenv</code><code>source myenv/bin/activate</code> Install required libraries Install the required libraries on the target system using <code>requirements.txt</code>. <code>pip install -r requirements.txt</code> Run the script Execute your Python script on the target system. <code>python data_analysis.py</code>"},{"location":"Python/1.11_ifelifelse.html","title":"If-Elif-Else","text":""},{"location":"Python/1.11_ifelifelse.html#if-elif-else-in-python","title":"<code>if:</code> <code>elif:</code> <code>else:</code> in Python","text":"<ul> <li><code>if</code>: Checks a condition. If true, runs the code block.</li> <li><code>elif</code>: Checks another condition if the previous ones were false. Runs the code block if true.</li> <li><code>else</code>: Runs the code block if all previous conditions were false.</li> </ul>"},{"location":"Python/1.11_ifelifelse.html#some-ifelifelse-examples","title":"Some ifelifelse examples","text":"<p><pre><code>#demo.py\ndef main():\n  print(find_email(sys.argv))\n\n# If demo.py file is run directly (not imported as a module), then run the main() function.\nif __name__ == \"__main__\":\n  main()\n</code></pre> <pre><code>if user_input.lower() == \"yes\":\n    print(\"blabla\")\n</code></pre> <pre><code>stock = \"APPL\"\nNASDAQ  = [\"FB\", \"APPL\", \"NVDIA\"]\nif stock in NASDAQ:\n    print(\"..\")\n</code></pre></p> Code Examples Code Examples <pre>\nuser_input = \"Yes\"\nif user_input.lower() == \"yes\":\n    print(\"Yes\")\nelse:\n    print(\"No\")\n                </pre> <pre>\nfruit = \"apple\"\nfruits = [\"apple\", \"banana\", \"cherry\"]\nif fruit in fruits:\n    print(f\"{fruit} is in the list\")\nelse:\n    print(f\"{fruit} is not in the list\")\n                </pre> <pre>\nuser_input = \"Yes\"\nif user_input.lower() == \"yes\":\n    print(\"User said yes\")\nelse:\n    print(\"User did not say yes\")\n                </pre> <pre>\nfruit = \"apple\"\nfruits = [\"apple\", \"banana\", \"cherry\"]\nif fruit in fruits:\n    print(f\"{fruit} is in the list\")\nelse:\n    print(f\"{fruit} is not in the list\")\n                </pre> <pre>\nnum = 10\nif num % 2 == 0:\n    print(\"Even\")\nelse:\n    print(\"Odd\")\n                </pre> <pre>\nvalue = \"100\"\nif isinstance(value, str):\n    print(\"Value is a string\")\nelse:\n    print(\"Value is not a string\")\n                </pre> <pre>\nn = None\nif n is None:\n    print(\"n is None\")\nelse:\n    print(\"n is not None\")\n                </pre> <pre>\npermissions = ['read', 'write']\nif 'admin' in permissions:\n    print(\"Has admin access\")\nelif 'write' in permissions:\n    print(\"Has write access\")\nelse:\n    print(\"Has read-only access\")\n                </pre> <pre>\nconfig = {\"debug\": True}\nif config.get(\"debug\"):\n    print(\"Debugging mode is on\")\nelse:\n    print(\"Debugging mode is off\")\n                </pre> <pre>\ncolor = \"red\"\nif color == \"blue\":\n    print(\"Color is blue\")\nelif color == \"red\":\n    print(\"Color is red\")\nelse:\n    print(\"Color is neither blue nor red\")\n                </pre> <pre>\nx = 3\ny = \"3\"\nif x == int(y):\n    print(\"Equal values\")\nelse:\n    print(\"Different values\")\n                </pre> <pre>\ntemperature = 35\nif temperature &gt; 30:\n    print(\"It's hot\")\nelif 20 &lt;= temperature &lt;= 30:\n    print(\"It's warm\")\nelse:\n    print(\"It's cold\")\n                </pre>"},{"location":"Python/1.12_Operators.html","title":"Operators","text":""},{"location":"Python/1.12_Operators.html#1-1-is-false-but-1-1-is-error-why","title":"(1 == \"1\") is <code>False</code> but (1 &lt; \"1\") is <code>ERROR</code> Why?Summary:","text":"<p>When Python uses the <code>==</code> operator, it first checks if the two items are of the same type. If the types are different, Python will immediately return <code>False</code> without thinking twice. Hence, you get <code>False</code> and not an error.</p> <p>But when Python uses relational operators like (<code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code>), it wants the items to be of the same type. Why? Because you can't compare oranges with apples! Relational comparisons need the items to be of the same type to decide the order.</p> <p>Equality (<code>==</code>): Checks if the two values are the same type. Returns `False` if the types are different, without further comparison.</p> <pre>\nprint(1 == \"1\")  # This will return False\n  </pre> <p>Relational Operators (<code>&lt;</code>, <code>&gt;</code>, etc.): Require the values to be of the same or comparable types to evaluate their order. Raise a `TypeError` if the types are not comparable.</p> <pre>\nprint(1 &lt; \"1\")   # This will raise a TypeError\n  </pre> <p></p>"},{"location":"Python/1.12_Operators.html#use-and-not","title":"Use and not &amp;","text":""},{"location":"Python/1.13_For_Loops.html","title":"For Loops","text":""},{"location":"Python/1.13_For_Loops.html#python-for-loop","title":"Python for loop","text":"<p>Here us the syntax</p> for item in sequence: <p></p> <p>Break, continue, and pass are control flow statements in Python that are used to change the behavior of loops (and in the case of pass, to do nothing). </p> <p>Here's a short explanation of each item:</p>"},{"location":"Python/1.13_For_Loops.html#break","title":"Break","text":"<ul> <li><code>break</code>: Terminates the loop completely and transfers control to the first statement after the loop.   <pre><code>for number in range(10):\n    if number == 5:\n        break\n    print(number)\n</code></pre>   This loop will print numbers 0 to 4 and then stop.</li> </ul>"},{"location":"Python/1.13_For_Loops.html#continue","title":"Continue","text":"<ul> <li><code>continue</code>: Skips the rest of the code inside the current loop iteration and moves to the next iteration of the loop.   <pre><code>for number in range(10):\n    if number % 2 == 0:\n        continue\n    print(number)\n</code></pre>   This loop will print all odd numbers between 0 and 9.</li> </ul>"},{"location":"Python/1.13_For_Loops.html#pass","title":"Pass","text":"<ul> <li><code>pass</code>: Does nothing and is used as a placeholder in loops, function definitions, or conditionals where syntactically some code is required but no action is needed.   <pre><code>for number in range(10):\n    if number &lt; 5:\n        pass  # Placeholder for future code\n    else:\n        print(number)\n</code></pre>   This loop will print numbers 5 to 9, doing nothing for numbers less than 5.</li> </ul>"},{"location":"Python/1.13_For_Loops.html#else","title":"Else","text":"<ul> <li><code>else</code> (used with loops): Executes when the loop completes normally (i.e., not terminated by a <code>break</code> statement).   <pre><code>for number in range(5):\n    print(number)\nelse:\n    print(\"Loop completed without break\")\n</code></pre>   This will print numbers 0 to 4 and then \"Loop completed without break\".</li> </ul>"},{"location":"Python/1.13_For_Loops.html#continue-vs-break-vs-pass","title":"Continue vs Break vs Pass","text":"<ul> <li><code>continue</code>: Skips to the next iteration of the loop.</li> <li><code>break</code>: Exits the loop immediately.</li> <li><code>pass</code>: Does nothing, acts as a placeholder.</li> </ul>"},{"location":"Python/1.13_For_Loops.html#for-loops-examples","title":"For loops Examples","text":"Code Examples Code Examples <pre>\n# Print \"Access Denied\" 5 times\nfor _ in range(5):\n    print(\"Access Denied\")\n                </pre> <pre>\n# Using list comprehension for conditional operations\nnumbers = [1, 2, 3, 4, 5, 6]\neven_numbers = [num for num in numbers if num % 2 == 0]\nprint(even_numbers)  # Output: [2, 4, 6]\n                </pre> <pre>\n# Using enumerate to get index and value\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfor index, fruit in enumerate(fruits):\n    print(f\"{index}: {fruit}\")\n                </pre> <pre>\n# Using zip to iterate over two lists\nnames = [\"Alice\", \"Bob\", \"Charlie\"]\nscores = [85, 90, 95]\nfor name, score in zip(names, scores):\n    print(f\"{name} scored {score}\")\n                </pre> <pre>\n# Using a dictionary in a for loop\nuser_info = {\"name\": \"Alice\", \"age\": 25, \"city\": \"New York\"}\nfor key, value in user_info.items():\n    print(f\"{key}: {value}\")\n                </pre> <pre>\n# Nested loops to print a multiplication table\nfor i in range(1, 6):\n    for j in range(1, 6):\n        print(f\"{i} x {j} = {i * j}\")\n    print()\n                </pre> <pre>\n# Using the range function with a step\nfor i in range(0, 20, 5):\n    print(i)\n                </pre> <pre>\n# Using a set in a for loop\nunique_numbers = {1, 2, 3, 4, 5}\nfor num in unique_numbers:\n    print(num)\n                </pre> <pre>\n# Using a for loop with else\nnumbers = [1, 2, 3, 4, 5]\nfor number in numbers:\n    if number == 3:\n        print(\"Found 3!\")\n        break\nelse:\n    print(\"3 not found\")\n                </pre> <pre>\n# Using a generator expression in a for loop\nsquares = (x * x for x in range(10))\nfor square in squares:\n    print(square)\n                </pre>"},{"location":"Python/1.14_enumerate.html","title":"Enumerate","text":""},{"location":"Python/1.14_enumerate.html#python-enumerate-function","title":"Python enumerate() function","text":"<p>Enumerate is a buil-in funciton in Python. Using this you can loop over a list (or any iterable) and at the same time you will have a counter telling you which round you are in.</p> <p>So you will get outputs like 0a, 1b, 2c..</p> <p>So, using enumerate, you get an index(trip counter) and item(item) in the list.</p> for index, item in enumerate(sequence):"},{"location":"Python/1.15_range_function.html","title":"Range","text":""},{"location":"Python/1.15_range_function.html#python-range-function","title":"Python range() function","text":"<p>The <code>range()</code> function gives you a bunch of numbers. An ordered bunch of numbers. Simple series like <code>1, 2, 3, 4</code>.</p> <pre><code># A typical use of range. This gives you 0, 1, 2\nrange(3)\n</code></pre> <p>Now, the output type of the <code>range()</code> function is a <code>range</code> object. Don't get confused, the output of range() is always a bunch of numbers. </p> <p>The <code>range()</code> function and <code>for</code> loops are like best buddies in Python. One creates a sequence of numbers, and the other loops through it.</p>"},{"location":"Python/1.15_range_function.html#range-syntax","title":"range() syntax","text":""},{"location":"Python/1.15_range_function.html#range-examples","title":"range() examples","text":"print(*(i for i in range(3))) 0 1 2 range(3) -&gt; 0, 1, 2 range(10)[-1] 9 print(*(fruits[i] for i in range(len(fruits)))) apple banana cherry date * unpacks the items print(*(i for i in range(10, 0, -2))) 10 8 6 4 2 Backwards using minus number list(             range(             5))          [0,1,2,3,4] print(*(fruits[i] for i in range(len(fruits))))"},{"location":"Python/1.16_built_in_functions.html","title":"Built-in","text":""},{"location":"Python/1.16_built_in_functions.html#os-library-in-python","title":"<code>os</code> Library in Python","text":"<p>Note: Import <code>os</code> at the beginning of your script: <code>import os</code></p> Function Example Get Current Working Directory <code>cwd = os.getcwd()</code><code>print(f\"Current Working Directory: {cwd}\")</code> Change Directory <code>os.chdir(\"/path/to/directory\")</code><code>print(f\"Changed Directory: {os.getcwd()}\")</code> List Directory Contents <code>contents = os.listdir(\".\")</code><code>print(f\"Directory Contents: {contents}\")</code> Make New Directory <code>os.makedirs(\"new_dir/sub_dir\", exist_ok=True)</code><code>print(\"Directories created\")</code> Remove Directory <code>os.rmdir(\"new_dir/sub_dir\")</code><code>print(\"Sub-directory removed\")</code> Remove File <code>os.remove(\"example.txt\")</code><code>print(\"File removed\")</code> Rename File or Directory <code>os.rename(\"old_name.txt\", \"new_name.txt\")</code><code>print(\"File renamed\")</code> Get File Size <code>size = os.path.getsize(\"example.txt\")</code><code>print(f\"File size: {size} bytes\")</code> Check if Path Exists <code>exists = os.path.exists(\"example.txt\")</code><code>print(f\"Path exists: {exists}\")</code> Check if Path is File <code>is_file = os.path.isfile(\"example.txt\")</code><code>print(f\"Is a file: {is_file}\")</code> Check if Path is Directory <code>is_dir = os.path.isdir(\"example\")</code><code>print(f\"Is a directory: {is_dir}\")</code> Join Paths <code>path = os.path.join(\"folder\", \"subfolder\", \"file.txt\")</code><code>print(f\"Joined path: {path}\")</code> Get Absolute Path <code>absolute_path = os.path.abspath(\"example.txt\")</code><code>print(f\"Absolute path: {absolute_path}\")</code> Get Directory Name <code>dir_name = os.path.dirname(\"/path/to/example.txt\")</code><code>print(f\"Directory name: {dir_name}\")</code> Get Base Name <code>base_name = os.path.basename(\"/path/to/example.txt\")</code><code>print(f\"Base name: {base_name}\")</code> Split Path <code>head, tail = os.path.split(\"/path/to/example.txt\")</code><code>print(f\"Head: {head}, Tail: {tail}\")</code> Split File Extension <code>root, ext = os.path.splitext(\"example.txt\")</code><code>print(f\"Root: {root}, Extension: {ext}\")</code> Execute a Shell Command <code>os.system(\"echo Hello World\")</code> Get Environment Variables <code>path = os.getenv(\"PATH\")</code><code>print(f\"PATH: {path}\")</code> Set Environment Variables <code>os.environ[\"MY_VAR\"] = \"my_value\"</code><code>print(f\"MY_VAR: {os.getenv('MY_VAR')}\")</code> Walk Through Directory Tree <code>for dirpath, dirnames, filenames in os.walk(\".\"):</code><code>print(f\"Found directory: {dirpath}\")</code><code>for file_name in filenames:</code> <code>print(f\"\\t{file_name}\")</code> File Permissions <code>os.chmod(\"example.txt\", 0o644)</code><code>print(\"Permissions changed\")</code> File Ownership <code>os.chown(\"example.txt\", uid, gid)</code><code>print(\"Ownership changed\")</code> Create a Symbolic Link <code>os.symlink(\"source.txt\", \"link.txt\")</code><code>print(\"Symbolic link created\")</code> Create a Hard Link <code>os.link(\"source.txt\", \"hard_link.txt\")</code><code>print(\"Hard link created\")</code> Get File Statistics <code>stats = os.stat(\"example.txt\")</code><code>print(f\"File statistics: {stats}\")</code>"},{"location":"Python/1.16_built_in_functions.html#ospath-in-python","title":"<code>os.path</code> in Python","text":"<p><code>os.path</code> is a part of Python's standard library that helps you work with file paths easily. <code>os.path</code> functions work on all operating systems like Windows, macOS, and Linux. </p> <p>You don\u2019t have to worry about different path styles on different systems.</p> <p>Here is an example which will include all main functianlities of os.path.</p> <pre><code>import os\n\n# Example file path\nexample_path = \"example/directory/file.txt\"\n\n# Get the directory name\ndirectory = os.path.dirname(example_path)\nprint(f\"Directory name: {directory}\")  # Output: example/directory\n\n# Get the base name\nfile_name = os.path.basename(example_path)\nprint(f\"File name: {file_name}\")  # Output: file.txt\n\n# Join paths\njoined_path = os.path.join(\"example\", \"directory\", \"newfile.txt\")\nprint(f\"Joined path: {joined_path}\")  # Output: example/directory/newfile.txt\n\n# Check if path exists\npath_exists = os.path.exists(example_path)\nprint(f\"Path exists: {path_exists}\")\n\n# Check if it is a file\nis_file = os.path.isfile(example_path)\nprint(f\"Is a file: {is_file}\")\n\n# Check if it is a directory\nis_directory = os.path.isdir(directory)\nprint(f\"Is a directory: {is_directory}\")\n\n# Get the absolute path\nabsolute_path = os.path.abspath(example_path)\nprint(f\"Absolute path: {absolute_path}\")\n\n# Split the path into directory and file name\nsplit_path = os.path.split(example_path)\nprint(f\"Split path: {split_path}\")  # Output: ('example/directory', 'file.txt')\n\n# Get the file extension\nfile_extension = os.path.splitext(example_path)[1]\nprint(f\"File extension: {file_extension}\")  # Output: .txt\n\n# Example relative path\nrelative_path = os.path.relpath(example_path, start=\"example\")\nprint(f\"Relative path: {relative_path}\")  # Output: directory/file.txt\n\n# Check the size of the file (only if it exists)\nif os.path.exists(example_path):\n    file_size = os.path.getsize(example_path)\n    print(f\"File size: {file_size} bytes\")\n\n# Check the last modification time (only if it exists)\nif os.path.exists(example_path):\n    modification_time = os.path.getmtime(example_path)\n    print(f\"Last modification time: {modification_time}\")\n\n# Demonstrate creating a new directory\nnew_directory_path = \"example/new_directory\"\nif not os.path.exists(new_directory_path):\n    os.makedirs(new_directory_path)\n    print(f\"Created new directory: {new_directory_path}\")\nelse:\n    print(f\"Directory already exists: {new_directory_path}\")\n</code></pre> <p>This program shows the following <code>os.path</code> functions: - <code>os.path.dirname()</code>: Get the directory name from a path. - <code>os.path.basename()</code>: Get the file name from a path. - <code>os.path.join()</code>: Join multiple path components. - <code>os.path.exists()</code>: Check if a path exists. - <code>os.path.isfile()</code>: Check if a path is a file. - <code>os.path.isdir()</code>: Check if a path is a directory. - <code>os.path.abspath()</code>: Get the absolute path. - <code>os.path.split()</code>: Split a path into directory and file name. - <code>os.path.splitext()</code>: Get the file extension. - <code>os.path.relpath()</code>: Get the relative path. - <code>os.path.getsize()</code>: Get the size of a file. - <code>os.path.getmtime()</code>: Get the last modification time of a file. - <code>os.makedirs()</code>: Create a new directory.</p> <p>Here is an example showing how <code>os.path</code> can be used so that the code handles both Windows and MAC folder systems without problems:</p> <p><pre><code>import os\n\n# Example file path components\nfolder = \"example\"\nsubfolder = \"directory\"\nfilename = \"file.txt\"\n\n# Join the components into a full path\nfull_path = os.path.join(folder, subfolder, filename)\nprint(f\"Full path: {full_path}\")\n\n# Check if the path exists\npath_exists = os.path.exists(full_path)\nprint(f\"Path exists: {path_exists}\")\n\n# Get the absolute path\nabsolute_path = os.path.abspath(full_path)\nprint(f\"Absolute path: {absolute_path}\")\n\n# Example output for different operating systems\nif os.name == 'nt':  # Windows\n    print(f\"Windows style path: {full_path}\")\nelse:  # Mac/Linux\n    print(f\"Unix style path: {full_path}\")\n</code></pre> By using <code>os.path</code>, you can make sure the code works in both windows and Linux/Mac and handles (backslash <code>\\</code> for Windows and forward slash <code>/</code> for Mac/Linux).</p>"},{"location":"Python/1.16_built_in_functions.html#python-built-in-functions","title":"Python built-in functions","text":"<p>These are built-in functions that are available to Python interpreter always. You don't have to pip install anything for this.</p> <p>I compiled the list from the following link</p> Function Description &amp; Syntax Usage Example <code>abs(x)</code>: Returns the absolute value of a number. <code>abs(-5)</code> returns <code>5</code> <code>aiter(iterable)</code>: Returns an asynchronous iterator for an asynchronous iterable. <code>async for item in aiter(async_iterable): pass</code> <code>all(iterable)</code>: Returns True if all elements of an iterable are true. <code>all([True, True, False])</code> returns <code>False</code> <code>anext(async_iterator)</code>: Retrieves the next item from an asynchronous iterator. <code>await anext(async_iterator)</code> <code>any(iterable)</code>: Returns True if any element of an iterable is true. <code>any([False, True, False])</code> returns <code>True</code> <code>ascii(object)</code>: Returns a string representation of an object with non-ASCII characters escaped. <code>ascii('\u00fc')</code> returns <code>'\\\\xfc'</code> <code>bin(x)</code>: Converts an integer to a binary string. <code>bin(10)</code> returns <code>'0b1010'</code> <code>bool(x)</code>: Converts a value to a boolean, returning either True or False. <code>bool(1)</code> returns <code>True</code> <code>breakpoint()</code>: Drops you into the debugger at the call site. <code>breakpoint()</code> <code>bytearray([source[, encoding[, errors]]])</code>: Returns a new array of bytes. <code>bytearray(b'hello')</code> returns <code>bytearray(b'hello')</code> <code>bytes([source[, encoding[, errors]]])</code>: Returns a new bytes object. <code>bytes('hello', 'utf-8')</code> returns <code>b'hello'</code> <code>callable(object)</code>: Returns True if the object appears callable. <code>callable(len)</code> returns <code>True</code> <code>chr(i)</code>: Returns the character that represents the specified unicode. <code>chr(97)</code> returns <code>'a'</code> <code>classmethod(function)</code>: Converts a method into a class method. <code>class C: @classmethod def f(cls): pass</code> <code>compile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1)</code>: Compiles source into a code object that can be executed by exec() or eval(). <code>compile('print(1)', '', 'exec')</code> <code>complex([real[, imag]])</code>: Creates a complex number. <code>complex(1, 2)</code> returns <code>(1+2j)</code> <code>delattr(object, name)</code>: Deletes the named attribute from an object. <code>delattr(obj, 'attr')</code> <code>dict(**kwargs)</code>: Creates a new dictionary. <code>dict(a=1, b=2)</code> returns <code>{'a': 1, 'b': 2}</code> <code>dir([object])</code>: Returns a list of the attributes and methods of any object. <code>dir()</code> <code>divmod(a, b)</code>: Returns a tuple containing the quotient and remainder when dividing two numbers. <code>divmod(10, 3)</code> returns <code>(3, 1)</code> <code>enumerate(iterable, start=0)</code>: Returns an enumerate object. <code>enumerate(['a', 'b'], 1)</code> returns <code>[(1, 'a'), (2, 'b')]</code> <code>eval(expression, globals=None, locals=None)</code>: Evaluates the specified expression. <code>eval('1 + 2')</code> returns <code>3</code> <code>exec(object[, globals[, locals]])</code>: Executes the specified code. <code>exec('print(\"Hello, World!\")')</code> <code>filter(function, iterable)</code>: Constructs an iterator from elements of iterable for which function returns true. <code>filter(lambda x: x &gt; 0, [1, -2, 3, 0])</code> <code>float([x])</code>: Converts a number or string to a floating point number. <code>float('3.14')</code> returns <code>3.14</code> <code>format(value[, format_spec])</code>: Formats a value using a format specification. <code>format(255, '02x')</code> returns <code>'ff'</code> <code>frozenset([iterable])</code>: Returns a new frozenset object, optionally with elements taken from iterable. <code>frozenset([1, 2, 3])</code> returns <code>frozenset({1, 2, 3})</code> <code>getattr(object, name[, default])</code>: Returns the value of the named attribute of an object. <code>getattr(obj, 'attr', None)</code> <code>globals()</code>: Returns a dictionary representing the current global symbol table. <code>globals()</code> <code>hasattr(object, name)</code>: Returns True if the object has the named attribute. <code>hasattr(obj, 'attr')</code> <code>hash(object)</code>: Returns the hash value of the object. <code>hash('hello')</code> <code>help([object])</code>: Invokes the built-in help system. <code>help(print)</code> <code>hex(x)</code>: Converts an integer to a hexadecimal string. <code>hex(255)</code> returns <code>'0xff'</code> <code>id(object)</code>: Returns the identity of an object. <code>id(3)</code> <code>input([prompt])</code>: Reads a line from input. <code>input('Enter your name: ')</code> <code>int([x[, base]])</code>: Converts a number or string to an integer. <code>int('10')</code> returns <code>10</code> <code>isinstance(object, classinfo)</code>: Returns True if the object is an instance of the class or of a subclass thereof. <code>isinstance(3, int)</code> <code>issubclass(class, classinfo)</code>: Returns True if the class is a subclass of classinfo. <code>issubclass(bool, int)</code> <code>iter(object[, sentinel])</code>: Returns an iterator object. <code>iter([1, 2, 3])</code> <code>len(s)</code>: Returns the length of an object. <code>len('hello')</code> returns <code>5</code> <code>list([iterable])</code>: Creates a new list. <code>list('hello')</code> returns <code>['h', 'e', 'l', 'l', 'o']</code> <code>locals()</code>: Updates and returns a dictionary representing the current local symbol table. <code>locals()</code> <code>map(function, iterable, ...)</code>: Applies function to every item of iterable and returns an iterator. <code>map(str.upper, ['a', 'b', 'c'])</code> <code>max(iterable, *[, key, default])</code>: Returns the largest item in an iterable or the largest of two or more arguments. <code>max([1, 2, 3])</code> <code>memoryview(obj)</code>: Returns a memory view object. <code>memoryview(b'abc')</code> <code>min(iterable, *[, key, default])</code>: Returns the smallest item in an iterable or the smallest of two or more arguments. <code>min([1, 2, 3])</code> <code>next(iterator[, default])</code>: Retrieves the next item from the iterator. <code>next(iter([1, 2, 3]))</code> <code>object()</code>: Returns a new featureless object. <code>object()</code> <code>oct(x)</code>: Converts an integer to an octal string. <code>oct(8)</code> returns <code>'0o10'</code> <code>open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)</code>: Opens a file and returns a corresponding file object. <code>open('file.txt', 'r')</code> <code>ord(c)</code>: Returns the Unicode code point for a single character. <code>ord('a')</code> returns <code>97</code> <code>pow(x, y[, z])</code>: Returns x to the power of y; if z is present, returns x to the power of y, modulo z. <code>pow(2, 3)</code> returns <code>8</code> <code>print(*objects, sep=' ', end='\\n', file=sys.stdout, flush=False)</code>: Prints the objects to the text stream file, separated by sep and followed by end. <code>print('hello')</code> <code>property(fget=None, fset=None, fdel=None, doc=None)</code>: Returns a property attribute. <code>class C: @property def x(self): return self._x</code> <code>range(stop)</code>: Returns an immutable sequence of numbers from 0 to stop <code>repr(object)</code>: Returns a string containing a printable representation of an object. <code>reversed(seq)</code>: Returns a reversed iterator. <code>list(reversed([1, 2, 3]))</code> returns <code>[3, 2, 1]</code> <code>round(number[, ndigits])</code>: Rounds a number to a given precision in decimal digits. <code>round(3.14159, 2)</code> returns <code>3.14</code> <code>set([iterable])</code>: Returns a new set object, optionally with elements taken from iterable. <code>set([1, 2, 2, 3])</code> returns <code>{1, 2, 3}</code> <code>setattr(object, name, value)</code>: Sets the value of the named attribute of an object. <code>setattr(obj, 'attr', 10)</code> <code>slice(stop)</code>: Returns a slice object representing the set of indices specified by range(start, stop, step). <code>slice(1, 5, 2)</code> returns <code>slice(1, 5, 2)</code> <code>sorted(iterable, *, key=None, reverse=False)</code>: Returns a new sorted list from the items in iterable. <code>sorted([3, 1, 2])</code> returns <code>[1, 2, 3]</code> <code>staticmethod(function)</code>: Converts a method into a static method. <code>class C: @staticmethod def f(): pass</code> <code>str(object='')</code>: Returns a string version of object. <code>str(123)</code> returns <code>'123'</code> <code>sum(iterable, /, start=0)</code>: Sums the items of an iterable from left to right and returns the total. <code>sum([1, 2, 3])</code> returns <code>6</code> <code>super([type[, object-or-type]])</code>: Returns a proxy object that delegates method calls to a parent or sibling class of type. <code>super().method()</code> <code>tuple([iterable])</code>: Returns a tuple object, optionally with elements taken from iterable. <code>tuple([1, 2, 3])</code> returns <code>(1, 2, 3)</code> <code>type(object)</code>: Returns the type of an object. <code>type(123)</code> returns <code>&lt;class 'int'&gt;</code> <code>vars([object])</code>: Returns the <code>__dict__</code> attribute for a module, class, instance, or any other object with a <code>__dict__</code> attribute. <code>vars()</code> <code>zip(*iterables)</code>: Returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. <code>list(zip([1, 2, 3], ['a', 'b', 'c']))</code> returns <code>[(1, 'a'), (2, 'b'), (3, 'c')]</code> <code>__import__(name, globals=None, locals=None, fromlist=(), level=0)</code>: Invoked by the <code>import</code> statement. <code>__import__('math')</code>"},{"location":"Python/1.17_withStatement.html","title":"With Statement","text":""},{"location":"Python/1.17_withStatement.html#the-with-statement-in-python","title":"The <code>with</code> Statement in Python","text":"<p>In Python, the <code>with</code> statement is very useful when you need to clean up after the code finishes. Cleanup includes things like closing connections and freeing memory.</p> <p>It also makes the code very clean and readable.</p> <p>In C#, we have the <code>using</code> function, which is similar to <code>with</code>.</p> <p>Let's find out more details about the <code>with</code> function in this article.</p>"},{"location":"Python/1.17_withStatement.html#what-is-the-with-statement","title":"What is the <code>with</code> Statement?","text":"<p>The <code>with</code> statement ensures that resources are properly acquired and released, handling setup and cleanup tasks automatically. It guarantees that resources are released when the block of code is done, even if exceptions occur.</p>"},{"location":"Python/1.17_withStatement.html#basic-syntax","title":"Basic Syntax","text":"<p>Here's the basic syntax of the <code>with</code> statement:</p> <p></p>"},{"location":"Python/1.17_withStatement.html#why-use-with","title":"Why Use <code>with</code>?","text":"<ul> <li>Automatic Resource Management: The <code>with</code> statement ensures that resources are released as soon as the block of code is executed, even if errors occur.</li> <li>Cleaner Code: Reduces the need for explicit resource management (like closing files), leading to cleaner and more readable code.</li> </ul>"},{"location":"Python/1.17_withStatement.html#examples","title":"Examples","text":"<p>Let's look at some practical and funny examples to see how the <code>with</code> statement works.</p> <ol> <li>Working with Files</li> </ol> <p>Opening and reading a file:</p> <pre><code>with open('lovers.txt', 'r') as file:\n    content = file.read()\n    print(\"Reading love notes...\")\n    print(content)\n</code></pre> <p>Writing to a file:</p> <pre><code>new_lovers = [\"Romeo\", \"Juliet\", \"Casablanca\"]\n\nwith open('lovers.txt', 'a') as file:\n    for lover in new_lovers:\n        file.write(lover + \" \u2764\ufe0f\\n\")\n        print(f\"{lover} added to the list of lovers!\")\n</code></pre> <ol> <li>Using Locks for Thread Safety</li> </ol> <p>When working with threads, you might need to use locks to prevent race conditions. The <code>with</code> statement makes this simple:</p> <pre><code>import threading\n\nlock = threading.Lock()\n\nwith lock:\n    print(\"Securing the love triangle...\")\n    # Critical section of code\n    print(\"Thread-safe love triangle management\")\n</code></pre> <ol> <li>Handling Database Connections</li> </ol> <p>Managing database connections can be complex, but the <code>with</code> statement can simplify it:</p> <p></p> <ol> <li>Custom Context Managers</li> </ol> <p>You can create your own context managers using classes and the <code>__enter__</code> and <code>__exit__</code> methods:</p> <pre><code>class LoveContext:\n    def __enter__(self):\n        print(\"Entering the love zone \u2764\ufe0f\")\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        print(\"Exiting the love zone \ud83d\udc94\")\n\nwith LoveContext():\n    print(\"Inside the love zone\")\n</code></pre> <p>This will output:</p> <pre><code>Entering the love zone \u2764\ufe0f\nInside the love zone\nExiting the love zone \ud83d\udc94\n</code></pre>"},{"location":"Python/1.17_withStatement.html#conclusion","title":"Conclusion","text":"<p>The usinng function in C# is same as the with function in Python. This is an example of using using function</p> <pre><code>using (StreamReader reader = new StreamReader(\"example.txt\"))\n{\n    string content = reader.ReadToEnd();\n    Console.WriteLine(content);\n}\n</code></pre>"},{"location":"Python/1.18_unittest_pytest.html","title":"Testing","text":""},{"location":"Python/1.18_unittest_pytest.html#introduction-to-pytest","title":"Introduction to <code>pytest</code>","text":"<p><code>pytest</code> is a popular tool for testing Python code. It's known for being easy to use and having many useful features.</p>"},{"location":"Python/1.18_unittest_pytest.html#key-features-of-pytest","title":"Key Features of <code>pytest</code>","text":"<ol> <li> <p>Simple Syntax: You write tests with plain Python functions starting with <code>test_</code>. Use standard <code>assert</code> statements for checks. This makes it easy to start testing.</p> </li> <li> <p>Rich Assertions: <code>pytest</code> provides clear failure reports. If a test fails, it shows what went wrong with details on variables and expressions.</p> </li> <li> <p>Fixtures: Fixtures help set up and clean up before and after tests. They let you reuse code for preparing test data or resources.</p> </li> <li> <p>Parametrization: You can run the same test with different inputs using <code>pytest</code>. This saves you from writing multiple similar tests.</p> </li> <li> <p>Plugins and Extensions: There are many plugins to add extra features like running tests in parallel, checking code coverage, and more.</p> </li> <li> <p>Test Discovery: <code>pytest</code> finds and runs tests automatically by looking for files and functions that follow certain naming rules.</p> </li> </ol> <p>These features make <code>pytest</code> a powerful and flexible tool for testing Python code.</p>"},{"location":"Python/1.18_unittest_pytest.html#writing-tests-with-pytest","title":"Writing Tests with <code>pytest</code>","text":"<p>To get started with <code>pytest</code>, install it using pip:</p> <pre><code>pip install pytest\n</code></pre> <p>Create a test file (e.g., <code>test_sample.py</code>) and write a simple test function:</p> <pre><code>def test_addition():\n    assert 1 + 1 == 2\n</code></pre> <p>Run the test using the <code>pytest</code> command:</p> <pre><code>pytest\n</code></pre> <p><code>pytest</code> will automatically find and run the test, providing a summary of the results.</p>"},{"location":"Python/1.18_unittest_pytest.html#advanced-features","title":"Advanced Features","text":"<ul> <li> <p>Fixtures: Define a fixture using the <code>@pytest.fixture</code> decorator to provide setup code for your tests:</p> <pre><code>import pytest\n\n@pytest.fixture\ndef sample_data():\n    return [1, 2, 3]\n\ndef test_sum(sample_data):\n    assert sum(sample_data) == 6\n</code></pre> </li> <li> <p>Parametrization: Use the <code>@pytest.mark.parametrize</code> decorator to run tests with multiple sets of parameters:</p> <pre><code>import pytest\n\n@pytest.mark.parametrize(\"a, b, expected\", [\n    (1, 1, 2),\n    (2, 3, 5),\n    (5, 5, 10)\n])\ndef test_add(a, b, expected):\n    assert a + b == expected\n</code></pre> </li> </ul>"},{"location":"Python/1.18_unittest_pytest.html#unit-test-example","title":"Unit test example","text":""},{"location":"Python/1.18_unittest_pytest.html#rearrangepy","title":"<code>rearrange.py</code>","text":"<pre><code>#!/usr/bin/env python3\n\nimport re\n\ndef rearrange_name(full_name):\n    match = re.search(r\"^([\\w .]*), ([\\w .]*)$\", full_name)\n    return \"{} {}\".format(match[2], match[1])\n</code></pre>"},{"location":"Python/1.18_unittest_pytest.html#rearrange_testpy","title":"<code>rearrange_test.py</code>","text":"<pre><code>#!/usr/bin/env python3\n\nimport unittest\nfrom rearrange import rearrange_name\n\nclass TestRearrange(unittest.TestCase):\n\n    def test_rearrange(self):\n        input_name = \"Lovelace, Ada\"\n        expected_output = \"Ada Lovelace\"\n        self.assertEqual(rearrange_name(input_name), expected_output)\n\n# Run the tests\nunittest.main()\n</code></pre>"},{"location":"Python/1.18_unittest_pytest.html#running-the-test","title":"Running the Test","text":"<pre><code>chmod +x rearrange_test.py\n./rearrange_test.py\n</code></pre>"},{"location":"Python/1.18_unittest_pytest.html#unittest-vs-pytest","title":"Unittest vs. Pytest","text":"<ul> <li>Unittest is built into Python and uses an object-oriented approach with special assert methods.</li> <li>Pytest needs to be imported and uses a functional approach with simple assert statements.</li> <li>Unittest automatically detects tests but needs to be run from the command line. Pytest runs tests automatically with the <code>test_</code> prefix.</li> <li>Both can work together; you can use pytest with unittest code.</li> </ul> <p>Choose based on your preference: unittest for built-in simplicity or pytest for easier, more readable tests.</p>"},{"location":"Python/1.18_unittest_pytest.html#conclusion","title":"Conclusion","text":"<p><code>pytest</code> is a handy tool for testing Python code. It\u2019s easy to use, has many features, and is well-supported by the community. Ideal for both small and big projects, it helps you write reliable tests quickly.</p>"},{"location":"Python/1.18_unittest_pytest.html#some-important-concepts-regarding-testing","title":"Some important concepts regarding testing","text":"<ol> <li>Black-box test:</li> <li> <p>This is a type of testing where the tester doesn\u2019t look at the internal code of the software. They just check if the software works as expected based on inputs and outputs.</p> </li> <li> <p>Integration test:</p> </li> <li> <p>This test checks if different parts of the software work well together. It ensures that the combined parts function correctly with each other and with external systems.</p> </li> <li> <p>Isolation:</p> </li> <li> <p>In testing, isolation means ensuring that a test only checks one specific part of the software. Any success or failure should be due to the part being tested, not because of something else.</p> </li> <li> <p>Regression test:</p> </li> <li> <p>This is a test done after fixing a bug to make sure that the bug doesn\u2019t come back. It checks if the new changes have affected the existing functionality.</p> </li> <li> <p>Load test:</p> </li> <li> <p>This type of testing checks how well the software performs under heavy use. It ensures that the software can handle a large number of users or high data loads without crashing.</p> </li> <li> <p>Smoke test:</p> </li> <li> <p>This is a basic test to check if the main functions of the software are working. It\u2019s like a quick check to see if the software is stable enough for more detailed testing.</p> </li> <li> <p>White-box test:</p> </li> <li> <p>This is a type of testing where the tester looks at the internal code and structure of the software. They check if the internal workings are correct and efficient.</p> </li> <li> <p>Test-driven development:</p> </li> <li>This is a software development process where tests are written before the actual code. Developers first write a test for a small part of the functionality, then write the code to pass that test, and finally, improve the code while keeping the test passing.</li> </ol>"},{"location":"Python/1.19_if_name_main.md.html","title":"1.19 if name main.md","text":""},{"location":"Python/1.19_if_name_main.md.html#understanding-if-__name__-__main__","title":"Understanding <code>if __name__ == '__main__':</code>","text":"<p>In Python, <code>if __name__ == '__main__':</code> is used to determine whether a script is run directly or imported as a module. This helps manage code execution depending on the context.</p>"},{"location":"Python/1.19_if_name_main.md.html#how-it-works","title":"How It Works","text":"<ol> <li> <p>Direct Execution: When you run a script directly (e.g., <code>python my_script.py</code>), <code>__name__</code> is set to <code>'__main__'</code>. The code inside the <code>if __name__ == '__main__':</code> block will run.</p> </li> <li> <p>Imported as a Module: When you import the script into another script (e.g., <code>import my_script</code>), <code>__name__</code> is set to the module's name (e.g., <code>'my_script'</code>). The code inside the <code>if __name__ == '__main__':</code> block will not execute.</p> </li> </ol>"},{"location":"Python/1.19_if_name_main.md.html#examples","title":"Examples","text":"<ol> <li>Basic Example</li> </ol> <pre><code># script1.py\ndef say_hello():\n    print(\"Hello!\")\n\nif __name__ == '__main__':\n    say_hello()\n</code></pre> <p>Running <code>python script1.py</code> will output \"Hello!\" because <code>say_hello()</code> is called directly.</p> <ol> <li>Testing Functions</li> </ol> <pre><code># script2.py\ndef add(a, b):\n    return a + b\n\nif __name__ == '__main__':\n    result = add(2, 3)\n    print(f\"Result: {result}\")\n</code></pre> <p>Running <code>python script2.py</code> will print \"Result: 5\". When imported, <code>add()</code> can be used without printing the result.</p> <ol> <li>Module Import Example</li> </ol> <pre><code># script3.py\ndef greet(name):\n    print(f\"Hi, {name}!\")\n\nif __name__ == '__main__':\n    greet(\"Alice\")\n</code></pre> <pre><code># script4.py\nimport script3\n\nscript3.greet(\"Bob\")\n</code></pre> <p>Running <code>python script4.py</code> will print \"Hi, Bob!\" but not \"Hi, Alice!\" since <code>greet(\"Alice\")</code> in <code>script3</code> only runs when <code>script3.py</code> is executed directly.</p> <ol> <li>Using Command-Line Arguments</li> </ol> <pre><code># script5.py\nimport sys\n\ndef main():\n    if len(sys.argv) &gt; 1:\n        print(f\"Arguments: {sys.argv[1:]}\")\n    else:\n        print(\"No arguments provided\")\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Running <code>python script5.py arg1 arg2</code> will print \"Arguments: ['arg1', 'arg2']\". When imported, <code>main()</code> won't run automatically.</p>"},{"location":"Python/1.19_if_name_main.md.html#takeaways","title":"Takeaways","text":"<ul> <li>Direct Execution: Code in <code>if __name__ == '__main__':</code> runs only when the script is executed directly.</li> <li>Module Import: Code in this block does not run when the script is imported as a module.</li> <li>Use Cases: Ideal for testing code, running scripts, or executing code that should not run on import.</li> </ul> <p>Using <code>if __name__ == '__main__':</code> helps keep code modular and avoids unintended executions when scripts are imported as modules.</p>"},{"location":"Python/1.1_Tuples.html","title":"Basic","text":"Table of contents      {: .text-delta } 1. TOC {:toc}"},{"location":"Python/1.1_Tuples.html#python-tuples","title":"Python (Tuples)","text":"<p> Tuples are collections of items where items have serial numbers, but they can't be changed - No add/remove/reorder. They can have duplicates. Tuples are like a necklace. Once you create them you can't change them. You can't add/remove items. You can't reorder items.</p> <p>Technically they are ordered collections of immutable objects. Phew!</p> <p> Python has 4 built-in data types - LSTD - List, Set, Tuple, Dictionary. </p> <p> Tuples and lists both store items, but there are key differences. **Tuples** can't be changed once created; you can't add, remove, or modify items. **Lists** can be changed; you can add, remove, or modify items.  Both can have duplicate items. </p>"},{"location":"Python/1.1_Tuples.html#tuples-vs-lists","title":"Tuples vs. Lists","text":"<p> Tuples and lists both store items, but there are key differences. **Tuples** can't be changed once created; you can't add, remove, or modify items. **Lists** can be changed; you can add, remove, or modify items.  Both can have duplicate items. </p> <pre><code># Tuples can have duplicates\nmy_tuple = (1, 2, 2, 3, 4, 4)\nprint(my_tuple)  # Output: (1, 2, 2, 3, 4, 4)\n\n# Lists can also have duplicates\nmy_list = [1, 2, 2, 3, 4, 4]\nprint(my_list)  # Output: [1, 2, 2, 3, 4, 4]\n</code></pre> <p> Tuples use `()`, and lists use `[]`. Use tuples for fixed data and lists for data that changes. </p>"},{"location":"Python/1.1_Tuples.html#features-of-tuples","title":"Features of Tuples","text":""},{"location":"Python/1.1_Tuples.html#ordered-and-indexed","title":"Ordered and Indexed","text":"<p>Tuple items have serial numbers(Index). And they stay the same. The first item stays first, the second stays second, and so on.</p> <p>I.e., Tuples are 'ordered'</p> <pre><code>my_tuple = (1, 2, 3)\nprint(my_tuple[0])  # Output: 1\nprint(my_tuple[1])  # Output: 2\n</code></pre>"},{"location":"Python/1.1_Tuples.html#immutablerigid","title":"Immutable(Rigid)","text":"<p>Once created, you can't change the items in a tuple. This means you can't add, remove, or replace items in a tuple. This makes tuples very reliable for storing data that shouldn't change.</p> <pre><code>my_tuple = (1, 2, 3)\n# Trying to change a value will cause an error\nmy_tuple[0] = 10  # Error: TypeError: 'tuple' object does not support item assignment\n\n# Trying to add an item will cause an error\nmy_tuple.append(4)  # Error: AttributeError: 'tuple' object has no attribute 'append'\n\n# Trying to remove an item will cause an error\nmy_tuple.remove(2)  # Error: AttributeError: 'tuple' object has no attribute 'remove'\n</code></pre>"},{"location":"Python/1.1_Tuples.html#hetrohetrogenxblabla-urrgh-all-datatypes-welcome","title":"Hetro..Hetrogenxblabla - urrgh! All datatypes welcome!","text":"<p>Tuples can hold items of different types, including other tuples.</p> <pre><code>mixed_tuple = (1, \"hello\", 3.14, (2, 4))\nprint(mixed_tuple)  # Output: (1, 'hello', 3.14, (2, 4))\n</code></pre> <p> Tuples take less memory than others </p>"},{"location":"Python/1.1_Tuples.html#creating-tuples","title":"Creating Tuples","text":"<p>There are a few ways to create tuples in Python:</p>"},{"location":"Python/1.1_Tuples.html#using-parentheses","title":"Using Parentheses <code>()</code>","text":"<pre><code># Creating an empty tuple\nempty_tuple = ()\n\n# Creating a tuple with items\nmy_tuple = (1, 2, 3)\n</code></pre>"},{"location":"Python/1.1_Tuples.html#without-parentheses","title":"Without Parentheses","text":"<p>You can also create a tuple by simply separating items with commas.</p> <pre><code>my_tuple = 1, 2, 3\n</code></pre>"},{"location":"Python/1.1_Tuples.html#using-the-tuple-function","title":"Using the <code>tuple()</code> Function","text":"<p>You can convert other data types (like lists) to tuples using the <code>tuple()</code> function.</p> <pre><code>my_list = [1, 2, 3]\nmy_tuple = tuple(my_list)\n</code></pre>"},{"location":"Python/1.1_Tuples.html#common-errors","title":"Common Errors","text":""},{"location":"Python/1.1_Tuples.html#trying-to-modify-a-tuple","title":"Trying to Modify a Tuple","text":"<p>Since tuples are immutable, trying to change an item will result in an error.</p> <pre><code>my_tuple = (1, 2, 3)\nmy_tuple[0] = 10  # Error: TypeError: 'tuple' object does not support item assignment\n</code></pre>"},{"location":"Python/1.1_Tuples.html#forgetting-comma-in-single-item-tuples","title":"Forgetting Comma in Single-Item Tuples","text":"<p>A tuple with one item needs a comma, otherwise, Python will not recognize it as a tuple.</p> <pre><code>single_item_tuple = (1,)\nprint(type(single_item_tuple))  # Output: &lt;class 'tuple'&gt;\n\nnot_a_tuple = (1)\nprint(type(not_a_tuple))  # Output: &lt;class 'int'&gt;\n</code></pre>"},{"location":"Python/1.1_Tuples.html#examples-of-using-tuples","title":"Examples of Using Tuples","text":""},{"location":"Python/1.1_Tuples.html#storing-coordinates","title":"Storing Coordinates","text":"<p>Tuples are great for storing fixed sets of data like coordinates.</p> <pre><code>coordinates = (10.0, 20.0)\nprint(coordinates)  # Output: (10.0, 20.0)\n</code></pre>"},{"location":"Python/1.1_Tuples.html#returning-multiple-values-from-functions","title":"Returning Multiple Values from Functions","text":"<p>Functions can return multiple values using tuples.</p> <pre><code>def get_name_and_age():\n    name = \"Alice\"\n    age = 30\n    return name, age\n\nname, age = get_name_and_age()\nprint(name)  # Output: Alice\nprint(age)  # Output: 30\n</code></pre>"},{"location":"Python/1.1_Tuples.html#using-tuples-as-dictionary-keys","title":"Using Tuples as Dictionary Keys","text":"<p>Because tuples are immutable, they can be used as keys in dictionaries.</p> <pre><code>location_data = {}\nlocation = (10.0, 20.0)\nlocation_data[location] = \"Location 1\"\nprint(location_data)  # Output: {(10.0, 20.0): 'Location 1'}\n</code></pre> <p>Did you know this? <pre><code>\"Hello, %s! You're %s years old.\" % (\"Kim John\", 150)\n</code></pre></p>"},{"location":"Python/1.1_Tuples.html#summary","title":"Summary","text":"<ul> <li>Tuples: Like lists but can't modify.</li> <li>Create: Using <code>()</code>, commas, or <code>tuple()</code>.</li> <li>Features: Ordered, immutable, and can hold different data types.</li> <li>Use Cases: Fixed collections, multiple return values, dictionary keys.</li> </ul> <p>Tuples are useful to put constant items in a group. They are easy to understand and make code easy to read.</p>"},{"location":"Python/1.1_Tuples.html#test-your-knowledge","title":"Test your knowledge","text":"<p>Highlight the answer section to reveal!</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-indexing","title":"Question - Tuple Indexing","text":"<p> What will be the output of the following statement?</p> <pre><code>my_tuple = (1, 2, 3, 4, 5)\nprint(my_tuple[-1])\n</code></pre> <p>Answer: 5</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-containing-a-list","title":"Question - Tuple Containing a List","text":"<p> What will happen if you try to include a list inside a tuple?</p> <pre><code>my_tuple = (1, [2, 3], 4)\nmy_tuple[1][0] = 5\nprint(my_tuple)\n</code></pre> <p>Answer: (1, [5, 3], 4)</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-concatenation","title":"Question - Tuple Concatenation","text":"<p> What will be the output of the following statement?</p> <pre><code>tuple1 = (1, 2)\ntuple2 = (3, 4)\nprint(tuple1 + tuple2)\n</code></pre> <p>Answer: (1, 2, 3, 4)</p>"},{"location":"Python/1.1_Tuples.html#question-immutable-tuples","title":"Question - Immutable Tuples","text":"<p> What will happen if you try to change an element of a tuple?</p> <pre><code>my_tuple = (1, 2, 3)\nmy_tuple[0] = 4\n</code></pre> <p>Answer: TypeError: 'tuple' object does not support item assignment</p>"},{"location":"Python/1.1_Tuples.html#question-single-item-tuple","title":"Question - Single-Item Tuple","text":"<p> How do you create a tuple with a single item?</p> <pre><code>single_item_tuple = (1)\nprint(type(single_item_tuple))\n</code></pre> <p>Answer: &lt;class 'int'&gt;</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-methods","title":"Question - Tuple Methods","text":"<p> Which method would you use to find the index of a value in a tuple?</p> <pre><code>my_tuple = (10, 20, 30, 40)\nprint(my_tuple.index(30))\n</code></pre> <p>Answer: 2</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-length","title":"Question - Tuple Length","text":"<p> What will be the output of the following statement?</p> <pre><code>my_tuple = (1, 2, 3, 4, 5)\nprint(len(my_tuple))\n</code></pre> <p>Answer: 5</p>"},{"location":"Python/1.1_Tuples.html#question-nested-tuples","title":"Question - Nested Tuples","text":"<p> What will be the output of the following statement?</p> <pre><code>nested_tuple = (1, (2, 3), 4)\nprint(nested_tuple[1])\n</code></pre> <p>Answer: (2, 3)</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-slicing","title":"Question - Tuple Slicing","text":"<p> What will be the output of the following statement?</p> <pre><code>my_tuple = (1, 2, 3, 4, 5)\nprint(my_tuple[1:3])\n</code></pre> <p>Answer: (2, 3)</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-containment","title":"Question - Tuple Containment","text":"<p> What will be the output of the following statement?</p> <pre><code>my_tuple = (1, 2, 3, 4, 5)\nprint(3 in my_tuple)\n</code></pre> <p>Answer: True</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-unpacking","title":"Question - Tuple Unpacking","text":"<p> What will be the output of the following statement?</p> <pre><code>my_tuple = (1, 2, 3)\na, b, c = my_tuple\nprint(a, b, c)\n</code></pre> <p>Answer: 1 2 3</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-multiplication","title":"Question - Tuple Multiplication","text":"<p> What will be the output of the following statement?</p> <pre><code>my_tuple = (1, 2, 3)\nprint(my_tuple * 2)\n</code></pre> <p>Answer: (1, 2, 3, 1, 2, 3)</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-conversion","title":"Question - Tuple Conversion","text":"<p> What will be the output of the following statement?</p> <pre><code>my_list = [1, 2, 3]\nmy_tuple = tuple(my_list)\nprint(my_tuple)\n</code></pre> <p>Answer: (1, 2, 3)</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-with-mixed-types","title":"Question - Tuple with Mixed Types","text":"<p> What will be the output of the following statement?</p> <pre><code>my_tuple = (1, \"hello\", 3.14)\nprint(my_tuple[1])\n</code></pre> <p>Answer: hello</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-indexing_1","title":"Question - Tuple Indexing","text":"<p> What will be the output of the following statement?</p> <pre><code>my_tuple = (1, 2, 3, 4, 5)\nprint(my_tuple[-1])\n</code></pre> <p>Answer: 5</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-containing-a-list_1","title":"Question - Tuple Containing a List","text":"<p> What will happen if you try to include a list inside a tuple?</p> <pre><code>my_tuple = (1, [2, 3], 4)\nmy_tuple[1][0] = 5\nprint(my_tuple)\n</code></pre> <p>Answer: (1, [5, 3], 4)</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-concatenation_1","title":"Question - Tuple Concatenation","text":"<p> What will be the output of the following statement?</p> <pre><code>tuple1 = (1, 2)\ntuple2 = (3, 4)\nprint(tuple1 + tuple2)\n</code></pre> <p>Answer: (1, 2, 3, 4)</p>"},{"location":"Python/1.1_Tuples.html#question-immutable-tuples_1","title":"Question - Immutable Tuples","text":"<p> What will happen if you try to change an element of a tuple?</p> <pre><code>my_tuple = (1, 2, 3)\nmy_tuple[0] = 4\n</code></pre> <p>Answer: TypeError: 'tuple' object does not support item assignment</p>"},{"location":"Python/1.1_Tuples.html#question-single-item-tuple_1","title":"Question - Single-Item Tuple","text":"<p> How do you create a tuple with a single item?</p> <pre><code>single_item_tuple = (1)\nprint(type(single_item_tuple))\n</code></pre> <p>Answer: &lt;class 'int'&gt;</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-methods_1","title":"Question - Tuple Methods","text":"<p> Which method would you use to find the index of a value in a tuple?</p> <pre><code>my_tuple = (10, 20, 30, 40)\nprint(my_tuple.index(30))\n</code></pre> <p>Answer: 2</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-length_1","title":"Question - Tuple Length","text":"<p> What will be the output of the following statement?</p> <pre><code>my_tuple = (1, 2, 3, 4, 5)\nprint(len(my_tuple))\n</code></pre> <p>Answer: 5</p>"},{"location":"Python/1.1_Tuples.html#question-nested-tuples_1","title":"Question - Nested Tuples","text":"<p> What will be the output of the following statement?</p> <pre><code>nested_tuple = (1, (2, 3), 4)\nprint(nested_tuple[1])\n</code></pre> <p>Answer: (2, 3)</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-slicing_1","title":"Question - Tuple Slicing","text":"<p> What will be the output of the following statement?</p> <pre><code>my_tuple = (1, 2, 3, 4, 5)\nprint(my_tuple[1:3])\n</code></pre> <p>Answer: (2, 3)</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-containment_1","title":"Question - Tuple Containment","text":"<p> What will be the output of the following statement?</p> <pre><code>my_tuple = (1, 2, 3, 4, 5)\nprint(3 in my_tuple)\n</code></pre> <p>Answer: True</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-unpacking_1","title":"Question - Tuple Unpacking","text":"<p> What will be the output of the following statement?</p> <pre><code>my_tuple = (1, 2, 3)\na, b, c = my_tuple\nprint(a, b, c)\n</code></pre> <p>Answer: 1 2 3</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-multiplication_1","title":"Question - Tuple Multiplication","text":"<p> What will be the output of the following statement?</p> <pre><code>my_tuple = (1, 2, 3)\nprint(my_tuple * 2)\n</code></pre> <p>Answer: (1, 2, 3, 1, 2, 3)</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-conversion_1","title":"Question - Tuple Conversion","text":"<p> What will be the output of the following statement?</p> <pre><code>my_list = [1, 2, 3]\nmy_tuple = tuple(my_list)\nprint(my_tuple)\n</code></pre> <p>Answer: (1, 2, 3)</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-with-mixed-types_1","title":"Question - Tuple with Mixed Types","text":"<p> What will be the output of the following statement?</p> <pre><code>my_tuple = (1, \"hello\", 3.14)\nprint(my_tuple[1])\n</code></pre> <p>Answer: hello</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-assignment","title":"Question - Tuple Assignment","text":"<p> What will happen when you try to reassign a tuple variable?</p> <pre><code>my_tuple = (1, 2, 3)\nmy_tuple = (4, 5, 6)\nprint(my_tuple)\n</code></pre> <p>Answer: (4, 5, 6)</p>"},{"location":"Python/1.1_Tuples.html#question-tuple-immutability-with-list","title":"Question - Tuple Immutability with List","text":"<p> What will be the output of the following statement?</p> <pre><code>my_tuple = (1, [2, 3], 4)\nmy_tuple[1].append(5)\nprint(my_tuple)\n</code></pre> <p>Answer: (1, [2, 3, 5], 4)</p>"},{"location":"Python/1.1_Tuples.html#examples","title":"Examples","text":"<pre><code>def biography_list(people):\n    # Iterate over each \"person\" in the given \"people\" list of tuples. \n    for item in people:\n        # This is the highlight of the code   \n        name, age, profession = item # The tuple gets assigned into 3 variables\n\n        # Format the required sentence and place the 3 variables \n        # in the correct placeholders using the .format() method.\n        print(\"{} is {} years old and works as {}\".format(name, age, profession))\n\n# Call to the function:\nbiography_list([(\"Dilwali\", 30, \"a Dulhania\"), (\"Raj\", 35, \"nam to suna hi hoga\"), (\"Maria\", 25, \"oh maria\")])\n\n# Output:\n# Ira is 30 years old and works as a Chef\n# Raj is 35 years old and works as a Lawyer\n# Maria is 25 years old and works as an Engineer\n</code></pre>"},{"location":"Python/1.2_Tuples_Advanced.html","title":"Advanced","text":""},{"location":"Python/1.2_Tuples_Advanced.html#advanced-tuples","title":"Advanced Tuples","text":"<p>In this article, we\u2019ll explore some advanced features and uses of tuples in Python.</p>"},{"location":"Python/1.2_Tuples_Advanced.html#using-parentheses-with-tuples","title":"Using Parentheses with Tuples","text":""},{"location":"Python/1.2_Tuples_Advanced.html#interpolating-values-in-a-string","title":"Interpolating Values in a String","text":"<p>When using the <code>%</code> operator to insert values into a string, you need to wrap the values in a tuple using parentheses.</p> <pre><code># Correct way\nprint(\"Hi, %s! You are %s years old.\" % (\"Ravi\", 30))\n# Output: 'Hi, Ravi! You are 30 years old.'\n\n# Incorrect way\nprint(\"Hi, %s! You are %s years old.\" % \"Ravi\", 30)\n# Output: TypeError: not enough arguments for format string\n</code></pre> <p>In the first example, the values are wrapped in a tuple, so it works. The second example raises an error because the values are not in a tuple.</p>"},{"location":"Python/1.2_Tuples_Advanced.html#creating-single-item-tuples","title":"Creating Single-Item Tuples","text":"<p>To make a tuple with only one item, you need to include a comma after the item.</p> <pre><code>one_word = \"Pranam\",\nprint(one_word)  # Output: ('Pranam',)\n\none_number = (88,)\nprint(one_number)  # Output: (88,)\n</code></pre> <p>The comma is necessary to make it a tuple and not just a regular string or number.</p>"},{"location":"Python/1.2_Tuples_Advanced.html#using-the-tuple-constructor","title":"Using the <code>tuple()</code> Constructor","text":"<p>You can use the <code>tuple()</code> function to make tuples from a list, set, dictionary, or string. If you call <code>tuple()</code> without any arguments, it creates an empty tuple.</p>"},{"location":"Python/1.2_Tuples_Advanced.html#examples","title":"Examples:","text":"<pre><code>print(tuple([\"Asharam Bapu\", 28, 5.9, \"India\"]))\n# Output: ('Asharam Bapu', 28, 5.9, 'India')\n\nprint(tuple(\"Developer\"))\n# Output: ('D', 'e', 'v', 'e', 'l', 'o', 'p', 'e', 'r')\n\nprint(tuple({\n    \"make\": \"Honda\",\n    \"model\": \"Civic\",\n    \"year\": 2021,\n}.values()))\n# Output: ('Honda', 'Civic', 2021)\n\nprint(tuple())\n# Output: ()\n</code></pre>"},{"location":"Python/1.2_Tuples_Advanced.html#accessing-items-in-a-tuple-indexing","title":"Accessing Items in a Tuple: Indexing","text":"<p>You can get items from a tuple using their index numbers. Indexes start from 0.</p>"},{"location":"Python/1.2_Tuples_Advanced.html#example","title":"Example:","text":"<pre><code>person = (\"Sita\", 22, 5.4, \"Nepal\")\nprint(person[0])  # Output: 'Sita'\nprint(person[1])  # Output: 22\nprint(person[3])  # Output: 'Nepal'\n</code></pre> <p>You can also use negative indexes to get items from the end.</p>"},{"location":"Python/1.2_Tuples_Advanced.html#example_1","title":"Example:","text":"<pre><code>print(person[-1])  # Output: 'Nepal'\nprint(person[-2])  # Output: 5.4\n</code></pre>"},{"location":"Python/1.2_Tuples_Advanced.html#retrieving-multiple-items-from-a-tuple-slicing","title":"Retrieving Multiple Items From a Tuple: Slicing","text":"<p>Slicing allows you to get parts of a tuple.</p>"},{"location":"Python/1.2_Tuples_Advanced.html#example_2","title":"Example:","text":"<pre><code>days = (\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\")\nprint(days[:3])  # Output: ('Sunday', 'Monday', 'Tuesday')\nprint(days[3:])  # Output: ('Wednesday', 'Thursday', 'Friday', 'Saturday')\n</code></pre>"},{"location":"Python/1.2_Tuples_Advanced.html#exploring-tuple-immutability","title":"Exploring Tuple Immutability","text":"<p>Tuples are immutable, which means you can\u2019t change, add, or remove items after creating them.</p>"},{"location":"Python/1.2_Tuples_Advanced.html#example_3","title":"Example:","text":"<pre><code>person = (\"Sita\", 22, 5.4, \"Nepal\")\n# Trying to change a value will cause an error\nperson[3] = \"India\"\n# Output: TypeError: 'tuple' object does not support item assignment\n\n# Trying to delete an item will cause an error\ndel person[2]\n# Output: TypeError: 'tuple' object doesn't support item deletion\n</code></pre>"},{"location":"Python/1.2_Tuples_Advanced.html#packing-and-unpacking-tuples","title":"Packing and Unpacking Tuples","text":""},{"location":"Python/1.2_Tuples_Advanced.html#example_4","title":"Example:","text":"<pre><code># Packing a tuple\ncoordinates = (19.0760, 72.8777)\n\n# Unpacking a tuple\nlat, lon = coordinates\nprint(lat)  # Output: 19.0760\nprint(lon)  # Output: 72.8777\n</code></pre>"},{"location":"Python/1.2_Tuples_Advanced.html#returning-tuples-from-functions","title":"Returning Tuples From Functions","text":"<p>Functions can return multiple values as tuples.</p>"},{"location":"Python/1.2_Tuples_Advanced.html#example_5","title":"Example:","text":"<pre><code>def min_max(values):\n    if not values:\n        raise ValueError(\"input list must not be empty\")\n    return min(values), max(values)\n\nresult = min_max([7, 2, 8, 4, 5])\nprint(result)  # Output: (2, 8)\nprint(type(result))  # Output: &lt;class 'tuple'&gt;\n</code></pre>"},{"location":"Python/1.2_Tuples_Advanced.html#concatenating-and-repeating-tuples","title":"Concatenating and Repeating Tuples","text":""},{"location":"Python/1.2_Tuples_Advanced.html#concatenating-tuples","title":"Concatenating Tuples","text":"<pre><code>name = (\"Rohit\",)\nsurname = (\"Sharma\",)\nfull_name = name + surname\nprint(full_name)\n# Output: ('Rohit', 'Sharma')\n</code></pre>"},{"location":"Python/1.2_Tuples_Advanced.html#repeating-tuples","title":"Repeating Tuples","text":"<pre><code>numbers = (1, 2, 3)\nprint(numbers * 2)\n# Output: (1, 2, 3, 1, 2, 3)\n</code></pre>"},{"location":"Python/1.2_Tuples_Advanced.html#reversing-and-sorting-tuples","title":"Reversing and Sorting Tuples","text":""},{"location":"Python/1.2_Tuples_Advanced.html#reversing-a-tuple","title":"Reversing a Tuple","text":"<pre><code>days = (\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\")\nreversed_days = days[::-1]\nprint(reversed_days)\n# Output: ('Saturday', 'Friday', 'Thursday', 'Wednesday', 'Tuesday', 'Monday', 'Sunday')\n</code></pre>"},{"location":"Python/1.2_Tuples_Advanced.html#sorting-a-tuple","title":"Sorting a Tuple","text":"<pre><code>ages = (45, 23, 67, 12, 34)\nprint(sorted(ages))\n# Output: [12, 23, 34, 45, 67]\n</code></pre>"},{"location":"Python/1.2_Tuples_Advanced.html#traversing-tuples-in-python","title":"Traversing Tuples in Python","text":""},{"location":"Python/1.2_Tuples_Advanced.html#using-a-for-loop","title":"Using a for Loop","text":"<pre><code>monthly_sales = (\n    (\"January\", 12000),\n    (\"February\", 14000),\n    (\"March\", 13000),\n    (\"April\", 15000),\n)\n\ntotal_sales = 0\nfor month, sales in monthly_sales:\n    total_sales += sales\n\nprint(total_sales)  # Output: 54000\n</code></pre>"},{"location":"Python/1.2_Tuples_Advanced.html#using-a-list-comprehension","title":"Using a List Comprehension","text":"<pre><code>numbers = (\"10\", \"20\", \"30\")\nprint(tuple(int(num) for num in numbers))\n# Output: (10, 20, 30)\n</code></pre>"},{"location":"Python/1.2_Tuples_Advanced.html#other-features-of-tuples","title":"Other Features of Tuples","text":""},{"location":"Python/1.2_Tuples_Advanced.html#count-and-index-methods","title":".count() and .index() Methods","text":"<pre><code>fruits = (\"mango\", \"banana\", \"apple\", \"mango\", \"mango\", \"kiwi\", \"banana\")\nprint(fruits.count(\"mango\"))  # Output: 3\nprint(fruits.index(\"kiwi\"))  # Output: 5\n</code></pre>"},{"location":"Python/1.2_Tuples_Advanced.html#membership-tests","title":"Membership Tests","text":"<pre><code>languages = (\"Hindi\", \"English\", \"Tamil\", \"Telugu\")\nprint(\"Tamil\" in languages)  # Output: True\nprint(\"Bengali\" not in languages)  # Output: True\n</code></pre>"},{"location":"Python/1.2_Tuples_Advanced.html#getting-the-length-of-a-tuple","title":"Getting the Length of a Tuple","text":"<pre><code>details = (\"Rajesh\", \"Teacher\", \"Delhi\", 45)\nprint(len(details))  # Output: 4\n</code></pre>"},{"location":"Python/1.2_Tuples_Advanced.html#comparing-tuples","title":"Comparing Tuples","text":"<pre><code>print((3, 4) == (3, 4))  # Output: True\nprint((10, 15, 20) &lt; (20, 15, 10))  # Output: True\nprint((7, 8, 9) &lt;= (7, 8, 9))  # Output: True\n</code></pre>"},{"location":"Python/1.2_Tuples_Advanced.html#common-traps","title":"Common Traps","text":"<p>When creating a one-item tuple, don't forget the trailing comma.</p> <pre><code>items = (99,)\nprint(type(items))  # Output: &lt;class 'tuple'&gt;\n</code></pre> <p>Tuples containing mutable objects, like lists, can't be used as dictionary keys.</p> <pre><code># This will raise an error\ncities_info = {\n    (\"Mumbai\", [18.975, 72.8258]): \"Financial Capital\",\n}\n# Output: TypeError: unhashable type: 'list'\n</code></pre>"},{"location":"Python/1.3_List.html","title":"Lists","text":""},{"location":"Python/1.3_List.html#understanding-pythons-sort-method","title":"Understanding Python's <code>sort()</code> Method","text":"<p>Let's start with a surprising example:</p> <pre><code>numbers = [3, 2, 4, 5, 1]\n\nsorted_numbers = numbers.sort()\n# THIS WILL OUTPUT None\nprint(sorted_numbers)  # Output: None\n</code></pre> <p>In this example, the output is <code>None</code>. Surprised and expected [1,2,3,4,5]?.</p>"},{"location":"Python/1.3_List.html#lets-find-out-what-happens-in-the-backend","title":"Let's find out what happens in the backend","text":"<p>The <code>sort()</code> method sorts a list without creating a new list.</p> <p>The <code>sort()</code> method sorts the elements of a list in place, meaning it modifies the original list directly without creating a new list. It does not return a new sorted list, but rather updates the existing list and returns <code>None</code>.</p> <p>This is the key difference between the <code>sort()</code> method and the <code>sorted()</code> function. The <code>sorted()</code> function returns a new list that is sorted, while the <code>sort()</code> method sorts the list it is called on and does not create a new list.</p>"},{"location":"Python/1.3_List.html#lets-see-some-examples","title":"Let's see some examples","text":"<p>Example 1: Using <code>sort()</code> method</p> <pre><code>numbers = [3, 2, 4, 5, 1]\n\nnumbers.sort()\n\nprint(numbers)  # Output: [1, 2, 3, 4, 5]\n</code></pre> <p>Example 2: Using <code>sorted()</code> function</p> <pre><code>numbers = [3, 2, 4, 5, 1]\n\nsorted_numbers = sorted(numbers)\n\nprint(sorted_numbers)  # Output: [1, 2, 3, 4, 5]\nprint(numbers)  # Output: [3, 2, 4, 5, 1]\n</code></pre>"},{"location":"Python/1.3_List.html#sorting-in-reverse-order","title":"Sorting in reverse order","text":"<pre><code>sorted_numbers = sorted(numbers, reverse=True)\n</code></pre>"},{"location":"Python/1.4_Dictionaries.html","title":"Dictionaries","text":""},{"location":"Python/1.4_Dictionaries.html#dictionaries-are-an-ordered-collection-of-key-value-pairs-python-37-and-later","title":"Dictionaries are an Ordered Collection of Key-Value Pairs (Python 3.7 and Later)","text":"<p>Starting from Python 3.7, dictionaries maintain the insertion order of items. This means the order in which you add key-value pairs is preserved when you iterate over the dictionary.</p>"},{"location":"Python/1.4_Dictionaries.html#lets-look-at-examples","title":"Let's look at examples","text":"<pre><code># Creating a dictionary\nperson = {\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"city\": \"Wonderland\"\n}\n\n# Adding a new key-value pair\nperson[\"email\"] = \"alice@example.com\"\n\n# Iterating over the dictionary\nfor key, value in person.items():\n    print(key, value)\n</code></pre> <p>Output: <pre><code>name Alice\nage 30\ncity Wonderland\nemail alice@example.com\n</code></pre></p> <p>In this example, the order of key-value pairs is the same as the order in which they were added to the dictionary.</p>"},{"location":"Python/1.5_Lamda_Functions.html","title":"Lambda","text":""},{"location":"Python/1.5_Lamda_Functions.html#python-lamda-functions-no-name-one-line-functions","title":"Python Lamda  Functions. No-name. One-line functions","text":"<p>Lamda function is a small function having just one expression and written in just one line. They don't need any function name. They are very handy when you need a small one-line code.</p> <p> Lamda functions always start with lamda </p>"},{"location":"Python/1.5_Lamda_Functions.html#what-is-lambda","title":"What is Lambda?","text":"<p>A lambda function is a small function you write in one line, without even naming it. Here is how a normal function and its lamda counterpart would look like:</p> <p></p>"},{"location":"Python/1.5_Lamda_Functions.html#where-to-use-lambda","title":"Where to Use Lambda","text":"<p>Lambda functions are handy for small, quick tasks where you don\u2019t need a full function. Here are a few places where they are commonly used:</p>"},{"location":"Python/1.5_Lamda_Functions.html#1-sorting-with-custom-rules","title":"1. Sorting with Custom Rules","text":"<p>Let\u2019s say you have a list of names and you want to sort them by length instead of alphabetically. Normally, you would write a function, but with a lambda, it\u2019s easy:</p> <pre><code>names = ['Rahul', 'Amit', 'Zara', 'Pooja']\nsorted_names = sorted(names, key=lambda x: len(x))\n</code></pre> <p>     Use lambda for sorting lists in a custom way. </p>"},{"location":"Python/1.5_Lamda_Functions.html#2-filtering-lists","title":"2. Filtering Lists","text":"<p>If you have a list of numbers and you only want the even ones, lambda makes it easy:</p> <pre><code>numbers = [1, 2, 3, 4, 5, 6, 7, 8]\neven_numbers = list(filter(lambda x: x % 2 == 0, numbers))\n</code></pre> <p>     Lambda functions are perfect for filtering lists. </p>"},{"location":"Python/1.5_Lamda_Functions.html#3-mapping-data","title":"3. Mapping Data","text":"<p>Let\u2019s say you want to double every number in a list. Instead of writing a loop, you can use <code>map()</code> with a lambda:</p> <pre><code>numbers = [1, 2, 3, 4]\ndoubled = list(map(lambda x: x * 2, numbers))\n</code></pre> <p>     Use lambda to quickly transform data in lists. </p>"},{"location":"Python/1.5_Lamda_Functions.html#4-filtering-with-lambda","title":"4. Filtering with Lambda","text":"<p>If you need to filter out items from a list based on a condition, <code>filter</code> with a <code>lambda</code> function is a quick solution:</p> <pre><code>numbers = [5, 12, 17, 24, 29]\nfiltered_numbers = list(filter(lambda x: x &gt; 15, numbers))\n</code></pre> <p>     Lambda is great for filtering lists based on specific conditions. </p>"},{"location":"Python/1.5_Lamda_Functions.html#5-applying-lambda-to-a-dataframe","title":"5. Applying Lambda to a DataFrame","text":"<p>When working with data, you can use <code>apply</code> with a <code>lambda</code> function to perform operations on each element in a DataFrame column:</p> <pre><code>df['new_column'] = df['existing_column'].apply(lambda x: x + 10)\n</code></pre> <p>     Apply lambda functions to DataFrame columns for quick data manipulation. </p>"},{"location":"Python/1.5_Lamda_Functions.html#6-combining-lambda-with-list-comprehensions","title":"6. Combining Lambda with List Comprehensions","text":"<p>Sometimes, you might want to use a lambda function within a list comprehension for more complex operations:</p> <pre><code>result = [(lambda x: x * 2)(x) for x in range(5)]\n</code></pre> <p>     Combine lambda functions with list comprehensions. </p>"},{"location":"Python/1.5_Lamda_Functions.html#7-lamda-with-if-elif-else","title":"7. Lamda with IF-ELIF-ELSE","text":"<p>You can use a lambda function to quickly check if a number is odd or even:</p> <pre><code>odd_even_lambda = lambda i: 'Number is Even' if i % 2 == 0 else 'Number is Odd'\n</code></pre> <p>     You can use lamda with if  IF-ELIF-ELSE logic </p>"},{"location":"Python/1.5_Lamda_Functions.html#lets-recap","title":"Let's recap","text":"<p>Don\u2019t expect lambda functions to shrink your entire code. They\u2019re only meant for small, one-line tasks where you don\u2019t need to name the function.</p>"},{"location":"Python/1.9_Func_Modl_Lib.html","title":"Overview","text":""},{"location":"Python/1.9_Func_Modl_Lib.html#python-functions-modules-and-libraries","title":"Python Functions, Modules, and Libraries","text":"<p>  Let's clear the confusion about Python libraries, modules, functions &amp; methods.  </p>"},{"location":"Python/1.9_Func_Modl_Lib.html#functions-and-methods","title":"Functions and Methods","text":""},{"location":"Python/1.9_Func_Modl_Lib.html#functions","title":"Functions","text":"<p>A function is a block of code inside def. Functions are stanalone and indepandant. Here\u2019s a simple function:</p> <pre><code>def greet(name):\n    return f\"Hello, {name}!\"\n\nprint(greet(\"Alice\"))  # Output: Hello, Alice!\n</code></pre>"},{"location":"Python/1.9_Func_Modl_Lib.html#methods","title":"Methods","text":"<p>A methods are like functions, but they are connected to an object; Methods are present inside a class and when you call them you need to give them live objects of that class.</p> <p>Here's an example of a method:</p> <pre><code>class Greeter:\n    def __init__(self, name):\n        self.name = name\n\n    def greet(self):\n        return f\"Hello, {self.name}!\"\n\ngreeter = Greeter(\"Alice\") # Creating a live object of a class\nprint(greeter.greet())  # Giving the method a live object\n</code></pre> Summary Functions:          Standalone blocks of code inside a <code>def:</code>.          E.g. <code>math.sqrt()</code>, <code>os.path.exists()</code>, <code>json.loads()</code> Methods:          Functions that you write inside a class. They should always be inside a class and usually include an <code>__init__</code> method for initialization.          E.g. <code>list.append()</code>, <code>str.upper()</code>, <code>dict.items()</code>"},{"location":"Python/1.9_Func_Modl_Lib.html#modules","title":"Modules","text":"<p>A python module is a <code>.py</code> file. E.g. <code>ikea.py</code>. It contains classes, functions and other code. You import it using <code>import ikea</code> and get all the funcs clases in your new code.</p> <p>Usually the module name and its filename is same. math module is math.py</p> <p>Example of a Module:</p> <p>Create a file named <code>ikea.py</code>:</p> <pre><code># ikea.py\ndef add(a, b):\n    return a + b\n\ndef subtract(a, b):\n    return a - b\n</code></pre> <p><code>ikea.py</code> becomes ikea module. You can import it now to use it:</p> <pre><code># main.py\nimport ikea\n\nikea.add(1,2) # modulename.function\n</code></pre>"},{"location":"Python/1.9_Func_Modl_Lib.html#import-modulename-vs-from-modulename-import-funcname","title":"<code>import moduleName</code> vs <code>from moduleName import funcName</code>","text":"<ul> <li><code>import module</code>:   <pre><code>import ikea   #Imports the full module\n\nikea.add()    #To use functions. Use moduleName.funcName\n</code></pre></li> <li><code>from moduleName import funcName</code></li> </ul> <pre><code>from ikea import add   # Imports only a function(s) from that module\nadd() #To use. Directly use the function. No ModuleName required.\n</code></pre>"},{"location":"Python/1.9_Func_Modl_Lib.html#import-module-vs-from-module-import","title":"<code>import module</code> vs <code>from module import *</code>","text":"<ul> <li><code>import module</code>:</li> <li>Imports the entire module with the packing: Module is imported as a complete package(with the cover).</li> <li>Access: To use the modules functions you need to add <code>moduleName.</code>.</li> <li>Example:     <pre><code>import math\nprint(math.sqrt(16))  # Output: 4.0\n</code></pre></li> <li> <p>Benefits: Keeps the module's namespace separate, which helps avoid naming conflicts. It also makes it clear where each function, class, or variable comes from.</p> </li> <li> <p><code>from module import *</code>:</p> </li> <li>Imports all functions, classes etc. but without the package(cover) directly into the current namespace: This means each function, class, and variable in the module is individually imported into your current namespace(cover is not imported).</li> <li>Access: You can use the imported items directly without the module name prefix.</li> <li>Example:     <pre><code>from math import *\nprint(sqrt(16))  # Output: 4.0\n</code></pre></li> <li>Issues: Can create naming conflict. Say, you created your own <code>sqrt</code> function, Python won't know which <code>sqrt</code> to use(math's or yours?). This makes it hard to trace where functions, classes, or variables come from, especially in large programs or many modules.</li> </ul>"},{"location":"Python/1.9_Func_Modl_Lib.html#libraries","title":"Libraries","text":"<p>A library is a collection of modules. Libraries contain similar module for similar tasks.</p>"},{"location":"Python/1.9_Func_Modl_Lib.html#example-using-the-pandas-library","title":"Example: Using the <code>pandas</code> Library","text":"<p>The <code>pandas</code> library has many modules for data manipulation and analysis. When you import <code>pandas</code>, you get access to all its tools.</p> <pre><code>import pandas as pd # Importing the entire library!\n\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35]\n}\n\ndf = pd.DataFrame(data) # Here, `DataFrame` is a module in the `pandas` library\nprint(df)\n</code></pre>"},{"location":"Python/1.9_Func_Modl_Lib.html#conclusion","title":"Conclusion","text":"<ul> <li>Python Universe: The entire Python ecosystem.</li> <li>Galaxies: Libraries (e.g., NumPy, Pandas).</li> <li>Solar Systems: Modules within libraries.</li> <li>Planets: Classes within modules.</li> <li>Moons: Methods within classes.</li> <li>Satellites: Functions within modules.</li> <li>Asteroids/Comets: Variables within functions or classes.</li> </ul>"},{"location":"Python/1.9_Func_Modl_Lib.html#installing-libraries-in-python","title":"Installing Libraries in Python","text":"<p>  To install libraries in Python, you typically use the `pip` command from the command prompt or terminal. `pip` is a package manager for Python that allows you to install, upgrade, and manage libraries.  </p>"},{"location":"Python/1.9_Func_Modl_Lib.html#installing-libraries-from-the-command-prompt","title":"Installing Libraries from the Command Prompt","text":"<p>To install a library, open your command prompt or terminal and type: <pre><code>pip install libraryname\n</code></pre> For example, to install the <code>pandas</code> library, you would type: <pre><code>pip install pandas\n</code></pre></p>"},{"location":"Python/1.9_Func_Modl_Lib.html#installing-libraries-inside-code","title":"Installing Libraries Inside Code","text":"<p>You can also install libraries from within your Python code using the <code>os</code> module to run shell commands: <pre><code>import os\nos.system('pip install pandas')\n</code></pre></p> <p>However, it is more common to use the command prompt for installing libraries to avoid unnecessary overhead in your scripts.</p>"},{"location":"Python/1.9_Func_Modl_Lib.html#sources-to-find-libraries","title":"Sources to Find Libraries","text":"<p>You can find Python libraries on the Python Package Index (PyPI) website, pypi.org. PyPI is the official repository for Python packages where you can search for and learn about different libraries.</p>"},{"location":"Python/1.9_Func_Modl_Lib.html#managing-libraries-when-sharing-python-code","title":"Managing libraries When Sharing Python Code","text":"<p>  When you write a Python script that relies on external libraries, you need to ensure those libraries are installed on any system where the script will run. Here\u2019s how you can manage dependencies using a `requirements.txt` file. </p>"},{"location":"Python/1.9_Func_Modl_Lib.html#creating-a-python-script-with-dependencies","title":"Creating a Python Script with Dependencies","text":"<p>Suppose you have a Python script named <code>data_analysis.py</code> that uses the <code>pandas</code> library:</p> <pre><code># data_analysis.py\nimport pandas as pd\n\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35]\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n</code></pre>"},{"location":"Python/1.9_Func_Modl_Lib.html#creating-a-requirementstxt-file","title":"Creating a <code>requirements.txt</code> File","text":"<p>To manage dependencies, create a <code>requirements.txt</code> file that lists all the libraries your script needs. You can generate this file automatically if the libraries are already installed in your environment:</p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre> <p>This command will create a <code>requirements.txt</code> file with contents similar to:</p> <pre><code>pandas==1.3.3\n</code></pre>"},{"location":"Python/1.9_Func_Modl_Lib.html#installing-dependencies-on-another-system","title":"Installing Dependencies on Another System","text":"<p>When someone else wants to run your <code>data_analysis.py</code> script on another system, they should follow these steps:</p> <ol> <li> <p>Clone or Copy the Project: Get the script and the <code>requirements.txt</code> file onto their system.</p> </li> <li> <p>Create a Virtual Environment (Optional but Recommended):     <pre><code>python -m venv myenv\nsource myenv/bin/activate  # On Windows use `myenv\\Scripts\\activate`\n</code></pre></p> <p>Newer systems(linux versions) now makes it mandatory to install venv.</p> </li> <li> <p>Install Dependencies:    <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> </ol> <p>This command reads the <code>requirements.txt</code> file and installs all the listed libraries. This way the script will not run into can't find xxx error.</p>"},{"location":"Python/1.9_Func_Modl_Lib.html#example-workflow","title":"Example Workflow","text":"<p>Let me explain it with a sample workflow:</p> <ol> <li>On the Development System:</li> </ol> <pre><code># Create the Python script\necho \"\nimport pandas as pd\n\ndata = {\n    'Name': ['Donald', 'Biden', 'Mia Khalifa'],\n    'Age': [25, 30, 35]\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n\" &gt; data_analysis.py\n\n# Install pandas if not already installed\npip install pandas\n\n# Generate requirements.txt\npip freeze &gt; requirements.txt\n</code></pre> <ol> <li> <p>On the Target System:</p> <p>Using a terminal or CMD.    <pre><code># Clone project or copy files\nscp user@development_system:/path/to/project/* /path/to/local/directory/\n\n# Go to the project directory\ncd /path/to/local/directory\n\n# (Optional) Create venv(virtual env)\npython -m venv myenv\nsource myenv/bin/activate  # If you use windows you may have to use `myenv\\Scripts\\activate`\n\n# Install required libraries\npip install -r requirements.txt\n\n# Run the script\npython data_analysis.py\n</code></pre></p> <p>This is how you ensure that all necessary dependancies are available on the other machine where the script is re-run.</p> </li> </ol>"},{"location":"Python/Linux.html","title":"Some essential unix commands","text":"Command Description Example <code>id</code> Displays user and group information for the current user. <code>id dwdas</code> <code>sudo su</code> Switches to the root user. <code>sudo su</code> <code>ls -la</code> Lists all files and directories, including hidden ones, with detailed information. <code>ls -la</code> <code>chown</code> Changes the ownership of files and directories. <code>chown -R dwdas:dwdas /user/hive</code> <code>rm -rf</code> Removes everything inside a specified directory but leaves the directory itself intact. <code>rm -rf /home/dwdas/*</code> <code>mkdir -p</code> Creates directories, including parent directories if they don't exist. <code>mkdir -p /user/hive/warehouse</code> <code>whoami</code> Displays the current username. <code>whoami</code> <code>printenv</code> Displays environment variables. <code>printenv SPARK_HOME</code> <code>cd</code> Changes the current directory. <code>cd /user/hive</code> <code>pwd</code> Prints the current working directory. <code>pwd</code> <code>cp</code> Copies files or directories. <code>cp source_file destination_file</code> <code>mv</code> Moves or renames files or directories. <code>mv old_name new_name</code> <code>chmod</code> Changes the permissions of files or directories. <code>chmod 755 script.sh</code> <code>top</code> Displays real-time system information and process details. <code>top</code> <code>ps</code> Displays currently running processes. <code>ps aux</code> <code>kill</code> Terminates a process. <code>kill -9 process_id</code> <code>grep</code> Searches for patterns within files. <code>grep \"search_term\" filename</code> <code>find</code> Searches for files and directories. <code>find /path -name filename</code> <code>df</code> Displays disk space usage. <code>df -h</code> <code>du</code> Displays disk usage of files and directories. <code>du -sh directory</code> <code>tar</code> Archives files. <code>tar -cvf archive_name.tar directory</code> <code>curl</code> Transfers data from or to a server. <code>curl -O http://example.com/file</code> <code>wget</code> Downloads files from the internet. <code>wget http://example.com/file</code> <code>nano</code> A simple text editor. <code>nano filename</code> <code>vim</code> A more advanced text editor. <code>vim filename</code> <code>ssh</code> Connects to a remote machine via SSH. <code>ssh user@hostname</code> <code>scp</code> Copies files between hosts over SSH. <code>scp local_file user@remote_host:/remote_path</code> <code>docker</code> Manages Docker containers. <code>docker ps</code> <code>systemctl</code> Manages system services. <code>systemctl status service_name</code> <code>service</code> Manages system services (older systems). <code>service service_name start</code> <code>alias</code> Creates a shortcut for a command. <code>alias ll='ls -la'</code> <code>history</code> Shows the command history. <code>history</code>"},{"location":"Python/Pyspark.html","title":"Get all session information","text":"<pre><code>import os\nimport sys\nfrom pyspark.sql import SparkSession\n\n# Initialize a Spark session\nspark = SparkSession.builder.appName(\"ExampleApp\").getOrCreate()\n\n# Print Spark information in a presentable format\nprint(f\"Spark version: {spark.version}\")\nprint(f\"Application name: {spark.sparkContext.appName}\")\nprint(f\"Master URL: {spark.sparkContext.master}\")\nprint(f\"Application ID: {spark.sparkContext.applicationId}\")\nprint(f\"Number of executors: {len(spark.sparkContext._jsc.sc().statusTracker().getExecutorInfos())}\")\nprint(f\"Cores per executor: {spark.sparkContext.defaultParallelism}\")\n\n# Try to get the memory per executor configuration\ntry:\n    executor_memory = spark.conf.get(\"spark.executor.memory\")\n    print(f\"Memory per executor: {executor_memory}\")\nexcept Exception as e:\n    print(\"Memory per executor: Configuration not found.\")\n\n# Print all configuration properties in a formatted way\nprint(\"\\nAll configuration properties:\")\nfor conf_key, conf_value in spark.sparkContext.getConf().getAll():\n    print(f\"  {conf_key}: {conf_value}\")\n\n# Print active jobs and stages using correct methods\nprint(f\"\\nActive jobs: {spark.sparkContext.statusTracker().getActiveJobIds()}\")\nprint(f\"Active stages: {spark.sparkContext.statusTracker().getActiveStageIds()}\")\n\n# Print additional environment information\nprint(f\"\\nSpark home: {os.getenv('SPARK_HOME')}\")\nprint(f\"Java home: {os.getenv('JAVA_HOME')}\")\nprint(f\"Python version: {sys.version}\")\nprint(f\"Python executable: {sys.executable}\")\nprint(f\"Current working directory: {os.getcwd()}\")\nprint(f\"User home directory: {os.path.expanduser('~')}\")\nprint(f\"System PATH: {os.getenv('PATH')}\")\n</code></pre>"},{"location":"Python/Pyspark.html#analyse-a-dataframe","title":"Analyse a dataframe","text":"<pre><code># 1. Print the Schema\nprint(\"# 1. Schema of the DataFrame:\")\ndf.printSchema()  # Displays the schema (column names, data types, nullability)\n\n# 2. Show the First Few Rows\nprint(\"\\n# 2. First 10 Rows of the DataFrame:\")\ndf.show(10, truncate=False)  # Displays the first 10 rows without truncating columns\n\n# 3. Describe the Data\nprint(\"\\n# 3. Summary Statistics for Numeric Columns:\")\ndf.describe().show()  # Provides summary statistics for numeric columns\n\n# 4. Count the Number of Rows\nrow_count = df.count()  # Returns the total number of rows\nprint(f\"\\n# 4. Total Number of Rows: {row_count}\")\n\n# 5. Check for Null Values\nfrom pyspark.sql.functions import isnull, when, count\nprint(\"\\n# 5. Number of Null Values in Each Column:\")\ndf.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()  # Counts null values in each column\n\n# 6. Get Column Names\ncolumns = df.columns  # Returns a list of column names\nprint(f\"\\n# 6. Column Names: {columns}\")\n\n# 7. Summary Statistics for All Columns\nprint(\"\\n# 7. Summary Statistics for All Columns:\")\ndf.summary().show()  # Provides summary statistics for all columns\n\n# 8. Distinct Values in a Column\nprint(\"\\n# 8. Distinct Values in the 'Invoice' Column:\")\ndf.select(\"Invoice\").distinct().show()  # Shows all distinct values in the \"Invoice\" column\n\n# 9. Data Types of Columns\nprint(\"\\n# 9. Data Types of Each Column:\")\nprint(df.dtypes)  # Returns a list of tuples with each column\u2019s name and data type\n\n# 10. Display Summary Statistics for Specific Columns\nprint(\"\\n# 10. Summary Statistics for 'Quantity' and 'Price' Columns:\")\ndf.describe(\"Quantity\", \"Price\").show()  # Summary stats for \"Quantity\" and \"Price\" columns\n\n# 11. Group and Aggregate Data\nprint(\"\\n# 11. Group and Count by 'Country':\")\ndf.groupBy(\"Country\").count().show()  # Groups data by \"Country\" and counts occurrences\n\n# 12. Sample Data\nprint(\"\\n# 12. Random 10% Sample of the DataFrame:\")\ndf.sample(0.1).show()  # Returns a 10% random sample of the DataFrame\n\n# 13. Check DataFrame Size\nrow_count = df.count()\ncolumn_count = len(df.columns)\nprint(f\"\\n# 13. Number of Rows: {row_count}, Number of Columns: {column_count}\")\n\n# 14. Drop Duplicates\nprint(\"\\n# 14. DataFrame After Dropping Duplicates Based on 'InvoiceNo':\")\ndf.dropDuplicates([\"InvoiceNo\"]).show()  # Removes duplicate rows based on \"InvoiceNo\"\n\n# 15. Check DataFrame Memory Usage\nmemory_usage = df.rdd.map(lambda row: len(str(row))).reduce(lambda a, b: a + b)\nprint(f\"\\n# 15. Estimated Memory Usage: {memory_usage} bytes\")\n</code></pre>"},{"location":"Python/Pyspark.html#common-df-operations-part-1","title":"Common df operations - Part 1","text":"Operation Description Example select() Selects a subset of columns. <code>df.select(\"col1\", \"col2\")</code> filter() Filters rows based on a condition. <code>df.filter(df[\"col\"] &gt; 10)</code> withColumn() Adds or replaces a column. <code>df.withColumn(\"new_col\", df[\"col\"] * 2)</code> drop() Drops one or more columns. <code>df.drop(\"col1\", \"col2\")</code> groupBy() Groups rows by specified columns. <code>df.groupBy(\"col\").count()</code> agg() Aggregates data based on a specified column or expression. <code>df.groupBy(\"col\").agg({\"col2\": \"sum\"})</code> orderBy() Sorts the DataFrame by specified columns. <code>df.orderBy(\"col\")</code> join() Joins two DataFrames. <code>df1.join(df2, df1[\"col1\"] == df2[\"col2\"], \"inner\")</code> union() Unions two DataFrames. <code>df1.union(df2)</code> distinct() Removes duplicate rows. <code>df.distinct()</code> dropDuplicates() Drops duplicate rows based on specified columns. <code>df.dropDuplicates([\"col1\", \"col2\"])</code> sample() Returns a random sample of the DataFrame. <code>df.sample(0.1)</code> count() Returns the number of rows in the DataFrame. <code>df.count()</code> show() Displays the first n rows of the DataFrame. <code>df.show(5)</code> collect() Returns all the rows as a list of Row objects. <code>df.collect()</code> repartition() Repartitions the DataFrame into the specified number of partitions. <code>df.repartition(10)</code> coalesce() Reduces the number of partitions in the DataFrame. <code>df.coalesce(1)</code> cache() Caches the DataFrame in memory. <code>df.cache()</code> persist() Persists the DataFrame with the specified storage level. <code>df.persist(StorageLevel.DISK_ONLY)</code> createOrReplaceTempView() Creates or replaces a temporary view with the DataFrame. <code>df.createOrReplaceTempView(\"temp_view\")</code> write Writes the DataFrame to a specified format (e.g., Parquet, CSV). <code>df.write.format(\"parquet\").save(\"path/to/save\")</code> printSchema() Prints the schema of the DataFrame. <code>df.printSchema()</code> dtypes Returns a list of tuples with column names and data types. <code>df.dtypes</code> schema Returns the schema of the DataFrame as a <code>StructType</code>. <code>df.schema</code> describe() Computes basic statistics for numeric and string columns. <code>df.describe().show()</code> explain() Prints the physical plan to execute the DataFrame query. <code>df.explain()</code> head() Returns the first n rows as a list of Row objects. <code>df.head(5)</code> first() Returns the first row as a Row object. <code>df.first()</code> withColumnRenamed() Renames an existing column. <code>df.withColumnRenamed(\"old_name\", \"new_name\")</code> cast() Converts the data type of a DataFrame column. <code>df.withColumn(\"age\", col(\"age\").cast(\"integer\"))</code> alias() Assigns an alias to a DataFrame column or DataFrame. <code>df.select(col(\"col1\").alias(\"new_col1\"))</code>"},{"location":"Python/Pyspark.html#common-df-operations-part-2","title":"Common df operations - Part 2","text":"Operation Description Example select() Selects a subset of columns. <code>df.select(\"col1\", \"col2\")</code> filter() Filters rows based on a condition. <code>df.filter(df[\"col\"] &gt; 10)</code> withColumn() Adds or replaces a column. <code>df.withColumn(\"new_col\", df[\"col\"] * 2)</code> drop() Drops one or more columns. <code>df.drop(\"col1\", \"col2\")</code> groupBy() Groups rows by specified columns. <code>df.groupBy(\"col\").count()</code> agg() Aggregates data based on a specified column or expression. <code>df.groupBy(\"col\").agg({\"col2\": \"sum\"})</code> orderBy() Sorts the DataFrame by specified columns. <code>df.orderBy(\"col\")</code> join() Joins two DataFrames. <code>df1.join(df2, df1[\"col1\"] == df2[\"col2\"], \"inner\")</code> union() Unions two DataFrames. <code>df1.union(df2)</code> distinct() Removes duplicate rows. <code>df.distinct()</code> dropDuplicates() Drops duplicate rows based on specified columns. <code>df.dropDuplicates([\"col1\", \"col2\"])</code> sample() Returns a random sample of the DataFrame. <code>df.sample(0.1)</code> count() Returns the number of rows in the DataFrame. <code>df.count()</code> show() Displays the first n rows of the DataFrame. <code>df.show(5)</code> collect() Returns all the rows as a list of Row objects. <code>df.collect()</code> repartition() Repartitions the DataFrame into the specified number of partitions. <code>df.repartition(10)</code> coalesce() Reduces the number of partitions in the DataFrame. <code>df.coalesce(1)</code> cache() Caches the DataFrame in memory. <code>df.cache()</code> persist() Persists the DataFrame with the specified storage level. <code>df.persist(StorageLevel.DISK_ONLY)</code> createOrReplaceTempView() Creates or replaces a temporary view with the DataFrame. <code>df.createOrReplaceTempView(\"temp_view\")</code> write Writes the DataFrame to a specified format (e.g., Parquet, CSV). <code>df.write.format(\"parquet\").save(\"path/to/save\")</code> printSchema() Prints the schema of the DataFrame. <code>df.printSchema()</code> dtypes Returns a list of tuples with column names and data types. <code>df.dtypes</code> schema Returns the schema of the DataFrame as a <code>StructType</code>. <code>df.schema</code> describe() Computes basic statistics for numeric and string columns. <code>df.describe().show()</code> explain() Prints the physical plan to execute the DataFrame query. <code>df.explain()</code> head() Returns the first n rows as a list of Row objects. <code>df.head(5)</code> first() Returns the first row as a Row object. <code>df.first()</code> withColumnRenamed() Renames an existing column. <code>df.withColumnRenamed(\"old_name\", \"new_name\")</code> cast() Converts the data type of a DataFrame column. <code>df.withColumn(\"age\", col(\"age\").cast(\"integer\"))</code> alias() Assigns an alias to a DataFrame column or DataFrame. <code>df.select(col(\"col1\").alias(\"new_col1\"))</code>"},{"location":"Python/Pyspark.html#withcolumn-and-withcolumnrenamed-in-pyspark","title":"<code>withColumn</code> and <code>withColumnRenamed</code> in PySpark","text":"Technique Description Example Code Add New Column Add a new column with a constant value <code>storesDF.withColumn(\"constantColumn\", lit(1))</code> Rename Column Rename an existing column <code>storesDF.withColumnRenamed(\"oldColumnName\", \"newColumnName\")</code> Modify Existing Column Modify an existing column <code>storesDF.withColumn(\"existingColumn\", col(\"existingColumn\") * 2)</code> Split Column into Multiple Split a column into multiple columns <code>storesDF.withColumn(\"part1\", split(col(\"compositeColumn\"), \"_\").getItem(0)).withColumn(\"part2\", split(col(\"compositeColumn\"), \"_\").getItem(1))</code> Cast Column to New Type Cast a column to a new type <code>storesDF.withColumn(\"castedColumn\", col(\"columnToCast\").cast(\"Integer\"))</code> Apply UDF Apply a user-defined function (UDF) <code>storesDF.withColumn(\"newColumn\", udfFunction(col(\"existingColumn\")))</code> Conditional Column Add a column based on a condition <code>storesDF.withColumn(\"newColumn\", when(col(\"conditionColumn\") &gt; 0, \"Positive\").otherwise(\"Negative\"))</code> Add Column from Expression Add a column from an expression <code>storesDF.withColumn(\"expressionColumn\", col(\"columnA\") + col(\"columnB\"))</code> Drop Column Drop a column <code>storesDF.drop(\"columnToDrop\")</code> Add Column with SQL Expression Add a column using SQL expression <code>storesDF.withColumn(\"newColumn\", expr(\"existingColumn + 1\"))</code>"},{"location":"Python/PythonScripts.html","title":"Csv files creator with random data","text":""},{"location":"Python/PythonScripts.html#employee-csv-creator","title":"Employee CSV creator","text":"<p>Here's a Python script to create CSV files with random values for specific fields. You can decide how many files to create and how many rows each file should have. This script uses the <code>Faker</code> library to generate realistic random data.</p> <p>So, before running the script, you must install <code>Faker</code>, or you will get <code>ModuleNotFoundError: No module named 'faker'</code>.</p> <pre><code>pip install faker\n</code></pre> <p>Then, run the script from Jupyter notebook or save it as a .py file and run it from the terminal:</p> <pre><code>import csv\nimport os\nfrom faker import Faker\n\n# Initialize Faker. This is the non-sense value creator. Haha\nfake = Faker()\n\ndef generate_random_data(num_rows):\n    data = []\n    for _ in range(num_rows):\n        row = {\n            'id': fake.unique.random_int(min=1, max=1000000),\n            'first_name': fake.first_name(),\n            'last_name': fake.last_name(),\n            'passport_number': fake.unique.bothify(text='???########'),\n            'country': fake.country(),\n            'date_of_birth': fake.date_of_birth(minimum_age=18, maximum_age=80),\n            'profession': fake.job()\n        }\n        data.append(row)\n    return data\n\ndef create_csv_file(file_name, num_rows):\n    data = generate_random_data(num_rows)\n    with open(file_name, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=data[0].keys())\n        writer.writeheader()\n        writer.writerows(data)\n\ndef create_multiple_files(num_files, num_rows):\n    for i in range(num_files):\n        file_name = f'file_{i+1}.csv'\n        create_csv_file(file_name, num_rows)\n        print(f'Created {file_name}')\n\n# Provide the number of files and no of rows per file\nnum_files = 5  # Change this to the number of files you want to create\nnum_rows = 100  # Change this to the number of rows per file\n\ncreate_multiple_files(num_files, num_rows)\n</code></pre> <p>Note: The files are outputted to the current working directory. To know use this command: <pre><code>import os\nos.getcwd()\n</code></pre></p>"},{"location":"Python/PythonScripts.html#output-information","title":"Output Information","text":"<p>The script creates CSV files with the following columns: - <code>id</code>: A unique identifier for each row - <code>first_name</code>: A randomly generated first name - <code>last_name</code>: A randomly generated last name - <code>passport_number</code>: A randomly generated passport number - <code>country</code>: A randomly generated country name - <code>date_of_birth</code>: A randomly generated date of birth - <code>profession</code>: A randomly generated profession</p>"},{"location":"Python/PythonScripts.html#sample-row","title":"Sample Row","text":"<p>Here is an example of what a row in the CSV file might look like: <pre><code>id,first_name,last_name,passport_number,country,date_of_birth,profession\n123456,John,Doe,ABC123456789,United States,1980-05-15,Software Developer\n</code></pre></p>"},{"location":"Python/PythonScripts.html#customization","title":"Customization","text":"<p>To change the number of files created, modify the <code>num_files</code> variable. To change the number of rows in each file, modify the <code>num_rows</code> variable. For example, to create 10 files with 50 rows each, set:</p> <pre><code>num_files = 10\nnum_rows = 50\n</code></pre> <p>Then, run the script, and it will generate the specified number of CSV files with the desired number of rows.</p>"},{"location":"Python/PythonScripts.html#insurance-policy-issuance-process","title":"Insurance - Policy Issuance Process","text":"<p>Python script to generate CSV files specifically for the Policy Issuance Process. </p> <pre><code>import csv\nimport os\nfrom faker import Faker\nfrom datetime import datetime\n\n# Initialize Faker\nfake = Faker()\n\ndef generate_random_data(num_rows, start_index):\n    data = []\n    for i in range(num_rows):\n        # Use timestamp and start_index to generate a unique policy number\n        policy_number = f'{start_index + i}_{int(datetime.now().timestamp() * 1000)}'\n\n        row = {\n            'policy_number': policy_number,\n            'policyholder_name': fake.name(),\n            'insured_name': fake.name(),\n            'policy_type': fake.random_element(elements=('Life', 'Health', 'Auto', 'Home')),\n            'effective_date': fake.date_this_decade(),\n            'expiration_date': fake.date_this_decade(),\n            'premium_amount': round(fake.random_number(digits=5), 2),\n            'beneficiary_name': fake.name(),\n            'beneficiary_contact': fake.phone_number(),\n            'agent_name': fake.name(),\n            'agent_contact': fake.phone_number(),\n            'coverage_details': fake.sentence(nb_words=6),\n            'endorsements_riders': fake.sentence(nb_words=4),\n            'underwriter_name': fake.name()\n        }\n        data.append(row)\n    return data\n\ndef create_csv_file(file_name, num_rows, start_index):\n    data = generate_random_data(num_rows, start_index)\n    with open(file_name, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=data[0].keys())\n        writer.writeheader()\n        writer.writerows(data)\n\ndef create_multiple_files(num_files, num_rows):\n    for i in range(num_files):\n        file_name = f'policy_issuance_file_{i+1}.csv'\n        create_csv_file(file_name, num_rows, i * num_rows)\n        print(f'Created {file_name}')\n\n# Specify the number of files and rows per file\nnum_files = 5  # Change this to the number of files you want to create\nnum_rows = 100  # Change this to the number of rows per file\n\ncreate_multiple_files(num_files, num_rows)\n</code></pre>"},{"location":"Python/PythonScripts.html#explanation","title":"Explanation:","text":"<ol> <li> <p>Fields:</p> <ul> <li><code>policy_number</code>: A unique identifier for the policy.</li> <li><code>policyholder_name</code>: The name of the person holding the policy.</li> <li><code>insured_name</code>: The name of the person covered by the policy.</li> <li><code>policy_type</code>: The type of insurance (e.g., Life, Health, Auto, Home).</li> <li><code>effective_date</code>: The start date of the policy.</li> <li><code>expiration_date</code>: The end date of the policy.</li> <li><code>premium_amount</code>: The cost of the policy.</li> <li><code>beneficiary_name</code>: The name of the beneficiary.</li> <li><code>beneficiary_contact</code>: The contact details of the beneficiary.</li> <li><code>agent_name</code>: The name of the agent.</li> <li><code>agent_contact</code>: The contact details of the agent.</li> <li><code>coverage_details</code>: The specific coverage information.</li> <li><code>endorsements_riders</code>: Any additional terms or coverage.</li> <li><code>underwriter_name</code>: The name of the underwriter.</li> </ul> </li> <li> <p>generate_random_data(num_rows): Generates a list of dictionaries with random data for the specified number of rows.</p> </li> <li> <p>create_csv_file(file_name, num_rows): Creates a CSV file with the given file name and writes the generated random data into it.</p> </li> <li> <p>create_multiple_files(num_files, num_rows): Creates the specified number of CSV files, each with the specified number of rows.</p> </li> </ol> <p>To customize, you can change <code>num_files</code> to the number of files you want to create and <code>num_rows</code> to the number of rows per file. Save this script as <code>generate_policy_issuance_csv.py</code> and run it to generate your files.</p>"},{"location":"Python/GraphAPIJupyter/GraphAPIUsingJuputer.html","title":"Graph API","text":"<ul> <li>Running Microsoft Graph quick start Python code from a jupyter notebook </li> <li>Background</li> <li>Getting Started<ul> <li>1. Download the Sample Code</li> <li>2. Setting Up the Notebook</li> <li>3. Adapting the Code for Jupyter</li> <li>4. Finalizing the Setup</li> </ul> </li> <li>Cnclusion</li> </ul>"},{"location":"Python/GraphAPIJupyter/GraphAPIUsingJuputer.html#running-microsoft-graph-quick-start-python-code-from-a-jupyter-notebook","title":"Running Microsoft Graph quick start Python code from a jupyter notebook","text":""},{"location":"Python/GraphAPIJupyter/GraphAPIUsingJuputer.html#background","title":"Background","text":"<p>In this guide I will show you how to run Microsoft's example code in a Jupyter notebook. Usually, this code is meant to be run in a terminal, but with a few small changes, you can run it in a notebook instead. Many people like this way because it's more familiar and easy to use. We'll focus on how to make these changes so the code works in Jupyter, without getting into more details about the code.</p>"},{"location":"Python/GraphAPIJupyter/GraphAPIUsingJuputer.html#getting-started","title":"Getting Started","text":""},{"location":"Python/GraphAPIJupyter/GraphAPIUsingJuputer.html#1-download-the-sample-code","title":"1. Download the Sample Code","text":"<ol> <li>Navigate to the Microsoft Graph Quick Start page.</li> </ol> <p> 2. Select Python as your language of choice.</p> <p> 3. Click on Get a client ID and log in using your personal, work, or school account. Note: An Outlook account is necessary for this step.</p> <p> 4. Upon successful login, a client ID will be presented to you. Make sure to save this ID.</p> <p> 5. Click on Download the code sample and save the msgraph-training-python.zip file. Unzip its contents into a directory where you have permission to run Python code, such as a Visual Studio Code workspace.</p> <p> </p>"},{"location":"Python/GraphAPIJupyter/GraphAPIUsingJuputer.html#2-setting-up-the-notebook","title":"2. Setting Up the Notebook","text":"<ol> <li>Open a Jupyter notebook in Visual Studio Code (VS Code) and navigate (<code>cd</code>) to the <code>graphtutorial</code> folder (e.g., <code>%cd &lt;Path to&gt;\\graphtutorial</code>).    </li> <li>Install the required dependencies by running <code>pip install -r requirements.txt</code> in a notebook cell. This process may take about a minute.</li> </ol>"},{"location":"Python/GraphAPIJupyter/GraphAPIUsingJuputer.html#3-adapting-the-code-for-jupyter","title":"3. Adapting the Code for Jupyter","text":"<ol> <li>Attempt to run the <code>main.py</code> file directly in a notebook cell with the command <code>%run main.py</code>. This will likely result in an error due to a conflict between Jupyter's and asyncio's event loops.</li> </ol> <p> 2. To resolve this, install the <code>nest_asyncio</code> package with <code>pip install nest_asyncio</code>. This package allows for the nesting of asyncio's event loop, facilitating the running of asynchronous code in Jupyter.</p> <p> 3. In a new cell, copy and paste the entire code from <code>main.py</code> and add the following lines just before the <code>main()</code> function:</p> <pre><code>```python\nimport nest_asyncio\nnest_asyncio.apply()\n```\n</code></pre> <p></p> <ol> <li>Comment out the last line of the script to prevent it from executing immediately.</li> </ol> <p> 5. Run the modified cell. You should not see any output yet. 6. In a new cell, execute <code>await main()</code> to run the main function asynchronously. This should produce the expected output.</p> <p></p>"},{"location":"Python/GraphAPIJupyter/GraphAPIUsingJuputer.html#4-finalizing-the-setup","title":"4. Finalizing the Setup","text":"<ol> <li>Follow the on-screen instructions and navigate to Microsoft Device Login to enter the provided code. Complete the login process as prompted.</li> </ol> <p> 2. A selection box or palette will appear at the top of VS Code. Enter your choice (e.g., <code>2</code> to view emails) and press Enter.</p> <p> 3. Once you've completed your tasks, enter <code>0</code> at the top of the input box and press Enter to conclude the session.</p> <p></p>"},{"location":"Python/GraphAPIJupyter/GraphAPIUsingJuputer.html#cnclusion","title":"Cnclusion","text":"<p>I have put the juputer notebook vrsion of the code here. All you have to do is place it in your \\msgraph-training-python\\graphtutorial folder. Open it and run cell by cell.</p>"},{"location":"SQL/1.0.0_dbt.html","title":"Overview","text":""},{"location":"SQL/1.0.0_dbt.html#what-is-dbt","title":"What is dbt?","text":"<p>dbt stands for data built tool. It's a tool to run SQL as workflows. It's almost all-SQL.</p> <p>dbt is the T in ELT. When you use dbt the data is already present in the final destination.</p>"},{"location":"SQL/1.0.0_dbt.html#why-dbt","title":"Why dbt?","text":"<p>Opensource. Free. Development-Testing-Deployment-Documentation-DataLineage So easily. Integrates so well with CI/CD. Resusage code with Macros and Jinja?</p>"},{"location":"SQL/1.0.0_dbt.html#so-why-not-just-use-a-sql-notebook-in-databricks","title":"So why not just use a SQL notebook in databricks?","text":"<p>Have you seen how notebooks run, one cell after another. Do you think workflows are always like a train? Long and linear? Workflows have branches like trees. dbt is far more than what databricks sql notebook's one cell at a time offers.</p> <p>So, to summarize, with dbt, you can run sql on your data which is already <code>inside</code> like enjoying in house like lakehouse, beachhouse, warehouse etc.</p>"},{"location":"SQL/1.0.0_dbt.html#explain-more","title":"Explain more","text":"<p>With dbt you can put your complex sql into small small chunks. All your coding good practices like modular kitchen, git, CI/ABCD you can do with dbt.</p>"},{"location":"SQL/1.0.0_dbt.html#how-to-install-dbt-is-it-local-or-on-cloud","title":"How to install dbt? Is it local or on cloud?","text":"<p>DBT has two ways to install:</p> <p>Local(dbt core): Here you just install a command line tool. It conneccts with databases etc with 'adapters'</p> <p>Cloud(dbt cloud): This is not just cloud. It offers all fancy stuffs like: User interface, job scheduling, CI/CD, hosting documentation, monitoring, alerting, integrated IDE, CLI(to connect from local)</p>"},{"location":"SQL/1.0.0_dbt.html#steps-install-dbt-locally-on-windows","title":"Steps Install dbt locally on Windows","text":"<ul> <li>Install python on your system.</li> <li>Install VS Code</li> <li>Install python and dbt extensions in VS Code</li> </ul>"},{"location":"SQL/1.0.0_dbt.html#awesome-links","title":"Awesome links","text":"<p>DBT Guides DBT Guides</p>"},{"location":"SQL/1.0.1_setup_simple_dbt_project.html","title":"Setup Project","text":""},{"location":"SQL/1.0.1_setup_simple_dbt_project.html#background","title":"Background","text":"<p>Here, I will show you how to setup a simple dbt project setup from scratch. You will just need a windows laptop for this as I have done it on my Windows machine.</p>"},{"location":"SQL/1.0.1_setup_simple_dbt_project.html#google-console-pre-requisite-setup","title":"Google console pre-requisite setup","text":""},{"location":"SQL/1.0.1_setup_simple_dbt_project.html#step-1-create-a-google-cloud-project","title":"Step 1: Create a Google Cloud Project","text":"<ol> <li>Go to the Google Cloud Console: Google Cloud Console.</li> <li>Create a New Project:</li> <li>In the top left corner, click on the Project dropdown and select New Project.</li> <li>Enter a Project Name (e.g., <code>my-dbt-project</code>).</li> <li> <p>Note down the Project ID; you\u2019ll need this later. This is typically something like <code>my-dbt-project-123456</code>.</p> </li> <li> <p>Set the Project as Active:</p> </li> <li>Click on the Project dropdown again and select your newly created project to make it active.</li> </ol>"},{"location":"SQL/1.0.1_setup_simple_dbt_project.html#step-2-enable-bigquery-api","title":"Step 2: Enable BigQuery API","text":"<ol> <li>Enable the BigQuery API:</li> <li>In the Google Cloud Console, navigate to APIs &amp; Services &gt; Library.</li> <li>Search for \"BigQuery API\" and click Enable.</li> </ol>"},{"location":"SQL/1.0.1_setup_simple_dbt_project.html#step-3-create-a-service-account","title":"Step 3: Create a Service Account","text":"<ol> <li>Create a Service Account:</li> <li>Go to IAM &amp; Admin &gt; Service Accounts in the Google Cloud Console.</li> <li>Click + CREATE SERVICE ACCOUNT.</li> <li>Enter a Service Account Name (e.g., <code>dbt-service-account</code>).</li> <li> <p>Click Create and Continue.</p> </li> <li> <p>Grant the Service Account Permissions:</p> </li> <li>Under Role, select BigQuery Admin. This gives the service account full access to BigQuery.</li> <li> <p>Click Continue and then Done.</p> </li> <li> <p>Create and Download the JSON Key:</p> </li> <li>In the Service Accounts list, find your new service account and click on it.</li> <li>Go to the Keys tab, click Add Key &gt; Create New Key.</li> <li>Select JSON and click Create.</li> <li>The JSON key file will be downloaded to your computer. This is the key file you will use in dbt.</li> </ol>"},{"location":"SQL/1.0.1_setup_simple_dbt_project.html#step-4-set-up-google-cloud-sdk-optional-but-recommended","title":"Step 4: Set Up Google Cloud SDK (Optional, but Recommended)","text":"<ol> <li>Install Google Cloud SDK:</li> <li>If you don\u2019t already have it installed, you can download it from Google Cloud SDK.</li> <li> <p>Follow the instructions to install it on your machine.</p> </li> <li> <p>Authenticate with the SDK:</p> </li> <li>Open a terminal and run:      <pre><code>gcloud auth login\n</code></pre></li> <li> <p>This will open a browser window where you can log in with your Google account.</p> </li> <li> <p>Set the Active Project:</p> </li> <li>Run the following command to set your project as the active project:      <pre><code>gcloud config set project your_project_id\n</code></pre></li> <li>Replace <code>your_project_id</code> with the actual Project ID you noted earlier.</li> </ol> <p>Reference: https://www.youtube.com/watch?v=DzxtCxi4YaA https://robust-dinosaur-2ef.notion.site/PUBLIC-Retail-Project-af398809b643495e851042fa293ffe5b</p>"},{"location":"SQL/1.0.1_setup_simple_dbt_project.html#step-5-create-a-bigquery-dataset","title":"Step 5: Create a BigQuery Dataset","text":"<ol> <li>Navigate to BigQuery in the Google Cloud Console.</li> <li>Create a Dataset:</li> <li>Click on your project name to expand it, then click Create Dataset.</li> <li>Enter a Dataset ID (e.g., <code>my_dataset</code>).</li> <li>Configure location and other settings as needed, then click Create Dataset.</li> </ol>"},{"location":"SQL/1.0.1_setup_simple_dbt_project.html#step-6-note-down-the-information","title":"Step 6: Note Down the Information","text":"<p>Make sure you have the following information handy: - Project ID: Your Google Cloud Project ID (e.g., <code>my-dbt-project-123456</code>). - Service Account JSON Key File: The path to the JSON file you downloaded (e.g., <code>/path/to/your-service-account-file.json</code>). - Dataset ID: The ID of the dataset you created (e.g., <code>my_dataset</code>).</p>"},{"location":"SQL/1.0.1_setup_simple_dbt_project.html#step-7-start-creating-your-dbt-project","title":"Step 7: Start Creating Your dbt Project","text":"<p>Now you\u2019re ready to create your dbt project using the above information. When dbt asks for: - BigQuery: Choose BigQuery as your data warehouse. - Key File Path: Provide the path to the JSON key file. - Project ID: Enter the Google Cloud Project ID. - Dataset: Enter the Dataset ID you created in BigQuery.</p>"},{"location":"SQL/1.0.1_setup_simple_dbt_project.html#step-1-install-dbt-and-bigquery-adapter","title":"Step 1: Install dbt and BigQuery Adapter","text":"<ol> <li>Just create a folder, say, dbt-bigquery, CD to it and run <code>python -m venv dbt-venv</code>. You will see a folder created <code>dbt-venv</code></li> <li>Activate the dbt-venv by running <code>dbt-venv\\Scripts\\activate</code></li> <li>Run the following commands to install dbt and the BigQuery adapter:    <pre><code>pip install dbt-core dbt-bigquery\n</code></pre>    This will install dbt and the necessary adapter to connect to BigQuery.</li> </ol>"},{"location":"SQL/1.0.1_setup_simple_dbt_project.html#step-3-initialize-a-dbt-project","title":"Step 3: Initialize a dbt Project","text":"<ol> <li> <p>In your terminal inside Visual Studio Code, run:    <pre><code>dbt init your_project_name\n</code></pre>    Replace <code>your_project_name</code> with the name you want for your dbt project.</p> </li> <li> <p>Navigate to the newly created project directory:    <pre><code>cd your_project_name\n</code></pre></p> </li> </ol>"},{"location":"SQL/1.0.1_setup_simple_dbt_project.html#step-4-configure-your-dbt-project-for-bigquery","title":"Step 4: Configure Your dbt Project for BigQuery","text":"<ol> <li>Open the <code>profiles.yml</code> file:</li> <li>If you don't have one, you can create it in <code>~/.dbt/</code>.</li> <li>The file should look like this:      <pre><code>your_project_name:\n  target: dev\n  outputs:\n    dev:\n      type: bigquery\n      method: service-account\n      project: your-gcp-project-id\n      dataset: your_dataset_name\n      threads: 1\n      keyfile: /path/to/your-service-account-file.json\n      timeout_seconds: 300\n      location: your_location\n</code></pre></li> <li> <p>Replace <code>your_project_name</code>, <code>your-gcp-project-id</code>, <code>your_dataset_name</code>, and <code>/path/to/your-service-account-file.json</code> with your actual project details.</p> </li> <li> <p>Save the file.</p> </li> </ol>"},{"location":"SQL/1.0.1_setup_simple_dbt_project.html#step-5-create-models-and-start-developing","title":"Step 5: Create Models and Start Developing","text":"<ol> <li>Open the <code>models</code> folder in your dbt project.</li> <li>Create a new <code>.sql</code> file for each model you want to define. For example:    <pre><code>-- my_first_model.sql\nSELECT *\nFROM `your_gcp_project.your_dataset.your_table`\n</code></pre>    Replace <code>your_gcp_project</code>, <code>your_dataset</code>, and <code>your_table</code> with the actual names.</li> </ol>"},{"location":"SQL/1.0.1_setup_simple_dbt_project.html#step-6-run-your-dbt-models","title":"Step 6: Run Your dbt Models","text":"<ol> <li>In the terminal, inside your dbt project directory, run:    <pre><code>dbt run\n</code></pre>    This will execute your models and create the tables or views in BigQuery.</li> </ol>"},{"location":"SQL/1.0.1_setup_simple_dbt_project.html#step-7-check-your-work","title":"Step 7: Check Your Work","text":"<ol> <li>Check BigQuery to see if your tables or views have been created as expected.</li> </ol>"},{"location":"SQL/1.0.1_setup_simple_dbt_project.html#step-8-version-control-optional","title":"Step 8: Version Control (Optional)","text":"<ol> <li>Initialize a Git repository in your project directory:    <pre><code>git init\n</code></pre></li> <li>Add and commit your files:    <pre><code>git add .\ngit commit -m \"Initial commit\"\n</code></pre></li> </ol>"},{"location":"SQL/1.0.1_setup_simple_dbt_project.html#final-tips","title":"Final Tips","text":"<ul> <li>Always test your models by running <code>dbt run</code> regularly to ensure everything is working.</li> <li>Use dbt docs to generate documentation with <code>dbt docs generate</code>.</li> </ul> <p>That's it! You've set up a dbt project for BigQuery using Visual Studio Code. Happy developing!</p>"},{"location":"SQL/InputAndOutputProperties.html","title":"Understanding External Columns and Output Columns in SSIS","text":"<p>In SSIS (SQL Server Integration Services), \"External Columns\" and \"Output Columns\" play crucial roles in managing how data moves through the data flow. </p> <p></p>"},{"location":"SQL/InputAndOutputProperties.html#external-columns","title":"External Columns","text":"<ul> <li>What It Is: External columns represent how the data appears in your source file (like a CSV file). These columns reflect the structure and data types as defined in the source.</li> <li>Why It Matters: They tell SSIS what kind of data is in each column, such as whether it\u2019s text, a number, or a date, and how long the text can be.</li> <li>Example: If your CSV file has a column called <code>coverage_details</code> and the longest text in this column is 200 characters, you need to set the length of this column to 200 in External Columns. This tells SSIS to expect up to 200 characters of text in that column.</li> </ul>"},{"location":"SQL/InputAndOutputProperties.html#output-columns","title":"Output Columns","text":"<ul> <li>What It Is: Output columns are the columns that SSIS will pass on to the next step in your data flow. These are the columns that will be used by other components like Data Conversion or SQL Server Destination.</li> <li>Why It Matters: They define how the data is passed from the Flat File Source to other components in your data flow. The data types and lengths must match or be compatible with the next step\u2019s requirements.</li> <li>Example: The <code>coverage_details</code> column in Output Columns should be set to the same length (200 characters in this case) as in External Columns. This ensures that the data from the source file is correctly transferred to the next component without errors.</li> </ul>"},{"location":"SQL/InputAndOutputProperties.html#where-to-find-external-columns-and-output-columns","title":"Where to Find External Columns and Output Columns","text":"<p>External Columns and Output Columns is mainly found in components that handle data sources and transformations in the Data Flow task (like Flat File Source, OLE DB Source, Excel Source, or Data Conversion Transformation).</p> <p>How to See Them: 1. Double-click the component in the Data Flow task. 2. Click on the \"Advanced Editor\u2026\" button. 3. Go to the \"Input and Output Properties\" tab. 4. You will see sections for \"External Columns\" and \"Output Columns\".</p> <p>Example for Flat File Source: </p> <p>Example for OLE DB Source: </p> <p>This way, you can easily find and manage External Columns and Output Columns in SSIS to ensure your data flows correctly.</p>"},{"location":"SQL/InputAndOutputProperties.html#common-errors-related-to-external-columns-and-output-columns","title":"Common errors related to External Columns and Output Columns","text":"<p>If External Columns and Output Columns are not set correctly, various types of errors can occur:</p> <ol> <li> <p>Truncation Errors If <code>coverage_details</code> is set to 50 characters in External Columns but the actual data is 200 characters, SSIS will try to fit long text into a smaller space, causing a truncation error. The error message will say, \"Text was truncated or one or more characters had no match in the target code page.\"</p> </li> <li> <p>Data Conversion Errors If <code>coverage_details</code> is defined as a numeric column in SSIS but the source file contains text data, a conversion error will occur. The error message will say, \"Data conversion failed. The data conversion for column 'coverage_details' returned status value 2 and status text 'The value could not be converted because of a potential loss of data.'\"</p> </li> <li> <p>Mapping Errors happen when the source column and destination column are not compatible in terms of data types or lengths. For example, if <code>coverage_details</code> is set to 200 characters in SSIS but the SQL Server table defines it as 100 characters, SSIS will not be able to map the data correctly. The error message will say, \"Cannot convert between Unicode and non-Unicode string data types.\"</p> </li> </ol>"},{"location":"SQL/MSSQL_Versions.html","title":"SQL Server Versions","text":"Product Release Date RTM Version Support End Date Notes SQL Server 2022 2022-11 16.0.1000.6 2028-01 No more Service Packs SQL Server 2019 2019-11 15.0.2000.5 2025-01 No more Service Packs SQL Server 2017 2017-10 14.0.1000.169 2022-10 No more Service Packs SQL Server 2016 2016-06 13.0.1601.5 2021-07 SQL Server 2014 2014-04 12.0.2000.8 2019-07 Obsolete versions \u2013 out of support SQL Server 2012 2012-03 11.0.2100.60 2017-07 Obsolete versions \u2013 out of support SQL Server 2008 R2 2010-04 10.50.1600.1 2014-07 Obsolete versions \u2013 out of support SQL Server 2008 2008-08 10.0.1600.22 2014-07 Obsolete versions \u2013 out of support SQL Server 2005 2005-11 9.0.1399.06 2011-04 Obsolete versions \u2013 out of support SQL Server 2000 2000-11 8.0.194 2008-04 Obsolete versions \u2013 out of support SQL Server 7.0 1998-11 7.0.623 2005-12 Obsolete versions \u2013 out of support SQL Server 6.5 1996-06 6.50.201 2002-01 Obsolete versions \u2013 out of support SQL Server 6.0 1995-06 6.00.121 1999-03 Obsolete versions \u2013 out of support"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html","title":"Project 1 - ETL Flat Files to MSSQL","text":"<p>This project is a small version of a real-time project often used in the banking and insurance sectors. Here, data is received from different sources as flat files (like .csv or .xml). In banking, this file is sometimes called a control file. The file may come from Dynamics 365,right-fax servers, through web services, etc. This data needs to be ingested into SQL Server. The data comes from various branches or field offices throughout the day, and the workflow needs to run at a regular time each day to process all the files.</p> <p>This kind of ETL is best suited for handling large volumes of data in batch mode, such as end-of-day processing for banks etc.</p>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#project-overview","title":"Project Overview","text":"<ul> <li> <p>Data Source: A folder containing .csv files with the following data:   <pre><code>policy_number,policyholder_name,insured_name,policy_type,effective_date,expiration_date,premium_amount,beneficiary_name,beneficiary_contact,agent_name,agent_contact,coverage_details,endorsements_riders,underwriter_name\n0_1722756562687,Pamela Bennett,Steven Dunn,Home,2021-08-24,2021-02-03,83572,Dana Hampton,366-752-7514x4594,Troy Young,+1-818-775-4416x2930,Set like go entire ground.,Drive generation major.,Candice Williams\n</code></pre></p> </li> <li> <p>File Naming Convention: The CSV file names will be in the format <code>policy_issuance_file_x.csv</code> where x is 1, 2, 3...</p> </li> <li> <p>SSIS Workflow: The workflow will contain a Foreach Loop container. Inside the Foreach Loop, there will be a Data Flow Task. The Data Flow Task will contain: Flat File Source -&gt; Data Conversion -&gt; OLE DB Destination.</p> </li> <li> <p>Setup Required: You can develop this easily on a Windows machine. You will need MSSQL server. Visual Studio(Community edition will do). SSDT(to create the SSIS package in VS).</p> </li> </ul>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#step-1-create-the-required-database-in-mssql","title":"Step 1: Create the Required Database in MSSQL","text":"<p>Open SQL Server Management Studio (SSMS) and run the following SQL script to create the database and table.</p> <pre><code>-- Create the database\nCREATE DATABASE InsuranceDB;\nGO\n\n-- Use the newly created database\nUSE InsuranceDB;\nGO\n\n-- Create the table\nCREATE TABLE PolicyIssuance (\n    policy_number NVARCHAR(50) PRIMARY KEY,\n    policyholder_name NVARCHAR(100),\n    insured_name NVARCHAR(100),\n    policy_type NVARCHAR(50),\n    effective_date DATE,\n    expiration_date DATE,\n    premium_amount DECIMAL(10, 2),\n    beneficiary_name NVARCHAR(100),\n    beneficiary_contact NVARCHAR(50),\n    agent_name NVARCHAR(100),\n    agent_contact NVARCHAR(50),\n    coverage_details NVARCHAR(255),\n    endorsements_riders NVARCHAR(255),\n    underwriter_name NVARCHAR(100)\n);\nGO\n\n-- Verify the table creation\nSELECT * FROM PolicyIssuance;\nGO\n</code></pre> <p>To check the details of the table created, use:</p> <pre><code>sp_help PolicyIssuance;\n</code></pre>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#step-2-create-the-ssis-package","title":"Step 2: Create the SSIS Package","text":""},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#configure-the-foreach-loop-container","title":"Configure the Foreach Loop Container","text":"<ul> <li>Open SQL Server Data Tools (SSDT) or Visual Studio with SSIS extension installed.</li> <li>Create a new SSIS project.</li> <li>Create a variable <code>var_FilePath</code> (String) to store the full path of the current file being processed.</li> <li>Drag a Foreach Loop Container onto the Control Flow tab.</li> <li>Double-click the Foreach Loop Container to open the editor.</li> <li> <p>In the Collection tab:</p> <ul> <li>Set Enumerator to Foreach File Enumerator.</li> <li>Set Folder to <code>C:\\Users\\dwaip\\Desktop\\sample_csvs\\</code>.</li> <li>Set Files to <code>policy_issuance_file_*.csv</code> to process all files matching this pattern.</li> </ul> <p></p> </li> <li> <p>In the Variable Mappings tab:</p> <ul> <li>Add the <code>FilePath</code> variable and set the Index to 0.</li> </ul> <p></p> </li> <li> <p>Drag a Data Flow Task into the Foreach Loop Container.</p> </li> </ul>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#configure-the-data-flow-task","title":"Configure the Data Flow Task","text":"<ul> <li>Double-click the Data Flow Task to open the Data Flow tab.</li> <li>Add a Flat File Source to the Data Flow.</li> <li>Double-click the Flat File Source to configure it.</li> </ul>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#configure-flat-file-connection-manager","title":"Configure Flat File Connection Manager","text":"<ul> <li>In the Flat File Source Editor, click New to create a new Flat File Connection Manager.</li> <li>Set up the connection manager using any one of the sample CSV files (e.g., <code>policy_issuance_file_1.csv</code>) to configure the columns.</li> <li>Once configured, click OK.</li> </ul>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#set-connection-string-to-dynamic","title":"Set Connection String to Dynamic","text":"<ul> <li> <p>In the Flat File Connection Manager:</p> <ul> <li>Go to Properties and find Expressions.</li> <li>Click the ellipsis (<code>...</code>) next to Expressions.</li> <li>In the Property dropdown, select ConnectionString.</li> <li>Set the expression to use the <code>var_FilePath</code> variable: <code>@[User::var_FilePath]</code>.</li> </ul> <p></p> <p>If the setting is correct you will see this:</p> <p></p> </li> </ul>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#add-data-conversion-transformation","title":"Add Data Conversion Transformation","text":"<ul> <li>Add a Data Conversion Transformation.</li> <li>Configure it to convert the necessary columns :<ul> <li><code>policyholder_name</code> (original) converted to <code>Copy of policyholder_name</code></li> <li><code>insured_name</code> (original) converted to <code>Copy of insured_name</code></li> <li>Other columns..</li> </ul> </li> </ul>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#add-sql-server-destination","title":"Add SQL Server Destination","text":"<ul> <li>Add an OLE DB Destination to the Data Flow.</li> <li>Connect the Data Conversion Transformation to the OLE DB Destination.</li> <li>Configure the OLE DB Destination to map the columns correctly to the <code>PolicyIssuance</code> table.</li> </ul>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#map-columns-in-ole-db-destination","title":"Map Columns in OLE DB Destination","text":"<ul> <li>Open the SQL Server Destination Editor.</li> <li>Go to the Mappings Tab.</li> <li>Map the converted columns:<ul> <li><code>Copy of policyholder_name</code> to <code>policyholder_name</code></li> <li><code>Copy of insured_name</code> to <code>insured_name</code></li> <li><code>Copy of beneficiary_name</code> to <code>beneficiary_name</code></li> <li>Similarly, map other converted columns as needed.</li> </ul> </li> </ul>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#test-run-the-package","title":"Test Run the Package","text":"<ul> <li>Save the package.</li> <li>Right-click the package and select Execute Package.</li> <li>Check the MSSQL table to verify the data has been loaded correctly.</li> </ul>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#step-3-deploy-and-schedule-the-ssis-package-in-mssql-server","title":"Step 3: Deploy and Schedule the SSIS Package in MSSQL Server","text":"<p>To deploy your SSIS package and schedule it to run once a day, follow these steps:</p> <p></p>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#deploy-the-ssis-package","title":"Deploy the SSIS Package","text":"<ul> <li>Deploy the Package:</li> <li>Right-click the project in the Solution Explorer and select \"Build\" to ensure there are no errors.</li> <li>Right-click the project and select \"Deploy\".</li> </ul> <ul> <li> <p>The Integration Services Deployment Wizard will open. Follow the steps:</p> <ul> <li>Select SSIS in SQL Server as the deployment target.</li> </ul> <p></p> <ul> <li>Provide the server name and path where the package will be deployed.</li> </ul> <p></p> <ul> <li>Finally, click Deploy to deploy the package.</li> </ul> <p></p> <ul> <li>If everything goes well, your package will be deployed.</li> </ul> <p></p> </li> </ul>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#schedule-the-package-using-sql-server-agent","title":"Schedule the Package using SQL Server Agent","text":""},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#create-a-new-sql-server-agent-job","title":"Create a New SQL Server Agent Job","text":"<ul> <li>Connect to your SQL Server instance. Expand the SQL Server Agent node. Right-click Jobs and select New Job.</li> </ul> <ul> <li>In the \"New Job\" window, provide a name for the job (e.g., \"Daily SSIS Package Run\").</li> </ul>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#set-job-steps","title":"Set Job Steps","text":"<ul> <li>In the Steps page, click New to create a new step.</li> <li>Set the Step name (e.g., \"Run SSIS Package\").</li> <li>Set Type to SQL Server Integration Services Package.</li> <li>Set \"Package source\" to \"SSIS Catalog\".</li> <li>Select the server and the package you deployed.</li> <li>Click \"OK\" to save the step.</li> </ul>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#set-job-schedule","title":"Set Job Schedule","text":"<ul> <li>In the \"Schedules\" page, click \"New\" to create a new schedule.</li> <li>Provide a name for the schedule (e.g., \"Daily Schedule\").</li> <li>Set the frequency to \"Daily\".</li> <li>Set the time of day you want the package to run.</li> <li>Click \"OK\" to save the schedule.</li> </ul>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#set-notifications-optional","title":"Set Notifications (Optional)","text":"<ul> <li>In the \"Notifications\" page, set up notifications to receive alerts in case of job failure or success.</li> </ul>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#save-the-job","title":"Save the Job","text":"<ul> <li>Click \"OK\" to save the job.</li> </ul>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#common-errors-and-their-solutions","title":"Common Errors and Their Solutions","text":"<p>The most common errors will be related to size mismatch and type mismatch. There will also be Unicode-non-Unicode type mismatch issues.</p> <p>For text files, choose Unicode string [DT_WSTR] as the output which can be mapped to MSSQL server NVARCHAR variables.</p> <p>I was able to ingest data from .csv files correctly using the DT_STR format. This means the data can come in as DT_STR, but it needs to go into MSSQL as DT_WSTR.</p> <p>The <code>coverage_details</code> field was the largest in the .csv file and caused the most errors. So, I will explain the errors using this field as an example.</p>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#input-and-output-properties-tab-common-place-for-error-troubleshooting","title":"Input and Output Properties Tab - Common place for error troubleshooting","text":"<p>Go to the Input and Output Properties in the Advanced Editor (right-click the step).</p> <p></p>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#truncation-errors","title":"Truncation Errors","text":"<p>If <code>coverage_details</code> is set to 50 characters in External Columns but the actual data is 200 characters, SSIS will try to fit long text into a smaller space, causing a truncation error. The error message will say, \"Text was truncated or one or more characters had no match in the target code page.\"</p>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#data-conversion-errors","title":"Data Conversion Errors","text":"<p>If <code>coverage_details</code> is defined as a numeric column in SSIS but the source file contains text data, a conversion error will occur. The error message will say, \"Data conversion failed. The data conversion for column 'coverage_details' returned status value 2 and status text 'The value could not be converted because of a potential loss of data.'\"</p> <p></p>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#mapping-errors","title":"Mapping Errors","text":"<p>These happen when the source column and destination column are not compatible in terms of data types or lengths. For example, if <code>coverage_details</code> is set to 200 characters in SSIS but the SQL Server table defines it as 100 characters, SSIS will not be able to map the data correctly. The error message will say, \"Cannot convert between Unicode and non-Unicode string data types.\"</p>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#ssis-toolbox-not-visible","title":"SSIS toolbox not visible","text":"<p>In the newer version of VS sometimes the SSIS Toolbox might be absent. You just have to right-click and select SSIS Toolbox. That's it.</p> <p></p>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#ssisdb-not-found","title":"SSISDB not found","text":"<p>An Integration Services catalog (SSISDB) was not found on this server instance. To deploy a project to this server, you must create the SSISDB catalog. Open the Create Catalog dialog box from the Integration Services Catalogs node.</p> <p></p>"},{"location":"SQL/Project_1-ETL-CSV-MSSQL.html#resolution","title":"Resolution","text":"<ul> <li>Open SSMS and connect to your SQL Server instance.</li> <li>In the Object Explorer, expand the \"Integration Services Catalogs\" node.</li> <li>Right-click and select \"Create Catalog\u2026\".</li> </ul> <ul> <li>In the \"Create Catalog\" dialog box, check the \"Enable CLR Integration\" checkbox.</li> </ul> <ul> <li>Enter a password for the SSISDB database master key. This password will be used to secure the catalog.</li> <li>Click \"OK\" to create the catalog. You will see a SSISDB created inside the Integration Services Catalogs folder. Now, you can deploy your projects here.</li> </ul>"},{"location":"SQL/Project_2-UsingWebServicesInSSIS.html","title":"Project 2","text":""},{"location":"SQL/Project_2-UsingWebServicesInSSIS.html#how-to-use-web-services-in-ssis-script-task","title":"How to Use Web Services in SSIS Script Task","text":""},{"location":"SQL/Project_2-UsingWebServicesInSSIS.html#introduction","title":"Introduction","text":"<p>Using web services in SSIS (SQL Server Integration Services) can be quite helpful when you need to get data from an external system. Here\u2019s a simple guide on how to do this using the Script Task in SSIS.</p>"},{"location":"SQL/Project_2-UsingWebServicesInSSIS.html#steps-to-use-web-services-in-ssis-script-task","title":"Steps to Use Web Services in SSIS Script Task","text":"<ol> <li> <p>Create a Proxy Class for the Web Service</p> </li> <li> <p>First, you need to create a Proxy Class using WSDL (Web Services Description Language). This will help your SSIS package understand how to communicate with the web service.</p> </li> <li> <p>Open the .NET command prompt and run this command:</p> <pre><code>wsdl /language:VB http://localhost:8080/WebService/Service1.asmx?WSDL /out:C:\\WebService1.vb\n</code></pre> </li> <li> <p>This command creates a file called <code>WebService1.vb</code> which is your Proxy Class.</p> </li> </ol> <p>By default, it uses C Sharp if you don\u2019t specify the language. In SSIS Script Task 2005, you can only use Visual Basic .NET, so generate the Proxy Class with VB if you are using SSIS 2005. For those unfamiliar with WSDL.EXE, it's a tool to generate code for XML Web Services and clients using ASP.NET from WSDL files, XSD schemas, and .discomap documents. It can be used with disco.exe.</p> <ol> <li> <p>Publish Your Web Service to IIS</p> </li> <li> <p>In your web service project, right-click and select \"Publish\".</p> </li> <li> <p>Choose the web site where you want to publish your web service.</p> </li> <li> <p>Add the Proxy Class to Your SSIS Project</p> </li> <li> <p>Open your SSIS package.</p> </li> <li>Drag and drop a Script Task onto the Control Flow area.</li> <li>Right-click on the Script Task and select \"Edit\".</li> <li>In the Script Task Editor, go to the \"Script\" section and click on \"Design Script\".</li> <li>In the Script Editor, open Project Explorer (if it\u2019s not visible, go to View -&gt; Project Explorer).</li> <li> <p>Add your Proxy Class (<code>WebService1.vb</code>) to the project:</p> <ul> <li>Right-click on your project in Project Explorer.</li> <li>Select \"Add\" -&gt; \"Existing Item\" and add the <code>WebService1.vb</code> file.</li> </ul> </li> <li> <p>Edit the Script in the Script Task</p> </li> <li> <p>Ensure that you include necessary namespaces in your script. Add these at the top of your Proxy Class file:</p> <pre><code>Imports System.Web.Services\nImports System.Xml.Serialization\n</code></pre> </li> <li> <p>In the <code>ScriptMain</code> class, create an object of the Proxy Class and call the web service methods. Here\u2019s an example:</p> <pre><code>Public Sub Main()\n    Dim ws As New Service1\n    MsgBox(\"Square of 2: \" &amp; ws.Square(2))\n    Dts.TaskResult = Dts.Results.Success\nEnd Sub\n</code></pre> </li> <li> <p>Handle Authentication Errors</p> </li> <li> <p>If you get an HTTP 401 Unauthorized error, add credentials to your web service call. Modify your <code>Main</code> method like this:</p> <pre><code>Public Sub Main()\n    Dim ws As New Service1\n    ws.Credentials = New System.Net.NetworkCredential(\"username\", \"password\", \"domain\")\n    MsgBox(\"Square of 2: \" &amp; ws.Square(2))\n    Dts.TaskResult = Dts.Results.Success\nEnd Sub\n</code></pre> </li> </ol>"},{"location":"SQL/SQL.html","title":"Essential SparkSQL commands","text":"Command Description Example <code>SET -v</code> Shows all Spark SQL settings. <code>SET -v;</code> <code>SHOW DATABASES</code> Lists all databases. <code>SHOW DATABASES;</code> <code>CREATE DATABASE</code> Creates a new database. <code>CREATE DATABASE db_name;</code> <code>DROP DATABASE</code> Drops an existing database. <code>DROP DATABASE db_name;</code> <code>USE</code> Sets the current database. <code>USE db_name;</code> <code>SHOW TABLES</code> Lists all tables in the current database. <code>SHOW TABLES;</code> <code>DESCRIBE TABLE</code> Provides the schema of a table. <code>DESCRIBE TABLE table_name;</code> <code>DESCRIBE EXTENDED</code> Provides detailed information about a table, including metadata. <code>DESCRIBE EXTENDED table_name;</code> <code>CREATE TABLE</code> Creates a new table. <code>CREATE TABLE table_name (col1 INT, ...);</code> <code>CREATE EXTERNAL TABLE</code> Creates an external table. <code>CREATE EXTERNAL TABLE table_name ...;</code> <code>DROP TABLE</code> Drops an existing table. <code>DROP TABLE table_name;</code> <code>INSERT INTO</code> Inserts data into a table. <code>INSERT INTO table_name VALUES (...);</code> <code>SELECT</code> Queries data from tables. <code>SELECT * FROM table_name;</code> <code>CREATE VIEW</code> Creates a view based on a query. <code>CREATE VIEW view_name AS SELECT ...;</code> <code>DROP VIEW</code> Drops an existing view. <code>DROP VIEW view_name;</code> <code>CACHE TABLE</code> Caches a table in memory. <code>CACHE TABLE table_name;</code> <code>UNCACHE TABLE</code> Removes a table from the in-memory cache. <code>UNCACHE TABLE table_name;</code> <code>SHOW COLUMNS</code> Lists all columns of a table. <code>SHOW COLUMNS FROM table_name;</code> <code>ALTER TABLE</code> Changes the structure of an existing table. <code>ALTER TABLE table_name ADD COLUMNS (...);</code> <code>TRUNCATE TABLE</code> Removes all rows from a table without deleting the table. <code>TRUNCATE TABLE table_name;</code> <code>MSCK REPAIR TABLE</code> Recovers partitions of a table. <code>MSCK REPAIR TABLE table_name;</code> <code>SHOW PARTITIONS</code> Lists all partitions of a table. <code>SHOW PARTITIONS table_name;</code> <code>EXPLAIN</code> Provides a detailed execution plan for a query. <code>EXPLAIN SELECT * FROM table_name;</code> <code>SHOW CREATE TABLE</code> Displays the <code>CREATE TABLE</code> statement for an existing table. <code>SHOW CREATE TABLE table_name;</code> <code>LOAD DATA</code> Loads data into a table from a file. <code>LOAD DATA INPATH 'path' INTO TABLE ...;</code> <code>SET</code> Sets a Spark SQL configuration property. <code>SET property_name=value;</code> <code>SHOW FUNCTIONS</code> Lists all functions available in the Spark SQL environment. <code>SHOW FUNCTIONS;</code> <code>DESCRIBE FUNCTION</code> Provides information about a function. <code>DESCRIBE FUNCTION function_name;</code>"},{"location":"SQL/SQL.html#openrowset-the-powerful-transact-sql-for-data-engineering","title":"OPENROWSET - The Powerful Transact-SQL for Data Engineering.","text":""},{"location":"SQL/SQL.html#what-is-openrowset","title":"What is OPENROWSET?","text":"<p><code>OPENROWSET</code> is a Transact-SQL function. Using this, you can create a select statement from external data sources (CSV, Parquet, JSON, Delta, ADLS, Blob) as if it were a local database. You don't need to create a table or linked service to run this. Hence, it's fast and requires fewer lines of code.</p> <p>It is used in SQL Server, Azure SQL DB, Synapse Analytics, Fabric, and Databricks.</p> <p> For `OPENROWSET` to work, you need to ensure that the Synapse workspace or the query has the appropriate authentication to access the external data source. </p>"},{"location":"SQL/SQL.html#practical-scenarios-where-openrowset-is-preferred","title":"Practical scenarios where OPENROWSET is preferred","text":"<ol> <li>Ad-Hoc Data Retrieval:</li> <li>Scenario: A business analyst needs to quickly retrieve data from an Excel file for an ad-hoc report.</li> <li> <p>Reason: OPENROWSET allows for on-the-fly access to the Excel file without the need to import it into SQL Server first, providing immediate data retrieval for analysis.</p> </li> <li> <p>Cross-Database Queries:</p> </li> <li>Scenario: A developer needs to run a query that joins tables from different SQL Server instances.</li> <li> <p>Reason: OPENROWSET can be used to run distributed queries across different servers, avoiding the need to set up linked servers which can be more complex and require additional configuration and permissions.</p> </li> <li> <p>Accessing External Data Sources:</p> </li> <li>Scenario: A company needs to integrate data from a remote Oracle database for a one-time data migration task.</li> <li> <p>Reason: OPENROWSET provides a quick way to query data from the Oracle database using an OLE DB provider, bypassing the need for more permanent and complex solutions like linked servers or SSIS packages.</p> </li> <li> <p>Temporary Data Access:</p> </li> <li>Scenario: During a data validation process, a data engineer needs to access data from a CSV file provided by a client.</li> <li> <p>Reason: OPENROWSET allows the engineer to read the CSV file directly into a SQL query, facilitating temporary data access without the overhead of creating permanent database tables or ETL processes.</p> </li> <li> <p>Data Import for Development and Testing:</p> </li> <li>Scenario: Developers need to import data from various formats (e.g., Excel, CSV) into a development environment for testing purposes.</li> <li>Reason: OPENROWSET can be used to import data directly into the development environment without the need for pre-configured data import processes, speeding up the testing and development cycles.</li> </ol>"},{"location":"SQL/SQL.html#syntax","title":"Syntax","text":"<pre><code>-- General Syntax\nOPENROWSET(\n    BULK 'file_path',\n    FORMAT = 'file_format',\n    [DATA_SOURCE = 'data_source_name'],\n    [WITH ( column_name column_type [ 'column_ordinal' | 'json_path' ] )]\n) AS alias_name\n\n-- Example for CSV\nSELECT * \nFROM OPENROWSET(\n    BULK 'https://storageaccount.blob.core.windows.net/container/file.csv',\n    FORMAT = 'CSV',\n    FIRSTROW = 2,\n    FIELDTERMINATOR = ',',\n    ROWTERMINATOR = '\\n'\n) AS [result]\n\n-- Example for Parquet\nSELECT * \nFROM OPENROWSET(\n    BULK 'https://storageaccount.blob.core.windows.net/container/file.parquet',\n    FORMAT = 'PARQUET'\n) AS [result]\n\n-- Example for Delta Lake\nSELECT * \nFROM OPENROWSET(\n    BULK 'https://storageaccount.blob.core.windows.net/container/file.delta',\n    FORMAT = 'DELTA'\n) AS [result]\n</code></pre>"},{"location":"SQL/SQL.html#key-options","title":"Key Options","text":"<ul> <li>BULK: Specifies the path to the file(s).</li> <li>FORMAT: Specifies the file format (<code>CSV</code>, <code>PARQUET</code>, or <code>DELTA</code>).</li> <li>DATA_SOURCE: (Optional) Specifies the external data source.</li> <li>WITH: Defines the schema of the columns to be read.</li> </ul>"},{"location":"SQL/SQL.html#options-for-csv","title":"Options for CSV","text":"<ul> <li>FIELDTERMINATOR: Character used to separate fields (default is <code>,</code>).</li> <li>ROWTERMINATOR: Character used to separate rows (default is <code>\\n</code>).</li> <li>FIRSTROW: Specifies the first row to read (default is 1).</li> <li>HEADER_ROW: Indicates if the CSV file contains a header row (<code>TRUE</code> or <code>FALSE</code>).</li> </ul>"},{"location":"SQL/SQL.html#examples","title":"Examples","text":"<ol> <li> <p>Reading a CSV file without specifying a schema: <pre><code>SELECT * \nFROM OPENROWSET(\n    BULK 'https://storageaccount.blob.core.windows.net/container/data.csv',\n    FORMAT = 'CSV'\n) AS [data]\n</code></pre></p> </li> <li> <p>Reading specific columns from a CSV file: <pre><code>SELECT * \nFROM OPENROWSET(\n    BULK 'https://storageaccount.blob.core.windows.net/container/data.csv',\n    FORMAT = 'CSV',\n    FIELDTERMINATOR = ',',\n    ROWTERMINATOR = '\\n',\n    FIRSTROW = 2\n)\nWITH (\n    col1 VARCHAR(50),\n    col2 INT,\n    col3 DATE\n) AS [data]\n</code></pre></p> </li> <li> <p>Reading a Parquet file: <pre><code>SELECT * \nFROM OPENROWSET(\n    BULK 'https://storageaccount.blob.core.windows.net/container/data.parquet',\n    FORMAT = 'PARQUET'\n) AS [data]\n</code></pre></p> </li> <li> <p>Reading a Delta Lake file: <pre><code>SELECT * \nFROM OPENROWSET(\n    BULK 'https://storageaccount.blob.core.windows.net/container/data.delta',\n    FORMAT = 'DELTA'\n) AS [data]\n</code></pre></p> </li> </ol>"},{"location":"SQL/SQL.html#what-other-options-we-have","title":"What other options we have?","text":"Alternative Usage Scenario Efficiency Microsoft Recommendation BULK INSERT Loading large volumes of data from a file into a SQL Server table. High for large files Use when importing large data volumes directly. PolyBase Querying external data in Hadoop, Azure Blob Storage, and Azure Data Lake. High Use for big data environments and distributed systems. EXTERNAL TABLE Accessing external data sources seamlessly as if they are regular SQL tables. Moderate to High Recommended for persistent access to external data. Linked Servers Connecting SQL Server to other data sources, like another SQL Server or an OLE DB data source. Varies Use for diverse data sources and quick data joins. Databricks Reading data from various sources using Spark SQL (not native to Databricks but works with Synapse). High for big data Use for Spark SQL integrations and large-scale analytics."},{"location":"SQL/SQL.html#approx_count_distinct-when-countdistinct-colname-runs-forever","title":"APPROX_COUNT_DISTINCT when COUNT(DISTINCT ColName) runs forever","text":"<p>Imagine your table has 1 trillion rows. You want to count distinct customer names. Do you think <code>COUNT(DISTINCT CustomerName)</code> will show you results in your lifetime?</p> <p>No, for such scenarios, use <code>APPROX_COUNT_DISTINCT(ColName)</code>. It will give you around 97% accuracy.</p>"},{"location":"SQL/SQL.html#how-to-run-it","title":"How to run it?","text":"<p>Pretty simple and straightforward.</p> <p>Normal Count <pre><code>SELECT COUNT(DISTINCT CustomerName) FROM Customers;\n</code></pre></p> <p>Approximate Count</p> <pre><code>SELECT APPROX_COUNT_DISTINCT(CustomerName) FROM Customers; \n</code></pre>"},{"location":"SQL/SQL.html#some-fun-facts-about-this-t-sql-function","title":"Some Fun facts about this T-SQL function","text":"<p>In MS SQL Server <code>APPROX_COUNT_DISTINCT</code> it first came in SQL Server 2019. It is available in Synapse SQL. These other brands support this function:</p> <ol> <li>Google BigQuery</li> <li>Amazon Redshift: It offers the <code>APPROXIMATE COUNT(DISTINCT column)</code> function, which serves a similar purpose.</li> <li>PostgreSQL: The <code>approx_count_distinct</code> extension is available through the <code>HLL (HyperLogLog)</code> extension, which allows for approximate distinct counting.</li> <li>Apache Spark: The <code>approx_count_distinct</code> function is available in Spark SQL, which uses HyperLogLog for approximate counting.</li> </ol>"},{"location":"SQL/SQL.html#practice-joins-with-mssql-adventureworkslt2016","title":"Practice Joins with MSSQL AdventureWorksLT2016","text":"<p>Download MSSQL Sample database AdventureWorksLT2016.bak and restore it. It will give you a full enviornment.</p> <p>https://github.com/Microsoft/sql-server-samples/releases/download/adventureworks/AdventureWorksLT2016.bak</p>"},{"location":"SQL/SQL.html#table-descriptions","title":"Table Descriptions","text":"<p>Here is a brief description of some of the key tables in the AdventureWorksLT2016 database:</p> <ul> <li>SalesLT.Customer: Contains customer information such as CustomerID, FirstName, LastName, EmailAddress, etc.</li> <li>SalesLT.SalesOrderHeader: Contains sales order information including SalesOrderID, OrderDate, CustomerID, TotalDue, etc.</li> <li>SalesLT.SalesOrderDetail: Contains details of each sales order, such as SalesOrderID, ProductID, OrderQty, UnitPrice, etc.</li> <li>SalesLT.Product: Contains product information including ProductID, Name, ProductNumber, Color, ListPrice, etc.</li> <li>SalesLT.ProductCategory: Contains information about product categories including ProductCategoryID and Name.</li> <li>SalesLT.ProductModel: Contains information about product models including ProductModelID and Name.</li> <li>SalesLT.Address: Contains address information including AddressID, AddressLine1, City, StateProvince, etc.</li> <li>SalesLT.CustomerAddress: A junction table linking customers to addresses.</li> </ul>"},{"location":"SQL/SQL.html#questions","title":"Questions","text":""},{"location":"SQL/SQL.html#question-1-list-customers-with-their-orders","title":"Question 1: List Customers with Their Orders","text":"<p>Question: Retrieve a list of customers along with their order details. Include the customer's first name, last name, and order date. Only include customers who have placed at least one order.</p> <p>Hint:  - Use <code>SalesLT.Customer</code> to get customer information. - Use <code>SalesLT.SalesOrderHeader</code> to get order information. - Use an INNER JOIN to connect these tables.</p>"},{"location":"SQL/SQL.html#question-2-list-products-and-their-categories","title":"Question 2: List Products and Their Categories","text":"<p>Question: Retrieve a list of products along with their category names. Include all products, even those that do not belong to any category.</p> <p>Hint: - Use <code>SalesLT.Product</code> to get product information. - Use <code>SalesLT.ProductCategory</code> to get category information. - Use a LEFT JOIN to include all products.</p>"},{"location":"SQL/SQL.html#question-3-find-customers-and-their-addresses","title":"Question 3: Find Customers and Their Addresses","text":"<p>Question: Retrieve a list of customers and their addresses. Include all customers, even if they do not have an address listed.</p> <p>Hint: - Use <code>SalesLT.Customer</code> to get customer information. - Use <code>SalesLT.CustomerAddress</code> to get the relationship between customers and addresses. - Use <code>SalesLT.Address</code> to get address information. - Use a LEFT JOIN to include all customers.</p>"},{"location":"SQL/SQL.html#question-4-list-all-products-and-their-order-details","title":"Question 4: List All Products and Their Order Details","text":"<p>Question: Retrieve a list of all products and their order details. Include all products and orders, even if they do not match.</p> <p>Hint: - Use <code>SalesLT.Product</code> to get product information. - Use <code>SalesLT.SalesOrderDetail</code> to get order details. - Use a FULL OUTER JOIN to include all products and orders.</p>"},{"location":"SQL/SQL.html#question-5-products-with-no-orders","title":"Question 5: Products with No Orders","text":"<p>Question: Find products that have not been ordered by any customer.</p> <p>Hint: - Use <code>SalesLT.Product</code> to get product information. - Use <code>SalesLT.SalesOrderDetail</code> to get order details. - Use a LEFT JOIN and filter to find products with no matching orders.</p>"},{"location":"SQL/SQL.html#question-6-calculate-total-sales-per-customer-using-cte","title":"Question 6: Calculate Total Sales per Customer Using CTE","text":"<p>Question: Use a Common Table Expression (CTE) to calculate the total sales amount for each customer.</p> <p>Hint: - Use <code>SalesLT.Customer</code> to get customer information. - Use <code>SalesLT.SalesOrderHeader</code> to get order information. - Use SUM function to calculate total sales. - Use a CTE to organize the query.</p>"},{"location":"SQL/SQL.html#comprehensive-list-of-sql-joins","title":"Comprehensive List of SQL Joins","text":""},{"location":"SQL/SQL.html#1-inner-join-also-known-as-join","title":"1. INNER JOIN (also known as JOIN)","text":"<ul> <li>Description: Returns only the rows where there is a match in both tables.</li> <li>Alternative Names: None</li> <li>Use Case: Used when you only want the matching rows between two tables. Common in relational queries.</li> <li>Example:      <pre><code>SELECT * \nFROM employees\nINNER JOIN departments\nON employees.department_id = departments.id;\n</code></pre></li> </ul>"},{"location":"SQL/SQL.html#2-left-join-also-known-as-left-outer-join","title":"2. LEFT JOIN (also known as LEFT OUTER JOIN)","text":"<ul> <li>Description: Returns all rows from the left table, and the matching rows from the right table. If no match, NULL values are returned for the right table.</li> <li>Alternative Names: LEFT OUTER JOIN</li> <li>Use Case: Used when you want all rows from the left table, even if there's no match in the right table.</li> <li>Example:      <pre><code>SELECT * \nFROM employees\nLEFT JOIN departments\nON employees.department_id = departments.id;\n</code></pre></li> </ul>"},{"location":"SQL/SQL.html#3-right-join-also-known-as-right-outer-join","title":"3. RIGHT JOIN (also known as RIGHT OUTER JOIN)","text":"<ul> <li>Description: Returns all rows from the right table, and the matching rows from the left table. If no match, NULL values are returned for the left table.</li> <li>Alternative Names: RIGHT OUTER JOIN</li> <li>Use Case: Used when you want all rows from the right table, even if there's no match in the left table.</li> <li>Example:      <pre><code>SELECT * \nFROM employees\nRIGHT JOIN departments\nON employees.department_id = departments.id;\n</code></pre></li> </ul>"},{"location":"SQL/SQL.html#4-full-join-also-known-as-full-outer-join","title":"4. FULL JOIN (also known as FULL OUTER JOIN)","text":"<ul> <li>Description: Returns all rows from both tables, with matching rows from both sides. Non-matching rows are returned as NULL.</li> <li>Alternative Names: FULL OUTER JOIN</li> <li>Use Case: Used when you need to include all rows from both tables, even if no matches exist.</li> <li>Example:      <pre><code>SELECT * \nFROM employees\nFULL JOIN departments\nON employees.department_id = departments.id;\n</code></pre></li> </ul>"},{"location":"SQL/SQL.html#5-cross-join","title":"5. CROSS JOIN","text":"<ul> <li>Description: Returns the Cartesian product of both tables. Each row from the first table is combined with each row from the second table.</li> <li>Alternative Names: None</li> <li>Use Case: Used when you want to combine every row from one table with every row from another table. Typically used for generating combinations or testing.</li> <li>Example:      <pre><code>SELECT * \nFROM products\nCROSS JOIN suppliers;\n</code></pre></li> </ul>"},{"location":"SQL/SQL.html#6-self-join","title":"6. SELF JOIN","text":"<ul> <li>Description: A table is joined with itself to compare rows within the same table.</li> <li>Alternative Names: None</li> <li>Use Case: Used for hierarchical relationships (e.g., finding managers for employees in the same table).</li> <li>Example:      <pre><code>SELECT e1.name AS Employee, e2.name AS Manager\nFROM employees e1\nLEFT JOIN employees e2\nON e1.manager_id = e2.id;\n</code></pre></li> </ul>"},{"location":"SQL/SQL.html#7-natural-join","title":"7. NATURAL JOIN","text":"<ul> <li>Description: Automatically joins tables based on columns with the same name in both tables. No need to explicitly specify the join condition.</li> <li>Alternative Names: None</li> <li>Use Case: Used when both tables have the same column names and you want an automatic join on those columns.</li> <li>Example:      <pre><code>SELECT * \nFROM employees\nNATURAL JOIN departments;\n</code></pre></li> </ul>"},{"location":"SQL/SQL.html#8-anti-join-also-known-as-left-anti-join","title":"8. ANTI JOIN (also known as LEFT ANTI JOIN)","text":"<ul> <li>Description: Returns rows from the left table where no matching rows exist in the right table.</li> <li>Alternative Names: LEFT ANTI JOIN</li> <li>Use Case: Used when you need rows from the left table that do not have a corresponding match in the right table.</li> <li>Example (using <code>LEFT JOIN</code> and filtering <code>NULL</code>):      <pre><code>SELECT employees.name\nFROM employees\nLEFT JOIN departments\nON employees.department_id = departments.id\nWHERE departments.id IS NULL;\n</code></pre></li> </ul>"},{"location":"SQL/SQL.html#9-semi-join-also-known-as-left-semi-join-or-right-semi-join","title":"9. SEMI JOIN (also known as LEFT SEMI JOIN or RIGHT SEMI JOIN)","text":"<ul> <li>Description: Returns rows from the left table where a match exists in the right table, but only columns from the left table are returned.</li> <li>Alternative Names: LEFT SEMI JOIN, RIGHT SEMI JOIN</li> <li>Use Case: Used when you need to know which rows in the left table have a match in the right table but only need the left table's columns.</li> <li>Example (using <code>EXISTS</code>):      <pre><code>SELECT employees.name\nFROM employees\nWHERE EXISTS (\n    SELECT 1 \n    FROM departments\n    WHERE employees.department_id = departments.id\n);\n</code></pre></li> </ul>"},{"location":"SQL/SQL.html#10-except-join","title":"10. EXCEPT JOIN","text":"<ul> <li>Description: Combines two result sets and returns rows from the left set that are not in the right set.</li> <li>Alternative Names: None</li> <li>Use Case: Used for set difference operations where you need to find rows in one table that don't exist in another.</li> <li>Example:      <pre><code>SELECT * FROM employees\nEXCEPT\nSELECT * FROM employees_in_department;\n</code></pre></li> </ul>"},{"location":"SQL/SQL.html#11-outer-apply","title":"11. OUTER APPLY","text":"<ul> <li>Description: Similar to a left join but with subqueries evaluated per row in the left table.</li> <li>Alternative Names: None</li> <li>Use Case: Used when you need a subquery or table-valued function evaluated once per row in the left table.</li> <li>Example:      <pre><code>SELECT employees.name, department_details.*\nFROM employees\nOUTER APPLY getDepartmentDetails(employees.department_id) AS department_details;\n</code></pre></li> </ul>"},{"location":"SQL/SQL.html#12-inner-apply","title":"12. INNER APPLY","text":"<ul> <li>Description: Similar to an inner join but with subqueries evaluated per row in the left table.</li> <li>Alternative Names: None</li> <li>Use Case: Used when you want to perform an inner join with a subquery or table-valued function evaluated per row.</li> <li>Example:      <pre><code>SELECT employees.name, department_details.*\nFROM employees\nINNER APPLY getDepartmentDetails(employees.department_id) AS department_details;\n</code></pre></li> </ul>"},{"location":"SQL/SQL.html#summary-table","title":"Summary Table","text":"Join Type Alternative Name(s) Description Use Case INNER JOIN JOIN Matches rows in both tables General use, finding common records LEFT JOIN LEFT OUTER JOIN All rows from left, matched from right When you need all rows from the left table RIGHT JOIN RIGHT OUTER JOIN All rows from right, matched from left When you need all rows from the right table FULL JOIN FULL OUTER JOIN All rows from both tables, with NULLs for non-matches When you need complete data from both tables CROSS JOIN None Cartesian product of both tables Generating combinations or large result sets SELF JOIN None Joining a table with itself For hierarchical relationships within a table NATURAL JOIN None Automatically joins on matching columns When tables have common column names ANTI JOIN LEFT ANTI JOIN Rows from the left table without a match in the right When you need records that don't match SEMI JOIN LEFT SEMI JOIN, RIGHT SEMI JOIN Rows from left with matching right rows, only left columns When you need to check for existence in the right table EXCEPT JOIN None Rows from left that are not in right For set difference operations OUTER APPLY None Subqueries evaluated once per row in the left table Using subqueries for each row INNER APPLY None Subqueries evaluated once per row in the left table, like an inner join Inner joins with subqueries"},{"location":"SQL/SQL.html#sql-keys","title":"SQL Keys","text":"Key Type Description Key Feature Primary Key Uniquely identifies each row in a table Ensures uniqueness and no NULL values Foreign Key Refers to a primary key in another table Creates relationships between tables Unique Key Ensures unique values in a column Allows one NULL value Candidate Key Potential primary key Can uniquely identify a row Composite Key Made of multiple columns Combines columns to create uniqueness Alternate Key Any candidate key not chosen as the primary Alternative unique identifier Superkey A set of columns that can uniquely identify a row May include extra columns Natural Key A key formed from real-world data Represents real-world attributes Surrogate Key An artificial key generated by the system No inherent meaning outside the database <p>SQL Concepts:</p> <ol> <li>What is the difference between <code>WHERE</code> and <code>HAVING</code> clauses in SQL?</li> <li> <p>The <code>WHERE</code> clause filters rows before any grouping is performed, while the <code>HAVING</code> clause filters groups after the <code>GROUP BY</code> operation.</p> </li> <li> <p>Explain the purpose of the <code>GROUP BY</code> clause.</p> </li> <li> <p>The <code>GROUP BY</code> clause groups rows that have the same values into summary rows, often used with aggregate functions like <code>COUNT</code>, <code>SUM</code>, <code>AVG</code>, <code>MAX</code>, or <code>MIN</code>.</p> </li> <li> <p>What are aggregate functions in SQL? Can you name a few?</p> </li> <li> <p>Aggregate functions perform a calculation on a set of values and return a single value. Examples include <code>COUNT</code>, <code>SUM</code>, <code>AVG</code>, <code>MAX</code>, and <code>MIN</code>.</p> </li> <li> <p>What is a primary key, and why is it important?</p> </li> <li> <p>A primary key is a unique identifier for a record in a table. It ensures that each record can be uniquely identified and prevents duplicate records.</p> </li> <li> <p>What is a foreign key, and how does it relate to a primary key?</p> </li> <li> <p>A foreign key is a field in one table that uniquely identifies a row of another table. It establishes a link between the two tables, ensuring referential integrity.</p> </li> <li> <p>Describe the different types of joins in SQL.</p> </li> <li> <ul> <li>INNER JOIN: Returns records that have matching values in both tables.</li> </ul> </li> <li>LEFT JOIN (or LEFT OUTER JOIN): Returns all records from the left table and the matched records from the right table; if no match, NULL values are returned for columns from the right table.</li> <li>RIGHT JOIN (or RIGHT OUTER JOIN): Returns all records from the right table and the matched records from the left table; if no match, NULL values are returned for columns from the left table.</li> <li> <p>FULL JOIN (or FULL OUTER JOIN): Returns all records when there is a match in either left or right table; if no match, NULL values are returned for columns from the table without a match.</p> </li> <li> <p>What is normalization in database design?</p> </li> <li> <p>Normalization is the process of organizing data to reduce redundancy and dependency by dividing large tables into smaller ones and defining relationships between them.</p> </li> <li> <p>What are indexes in SQL, and how do they improve query performance?</p> </li> <li> <p>Indexes are database objects that improve the speed of data retrieval operations on a table at the cost of additional space and decreased performance on data modification operations. They function similarly to an index in a book, allowing the database to find data more quickly.</p> </li> <li> <p>Explain the concept of a transaction in SQL.</p> </li> <li> <p>A transaction is a sequence of one or more SQL operations executed as a single unit. Transactions ensure data integrity and are governed by the ACID properties: Atomicity, Consistency, Isolation, and Durability.</p> </li> <li> <p>What is the purpose of the <code>DISTINCT</code> keyword?</p> <ul> <li>The <code>DISTINCT</code> keyword is used to return only distinct (different) values in the result set, eliminating duplicate records.</li> </ul> </li> </ol>"},{"location":"SQL/SQL_AdvancedTopics.html","title":"Advanced Topics","text":""},{"location":"SQL/SQL_AdvancedTopics.html#1-normalization-and-denormalization","title":"1. Normalization and Denormalization","text":"<ul> <li>Normalization is the process of organizing data to reduce redundancy and improve integrity. It involves dividing large tables into smaller, manageable ones and using foreign keys to link them.<ul> <li>First Normal Form (1NF): Each column contains atomic (indivisible) values, and each record is unique.</li> <li>Second Normal Form (2NF): Achieved by removing partial dependency, i.e., all non-key attributes must be fully dependent on the primary key.</li> <li>Third Normal Form (3NF): No transitive dependency, meaning non-key attributes cannot depend on other non-key attributes.</li> </ul> </li> <li>Denormalization: This involves combining tables to reduce the need for complex joins, improving query performance at the expense of redundancy.</li> </ul>"},{"location":"SQL/SQL_AdvancedTopics.html#2-joins","title":"2. Joins","text":"<ul> <li>Inner Join: Returns only rows where there is a match in both tables.</li> <li>Left Join (or Left Outer Join): Returns all rows from the left table, and matched rows from the right table; if no match, NULL values are returned.</li> <li>Right Join (or Right Outer Join): Similar to Left Join but returns all rows from the right table.</li> <li>Full Outer Join: Returns rows when there is a match in one of the tables.</li> <li>Cross Join: Returns the Cartesian product of the two tables, i.e., all combinations of rows.</li> <li>Self Join: A table is joined with itself, often using aliases.</li> </ul>"},{"location":"SQL/SQL_AdvancedTopics.html#3-subqueries","title":"3. Subqueries","text":"<ul> <li>Scalar Subquery: Returns a single value, and is often used in <code>SELECT</code>, <code>WHERE</code>, or <code>HAVING</code> clauses.      <pre><code>SELECT name\nFROM employees\nWHERE salary &gt; (SELECT AVG(salary) FROM employees);\n</code></pre></li> <li>Correlated Subquery: A subquery that refers to columns from the outer query.      <pre><code>SELECT e.name\nFROM employees e\nWHERE e.salary &gt; (SELECT AVG(salary) FROM employees WHERE department_id = e.department_id);\n</code></pre></li> <li>IN vs. EXISTS:<ul> <li><code>IN</code> is used for checking if a value is in a set.</li> <li><code>EXISTS</code> is used when the query checks for the presence of rows.</li> </ul> </li> </ul>"},{"location":"SQL/SQL_AdvancedTopics.html#4-indexes","title":"4. Indexes","text":"<ul> <li>Clustered Index: This type of index determines the physical order of data rows in the table. A table can have only one clustered index.</li> <li>Non-Clustered Index: This does not change the physical order of data. It creates a separate structure to speed up retrieval.</li> <li>Composite Index: Involves multiple columns. It can be used when queries commonly filter by more than one column.</li> </ul>"},{"location":"SQL/SQL_AdvancedTopics.html#5-transactions","title":"5. Transactions","text":"<ul> <li>ACID Properties: Ensures the database transactions are processed reliably.<ul> <li>Atomicity: A transaction is either fully completed or not executed at all.</li> <li>Consistency: The database remains in a valid state before and after the transaction.</li> <li>Isolation: Transactions are isolated from each other.</li> <li>Durability: Once a transaction is committed, it is permanent.</li> </ul> </li> <li>Transaction Control: <ul> <li><code>BEGIN TRANSACTION</code>: Starts a transaction.</li> <li><code>COMMIT</code>: Saves the changes.</li> <li><code>ROLLBACK</code>: Reverts the changes if there is an error.</li> </ul> </li> </ul>"},{"location":"SQL/SQL_AdvancedTopics.html#6-window-functions","title":"6. Window Functions","text":"<ul> <li>ROW_NUMBER(): Assigns a unique row number to each row in a result set.      <pre><code>SELECT name, ROW_NUMBER() OVER (ORDER BY salary DESC) AS rank\nFROM employees;\n</code></pre></li> <li>RANK() and DENSE_RANK(): Similar to <code>ROW_NUMBER()</code>, but handles ties differently.</li> <li>PARTITION BY: Divides the result set into partitions and performs operations on each partition.</li> </ul>"},{"location":"SQL/SQL_AdvancedTopics.html#7-triggers-and-stored-procedures","title":"7. Triggers and Stored Procedures","text":"<ul> <li>Triggers: Automatically executed or fired when certain events occur on a table or view (e.g., <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>).      <pre><code>CREATE TRIGGER update_salary\nAFTER UPDATE ON employees\nFOR EACH ROW\nBEGIN\n   -- trigger logic here\nEND;\n</code></pre></li> <li>Stored Procedures: A set of SQL statements stored in the database that can be executed as a program.      <pre><code>CREATE PROCEDURE GetEmployeeSalary(IN emp_id INT)\nBEGIN\n   SELECT salary FROM employees WHERE employee_id = emp_id;\nEND;\n</code></pre></li> <li>Functions: Similar to stored procedures, but typically return a value.</li> </ul>"},{"location":"SQL/SQL_AdvancedTopics.html#8-partitioning","title":"8. Partitioning","text":"<ul> <li>Range Partitioning: Dividing a table into partitions based on a range of values (e.g., dates).</li> <li>List Partitioning: Dividing a table into partitions based on a list of values.</li> <li>Hash Partitioning: Dividing the data into partitions based on a hash function.</li> </ul>"},{"location":"SQL/SQL_AdvancedTopics.html#9-advanced-sql-clauses","title":"9. Advanced SQL Clauses","text":"<ul> <li>WITH Clause (Common Table Expressions or CTEs): Makes a subquery easier to reference.      <pre><code>WITH DepartmentAvgSalary AS (\n   SELECT department_id, AVG(salary) AS avg_salary\n   FROM employees\n   GROUP BY department_id\n)\nSELECT e.name, e.salary\nFROM employees e\nJOIN DepartmentAvgSalary das ON e.department_id = das.department_id\nWHERE e.salary &gt; das.avg_salary;\n</code></pre></li> <li>GROUP BY ROLLUP and CUBE: Useful for generating subtotals and grand totals.      <pre><code>SELECT department, SUM(salary)\nFROM employees\nGROUP BY ROLLUP (department);\n</code></pre></li> </ul>"},{"location":"SQL/SQL_AdvancedTopics.html#10-data-warehousing-concepts","title":"10. Data Warehousing Concepts","text":"<ul> <li>Star Schema: A central fact table surrounded by dimension tables.</li> <li>Snowflake Schema: A more normalized form of the star schema, where dimension tables are further split into related tables.</li> <li>Fact Tables and Dimension Tables: Fact tables hold quantitative data, while dimension tables contain descriptive data (e.g., <code>products</code>, <code>dates</code>).</li> <li>ETL Process: Extract, Transform, Load; used for moving data from operational databases to a data warehouse.</li> </ul>"},{"location":"SQL/SQL_AdvancedTopics.html#11-nosql-databases","title":"11. NoSQL Databases","text":"<ul> <li>For databases like MongoDB, Cassandra, or Redis, understanding data models (document, key-value, column-family) and how they differ from relational databases is important.</li> <li>Aggregation Pipeline (MongoDB): A framework for performing data transformations within MongoDB.</li> <li>Cassandra Queries: Using CQL (Cassandra Query Language) for column-family based databases.</li> </ul>"},{"location":"SQL/SQL_AdvancedTopics.html#12-optimizing-queries","title":"12. Optimizing Queries","text":"<ul> <li>EXPLAIN Plan: Analyzing the execution plan of a query can help identify bottlenecks.</li> <li>Query Hints: Directing the query optimizer to use specific indexes or join strategies.</li> <li>Materialized Views: Precomputed results that can be used for complex queries to improve performance.</li> <li>Partition Pruning: Ensuring that only relevant partitions are scanned to improve performance.</li> </ul>"},{"location":"SQL/SSIS.html","title":"SQL Server Integration Services","text":"<p>To put simply SSIS is the ETL tool for MSSQL Server. Before Azure Data Factory this was the ETL tool for Microosft ecosystem.</p> <p>Common activity with this tool is extracting data from XML, Csvs, .txt files and load into MSSQL Server.</p>"},{"location":"SQL/SSIS.html#how-to-install-ssis","title":"How to install SSIS","text":"<p>SSIS is installed as an option during MSSQL installation. There is no separate installation of this. </p> <p></p> <p></p>"},{"location":"SQL/SSIS.html#control-flow-and-data-flow","title":"Control Flow and Data Flow","text":"<p>The SSIS ETL workflow is made up two main components:</p> <p>Control Flow: This is the 'roadmap/blueprint/overall plan/sequence of steps' of the ETL workflow. It tell what steps to perform in which order. E.g.</p> <p>First check if the file exists, Then read the data from it, Then, put the data to SQL server.</p> <p>Control flow contains tasks and containers(for loop etc)</p> <p></p> <p>Data flow </p> <p>It is the actual transformation and movement of data. It contains Source, transformation and destination.</p> <p>Oversimplified summary: Control flow is the Workflow and Dataflow is the transformation.</p> <p>Remember, control flow is the workflow. Dataflow is the transformation. Dataflow task is a task in the control flow.</p>"},{"location":"SQL/SSIS.html#key-differences","title":"Key Differences","text":"Feature Control Flow Data Flow Basic Unit Task Transformation Focus Workflow and execution order Data movement and transformation Processing Sequential Parallel Examples Execute SQL, send email, file transfer Extract from database, clean data, load to data warehouse"},{"location":"SQL/SSIS.html#relationship-between-control-flow-and-data-flow","title":"Relationship between Control Flow and Data Flow","text":"<ul> <li>The control flow initiates and controls the data flow.</li> <li>A Data Flow Task is a type of task in the control flow.</li> <li>Multiple data flows can be executed within a single SSIS package.</li> </ul>"},{"location":"SQL/SSIS.html#ssis-sql-destination-types","title":"SSIS SQL Destination Types","text":"<p>To send data from an SSIS workflow to an MSSQL server, we have three options (called Data Flow Destinations). Here\u2019s a simple guide to help you decide which destination to choose for your SQL Server.</p> <p>In SSIS, a destination is a component(Module) that sends data into a database, etc.</p> <p></p> Destination Type What It Does When to Use Pros Cons ADO.NET Destination Loads data into various databases using ADO.NET connections. For MySQL, Oracle, and other ADO.NET compatible databases. Supports complex data types and handles large data well. Sometimes slower than OLE DB in some cases. OLE DB Destination Loads data into databases using OLE DB connections. Mainly for SQL Server or other OLE DB compatible databases. Generally faster for SQL Server, widely used in ETL processes. Less flexible with some modern data types compared to ADO.NET. SQL Server Destination Loads data directly into SQL Server databases. When you need the best performance for SQL Server. Fastest performance for loading data into SQL Server. Only works with SQL Server and needs local execution."},{"location":"SQL/SSIS.html#what-to-choose-for-mssql-server","title":"What to choose for MSSQL Server?","text":"<p>The SQL Server Destination is the fastest but needs the package to run on the same server as SQL Server and usually requires higher permissions.</p> <p>So, OLE DB Destination could be the best choice for most cases.</p>"},{"location":"SQL/SSIS.html#sql-server-management-studio","title":"SQL Server Management Studio","text":"<p>SQL Server Management Studio (SSMS) is a tool for managing SQL Server and Azure SQL Database. With SSMS, you can set up, check, and control your SQL Server and databases. You can use it to deploy, monitor, and upgrade your application's data parts, and to create queries and scripts.</p> <p>SSMS lets you query, design, and manage your databases and data warehouses, whether they are on your local computer or in the cloud.</p>"},{"location":"SQL/SSIS.html#ssms-installation","title":"SSMS Installation","text":"<p>SSMS can be freely downloaded from microsoft site and installed on your windows system. You can install it almost any version of WIndows. It doesn't require windwos server to be instaslled.</p>"},{"location":"SQL/SSRS.html","title":"[TBD]Overview","text":"<p>SQL Server Reporting Services 2008 (SSRS) is a tool in SQL Server 2008 for designing, developing, testing, and deploying reports. It works with the Business Intelligence Development Studio (BIDS) for all tasks related to report authoring and deployment. BIDS is included with SQL Server 2008.</p> <p>In this tutorial, we will cover the following topics to help you build a report successfully:</p> <ol> <li>Reporting Services Components</li> <li>Install Reporting Services</li> <li>Business Intelligence Development Studio (BIDS)</li> <li>Install Sample Database</li> <li>Create a Simple Report with the Wizard</li> <li>Create a Simple Report with the Report Designer</li> <li>Deploy Reports</li> <li>Configure Report Manager Security</li> </ol>"},{"location":"SQL/SSRS.html#reporting-services-components","title":"Reporting Services Components","text":"<p>SSRS uses two main databases: ReportServer and ReportServerTempDB. ReportServer stores reports, data sources, snapshots, subscriptions, etc., while ReportServerTempDB is for temporary storage. These databases are created automatically during the SQL Server installation if you choose the default setup. You can also create them manually using the Reporting Services Configuration Manager.</p>"},{"location":"SQL/SSRS.html#reporting-services-windows-service","title":"Reporting Services Windows Service","text":"<p>SSRS functionality is implemented as a Windows service. Understanding this service isn't essential for using SSRS but can be useful. The service provides several functions, as shown in the diagram below:</p> <p></p> <p>Key points about the Windows service:</p> <ul> <li>HTTP Listener: This is a new feature in SSRS 2008, removing the need for Internet Information Server (IIS).</li> <li>Report Manager: An ASP.NET application providing a browser-based interface for managing SSRS.</li> <li>Web Service: An ASP.NET application offering a programmatic interface to SSRS, used by Report Manager and can be used for custom implementations.</li> <li>Background Processing: Handles core services for SSRS.</li> </ul>"},{"location":"SQL/SSRS.html#report-designer","title":"Report Designer","text":"<p>The Report Designer in BIDS allows you to design, develop, test, and deploy reports. BIDS, included with SQL Server 2008, has a user-friendly interface and new visualizations. Non-developers can use Report Builder, but it is not covered in this tutorial.</p>"},{"location":"SQL/SSRS.html#installing-reporting-services","title":"Installing Reporting Services","text":"<p>To install SSRS, run the SQL Server 2008 SETUP.EXE and follow the steps, selecting Reporting Services in the Feature Selection dialog:</p> <p></p> <p>Ensure to check all items under Shared Features, especially BIDS, as it is used for designing, developing, testing, and deploying reports.</p> <p>In the Reporting Services Configuration dialog, select \"Install the native mode default configuration\":</p> <p></p> <p>This installs and configures SSRS automatically. After installation, you can start using SSRS immediately.</p>"},{"location":"SQL/SSRS.html#using-bids","title":"Using BIDS","text":"<p>BIDS helps in developing reports with an intuitive interface based on Microsoft's Visual Studio. To install BIDS, select the Business Intelligence Development Studio checkbox in the Shared Features section during the SQL Server installation. BIDS uses the concept of projects and solutions to organize reports.</p> <p>To create a new project: 1. Launch BIDS from the Microsoft SQL Server 2008 program group. 2. Click File &gt; New Project. 3. Select Visual Studio Solutions under Project Types and Blank Solution under Visual Studio installed templates. Enter the name and location:</p> <p></p> <ol> <li>Click OK to create a blank solution.</li> </ol> <p>Next, add a project to the solution: 1. Click File &gt; Add &gt; New Project. 2. Select Report Server Project and fill in the details:</p> <p></p> <p>We are now ready to install the sample database to use in our reports.</p>"},{"location":"SQL/SSRS.html#installing-the-sample-database","title":"Installing the Sample Database","text":"<p>We will use the AdventureWorksLT sample database for our reports. This database is chosen because it is small and easy to set up.</p> <ol> <li>Download the AdventureWorksLT database from the Microsoft SQL Server Product Samples page on CodePlex.</li> <li>Run the installer (.msi file) and choose the folder for the database and log files. For a default SQL Server installation, use:</li> </ol> <pre><code>C:\\Program Files\\Microsoft SQL Server\\MSSQL10.MSSQLSERVER\\MSSQL\\DATA\n</code></pre> <ol> <li>After installation, attach the database to your SQL Server using SQL Server Management Studio:</li> </ol> <p></p> <ol> <li>In the Attach Databases dialog, click Add and navigate to the AdventureWorksLT_Data.mdf file:</li> </ol> <p></p> <ol> <li>Click OK to attach the database.</li> </ol>"},{"location":"SQL/SSRS.html#creating-a-simple-report-with-the-wizard","title":"Creating a Simple Report with the Wizard","text":"<p>The Report Wizard in BIDS helps you create a simple report quickly. </p> <ol> <li>Open your solution in BIDS.</li> <li>Right-click the Reports node and select Add New Report to launch the Report Wizard:</li> </ol> <p></p> <ol> <li> <p>Follow these steps in the wizard:</p> </li> <li> <p>Create a Shared Data Source: Define the data source your report will use.</p> </li> <li>Design a Query: Specify the SQL query to fetch the data for your report.</li> <li>Select a Report Type: Choose the layout type (e.g., tabular or matrix).</li> <li>Design a Table: Define the structure of the report.</li> <li>Choose the Table Layout: Select how the table should be displayed.</li> <li>Complete the Wizard: Finalize the report setup.</li> </ol>"},{"location":"SQL/SSRS.html#next-steps","title":"Next Steps","text":"<p>Now that you've created a simple report using the wizard, you can further customize it using the Report Designer in BIDS. The Report Designer allows you to define every aspect of your report, giving you more control and flexibility. Once you're comfortable with the basics, you can start building more sophisticated reports and dashboards.</p>"},{"location":"SQL/SSRS.html#creating-a-shared-data-source","title":"Creating a Shared Data Source","text":"<p>A Data Source contains the information needed to fetch the data for your report. SSRS can access data from relational databases, OLAP databases, and almost any data source with an ODBC or OLE DB driver.</p> <p>When creating a Data Source, you can specify it as shared, meaning it can be used by any report in the same project. It is generally a good idea to create Shared Data Sources. If a Data Source is not shared, its definition is stored inside the report and cannot be shared with other reports.</p> <p>In this section, we will go through the steps to create a Shared Data Source.</p>"},{"location":"SQL/SSRS.html#explanation","title":"Explanation","text":"<p>After launching the Report Wizard, you will see the Select the Data Source dialog as shown below:</p> <p></p> <p>Since our project does not have any Shared Data Sources yet, we need to define a new Data Source. Here are the details you need to provide:</p> <ul> <li>Name: Choose a descriptive name for the Data Source, like AdventureWorksLT. Avoid spaces in the name to prevent errors.</li> <li>Type: Select the appropriate type from the dropdown list. The default value, Microsoft SQL Server, is correct for our AdventureWorksLT database.</li> <li>Connection String: Enter the connection string for your Data Source. It is usually easier to click the Edit button to enter the details and have the connection string created for you.</li> <li>Edit button: Click this to open the Connection Properties dialog where you can enter the server name and select the database. The connection string will be created for you.</li> <li>Credentials button: Click this to open the Data Source Credentials dialog where you can specify the credentials to use when connecting to the Data Source.</li> <li>Make this a shared data source checkbox: Check this box to create a Shared Data Source, so any report in the same project can use it.</li> </ul> <p>Click the Edit button to open the Connection Properties dialog. Enter your server name and select the AdventureWorksLT database as shown below:</p> <p></p> <p>The Server name is where your SQL Server database is deployed. If you are running a named instance of SQL Server, specify it as SERVERNAME\\INSTANCENAME. If SQL Server is running locally, you can use localhost instead of SERVERNAME. Click the Test Connection button to verify the connection, then click OK to close the dialog.</p> <p>Next, click the Credentials button to open the Data Source Credentials dialog as shown below:</p> <p></p> <p>The default selection, Use Windows Authentication (Integrated Security), is fine for our purposes. This means Reporting Services will connect to the Data Source using the Windows credentials of the person running the report. When you deploy the report and Data Source for others to use, you can select a different option if needed. For now, we'll stick with the default.</p> <p>After completing these steps, the Select the Data Source dialog will look like this:</p> <p></p> <p>Click Next to proceed to the Design the Query dialog, which we will cover in the next section.</p>"},{"location":"SQL/SSRS.html#design-query-step","title":"Design Query Step","text":"<p>The Design Query step of the Report Wizard allows us to specify what data we want to retrieve from our Data Source and display in our report. In this section, we will explain how to define a query to fetch the data for our report.</p>"},{"location":"SQL/SSRS.html#explanation_1","title":"Explanation","text":"<p>The Design Query step in the Report Wizard displays the following dialog:</p> <p></p> <p>You can click the Query Builder button to build your query graphically or type it directly into the Query string textbox. Here is an example query you can use:</p> <pre><code>SELECT\n  c.ParentProductCategoryName,\n  c.ProductCategoryName,\n  SUM(d.LineTotal) AS Sales\nFROM SalesLT.Product p\nJOIN SalesLT.vGetAllCategories c \n  ON c.ProductCategoryID = p.ProductCategoryID\nJOIN SalesLT.SalesOrderDetail d \n  ON d.ProductID = p.ProductID  \nGROUP BY  \n  c.ParentProductCategoryName,\n  c.ProductCategoryName\nORDER BY\n  c.ParentProductCategoryName,\n  c.ProductCategoryName\n</code></pre> <p>This query provides a sales summary broken down by product category. Copy and paste this query into the Query string textbox in the Design Query dialog. Alternatively, you can click the Query Builder button and design the query graphically.</p> <p>Click Next to move to the Select Report Type dialog.</p>"},{"location":"SQL/SSRS.html#select-report-type-step","title":"Select Report Type Step","text":"<p>The Select Report Type step of the Report Wizard lets us choose between a Tabular or Matrix type of report. Here are the details of these report types.</p>"},{"location":"SQL/SSRS.html#explanation_2","title":"Explanation","text":"<p>The Select Report Type step in the Report Wizard shows the following dialog:</p> <p></p> <p>A tabular report has page headings, column headings, and subtotals running down the page. A matrix report allows defining fields on columns and rows and provides interactive drilldown capabilities. We will create a tabular report as it is simple and familiar.</p> <p>Click Next to move to the Design the Table dialog.</p>"},{"location":"SQL/SSRS.html#design-table-step","title":"Design Table Step","text":"<p>The Design Table step of the Report Wizard allows us to layout the available fields on our report, choosing between Page, Group, and Details.</p>"},{"location":"SQL/SSRS.html#explanation_3","title":"Explanation","text":"<p>The Design Table step in the Report Wizard displays the following dialog:</p> <p></p> <p>The Available fields list is populated based on the query you defined in the previous step. Click on a field, then click the appropriate button to place that field. Fill in the dialog as shown below:</p> <p></p>"},{"location":"SQL/SSRS.html#button-descriptions","title":"Button Descriptions:","text":"<ul> <li>Page: Use this to start a new page when the field value changes, e.g., each ParentProductCategory on a different page.</li> <li>Group: Group the fields in this list.</li> <li>Details: Fields in this list appear in each row of the report.</li> </ul> <p>Click Next to move to the Choose Table Layout dialog.</p>"},{"location":"SQL/SSRS.html#choose-table-layout-step","title":"Choose Table Layout Step","text":"<p>The Choose Table Layout step of the Report Wizard lets us choose a stepped or blocked layout and whether to include subtotals and enable drilldown.</p>"},{"location":"SQL/SSRS.html#explanation_4","title":"Explanation","text":"<p>The Choose Table Layout step in the Report Wizard displays the following dialog:</p> <p></p> <p>The default Stepped layout shows the groupings as above. Block layout saves space but disables drilldown. Including Subtotals provides intermediate totals based on groupings. Enabling drilldown initially hides details and allows expanding with a click on the plus icon.</p> <p>Fill in the dialog as shown below:</p> <p></p> <p>Click Next to move to the Choose Table Style dialog.</p>"},{"location":"SQL/SSRS.html#choose-table-style-step","title":"Choose Table Style Step","text":"<p>The Choose Table Style step of the Report Wizard allows us to select from different styles. This is purely cosmetic, offering different color schemes.</p>"},{"location":"SQL/SSRS.html#explanation_5","title":"Explanation","text":"<p>The Choose Table Style step in the Report Wizard displays the following dialog:</p> <p></p> <p>Choose a style from the list and click Next to move to the Completing the Wizard dialog.</p>"},{"location":"SQL/SSRS.html#completing-the-wizard-step","title":"Completing the Wizard Step","text":"<p>The Completing the Wizard step of the Report Wizard summarizes your choices from the previous dialogs.</p>"},{"location":"SQL/SSRS.html#explanation_6","title":"Explanation","text":"<p>The Completing the Wizard step in the Report Wizard displays the following dialog:</p> <p></p> <p>Provide a descriptive name for your report in the Report Name textbox, e.g., ReportWizardExample. You can check the Preview report checkbox to see what your report will look like. Review your choices in the summary. If you need to change anything, click the Back button to revisit the previous dialogs.</p> <p>Click the Finish button to generate your report. Your report will appear in the Solution Explorer as shown below:</p> <p></p> <p>The report will also be displayed in the Report Designer. Click the Preview tab to render your report. A portion of the report is shown below:</p> <p></p> <p>We will make a few changes to the report. Click the Design tab; you will see the following:</p> <p></p> <p>We'll add spaces in the heading, widen the columns, and format the sales numbers. Here are the steps:</p> <ol> <li>Add spaces in the heading: Click between the 't' and 'W', and between 'd' and 'E'.</li> <li>Widen the columns: Click in the ParentProductCategory cell. An Excel-like grid appears. Hover between the cells at the top, click, and drag to widen them.</li> <li>Format sales numbers: Click inside the [Sum(Sales)] column, locate Format in the Properties window, and type C0 to format the cell as currency with no decimals. Repeat for the [Sales] column.</li> </ol> <p>After making these changes, the report design should look like this:</p> <p></p> <p>Click the Preview tab to display the report:</p> <p></p> <p>Click the + icon to the left of the Parent Product Category Names to drill down to Product Category Name details.</p> <p>This completes our tutorial section on the Report Wizard.</p>"},{"location":"SQL/SSRS.html#creating-a-report-from-scratch-using-report-designer","title":"Creating a Report from Scratch Using Report Designer","text":"<p>In the prior section, we created a report using the Report Wizard in Business Intelligence Development Studio (BIDS). In this section, we will create a report from scratch using the Report Designer in BIDS. With the Report Designer, you start with an empty canvas and define every aspect of the report yourself, allowing you to create sophisticated reports and dashboards.</p> <p>We will complete the following steps to build a simple report:</p> <ol> <li>Add a new report to our project</li> <li>Create a shared data source</li> <li>Create a Dataset</li> <li>Configure a Table</li> </ol> <p>The following screenshot shows the report we will build as rendered in the Report Manager:</p> <p></p> <p>This report is based on the same query used in the earlier Report Wizard section. The plus sign icon to the left of the value in the Parent Product Category column allows us to drill down to the Product Category details.</p> <p>Now, let's start creating our report.</p>"},{"location":"SQL/SSRS.html#adding-a-new-report-to-our-project","title":"Adding a New Report to Our Project","text":"<p>The first step in creating a report is to add a new report to our project.</p>"},{"location":"SQL/SSRS.html#explanation_7","title":"Explanation","text":"<p>In the earlier section on Projects and Solutions, we created a blank solution and added a Report Server project to the solution. In the previous section, we added a new report by stepping through the Report Wizard. The BIDS Solution Explorer shows our Reports project along with the Shared Data Source and ReportWizardExample created in the previous section:</p> <p></p> <p>Right-click on the Reports node, then select Add &gt; New Item, which will display the Add New Item - Reports dialog. Fill in the dialog as shown below:</p> <p></p> <p>Click the Add button to add a new report to your project. Your new report will be displayed in the Report Designer. Let's review the Report Designer before we continue creating our report from scratch.</p> <p>There are three parts of the Report Designer:</p> <ul> <li>Design Surface: The palette where you lay out your report.</li> <li>Report Data: Allows you to define Data Sources, Datasets, Parameters, and Images. You can access built-in fields like Report Name and Page Number and drag and drop items onto the design surface.</li> <li>Toolbox: Contains the Report Items that you drag and drop onto the design surface, such as Table, Matrix, Rectangle, List, etc.</li> </ul> <p>When you add or open a report, the design surface will be displayed. After adding a report, you will see the following blank design surface:</p> <p></p> <p>You can display the Report Data and Toolbox areas by selecting them from the top-level View menu if they aren't shown. I prefer to position them to the left of the designer. The Report Data area is shown below:</p> <p></p> <p>In the screenshot above, Report Data and the Toolbox share the same area of the screen; click on the tab at the bottom to switch between them. The Toolbox contains the following elements that you will drag and drop onto the design surface:</p> <p></p> <p>Note the push pin icon</p> <p>in the heading of Report Data and the Toolbox. Clicking this toggles between showing the tab and hiding it, putting a button you can hover over to display the tab.</p> <p>You can customize what you see in the report designer and position it however you like. Click on the Report Data or Toolbox heading and drag it around to position it.</p> <p>Now, let's continue to the next section and create a Shared Data Source.</p>"},{"location":"SQL/SSRS.html#creating-a-shared-data-source_1","title":"Creating a Shared Data Source","text":"<p>We discussed the Shared Data Source in the earlier section on using the Report Wizard to create a new report. The Data Source contains the information that Reporting Services needs to retrieve the data for our report. A Shared Data Source can be used by any report in the same project. In this section, we will create a Shared Data Source.</p>"},{"location":"SQL/SSRS.html#explanation_8","title":"Explanation","text":"<p>To create a Shared Data Source, click the New button in the Report Data area, then select Data Source from the menu as shown below:</p> <p></p> <p>The Data Source Properties dialog will be displayed as shown below:</p> <p></p> <p>First, provide a name; enter AdventureWorksLT in the Name textbox. Since we already defined a Shared Data Source in the earlier section on using the Report Wizard, click the Use shared data source reference radio button and select AdventureWorksLT from the dropdown list. The Data Source Properties dialog is shown below:</p> <p></p> <p>At this point, we are done. If you need to create a new Shared Data Source, click the New button and complete the Shared Data Source Properties dialog. This is essentially the same as what we did in the Report Wizard section.</p> <p>We can now see our Shared Data Source in the Report Data area as shown below:</p> <p></p> <p>We are now ready to continue to the next section and create a Data Set.</p>"},{"location":"SQL/SSRS.html#creating-a-data-set","title":"Creating a Data Set","text":"<p>A Data Set contains a query that Reporting Services uses to retrieve the data for our report. This query could be a SQL statement like we used in the Design the Query step of the Report Wizard section; it could also be a stored procedure that we execute. In this section, we will define a new Data Set using the same query from the Report Wizard section.</p>"},{"location":"SQL/SSRS.html#explanation_9","title":"Explanation","text":"<p>To create a Data Set, right-click on the AdventureWorksLT Shared Data Source that we created in the previous section and select Add Dataset from the menu as shown below:</p> <p></p> <p>The Dataset Properties dialog will be displayed as shown below:</p> <p></p> <p>First, provide a name; enter Main in the Name textbox. Since we only have one Shared Data Source in our project, it will be selected automatically in the Data source dropdown. To define our query, you could click the Query Designer button and do it graphically or type in the query as we did in the Report Wizard section. Instead, click the Import button, which will initially display the familiar Open File dialog. Navigate to the report we created earlier in the Report Wizard section of the tutorial as shown below:</p> <p></p> <p>Click OK to display the Import Query dialog as shown below:</p> <p></p> <p>The above dialog displays the Datasets and their queries from the report. Our earlier report has only one Dataset, so just click the Import button. If the report had multiple Datasets, you could choose the Dataset from the list on the left.</p> <p>The Report Data area now shows our new Dataset and the list of available fields as shown below:</p> <p></p> <p>We are now ready to continue to the next section to configure a Table for our report layout.</p>"},{"location":"SQL/Windows_Functions.html","title":"Beginner\u2019s Guide to Window Functions in SQL","text":""},{"location":"SQL/Windows_Functions.html#what-if-you-could-perform-calculations-on-rows-of-data-but-still-see-every-row-meet-window-functions","title":"What if you could perform calculations on rows of data but still see every row? Meet Window Functions!","text":"<p>Imagine you're running a marathon, and you\u2019re curious not just about who won, but also how you rank within your age group, your city, or among your friends. Window Functions are like the \"rank tracker\" for your SQL data\u2014helping you zoom in on specific slices of your data while keeping the big picture in view.</p> <p>Think of it this way: Aggregate functions (like SUM, AVG) summarize data by squashing rows together. Window Functions, on the other hand, let you see both the summary and the individual details. It\u2019s like having your cake and eating it too!</p>"},{"location":"SQL/Windows_Functions.html#meet-the-cast-types-of-window-functions","title":"Meet the Cast: Types of Window Functions","text":"<p>Let\u2019s dive into the superstar Window Functions\u2014each has its own talent for tackling data challenges.</p>"},{"location":"SQL/Windows_Functions.html#1-aggregate-functions-with-over-clause","title":"1. Aggregate Functions with OVER Clause","text":""},{"location":"SQL/Windows_Functions.html#summing-things-up-without-losing-the-details","title":"\"Summing Things Up Without Losing the Details\"","text":"<p>Use these to calculate totals, averages, and more within defined windows. Perfect for seeing the forest and the trees.</p> <ul> <li>Scenario: You\u2019re managing a team and want to know each person\u2019s salary along with the total salary of their department.   <pre><code>SELECT department, employee_name, salary,\n       SUM(salary) OVER (PARTITION BY department) AS total_salary\nFROM employees;\n</code></pre> Output:   | Department | Employee Name | Salary | Total Salary |   |------------|---------------|--------|--------------|   | Sales      | Alice         | 5000   | 15000        |   | Sales      | Bob           | 7000   | 15000        |   | Sales      | Charlie       | 3000   | 15000        |</li> </ul>"},{"location":"SQL/Windows_Functions.html#2-ranking-functions","title":"2. Ranking Functions","text":""},{"location":"SQL/Windows_Functions.html#whos-on-top","title":"\"Who\u2019s on Top?\"","text":"<p>When you need to assign ranks or positions to rows, these functions are your go-to tools.</p>"},{"location":"SQL/Windows_Functions.html#row_number-the-uniquely-numbered-mvp","title":"ROW_NUMBER(): The Uniquely Numbered MVP","text":"<ul> <li> <p>Scenario: Rank employees by salary within each department.   <pre><code>SELECT department, employee_name, salary,\n       ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) AS rank\nFROM employees;\n</code></pre></p> </li> <li> <p>PROD Scenario: Here is a real-query used in a prod envirnment:</p> </li> </ul> <p></p> <p>This query uses the <code>ROW_NUMBER()</code> window function to deduplicate data based on the <code>integration_key</code> column. Here\u2019s how it works step by step:</p> <p>##### Breakdown of <code>ROW_NUMBER</code> and RANKING Logic:</p> <ol> <li> <p>Purpose of <code>ROW_NUMBER()</code>:</p> <ul> <li>It assigns a unique row number to each record within a partition defined by the <code>PARTITION BY</code> clause.</li> <li>The numbering starts at 1 for each partition, based on the ordering specified in the <code>ORDER BY</code> clause.</li> </ul> </li> <li> <p><code>PARTITION BY integration_key</code>:</p> <ul> <li>Divides the data into partitions, where each partition corresponds to a unique <code>integration_key</code>.</li> </ul> </li> <li> <p><code>ORDER BY versionnumber DESC, w_business_dt DESC</code>:</p> <ul> <li>Within each partition, the rows are sorted by:</li> <li><code>versionnumber</code> in descending order (higher version numbers appear first).</li> <li>If there are ties in <code>versionnumber</code>, they are further sorted by <code>w_business_dt</code> in descending order (more recent dates appear first).</li> </ul> </li> <li> <p>Alias <code>rn</code>:</p> <ul> <li>The <code>ROW_NUMBER()</code> function assigns a row number to each row in the partition. The row with the highest priority (based on the <code>ORDER BY</code> conditions) gets <code>rn = 1</code>.</li> </ul> </li> </ol> <p>##### Final Selection:</p> <ul> <li><code>WHERE data.rn = 1</code>:</li> <li> <p>This filters out all rows except the one with <code>rn = 1</code>, effectively keeping only the \"best\" row (highest versionnumber and latest date) for each <code>integration_key</code>.</p> </li> <li> <p><code>SELECT DISTINCT data.*</code>:</p> </li> <li>Ensures that there are no duplicate rows in the output.</li> </ul> <p>##### What Happens in the Query:</p> <ol> <li> <p>Deduplication:</p> <ul> <li>The CTE (<code>WITH deduplicated_data</code>) ensures each <code>integration_key</code> has exactly one row based on the ranking logic.</li> </ul> </li> <li> <p>Extraction:</p> <ul> <li>The main query fetches only the top-ranked rows (<code>rn = 1</code>) for each <code>integration_key</code>.</li> </ul> </li> </ol> <p>This approach is commonly used in SQL to handle deduplication when there\u2019s a clear ranking or prioritization logic. Let me know if you need additional examples or variations!</p>"},{"location":"SQL/Windows_Functions.html#rank-sharing-the-spotlight","title":"RANK(): Sharing the Spotlight","text":"<ul> <li>Assigns the same rank to rows with identical values, but skips the next rank.</li> <li>Scenario: Similar to <code>ROW_NUMBER()</code> but with ties.</li> </ul>"},{"location":"SQL/Windows_Functions.html#dense_rank-closing-the-gaps","title":"DENSE_RANK(): Closing the Gaps","text":"<ul> <li>No gaps in ranks, even with ties.</li> </ul>"},{"location":"SQL/Windows_Functions.html#ntilen-divide-and-conquer","title":"NTILE(n): Divide and Conquer","text":"<ul> <li>Divides rows into <code>n</code> equal parts\u2014great for quartiles or deciles.</li> </ul>"},{"location":"SQL/Windows_Functions.html#3-distribution-functions","title":"3. Distribution Functions","text":""},{"location":"SQL/Windows_Functions.html#how-do-you-stack-up","title":"\"How Do You Stack Up?\"","text":"<p>Use these to see how a value compares to others within a window.</p>"},{"location":"SQL/Windows_Functions.html#cume_dist-track-your-progress","title":"CUME_DIST(): Track Your Progress","text":"<ul> <li>Scenario: See the cumulative distribution of salaries.   <pre><code>SELECT employee_name, salary,\n       CUME_DIST() OVER (ORDER BY salary ASC) AS cumulative_distribution\nFROM employees;\n</code></pre></li> </ul>"},{"location":"SQL/Windows_Functions.html#percent_rank-relative-ranking-made-easy","title":"PERCENT_RANK(): Relative Ranking Made Easy","text":"<ul> <li>Similar to <code>CUME_DIST()</code>, but calculates the rank percentage.</li> </ul>"},{"location":"SQL/Windows_Functions.html#cheat-sheet-quick-reference-table","title":"Cheat Sheet: Quick Reference Table","text":"Function Superpower Handles Ties Gaps in Rank ROW_NUMBER() Unique numbering of rows No N/A RANK() Rank with ties, skips next Yes Yes DENSE_RANK() Rank with ties, no gaps Yes No NTILE(n) Split rows into <code>n</code> groups N/A N/A"},{"location":"SQL/Windows_Functions.html#your-turn-practice-makes-perfect","title":"Your Turn: Practice Makes Perfect","text":"<p>Here\u2019s a fun challenge to flex your SQL muscles. Given this table:</p> Region Salesperson Sales East John 5000 East Jane 7000 West Jack 8000 West Jill 3000"},{"location":"SQL/Windows_Functions.html#challenge","title":"Challenge:","text":"<ol> <li>Rank salespeople by their sales within each region.</li> <li>Calculate the cumulative sales for each region.</li> </ol>"},{"location":"SQL/connecting-with-dbt.html","title":"Connect Local dbt with MSSQL Server","text":"<p>This guide will take you through the steps to create a project with dbt and connect it to a Microsoft SQL Server (MSSQL) database.</p>"},{"location":"SQL/connecting-with-dbt.html#steps-to-follow","title":"Steps to follow","text":"<ol> <li> <p>Create .dbt Folder: Create a <code>.dbt</code> folder in your user directory, e.g., <code>C:\\Users\\dwaip\\.dbt</code>.</p> </li> <li> <p>Add profiles.yml: Inside the <code>.dbt</code> folder, place a <code>profiles.yml</code> file with the following content:</p> </li> </ol> <pre><code># Das: This profiles.yml configuration is tested for:\n# - SQL Server 2022 build 16.0.1121.4\n# - Python 3.12.3\n# - Registered adapter: sqlserver=1.7.4\n# - dbt version: 1.7.18\n# - Authentication: Windows Login\n\n# Profile name should match what's in your dbt_project.yml\nhello_mssql:\n\n# 'target' specifies the environment (e.g., dev, prod)\ntarget: dev\n\n# 'outputs' define configurations for environments\noutputs:\n\n   # Configuration for the 'dev' environment\n   dev:\n\n      # Database type\n      type: sqlserver\n\n      # ODBC driver for SQL Server\n      driver: 'ODBC Driver 17 for SQL Server'\n\n      # SQL Server name or IP\n      server: 'MOMO'\n\n      # Default port for SQL Server\n      port: 1433\n\n      database: 'InsuranceDB'\n      schema: 'dbo'\n\n      # Use Windows login credentials\n      trusted_connection: true\n\n      # Enable encryption for data transmission\n      encrypt: true\n\n      # Trust the server's SSL certificate (useful for self-signed certificates)\n      trust_cert: true\n</code></pre> <ol> <li> <p>Create Project Folder: Create a new folder on your laptop, e.g., <code>dbt_projects</code>, and navigate (CD) into it.</p> </li> <li> <p>Set Up Virtual Environment: Inside your project folder, create a virtual environment by running <code>python -m venv dbt_venv</code>.</p> </li> </ol> <p></p> <ol> <li> <p>Activate Virtual Environment: Activate the virtual environment by running <code>.\\Scripts\\activate</code>. The <code>activate.bat</code> or <code>activate.ps1</code> file is inside the <code>Scripts</code> folder.</p> </li> <li> <p>Install dbt and Adapter: Install dbt core and the SQL Server adapter by running <code>pip install dbt-core dbt-sqlserver</code>. Replace <code>dbt-sqlserver</code> with the appropriate adapter name if needed.</p> </li> <li> <p>Initialize dbt Project: Initialize your dbt project by running <code>dbt init [project_name]</code>, e.g., <code>dbt init hello_mssql</code>. This will create a folder named <code>hello_mssql</code>. Ensure the project name matches the one in your <code>profiles.yml</code>.</p> </li> <li> <p>Run dbt Debug: Navigate (CD) into the <code>hello_mssql</code> folder and run <code>dbt debug</code>. Ensure you run <code>dbt debug</code> from inside the project folder.</p> </li> </ol> <p></p>"},{"location":"SQL/connecting-with-dbt.html#connect-local-dbt-with-databricks","title":"Connect Local dbt with Databricks","text":"<ol> <li> <p>Set Up .dbt Folder: Create a <code>.dbt</code> folder in your user directory, e.g., <code>C:\\Users\\dwaip\\.dbt</code>.</p> </li> <li> <p>Add profiles.yml: Inside the <code>.dbt</code> folder, place a <code>profiles.yml</code> file with the following content:</p> </li> </ol> <pre><code>databricks:\n  outputs:\n    dev:\n      type: databricks\n      server_host: [databricks_host_url]\n      http_path: [http_path]\n      token: [access_token]\n      schema: [schema_name]\n      catalog: [catalog_name] # optional\n      warehouse: [warehouse_name] # optional\n  target: dev\n</code></pre> <p>Replace <code>[databricks_host_url]</code>, <code>[http_path]</code>, <code>[access_token]</code>, <code>[schema_name]</code>, <code>[catalog_name]</code>, and <code>[warehouse_name]</code> with your specific details.</p> <ol> <li> <p>Create Project Folder: Create a new folder on your laptop, e.g., <code>dbt_projects</code>, and navigate (CD) into it.</p> </li> <li> <p>Set Up Virtual Environment: Inside your project folder, create a virtual environment by running <code>python -m venv dbt_venv</code>.</p> </li> <li> <p>Activate Virtual Environment: Activate the virtual environment by running <code>.\\Scripts\\activate</code>. The <code>activate.bat</code> or <code>activate.ps1</code> file is inside the <code>Scripts</code> folder.</p> </li> <li> <p>Install dbt and Adapter: Install dbt core and the Databricks adapter by running <code>pip install dbt-core dbt-databricks</code>. </p> </li> <li> <p>Initialize dbt Project: Initialize your dbt project by running <code>dbt init [project_name]</code>, e.g., <code>dbt init hello_databricks</code>. This will create a folder named <code>hello_databricks</code>. Ensure the project name matches the one in your <code>profiles.yml</code>.</p> </li> <li> <p>Run dbt Debug: Navigate (CD) into the <code>hello_databricks</code> folder and run <code>dbt debug</code>. Ensure you run <code>dbt debug</code> from inside the project folder.</p> </li> </ol>"},{"location":"SQL/connecting-with-dbt.html#errors","title":"Errors","text":""},{"location":"SQL/connecting-with-dbt.html#1-profilesyml-file-is-invalid","title":"1. Profiles.yml File is Invalid","text":""},{"location":"SQL/connecting-with-dbt.html#error","title":"Error:","text":"<p>When running <code>dbt debug</code>, an error appeared saying the <code>profiles.yml</code> file was invalid, with the message: <code>'yes' is not valid under any of the given schemas</code>.</p>"},{"location":"SQL/connecting-with-dbt.html#resolution","title":"Resolution:","text":"<p>Change the value of <code>trusted_connection</code> from <code>'yes'</code> to <code>true</code> (without quotes) in the <code>profiles.yml</code> file. This ensures dbt correctly recognizes the trusted connection setting.</p>"},{"location":"SQL/connecting-with-dbt.html#2-dbt_projectyml-file-not-found","title":"2. dbt_project.yml File Not Found","text":""},{"location":"SQL/connecting-with-dbt.html#error_1","title":"Error:","text":"<p>The <code>dbt debug</code> command failed because the <code>dbt_project.yml</code> file was not found in the project directory.</p>"},{"location":"SQL/connecting-with-dbt.html#resolution_1","title":"Resolution:","text":"<p>Create a <code>dbt_project.yml</code> file in the project directory and ensure it correctly references the profile name used in the <code>profiles.yml</code> file.</p>"},{"location":"SQL/connecting-with-dbt.html#3-could-not-find-profile-named-hello_mssql","title":"3. Could Not Find Profile Named 'hello_mssql'","text":""},{"location":"SQL/connecting-with-dbt.html#error_2","title":"Error:","text":"<p>An error occurred because dbt couldn't find a profile named <code>hello_mssql</code>, even though the profile was set up in the <code>profiles.yml</code>.</p>"},{"location":"SQL/connecting-with-dbt.html#resolution_2","title":"Resolution:","text":"<p>Ensure that the <code>profile</code> in <code>dbt_project.yml</code> matches the profile name in <code>profiles.yml</code>. For example, if the profile name is <code>mssql_server</code> in <code>profiles.yml</code>, make sure <code>dbt_project.yml</code> references <code>mssql_server</code>.</p>"},{"location":"SQL/connecting-with-dbt.html#4-ssl-certificate-not-trusted","title":"4. SSL Certificate Not Trusted","text":""},{"location":"SQL/connecting-with-dbt.html#error_3","title":"Error:","text":"<p>The connection to SQL Server failed with the error: <code>The certificate chain was issued by an authority that is not trusted</code>.</p>"},{"location":"SQL/connecting-with-dbt.html#resolution_3","title":"Resolution:","text":"<p>To resolve this, add <code>trust_cert: true</code> to your <code>profiles.yml</code> file. This will bypass the SSL certificate validation error and allow dbt to connect to SQL Server.</p>"},{"location":"SQL/connecting-with-dbt.html#connect-local-dbt-with-spark","title":"Connect Local dbt with spark","text":"<p>Further reading https://github.com/dbt-labs/dbt-spark</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.0.1__Python_PySpark_Spark_Confusion.html","title":"Python, PySpark, and Spark - Removing the Confusion","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.0.1__Python_PySpark_Spark_Confusion.html#introduction","title":"Introduction","text":"<p>When you start learning about big data and Spark, there is a lot of confusion. What is Spark? What is PySpark? If I install Python and pyspark library, do I have Spark? Do I need Java? What about these Docker images with Jupyter Spark?</p> <p>Let me clear up all this confusion. I will explain what each component actually is and how they fit together.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.0.1__Python_PySpark_Spark_Confusion.html#what-is-spark-anyway","title":"What is Spark Anyway?","text":"<p>Apache Spark is a distributed computing engine. It is software that can process huge amounts of data by splitting the work across multiple computers. Instead of one computer doing all the work, Spark distributes the job to many computers working together.</p> <p>But here is the important part - Spark is a specialized application written in Scala. Scala is a programming language that runs on the Java Virtual Machine (JVM). So Spark is basically a complex program written in Scala, compiled to Java bytecode, and runs on JVM.</p> <p>Spark is not just any Java program. It has very specialized code that implements distributed computing. Things like: - Query optimizer (called Catalyst) - Execution planner - Task scheduler - Memory management - Shuffle mechanism for moving data between nodes - Fault tolerance logic - And much more</p> <p>All of this is the \"Spark engine\". This is what makes Spark special.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.0.1__Python_PySpark_Spark_Confusion.html#what-is-jvms-role","title":"What is JVM's Role?","text":"<p>JVM (Java Virtual Machine) is like an operating system for Java programs. It provides the runtime environment where Java and Scala programs can execute.</p> <p>JVM itself does not know anything about distributed computing or Spark. It just runs whatever Java/Scala application you give it.</p> <p>Think of it like this: - JVM = The stage - Spark = The actor performing on that stage</p> <p>You need the stage, but the stage alone does not give you the performance. You need the actual Spark application running on it.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.0.1__Python_PySpark_Spark_Confusion.html#what-is-pyspark","title":"What is PySpark?","text":"<p>PySpark is the Python API for Spark. It is a Python library that lets you write Python code to use Spark.</p> <p>But PySpark is just a wrapper. It is not Spark itself. When you write PySpark code, here is what actually happens:</p> <ol> <li>You write Python code using pyspark library</li> <li>PySpark translates your Python commands into instructions</li> <li>These instructions are sent to the Spark engine (running on JVM) through a bridge called Py4J</li> <li>The Spark engine does the actual work</li> <li>Results come back to your Python code</li> </ol> <p>So PySpark is like a translator. It lets Python talk to Spark, but the real work happens in the Spark engine on JVM.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.0.1__Python_PySpark_Spark_Confusion.html#the-full-picture","title":"The Full Picture","text":"<p>Let me put it all together:</p> <ul> <li>JVM = The runtime environment (like an OS for Java programs)</li> <li>Spark = A specialized application built on top of JVM with all the distributed computing logic</li> <li>PySpark = Python wrapper that talks to Spark via a bridge called Py4J</li> </ul> <p>All three are needed if you want to use Spark from Python.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.0.1__Python_PySpark_Spark_Confusion.html#does-installing-python-need-java","title":"Does Installing Python Need Java?","text":"<p>No, absolutely not.</p> <p>Python and Java are completely independent. You can install Python without Java, and it works perfectly fine for regular Python work - pandas, numpy, scikit-learn, whatever you want.</p> <p>You only need Java when you want to run Spark, because Spark is a Java/Scala application that runs on JVM.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.0.1__Python_PySpark_Spark_Confusion.html#what-happens-when-you-install-pyspark","title":"What Happens When You Install PySpark?","text":"<p>When you do <code>pip install pyspark</code>, two things get downloaded:</p> <ol> <li>The pyspark Python library - This has all the Python API code, the classes and functions you use</li> <li>The Spark binaries - The actual Spark application (JAR files with all the Scala/Java code)</li> </ol> <p>So installing pyspark gives you both the Python wrapper and the actual Spark engine.</p> <p>But here is the catch - you still need Java installed separately. The pyspark package does not include Java/JVM.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.0.1__Python_PySpark_Spark_Confusion.html#can-you-run-spark-without-java","title":"Can You Run Spark Without Java?","text":"<p>No, you cannot.</p> <p>Even after <code>pip install pyspark</code>, if you try to create a SparkSession without Java installed, it will fail. You will get an error saying Java is not found.</p> <p>The sequence should be: 1. Install Java (JRE or JDK) 2. Install Python 3. Install pyspark library (<code>pip install pyspark</code>) 4. Now you can run Spark</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.0.1__Python_PySpark_Spark_Confusion.html#does-installing-pyspark-make-it-real-spark","title":"Does Installing PySpark Make it \"Real Spark\"?","text":"<p>Yes, if you have Java installed.</p> <p>When you install pyspark (with Java present) and start a Spark session, you get real Spark running. Even if it runs only on your laptop.</p> <p>Why is it real Spark? Because you get:</p> <ol> <li>The actual Spark execution engine - The JVM process running Spark with all its internals</li> <li>Lazy evaluation - Your code builds a DAG (plan) first, then executes</li> <li>Distributed data structures - Your data is partitioned even in local mode</li> <li>Spark's query optimizer - Catalyst analyzes and optimizes your queries</li> <li>Spark's execution model - All the fault tolerance, caching, shuffle operations work the same</li> </ol> <p>The only difference between your laptop Spark and cluster Spark is WHERE the data partitions are. On laptop, all partitions are on one machine. On cluster, partitions are across multiple machines.</p> <p>But the Spark engine, the APIs, the execution model - everything is identical.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.0.1__Python_PySpark_Spark_Confusion.html#what-makes-spark-different-from-regular-python","title":"What Makes Spark Different from Regular Python?","text":"<p>This is important to understand. Forget about clusters and scale for a moment. What fundamentally makes Spark different from writing regular Python code?</p> <p>The execution model is different.</p> <p>When you write regular Python code with pandas, your code runs directly. Line by line. If you read a CSV, Python loads it into memory immediately. If you filter rows, Python does it right away. Everything is eager execution.</p> <p>Spark uses lazy evaluation. When you write Spark transformations, nothing happens immediately. Spark builds a plan of what you want to do. It creates a DAG (Directed Acyclic Graph) of operations. Only when you call an action (like <code>.show()</code> or <code>.write()</code>), Spark executes the entire plan.</p> <p>Why does this matter? Because Spark can optimize the whole workflow. It looks at all your operations together and figures out the most efficient way. It can skip unnecessary work, combine operations, reorder things for better performance.</p> <p>The data structures are different.</p> <p>In regular Python, you work with lists, arrays, or pandas DataFrames that live in your computer's memory as one object.</p> <p>In Spark, you work with RDDs (Resilient Distributed Datasets) or Spark DataFrames. These are not stored as one object. They are logical representations of data that is split into partitions. Even on your laptop in local mode, Spark divides your data into chunks. Each chunk can be processed independently.</p> <p>The computation engine is different.</p> <p>When you use pandas, your Python code directly manipulates the data. When you use PySpark, your Python code generates instructions sent to the Spark engine on JVM. The Spark engine then executes those instructions using its sophisticated optimizer and execution planner.</p> <p>Spark has Catalyst (query optimizer) and Tungsten (execution engine). These analyze your code and generate optimized execution plans. Regular Python libraries do not have this.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.0.1__Python_PySpark_Spark_Confusion.html#what-is-production-spark","title":"What is Production Spark?","text":"<p>Now let me talk about real production Spark.</p> <p>Production Spark runs on a cluster of machines. Not one laptop, but many servers working together. You have one master node coordinating everything. Then you have multiple worker nodes doing the actual data processing.</p> <p>In production, Spark typically runs on: - Hadoop YARN clusters - Kubernetes clusters - Standalone cluster mode - Cloud platforms like AWS EMR, Azure HDInsight, or Databricks</p> <p>The data is stored differently. In production, you work with huge datasets in HDFS, S3, Azure Blob Storage, or data lakes. We are talking terabytes or petabytes of data.</p> <p>Production Spark has proper resource management. You configure executor memory, number of cores, how to handle failures, etc. There is monitoring, logging, job scheduling, and many other operational concerns.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts.html","title":"Overview","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts.html#what-is-spark","title":"What is Spark?","text":"<p>Its mainly a processing engine more like Mapreduce v2. It has no internal storage sytem like HDFS, but uses external file system lie ADLS, HDFS, S3 etc. Its processing engine is called Spark Core. It has standalone resource manager by default to manage its clusters.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts.html#spark-architecture","title":"Spark Architecture","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts.html#spark-lifecycle","title":"Spark Lifecycle","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts.html#what-is-apache-hive","title":"What is Apache Hive?","text":"<p>Apache Hive is a database  like MSSQL Server. Its actually a data warehouse. It stores data in Hadoop File system(HDFS) as tables. Hive's query language is called HiveQL, similar to SQL. he Hive metastore stores information about Hive tables. Developed by Facebook, Hive later became an Apache project.</p> <p></p>"},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts.html#what-is-hadoop","title":"What is Hadoop?","text":"<p>Hadoop is a distributed file system and processing framework similar to an MSSQL cluster but designed for handling large-scale file systems and big data. Here are its main components:</p> <ul> <li>Hadoop Distributed File System (HDFS): The file system for big data, akin to NTFS or FAT in traditional systems.</li> <li>MapReduce: The older generation of big data processing, similar to Spark.</li> <li>YARN (Yet Another Resource Negotiator): The cluster manager.</li> <li>Hadoop Common: The set of shared libraries and utilities.</li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts.html#is-spark-replacing-mapreduce","title":"Is Spark replacing MapReduce?","text":"<p>Yes, Spark is like the next version of MapReduce.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts.html#is-branded-spark-synapsedatabricks-replacing-traditional-spark-hadoop-and-hive","title":"Is branded Spark (Synapse/Databricks) replacing traditional Spark, Hadoop, and Hive?","text":"<p>Imagine your company is new to data engineering and needs to process a lot of data. How long will it take to set up with Azure compared to using free open-source products on bare metal?</p> <p>With Azure, you just sign up and create the setup with a few clicks. If the company has the budget, the entire setup takes about an hour. On the other hand, using traditional Spark, Hive, and Hadoop, setting up servers, networks, installation, configuration, and connectivity can become a year-long project.</p> <p>That\u2019s the difference between open-source and paid services. Open-source is free but risky. Paid services cost money but are easy, fast, accountable, and well-maintained.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts.html#never-heard-of-hive-we-use-only-spark-and-synapse-analytics","title":"Never heard of Hive. We use only Spark and Synapse Analytics.","text":"<p>If you always use branded products like Synapse and Databricks, you might not use Hive much. However, Hive catalogs are still used in Databricks. Just run a command like <code>DESCRIBE EXTENDED TableName</code>, and you'll see where Hive is involved. But you don't have to worry about the setup.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts.html#spark-rdds","title":"Spark RDDs","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.0_Spark-Concepts.html#spark-in-memory-computing","title":"Spark In-memory computing","text":"<p>When we talk about in-memory its not just conventioal caching. Its actually storing data in RAM, processing in RAM etc. So, spark uses it and thats why its faster than mapreduce.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.10_Scala_Cheatsheet.html","title":"Scala Guide","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.10_Scala_Cheatsheet.html#scala-cheatsheet-for-spark","title":"Scala Cheatsheet for Spark","text":"Category Operation Code Snippet Basic Operations Variable Declaration <code>val x: Int = 10  // Immutable</code><code>var y: Int = 20  // Mutable</code> Collections <code>val list = List(1, 2, 3, 4, 5)</code><code>val array = Array(1, 2, 3, 4, 5)</code><code>val map = Map(\"a\" -&gt; 1, \"b\" -&gt; 2, \"c\" -&gt; 3)</code> Spark Setup Initialize Spark Session <code>import org.apache.spark.sql.SparkSession</code><code>val spark = SparkSession.builder.appName(\"Spark App\").config(\"spark.master\", \"local\").getOrCreate()</code> RDD Operations Create RDD <code>val rdd = spark.sparkContext.parallelize(Seq(1, 2, 3, 4, 5))</code> Transformations <code>val mappedRDD = rdd.map(_ * 2)</code><code>val filteredRDD = rdd.filter(_ &gt; 2)</code> Actions <code>val collected = rdd.collect()</code><code>val count = rdd.count()</code><code>val firstElement = rdd.first()</code> DataFrame Operations Create DataFrame <code>import spark.implicits._</code><code>val df = Seq((1, \"a\"), (2, \"b\"), (3, \"c\")).toDF(\"id\", \"value\")</code> Show DataFrame <code>df.show()</code> DataFrame Transformations <code>val filteredDF = df.filter($\"id\" &gt; 1)</code><code>val selectedDF = df.select(\"value\")</code><code>val withColumnDF = df.withColumn(\"new_column\", $\"id\" * 2)</code> SQL Queries <code>df.createOrReplaceTempView(\"table\")</code><code>val sqlDF = spark.sql(\"SELECT * FROM table WHERE id &gt; 1\")</code> Dataset Operations Create Dataset <code>case class Record(id: Int, value: String)</code><code>val ds = Seq(Record(1, \"a\"), Record(2, \"b\"), Record(3, \"c\")).toDS()</code> Dataset Transformations <code>val filteredDS = ds.filter(_.id &gt; 1)</code><code>val mappedDS = ds.map(record =&gt; record.copy(value = record.value.toUpperCase))</code> Conversions RDD to DataFrame <code>val rddToDF = rdd.toDF(\"numbers\")</code> DataFrame to RDD <code>val dfToRDD = df.rdd</code> DataFrame to Dataset <code>val dfToDS = df.as[Record]</code> Dataset to DataFrame <code>val dsToDF = ds.toDF()</code> Reading and Writing Data Read CSV <code>val csvDF = spark.read.option(\"header\", \"true\").csv(\"path/to/file.csv\")</code> Write CSV <code>df.write.option(\"header\", \"true\").csv(\"path/to/save\")</code> Read Parquet <code>val parquetDF = spark.read.parquet(\"path/to/file.parquet\")</code> Write Parquet <code>df.write.parquet(\"path/to/save\")</code> Common Data Engineering Functions GroupBy and Aggregations <code>val groupedDF = df.groupBy(\"id\").count()</code><code>val aggregatedDF = df.groupBy(\"id\").agg(sum(\"value\"))</code> Join Operations <code>val df1 = Seq((1, \"a\"), (2, \"b\")).toDF(\"id\", \"value1\")</code><code>val df2 = Seq((1, \"x\"), (2, \"y\")).toDF(\"id\", \"value2\")</code><code>val joinedDF = df1.join(df2, \"id\")</code> Window Functions <code>import org.apache.spark.sql.expressions.Window</code><code>import org.apache.spark.sql.functions._</code><code>val windowSpec = Window.partitionBy(\"id\").orderBy(\"value\")</code><code>val windowedDF = df.withColumn(\"rank\", rank().over(windowSpec))</code> UDFs (User-Defined Functions) <code>import org.apache.spark.sql.functions.udf</code><code>val addOne = udf((x: Int) =&gt; x + 1)</code><code>val dfWithUDF = df.withColumn(\"new_value\", addOne($\"id\"))</code>"},{"location":"Spark-DataBricks/1.0_Spark/1.11_Spark_Interview_Questions.html","title":"Common Spark Concepts which may be asked in an interview","text":"<p>In this article I will try to put some common Spark concepts in a tabular format(So that its compact). These are good concepts to remember. Also, they may be asked in Interview questions.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.11_Spark_Interview_Questions.html#types-of-join-strateries","title":"Types of join strateries","text":"Join Strategy Description Use Case Example Shuffle Hash Join Both DataFrames are shuffled based on the join keys, and then a hash join is performed. Useful when both DataFrames are large and have a good distribution of keys. <code>spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"false\")</code><code>df1.join(df2, \"key\")</code> Broadcast Hash Join One of the DataFrames is small enough to fit in memory and is broadcasted to all worker nodes. A hash join is then performed. Efficient when one DataFrame is much smaller than the other. <code>val broadcastDF = broadcast(df2)</code><code>df1.join(broadcastDF, \"key\")</code> Sort-Merge Join Both DataFrames are sorted on the join keys and then merged. This requires a shuffle if the data is not already sorted. Suitable for large DataFrames when the join keys are sorted or can be sorted efficiently. <code>spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"true\")</code><code>df1.join(df2, \"key\")</code> Cartesian Join (Cross Join) Every row of one DataFrame is paired with every row of the other DataFrame. Generally not recommended due to its computational expense, but can be used for generating combinations of all rows. <code>df1.crossJoin(df2)</code> Broadcast Nested Loop Join The smaller DataFrame is broadcasted, and a nested loop join is performed. Used when there are no join keys or the join condition is complex and cannot be optimized with hash or sort-merge joins. <code>val broadcastDF = broadcast(df2)</code><code>df1.join(broadcastDF)</code> Shuffle-and-Replicate Nested Loop Join Both DataFrames are shuffled and replicated to perform a nested loop join. Used for complex join conditions that cannot be handled by other join strategies. <code>df1.join(df2, expr(\"complex_condition\"))</code>"},{"location":"Spark-DataBricks/1.0_Spark/1.11_Spark_Interview_Questions.html#types-of-joins","title":"Types of Joins","text":"Join Type Description Example Inner Join Returns rows that have matching values in both DataFrames. <code>df1.join(df2, \"key\")</code> Outer Join Returns all rows when there is a match in either DataFrame. Missing values are filled with nulls. <code>df1.join(df2, Seq(\"key\"), \"outer\")</code> Left Outer Join Returns all rows from the left DataFrame, and matched rows from the right DataFrame. <code>df1.join(df2, Seq(\"key\"), \"left_outer\")</code> Right Outer Join Returns all rows from the right DataFrame, and matched rows from the left DataFrame. <code>df1.join(df2, Seq(\"key\"), \"right_outer\")</code> Left Semi Join Returns only the rows from the left DataFrame that have a match in the right DataFrame. <code>df1.join(df2, Seq(\"key\"), \"left_semi\")</code> Left Anti Join Returns only the rows from the left DataFrame that do not have a match in the right DataFrame. <code>df1.join(df2, Seq(\"key\"), \"left_anti\")</code> Cross Join Returns the Cartesian product of both DataFrames. Every row in the left DataFrame will be combined with every row in the right DataFrame. <code>df1.crossJoin(df2)</code> Self Join A join in which a DataFrame is joined with itself. This can be an inner, outer, left, or right join. <code>df.join(df, df(\"key1\") === df(\"key2\"))</code>"},{"location":"Spark-DataBricks/1.0_Spark/1.11_Spark_Interview_Questions.html#common-spark-optimization-techniques","title":"Common Spark optimization techniques","text":"Technique What it is When to use Example Caching and Persistence Storing data in memory for quick access When you need to reuse the same data multiple times <code>scala val cachedData = df.cache()</code> Broadcast Variables Sending a small dataset to all worker nodes When one dataset is much smaller than the other <code>scala val broadcastData = spark.broadcast(smallDF) largeDF.join(broadcastData.value, \"key\")</code> Partitioning Dividing data into smaller, manageable chunks When dealing with large datasets to improve parallel processing <code>scala val partitionedData = df.repartition(10, $\"key\")</code> Avoiding Shuffles Reducing the movement of data between nodes To improve performance by minimizing network overhead Use <code>mapPartitions</code> instead of <code>groupBy</code> when possible Coalesce Reducing the number of partitions When the data has become sparse after a transformation <code>scala val coalescedData = df.coalesce(1)</code> Predicate Pushdown Filtering data as early as possible in the processing To reduce the amount of data read and processed <code>scala val filteredData = df.filter($\"column\" &gt; 10)</code> Using the Right Join Strategy Choosing the most efficient way to join two datasets Based on the size and distribution of data Prefer broadcast joins for small datasets Tuning Spark Configurations Adjusting settings to optimize resource usage To match the workload and cluster resources <code>scala spark.conf.set(\"spark.executor.memory\", \"4g\")</code> Using DataFrames/Datasets API Leveraging the high-level APIs for optimizations To benefit from Catalyst optimizer and Tungsten execution engine <code>scala val df = spark.read.csv(\"data.csv\") df.groupBy(\"column\").count()</code> Vectorized Query Execution Processing multiple rows of data at a time For high-performance operations on large datasets Use built-in SQL functions and DataFrame methods"},{"location":"Spark-DataBricks/1.0_Spark/1.11_Spark_Interview_Questions.html#different-phases-of-spark-sql-engine","title":"Different phases of Spark-SQL engine","text":"Phase Description Details Example Parsing Converting SQL queries into a logical plan. The SQL query is parsed into an abstract syntax tree (AST). Converting <code>SELECT * FROM table WHERE id = 1</code> into an internal format. Analysis Resolving references and verifying the logical plan. Resolves column names, table names, and function names; checks for errors. Ensuring that the table <code>table</code> and the column <code>id</code> exist in the database. Optimization Improving the logical plan for better performance. Transforms the logical plan using various optimization techniques; applies rules via Catalyst optimizer. Reordering filters to reduce the amount of data processed early on. Physical Planning Converting the logical plan into a physical plan. Converts the optimized logical plan into one or more physical plans; selects the most efficient plan. Deciding whether to use a hash join or a sort-merge join. Code Generation Generating executable code from the physical plan. Generates Java bytecode to execute the physical plan; this code runs on Spark executors. Creating code to perform join operations, filter data, and compute results. Execution Running the generated code on the Spark cluster. Distributes generated code across the Spark cluster; executed by Spark executors; results collected and returned. Running join and filter operations on different nodes in the cluster and aggregating results."},{"location":"Spark-DataBricks/1.0_Spark/1.11_Spark_Interview_Questions.html#common-reasons-for-analysis-exception-in-spark","title":"Common reasons for analysis exception in Spark","text":"<p>Here are some common reasons why you might encounter an AnalysisException in Spark:</p> Reason Description Example Non-Existent Column or Table Column or table specified does not exist. Referring to a non-existent column <code>id</code>. Ambiguous Column Reference Same column name exists in multiple tables without qualification. Joining two DataFrames with the same column name <code>id</code>. Invalid SQL Syntax SQL query has syntax errors. Using incorrect SQL syntax like <code>SELCT</code>. Unsupported Operations Using an operation that Spark SQL does not support. Using an unsupported function. Schema Mismatch Schema of the DataFrame does not match the expected schema. Inserting data with different column types. Missing File or Directory Specified file or directory does not exist. Referring to a non-existent CSV file. Incorrect Data Type Operations expecting a specific data type are given the wrong type. Performing a math operation on a string column."},{"location":"Spark-DataBricks/1.0_Spark/1.11_Spark_Interview_Questions.html#flow-of-how-spark-works-internally","title":"Flow of how Spark works internally","text":"Component/Step Role/Process Function/Example Driver Program Entry point for the Spark application - Manages application lifecycle- Defines RDD transformations and actions SparkContext Acts as the master of the Spark application - Connects to cluster manager- Coordinates tasks Cluster Manager Manages the cluster of machines - Allocates resources to Spark applications- Examples: YARN, Mesos, standalone Executors Worker nodes that run tasks and store data - Execute assigned code- Return results to the driver- Cache data in memory for quick access Spark Application Submission Submitting the driver program to the cluster manager - Example: Submitting a job using <code>spark-submit</code> SparkContext Initialization Driver program initializes SparkContext - Example: <code>val sc = new SparkContext(conf)</code> Job Scheduling Driver program defines transformations and actions on RDDs/DataFrames - Example: <code>val rdd = sc.textFile(\"data.txt\").map(line =&gt; line.split(\" \"))</code> DAG (Directed Acyclic Graph) Creation Constructing a DAG of stages for the job - Stages are sets of tasks that can be executed in parallel- Example: A series of <code>map</code> and <code>filter</code> transformations create a DAG Task Execution Dividing the DAG into stages, creating tasks, and sending them to executors - Tasks are distributed across executors- Each executor processes a partition of the data Data Shuffling Exchanging data between nodes during operations like <code>reduceByKey</code> - Data is grouped by key across nodes- Example: Shuffling data for aggregations Result Collection Executors process the tasks and send the results back to the driver program - Example: Final results of <code>collect</code> or <code>count</code> are returned to the driver Job Completion Driver program completes the execution - Example: Driver terminates after executing <code>sc.stop()</code>"},{"location":"Spark-DataBricks/1.0_Spark/1.11_Spark_Interview_Questions.html#explain-dag-in-spark","title":"Explain DAG in Spark","text":"Topic Description Details Example DAG in Spark DAG stands for Directed Acyclic Graph. - Series of steps representing the operations on data.- Directed: Operations flow in one direction.- Acyclic: No cycles or loops. N/A Why We Need DAG Optimizes Execution, Fault Tolerance, and Parallel Processing. - Optimizes Execution: Spark can optimize operations.- Fault Tolerance: Recomputes lost data if a node fails.- Parallel Processing: Divides tasks into stages for parallel execution. N/A Without DAG No Optimization, No Fault Tolerance, and Less Parallelism. - No Optimization: Operations would run as written, slower performance.- No Fault Tolerance: Inefficient data recomputation.- Less Parallelism: Harder to parallelize tasks. N/A Example Example of a Spark job and DAG construction. - Read Data: <code>sc.textFile(\"file.txt\")</code>- Split Lines into Words: <code>data.flatMap(...)</code>- Map Words to Key-Value Pairs: <code>words.map(...)</code>- Reduce by Key: <code>wordCounts.reduceByKey(...)</code>- Collect Results: <code>wordCounts.collect()</code> <code>scala val data = sc.textFile(\"file.txt\") val words = data.flatMap(line =&gt; line.split(\" \")) val wordCounts = words.map(word =&gt; (word, 1)).reduceByKey(_ + _) wordCounts.collect()</code>"},{"location":"Spark-DataBricks/1.0_Spark/1.11_Spark_Interview_Questions.html#explain-sparksqlshufflepartitions-variable","title":"Explain spark.sql.shuffle.partitions Variable","text":"Topic Description Details Example spark.sql.shuffle.partitions Configuration setting for shuffle partitions - Default Value: 200 partitions- Defines the default number of partitions used when shuffling data for wide transformations <code>scala spark.conf.set(\"spark.sql.shuffle.partitions\", \"number_of_partitions\")</code> Purpose Optimize Performance and Control Data Distribution - Optimize Performance: Balances workload across the cluster- Control Data Distribution: Manages how data is distributed and processed during shuffle operations N/A When It's Used Wide Transformations and SQL Queries - Wide Transformations: <code>reduceByKey</code>, <code>groupByKey</code>, <code>join</code>, etc.- SQL Queries: Operations involving shuffling data like joins and aggregations N/A How to Set It Setting via Configuration and spark-submit - Configuration: <code>spark.conf.set(\"spark.sql.shuffle.partitions\", \"number_of_partitions\")</code>- spark-submit: <code>spark-submit --conf spark.sql.shuffle.partitions=number_of_partitions ...</code> N/A Example Default and Custom Settings - Default Setting: <code>scala val spark = SparkSession.builder.appName(\"Example\").getOrCreate() println(spark.conf.get(\"spark.sql.shuffle.partitions\")) // Output: 200</code>- Custom Setting: <code>scala val spark = SparkSession.builder.appName(\"Example\").getOrCreate() spark.conf.set(\"spark.sql.shuffle.partitions\", \"50\") val df = spark.read.json(\"data.json\") df.groupBy(\"column\").count().show()</code> <code>scala val spark = SparkSession.builder.appName(\"Example\").getOrCreate() println(spark.conf.get(\"spark.sql.shuffle.partitions\")) // Output: 200</code> Why Adjust This Setting? Small and Large Datasets - Small Datasets: Reduce the number of partitions to avoid too many small tasks, leading to overhead- Large Datasets: Increase the number of partitions to distribute data evenly and avoid large partitions that slow down processing N/A"},{"location":"Spark-DataBricks/1.0_Spark/1.12_Spark_Shuffle.html","title":"Shuffle","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.12_Spark_Shuffle.html#shuffle-in-spark-moving-data-between-partitions","title":"Shuffle in Spark - Moving Data Between Partitions","text":"<p>You know about partitions, where data is divided into different containers. But, suppose you are doing a <code>GroupBy</code> operation. You will need to bring the data together in one place.</p> <p>This process of bringing data from different partitions to one place is called shuffle.</p> <p>Shuffle happens during operations like <code>GroupBy</code>, <code>Join</code>, or <code>ReduceByKey</code>.</p> <p>However, shuffle is very expensive. Our goal should be to reduce it as much as possible. Here are some ways to reduce shuffle:</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.12_Spark_Shuffle.html#combine-operations-to-reduce-shuffles","title":"Combine Operations to Reduce Shuffles","text":"<p>Instead of doing separate operations that cause multiple shuffles, combine them into one.</p> <pre><code>df1 = df.groupBy(\"id\").count()\ndf2 = df1.filter(df1[\"count\"] &gt; 1)\n# Combined operations to reduce shuffles:\ndf_combined = df.groupBy(\"id\").count().filter(\"count &gt; 1\")\ndf_combined.show()\n</code></pre>"},{"location":"Spark-DataBricks/1.0_Spark/1.12_Spark_Shuffle.html#repartition-your-data","title":"Repartition Your Data","text":"<p>Repartition your data to balance the load and optimize data distribution.</p> <pre><code># Example DataFrame\ndf = spark.createDataFrame([(1, 'a'), (2, 'b'), (1, 'c'), (2, 'd')], [\"id\", \"value\"])\n\n# Repartitioning to optimize data distribution\ndf_repartitioned = df.repartition(\"id\")\ndf_repartitioned.show()\n</code></pre>"},{"location":"Spark-DataBricks/1.0_Spark/1.12_Spark_Shuffle.html#cache-data-for-reuse","title":"Cache Data for Reuse","text":"<p>Cache data that you need to use multiple times to avoid repeated shuffling.</p> <pre><code># Example DataFrame\ndf = spark.createDataFrame([(1, 'a'), (2, 'b'), (1, 'c'), (2, 'd')], [\"id\", \"value\"])\n\n# Caching intermediate DataFrame\ndf_cached = df.cache()\n\n# Using cached DataFrame multiple times\ndf_cached.groupBy(\"id\").count().show()\ndf_cached.filter(df_cached[\"id\"] == 1).show()\n</code></pre> <p>So, you can  operations, repartitioning data, and cahe frequently used data to reduce shuffle.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.13_SparkDatabaseTablesCatalogsMetastore.html","title":"Spark Database, Tables, Warehouse, Metastore &amp; Catalogs","text":"<ul> <li>A Spark Database is just a folder named databasename.db inside the spark-warehouse folder.</li> <li>A Managed/Internal/Spark-Metastore table is a subfolder within the databasename.db folder. Partitions are also stored as subfolders.</li> <li>The location of the warehouse folder is set by the spark.sql.warehouse.dir setting.</li> <li>If spark.sql.warehouse.dir is not set, Spark uses a default directory, usually a spark-warehouse folder in the current working directory of the application.</li> <li> <p>You can find out the warehouse directory by running the command <code>SET spark.sql.warehouse.dir</code> in the spark-sql prompt.</p> <p></p> </li> <li> <p>You can set the warehouse directory in your session with <code>.config(\"spark.sql.warehouse.dir\", \"/path/to/your/warehouse\")</code>.</p> </li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/1.13_SparkDatabaseTablesCatalogsMetastore.html#spark-managed-tables-aka-internal-spark-metastore-tables-using-spark-sql-shell","title":"SPARK Managed Tables (AKA Internal / Spark-Metastore Tables) Using Spark-SQL Shell","text":"<p>When you create tables in the spark-sql shell using commands like the one below, Spark will create a managed table. The table data will be stored in the <code>spark-warehouse</code> folder, and the Derby database (<code>metastore_db</code> folder) will contain its metadata.</p> <pre><code>CREATE TABLE Hollywood (name STRING);\n</code></pre> <p>The table will be permanent, meaning you can query it even after restarting Spark. Here is an example output of <code>DESCRIBE EXTENDED Hollywood</code> in the spark-sql shell:</p> <p></p> <ul> <li>Catalog: <code>spark_catalog</code> - Spark uses its own internal catalog to manage metadata.</li> <li>Database: <code>default</code> - The default database provided by Spark.</li> <li>Type: <code>MANAGED</code> - Indicates that Spark manages the table's lifecycle.</li> <li>Provider: <code>hive</code> - Refers to Spark's capability to handle Hive-compatible metadata.</li> <li>Serde Library: <code>org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</code> - Serialization and deserialization library.</li> <li>InputFormat: <code>org.apache.hadoop.mapred.TextInputFormat</code> - Input format for reading the table data.</li> <li>OutputFormat: <code>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</code> - Output format for writing the table data.</li> <li>Location: <code>file:/home/dwdas/spark-warehouse/hollywood</code> - File path where the table data is stored.</li> <li>Partition Provider: <code>Catalog</code> - Indicates that the catalog manages partitions.</li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/1.13_SparkDatabaseTablesCatalogsMetastore.html#key-takeaways","title":"Key Takeaways:","text":"<ul> <li>Built-in Catalog: Even without a standalone Hive installation, Spark provides managed table functionality by leveraging its built-in catalog and Hive-compatible features.</li> <li>SQL-like Operations: You can use SQL-like operations to manage tables within Spark.</li> <li>Embedded Deployment Mode: By default, Spark SQL uses an embedded deployment mode of a Hive metastore with an Apache Derby database.</li> <li>Production Use: The default embedded deployment mode is not recommended for production use due to the limitation of only one active SparkSession at a time.</li> </ul> <p> Remember: This Derby-mini-Hive method that Spark uses to manage internal tables has a limitation: only one active session is allowed at a time. Attempting multiple `spark-sql` sessions will result in a Derby database exception. </p> <p></p> <p>Would you take this to production?</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.13_SparkDatabaseTablesCatalogsMetastore.html#metastore-in-spark","title":"Metastore in Spark","text":"<p>The metastore in Spark stores metadata about tables, like their names and the locations of their files (e.g., Parquet files). In Spark, the metastore is typically configured in one of two common ways, but there are also more advanced options available.</p> <ol> <li> <p>Standalone Hive Metastore: You can install and configure a standalone Hive metastore server. This server would manage the metadata independently and communicate with your Spark application.</p> </li> <li> <p>Embedded Hive Metastore with Derby</p> </li> </ol> <p>Spark includes a built-in metastore that uses an embedded Apache Derby database. This database starts in the application's working directory and stores data in the <code>metastore_db</code> folder. It's a convenient, pseudo-metastore suitable for small applications or datasets. To use this, simply enable Hive support in your Spark session with <code>enableHiveSupport()</code>:</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"EmbeddedMetastoreExample\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n</code></pre> <p> Again: By default, Hive uses an embedded Apache Derby database to store its metadata. While this is a convenient option for initial setup, it has limitations. Notably, Derby can only support one active user at a time. This makes it unsuitable for scenarios requiring multiple concurrent Hive sessions. So, the solution is to use a standard database like MySQL/Postgrees or MSSQL as the metastore DB. And, let the poor derby take  some rest. </p>"},{"location":"Spark-DataBricks/1.0_Spark/1.13_SparkDatabaseTablesCatalogsMetastore.html#catalog-types-in-spark","title":"Catalog types in Spark","text":"<ol> <li> <p>In-Memory Catalog: Default catalog. Stores data memory. Everything vanishes when session is closed. For some quick queries etc.</p> </li> <li> <p>Hive Catalog: A mini-version comes shipped with Spark. You need to enable hive support to use it. Data is stoerd permanently. Can have a full-fledged hivve as well.</p> </li> <li> <p>JDBC Catalog: When you want a full-database like MSSQL to store the catalog information. Can be like Hive+MSSQL.</p> </li> <li> <p>Custom Catalogs: Custom catalogs can be implemented using ExtendedCatalogInterface. AWS glue, Synapese databricks.</p> </li> <li> <p>Delta Lake Catalog: When using Delta Lake, it provides its own catalog implementation.</p> </li> </ol>"},{"location":"Spark-DataBricks/1.0_Spark/1.13_SparkDatabaseTablesCatalogsMetastore.html#what-happens-when-you-enter-spark-sql-on-a-freshly-installed-spark-server","title":"What happens when you enter <code>spark-sql</code> on a freshly installed Spark server?","text":"<p>Imagine you have a fresh standalone Spark server. You log into the server through the terminal, and your current directory is <code>/home/dwdas</code>. The moment you enter the <code>spark-sql</code> command, Spark starts an embedded Derby database and creates a <code>metastore_db</code> folder in your current directory. This folder serves as the root of the Derby database. Essentially, Spark \"boots a Derby instance on <code>metastore_db</code>\".</p> <p>By default, a fresh Spark setup uses its in-house Derby database as a 'metastore' to store the names and file locations of the Spark tables you create. This is Spark's basic way to manage its tables. Though you can upgrade it and use an external metastore and other advanced features.</p> <p></p> <p> Remember: Your current directory is crucial because everything is created inside it. This often confuses new learners who forget this, making it hard to find their tables later. </p> <p> Remember: Spark SQL does not use a Hive metastore under the cover. By default, it uses in-memory catalogs if Hive support is not enabled. Spark-shell: By default, uses in-memory catalogs unless configured to use Hive metastore. Set <code>spark.sql.catalogImplementation</code> to <code>hive</code> or <code>in-memory</code> to control this. </p>"},{"location":"Spark-DataBricks/1.0_Spark/1.13_SparkDatabaseTablesCatalogsMetastore.html#what-happens-when-you-create-a-table-in-spark-sql","title":"What Happens When You Create a Table in Spark-SQL?","text":"<p>Let's open the spark-sql and enter this simple command:</p> <pre><code>CREATE TABLE movies (title STRING, genre STRING);\nINSERT INTO movies VALUES ('Inception', 'Sci-Fi');\n</code></pre> <p>You will see logs like these:</p> <pre><code>24/06/20 07:44:39 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n</code></pre> <p></p> <p>Explanation: - When you execute a <code>CREATE TABLE</code> statement without specifying a table provider, Spark\u2019s session catalog defaults to creating a Hive SerDe table. The session catalog interprets and resolves SQL commands.</p> <ul> <li> <p>Note: To create a native Spark SQL table, set <code>spark.sql.legacy.createHiveTableByDefault</code> to <code>false</code>.</p> </li> <li> <p>In <code>spark-defaults.conf</code>:</p> <pre><code>spark.sql.legacy.createHiveTableByDefault=false\n</code></pre> </li> <li> <p>In Spark SQL shell:</p> <pre><code>spark-sql --conf spark.sql.legacy.createHiveTableByDefault=false\n</code></pre> </li> <li> <p>In PySpark session:</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"example\") \\\n    .config(\"spark.sql.legacy.createHiveTableByDefault\", \"false\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n</code></pre> </li> </ul> <p>You might also see a warning like this:</p> <pre><code>24/06/20 07:44:40 WARN HiveMetaStore: Location: file:/home/dwdas/spark-warehouse/movies specified for non-external table: movies\n</code></pre> <p>Explanation: This indicates the default storage location for non-external tables is <code>spark-warehouse</code>.</p> <p>Note: Change the default location by setting <code>spark.sql.warehouse.dir</code>. To change the warehouse directory:</p> <ul> <li> <p>In <code>spark-defaults.conf</code>:</p> <pre><code>spark.sql.warehouse.dir=/your/custom/path\n</code></pre> </li> <li> <p>In Spark SQL shell:</p> <pre><code>spark-sql --conf spark.sql.warehouse.dir=/your/custom/path\n</code></pre> </li> <li> <p>In PySpark session:</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"example\") \\\n    .config(\"spark.sql.warehouse.dir\", \"/your/custom/path\") \\\n    .getOrCreate()\n</code></pre> </li> </ul> <p>When you run <code>spark-sql</code> with default settings, it will start a Derby database and create a <code>metastore_db</code> folder inside your current directory. So, be mindful of your current directory.</p> <p>If you create a table in a PySpark session, Spark will create both a <code>metastore_db</code> and a <code>spark-warehouse</code> folder.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.13_SparkDatabaseTablesCatalogsMetastore.html#key-takeaway","title":"Key Takeaway","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.14_Q%26A.html","title":"1.14 Q&A","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.14_Q%26A.html#q-a","title":"Q &amp; A","text":"<p>Which of the following statements about the Spark driver is incorrect?</p> <p>A. The Spark driver is the node in which the Spark application's main method runs to coordinate the Spark application. B. The Spark driver is horizontally scaled to increase overall processing throughput. C. The Spark driver contains the SparkContext object. D. The Spark driver is responsible for scheduling the execution of data by various worker nodes in cluster mode. E. The Spark driver should be as close as possible to worker nodes for optimal performance.  </p> <p>Answer: B. The Spark driver is horizontally scaled to increase overall processing throughput.</p> <p>Explanation: - A is correct because the Spark driver indeed runs the application's main method and coordinates the application. - B is incorrect because the Spark driver is a single entity and is not horizontally scalable. The driver manages and coordinates tasks but does not scale horizontally to increase throughput. - C is correct because the Spark driver contains the SparkContext object, which is the entry point for Spark functionality. - D is correct because the Spark driver schedules the execution of tasks on worker nodes. - E is correct because the driver should be close to the worker nodes to minimize latency and improve performance.</p> <p>Which of the following describes nodes in cluster-mode Spark?</p> <p>A. Nodes are the most granular level of execution in the Spark execution hierarchy. B. There is only one node and it hosts both the driver and executors. C. Nodes are another term for executors, so they are processing engine instances for performing computations. D. There are driver nodes and worker nodes, both of which can scale horizontally. E. Worker nodes are machines that host the executors responsible for the execution of tasks.  </p> <p>Answer: E. Worker nodes are machines that host the executors responsible for the execution of tasks.</p> <p>Explanation: - A is incorrect because tasks, not nodes, are the most granular level of execution. - B is incorrect because in a cluster mode, there are multiple nodes, including separate driver and worker nodes. - C is incorrect because nodes and executors are not the same; nodes host executors. - D is incorrect because typically, only worker nodes scale horizontally, not driver nodes. - E is correct because worker nodes are indeed machines that host executors, which execute the tasks assigned by the driver.</p> <p>Which of the following statements about slots is true? A. There must be more slots than executors. B. There must be more tasks than slots. C. Slots are the most granular level of execution in the Spark execution hierarchy. D. Slots are not used in cluster mode. E. Slots are resources for parallelization within a Spark application.  </p> <p>Answer: E. Slots are resources for parallelization within a Spark application.</p> <p>Explanation: - A is incorrect because there is no requirement for having more slots than executors. - B is incorrect because the number of tasks does not necessarily have to exceed the number of slots. - C is incorrect because tasks, not slots, are the most granular level of execution. - D is incorrect because slots are indeed used in cluster mode to enable parallel task execution. - E is correct because slots are resources that allow tasks to run in parallel, providing the means for concurrent execution.</p> <p>Which of the following is a combination of a block of data and a set of transformers that will run on a single executor? A. Executor B. Node C. Job D. Task E. Slot  </p> <p>Answer: D. Task</p> <p>Explanation: - A is incorrect because an executor is a process running on a worker node that executes tasks. - B is incorrect because a node can host multiple executors. - C is incorrect because a job is a higher-level construct encompassing multiple stages and tasks. - D is correct because a task is the unit of work that includes a block of data and the computation to be performed on it. - E is incorrect because a slot is a resource for parallel task execution, not the unit of work itself.</p> <p>Which of the following is a group of tasks that can be executed in parallel to compute the same set of operations on potentially multiple machines? A. Job B. Slot C. Executor D. Task E. Stage  </p> <p>Answer: E. Stage</p> <p>Explanation:</p> <ul> <li>A is incorrect because a job is an entire computation consisting of multiple stages.</li> <li>B is incorrect because a slot is a resource for parallel task execution.</li> <li>C is incorrect because an executor runs tasks on worker nodes.</li> <li>D is incorrect because a task is a single unit of work.</li> <li>E is correct because a stage is a set of tasks that can be executed in parallel to perform the same computation.</li> </ul> <p>Which of the following describes a shuffle?</p> <p>A. A shuffle is the process by which data is compared across partitions. B. A shuffle is the process by which data is compared across executors. C. A shuffle is the process by which partitions are allocated to tasks. D. A shuffle is the process by which partitions are ordered for write. E. A shuffle is the process by which tasks are ordered for execution.  </p> <p>Answer: A. A shuffle is the process by which data is compared across partitions.</p> <p>Explanation:</p> <ul> <li>A is correct because shuffling involves redistributing data across partitions to align with the needs of downstream transformations.</li> <li>B is incorrect because shuffling happens across partitions, not specifically executors.</li> <li>C is incorrect because partition allocation to tasks is not the definition of a shuffle.</li> <li>D is incorrect because shuffling is not about ordering partitions for write operations.</li> <li>E is incorrect because shuffling does not involve ordering tasks for execution.</li> </ul> <p>DataFrame df is very large with a large number of partitions, more than there are executors in the cluster. Based on this situation, which of the following is incorrect? Assume there is one core per executor.</p> <p>A. Performance will be suboptimal because not all executors will be utilized at the same time. B. Performance will be suboptimal because not all data can be processed at the same time. C. There will be a large number of shuffle connections performed on DataFrame df when operations inducing a shuffle are called. D. There will be a lot of overhead associated with managing resources for data processing within each task. E. There might be risk of out-of-memory errors depending on the size of the executors in the cluster.  </p> <p>Answer: A. Performance will be suboptimal because not all executors will be utilized at the same time.</p> <p>Explanation:</p> <ul> <li>A is incorrect because having more partitions than executors does not necessarily mean executors will be underutilized; they will process partitions sequentially.</li> <li>B is correct because more partitions than executors mean data processing cannot happen all at once, affecting performance.</li> <li>C is correct because a high number of partitions can lead to many shuffle operations.</li> <li>D is correct because managing many tasks increases overhead.</li> <li>E is correct because large data volumes can risk out-of-memory errors if executor memory is insufficient.</li> </ul> <p>Which of the following operations will trigger evaluation?</p> <p>A. DataFrame.filter() B. DataFrame.distinct() C. DataFrame.intersect() D. DataFrame.join() E. DataFrame.count()  </p> <p>Answer: E. DataFrame.count()</p> <p>Explanation: - A is incorrect because <code>DataFrame.filter()</code> is a transformation, which defines a computation but does not trigger it. - B is incorrect because <code>DataFrame.distinct()</code> is also a transformation. - C is incorrect because <code>DataFrame.intersect()</code> is a transformation. - D is incorrect because <code>DataFrame.join()</code> is a transformation. - E is correct because <code>DataFrame.count()</code> is an action that triggers the actual execution of the computation.</p> <p>Which of the following describes the difference between transformations and actions?</p> <p>A. Transformations work on DataFrames/Datasets while actions are reserved for native language objects. B. There is no difference between actions and transformations. C. Actions are business logic operations that do not induce execution while transformations are execution triggers focused on returning results. D. Actions work on DataFrames/Datasets while transformations are reserved for native language objects. E. Transformations are business logic operations that do not induce execution while actions are execution triggers focused on returning results.  </p> <p>Answer: E. Transformations are business logic operations that do not induce execution while actions are execution triggers focused on returning results.</p> <p>Explanation: - A is incorrect because both transformations and actions work on DataFrames/Datasets. - B is incorrect because there is a clear difference between transformations and actions. - C is incorrect because it incorrectly describes the role of transformations and actions. - D is incorrect because both transformations and actions work on DataFrames/Datasets. - E is correct because transformations define the computations without triggering them, while actions trigger the execution and return the results.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics.html","title":"Some common Pyspark Topics","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics.html#just-pyspark-vs-real-spark","title":"Just PySpark vs Real Spark","text":"<p>Here, I'll try to clear up the often-muddled area between PySpark and Full Spark installations. We will also touch upon different types of python installations available.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics.html#standalone-python-vs-anaconda-python","title":"Standalone Python vs. Anaconda Python","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics.html#standalone-python","title":"Standalone Python","text":"<p>Thi is the python you directly install from Python Software Foundation. Choose this for a lightweight setup, specific version control, and when using Python for general-purpose programming.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics.html#anaconda-python","title":"Anaconda Python","text":"<p>An open-source Python distribution for scientific computing and data science.Go for Anaconda for an easy-to-manage data science environment, especially when dealing with large datasets, machine learning, and analytics.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics.html#pyspark-vs-full-apache-spark-installation","title":"PySpark vs full Apache Spark Installation","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics.html#pyspark-via-pip","title":"PySpark via pip","text":"<p>     Many believe <code>pip install pyspark</code> installs the entire Apache Spark framework.      No, it does not. When you install PySpark via pip, it installs the      Python interface plus a minimal, standalone version of Apache Spark      that can run locally on your machine. This standalone version of Spark is what allows the      simulation of a Spark cluster environment on your single computer. Here's a breakdown: </p> <ol> <li> <p>Apache Spark in PySpark: </p> <ul> <li>The PySpark package installed via pip includes a lightweight, standalone Spark installation. This isn't the full-fledged, distributed Spark system typically used in large-scale setups but a minimal version that can run on a single machine.</li> <li>When you execute PySpark code after installing it via pip, you're actually running this local version of Spark.</li> </ul> </li> <li> <p>Local Mode Execution:</p> <ul> <li>In this \"local mode,\" Spark operates as if it's a cluster but is actually just using the resources (like CPU and memory) of your single machine. It simulates the behavior of a Spark cluster, which in a full setup would distribute processing tasks across multiple nodes (machines).</li> <li>This mode is incredibly useful for development, testing, and learning because it lets you write and test code that would normally run on a Spark cluster, without the need for setting up multiple machines.</li> </ul> </li> <li> <p>The Spark Context:</p> <ul> <li>When your PySpark code runs, it initializes a \"Spark context\" within your Python script. This context is the primary connection to the Spark execution environment and allows your Python script to access Spark's functionalities.</li> <li>In the pip-installed PySpark environment, this Spark context talks to the local Spark instance included in the PySpark package, not a remote or distributed cluster.</li> </ul> </li> </ol>"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics.html#full-apache-spark-installation","title":"Full Apache Spark Installation","text":"<p>Full Spark Installation involves setting up the complete Apache Spark framework, for building large-scale data processing applications, beyond the scope of PySpark alone. This is necessary for production-grade, large-scale data processing and when you need to harness the full power of Spark's distributed computing capabilities.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics.html#conclusion","title":"Conclusion","text":"<p>To sum it up, <code>pip install pyspark</code> actually installs both the Python interface to Spark (PySpark) and a minimal, local-only version of Apache Spark itself. This setup allows you to run Spark jobs as if you had a Spark cluster, but it's all happening within your own computer. The local mode is a simulation of a distributed Spark environment, suitable for learning, development, and processing small data sets.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics.html#finding-spark","title":"Finding Spark","text":"<p>When you install Spark using a package manager like <code>pip</code>, it places the Spark binaries and libraries inside the Python's <code>dist-packages</code> or <code>site-packages</code> directory. This is a different installation approach than downloading and extracting Spark directly from the Apache Spark website.</p> <p>Here's what you need to know:</p> <ol> <li>SPARK_HOME for pip-installed PySpark: If you've installed Spark via <code>pip</code>, then the equivalent <code>SPARK_HOME</code> would be <code>/usr/local/lib/python3.8/dist-packages/pyspark/</code>. This directory would contain all the necessary libraries and dependencies for Spark to run within a Python environment.</li> <li>Configuration and JARs: Configuration files, JARs, and other related resources would also be located within this directory structure. For instance, if you're looking for the location to place additional JAR files, it would likely be somewhere like <code>/usr/local/lib/python3.8/dist-packages/pyspark/jars/</code>.</li> <li>Using <code>spark-submit</code>: The <code>spark-submit</code> script in this directory is tailored for the pip-installed Spark and ensures that it picks up the correct libraries and configurations from this location.</li> </ol> <p>In summary, if you've installed Spark using <code>pip</code> within your Docker container, then the <code>/usr/local/lib/python3.8/dist-packages/pyspark/</code> directory is your primary point of reference for Spark configurations, binaries, and libraries. This is distinct from a standalone Spark installation, which would typically reside directly under <code>/usr/local/</code> or another directory, separate from the Python packages directory.</p> <p>What if echo $SPARK_HOME or %SPARK_HOME% returns nothing and you are not sure where spark is installed? Even though you instaleld it?</p> <p>Let's try a different approach.</p> <p>Spark's binaries (like <code>spark-submit</code> and <code>spark-shell</code>) are often a good hint as to where Spark might be installed.</p> <ol> <li>Search for Spark binaries:    Execute the following command within your Docker container:</li> </ol> <pre><code>find / -name \"spark-submit\" 2&gt;/dev/null\n</code></pre> <p>This command searches for the <code>spark-submit</code> binary, which is a common Spark binary. The directory containing this binary is likely your Spark installation directory.</p> <ol> <li>Once you've located the directory containing <code>spark-submit</code>, you'll typically find Spark's home directory one level up. For instance, if <code>spark-submit</code> is found in <code>/path/to/spark/bin/spark-submit</code>, then <code>/path/to/spark</code> is likely your <code>SPARK_HOME</code>.</li> </ol> <p>In my system it returned</p> <p><code>bash /usr/local/bin/spark-submit /usr/local/lib/python3.8/dist-packages/pyspark/bin/spark-submit</code></p> <p>How do I know where I have installed all the libraries or which is actually the right spark home?</p> <p>The path <code>/usr/local/lib/python3.8/dist-packages/pyspark/bin/spark-submit</code> suggests that this is the location where PySpark was installed as a Python package, likely via <code>pip</code> or a similar package manager.</p> <p>When you install Spark using a package manager like <code>pip</code>, it places the Spark binaries and libraries inside the Python's <code>dist-packages</code> or <code>site-packages</code> directory. This is a different installation approach than downloading and extracting Spark directly from the Apache Spark website.</p> <p>Here's what you need to know:</p> <ol> <li>SPARK_HOME for pip-installed PySpark: If you've installed Spark via <code>pip</code>, then the equivalent <code>SPARK_HOME</code> would be <code>/usr/local/lib/python3.8/dist-packages/pyspark/</code>. This directory would contain all the necessary libraries and dependencies for Spark to run within a Python environment.</li> <li>Configuration and JARs: Configuration files, JARs, and other related resources would also be located within this directory structure. For instance, if you're looking for the location to place additional JAR files, it would likely be somewhere like <code>/usr/local/lib/python3.8/dist-packages/pyspark/jars/</code>.</li> <li>Using <code>spark-submit</code>: The <code>spark-submit</code> script in this directory is tailored for the pip-installed Spark and ensures that it picks up the correct libraries and configurations from this location.</li> </ol> <p>In summary, if you've installed Spark using <code>pip</code> within your Docker container, then the <code>/usr/local/lib/python3.8/dist-packages/pyspark/</code> directory is your primary point of reference for Spark configurations, binaries, and libraries. This is distinct from a standalone Spark installation, which would typically reside directly under <code>/usr/local/</code> or another directory, separate from the Python packages directory.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.15_CommonPysparkTopics.html#frequently-asked-pyspark-questions","title":"Frequently asked Pyspark questions","text":"<p>In this section, I'll go over some common PySpark questions and their answers. These are basic questions that anyone using PySpark should be familiar with. So, let's get started :-)</p> <p>What is PySpark, how is different from Apache Spark?</p> <p>PySpark is the Python API for Apache Spark, allowing Python programmers to use Spark\u2019s large-scale data processing capabilities. Apache Spark is a unified analytics engine for large-scale data processing, originally written in Scala. PySpark provides a similar interface to Spark but allows for Python programming syntax and libraries.</p> <p>What's different between RDD, DataFrame, and Dataset in PySpark.</p> <p>RDD (Resilient Distributed Dataset) is the fundamental data structure of Spark, representing an immutable, distributed collection of objects that can be processed in parallel. DataFrame is a distributed collection of data organized into named columns, similar to a table in a relational database but with richer optimizations under the hood. Dataset is a type-safe version of DataFrame available in Scala and Java, offering the benefits of RDDs with the optimization benefits of DataFrames.</p> <p>How do you create a SparkSession in PySpark?</p> <p>You can create a SparkSession using the <code>SparkSession.builder</code> method, often initializing it with configurations such as <code>appName</code> to name your application and <code>master</code> to specify the cluster manager. For example: <code>spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()</code>.</p> <p>What are the advantages of using PySpark over traditional Python libraries like Pandas?</p> <p>PySpark provides distributed data processing capabilities, allowing for processing of large datasets that do not fit into memory on a single machine. It offers high-level APIs and supports complex ETL operations, real-time processing, and machine learning, unlike Pandas, which is limited by the memory of a single machine.</p> <p>What do you understand by lazy evaluation in PySpark.</p> <p>Lazy evaluation in PySpark means that execution will not start until an action is performed. Transformations in PySpark are lazy, meaning they define a series of operations on data but do not compute anything until the user calls an action. This allows Spark to optimize the execution plan for efficiency.</p> <p>How can you read a CSV  in PySpark?</p> <p>To read a CSV file using PySpark, you can use the <code>spark.read.csv</code> method, specifying the path to the CSV file. Options can be set for things like delimiter, header presence, and schema inference. For example: <code>df = spark.read.csv(\"path/to/csv\", header=True, inferSchema=True)</code>.</p> <p>Explain the actions and transformations in PySpark with examples.</p> <p>Transformations in PySpark create new RDDs, DataFrames, or Datasets from existing ones and are lazily evaluated. Examples include <code>map</code>, <code>filter</code>, and <code>groupBy</code>. Actions, on the other hand, trigger computation and return results. Examples include <code>count</code>, <code>collect</code>, and <code>show</code>. For instance, <code>rdd.filter(lambda x: x &gt; 10)</code> is a transformation, while <code>rdd.count()</code> is an action</p> <p>What are the various ways to select columns in a PySpark DataFrame? Columns in a PySpark DataFrame can be selected using the <code>select</code> method by specifying column names directly or using the <code>df[\"column_name\"]</code> syntax. You can also use SQL expressions with the <code>selectExpr</code> method.</p> <p>How do you handle missing or null values in PySpark DataFrames?</p> <p>Missing or null values in PySpark DataFrames can be handled using methods like <code>fillna</code> to replace nulls with specified values, <code>drop</code> to remove rows with null values, or <code>na.drop()</code> and <code>na.fill()</code> for more nuanced control.</p> <p>Explain the difference between map() and flatMap() functions in PySpark.</p> <p>The <code>map()</code> function applies a function to each element of an RDD, returning a new RDD with the results. <code>flatMap()</code>, on the other hand, applies a function to each element and then flattens the results into a new RDD. Essentially, <code>map()</code> returns elements one-to-one, while <code>flatMap()</code> can return 0 or more elements for each input.</p> <p>How do you perform joins in PySpark DataFrames?</p> <p>Joins in PySpark DataFrames are performed using the <code>join</code> method, specifying another DataFrame to join with, the key or condition to join on, and the type of join (e.g., inner, outer, left, right).</p> <p>Explain the significance of caching in PySpark and how it's implemented.</p> <p>Caching in PySpark is significant for optimization, allowing intermediate results to be stored in memory for faster access in subsequent actions. It's implemented using the <code>cache()</code> or <code>persist()</code> methods on RDDs or DataFrames, which store the data in memory or more persistent storage levels.</p> <p>What are User Defined Functions (UDFs) in PySpark, and when would you use them?</p> <p>UDFs in PySpark allow you to extend the built-in functions by defining custom functions in Python, which can then be used in DataFrame operations. They are useful for applying complex transformations or business logic that are not covered by Spark\u2019s built-in functions.</p> <p>How do you aggregate data in PySpark?</p> <p>Data in PySpark can be aggregated using methods like <code>groupBy</code> followed by aggregation functions such as <code>count</code>, <code>sum</code>, <code>avg</code>, etc. For example, <code>df.groupBy(\"column_name\").count()</code> would count the number of rows for each unique value in the specified column.</p> <p>Explain window functions and their usage in PySpark.</p> <p>Window functions in PySpark operate on a group of rows (a window) while returning a value for each row in the dataset. They are useful for operations like running totals, moving averages, and ranking without having to group the dataset. They are defined using the <code>Window</code> class and applied with functions like <code>rank</code>, <code>row_number</code>, etc.</p> <p>What strategies would you employ for optimizing PySpark jobs?</p> <p>Strategies for optimizing PySpark jobs include broadcasting large lookup tables, partitioning data effectively, caching intermediate results, minimizing shuffles, and using efficient serialization formats. Adjusting Spark configurations to match the job's needs can also improve performance.</p> <p>How does partitioning impact performance in PySpark?</p> <p>Proper partitioning in PySpark can significantly impact performance by ensuring that data is distributed evenly across nodes, reducing shuffles and improving parallel processing efficiency. Poor partitioning can lead to data skew and bottlenecks.</p> <p>Explain broadcast variables and their role in PySpark optimization.</p> <p>Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They are used to optimize performance in PySpark, especially when you have a large dataset that needs to be used across multiple nodes.</p> <p>How do you handle skewed data in PySpark?</p> <p>Handling skewed data in PySpark can involve strategies such as salting keys to distribute the data more evenly, repartitioning or coalescing, and custom partitioning schemes to avoid data skew and ensure balanced workload across nodes.</p> <p>Discuss the concept of accumulators in PySpark.</p> <p>Accumulators in PySpark are variables that are only \u201cadded\u201d to through an associative and commutative operation and can be used to implement counters or sums. PySpark ensures they are updated correctly across tasks.</p> <p>How do you handle streaming in PySpark?</p> <p>Working with structured streaming involves defining a DataStreamReader or DataStreamWriter with a schema, reading streaming data from various sources (like Kafka, sockets, or files), applying transformations, and then writing the output to a sink (like a file system, console, or memory).</p> <p>How can you handle schema evolution in PySpark?</p> <p>Schema evolution in PySpark can be handled by using options like <code>mergeSchema</code> in data sources that support schema merging (e.g., Parquet). It allows for the automatic merging of differing schemas in data files over time, accommodating the addition of new columns or changes in data types.</p> <p>Explain the difference between persist() and cache() in PySpark.</p> <p>Both <code>persist()</code> and <code>cache()</code> in PySpark are used to store the computation results of an RDD, DataFrame, or Dataset so they can be reused in subsequent actions. The difference is that <code>persist()</code> allows the user to specify the storage level (memory, disk, etc.), whereas <code>cache()</code> uses the default storage level (MEMORY_ONLY).</p> <p>How do you work with nested JSON data in PySpark?</p> <p>Working with nested JSON data in PySpark involves reading the JSON file into a DataFrame and then using functions like <code>explode</code> to flatten nested structures or <code>select</code> and <code>col</code> for accessing nested fields. PySpark's built-in functions for dealing with complex data types are also useful here.</p> <p>What is the purpose of the PySpark MLlib library?</p> <p>The purpose of the PySpark MLlib library is to provide machine learning algorithms and utilities for classification, regression, clustering, collaborative filtering, dimensionality reduction, and underlying optimization primitives. It allows for scalable and efficient execution of ML tasks on big data.</p> <p>How do you integrate PySpark with other Python libraries like NumPy and Pandas?</p> <p>Integration of PySpark with other Python libraries like NumPy and Pandas can be achieved through the use of PySpark's ability to convert DataFrames to and from Pandas DataFrames (<code>toPandas</code> and <code>createDataFrame</code> methods) and by using UDFs to apply functions that utilize these libraries on Spark DataFrames.</p> <p>Explain the process of deploying PySpark applications in a cluster.</p> <p>Deploying PySpark applications in a cluster involves packaging your application's code and dependencies, submitting the job to a cluster manager (like Spark Standalone, YARN, or Mesos) using the <code>spark-submit</code> script, and specifying configurations such as the number of executors, memory per executor, and the application's entry point.</p> <p>What are the best practices for writing efficient PySpark code?</p> <p>Best practices include using DataFrames for better optimization, avoiding UDFs when built-in functions are available, minimizing data shuffles, broadcasting large reference datasets, efficient data partitioning, and leveraging Spark's built-in functions for complex operations.</p> <p>How do you handle memory-related issues in PySpark?</p> <p>Handling memory-related issues involves optimizing Spark configurations such as executor memory, driver memory, and memory overhead. Tuning the size and number of partitions, avoiding large broadcast variables, and using disk storage when necessary can also help.</p> <p>Explain the significance of the Catalyst optimizer in PySpark.</p> <p>The Catalyst optimizer is a key component of Spark SQL that improves the performance of SQL and DataFrame queries. It optimizes query execution by analyzing query plans and applying optimization rules, such as predicate pushdown and constant folding, to generate an efficient physical plan.</p> <p>What are some common errors you've encountered while working with PySpark, and how did you resolve them?</p> <p>Common errors include out-of-memory errors, task serialization issues, and data skew. Resolving these issues typically involves tuning Spark configurations, ensuring efficient data partitioning, and applying strategies to handle large datasets and skewed data.</p> <p>How do you debug PySpark applications effectively?</p> <p>Effective debugging of PySpark applications involves checking Spark UI for detailed information on job execution, stages, and tasks, logging information at key points in the application, and using local mode for debugging simpler versions of the code.</p> <p>Explain the streaming capabilities of PySpark.</p> <p>PySpark supports structured streaming, a high-level API for stream processing that allows users to express streaming computations the same way they would express batch computations on static data. It supports event-time processing, window functions, and stateful operations.</p> <p>Can you explain model evaluation and hyperparameter tuning in PySpark.</p> <p>Model evaluation and hyperparameter tuning in PySpark can be performed using the MLlib library, which offers tools like CrossValidator for cross-validation and ParamGridBuilder for building a grid of parameters to search over. Evaluation metrics are available for assessing model performance.</p> <p>Name some common methods or tools do you use for testing PySpark code?</p> <p>Testing PySpark code can involve using the <code>pyspark.sql.functions.col</code> for column operations, the DataFrame API for data manipulation, and third-party libraries like PyTest for writing test cases. Mocking data and simulating Spark behavior in a local environment are also common practices.</p> <p>How do you ensure data quality and consistency in PySpark pipelines?</p> <p>Ensuring data quality and consistency involves implementing validation checks, using schema enforcement on DataFrames, employing data profiling and cleansing techniques, and maintaining data lineage and auditing processes.</p> <p>How do you perform machine learning tasks using PySpark MLlib?</p> <p>Performing machine learning tasks with PySpark MLlib involves using its DataFrame-based API for constructing ML pipelines, utilizing transformers and estimators for preprocessing and model training, and applying built-in algorithms for classification, regression, clustering, etc.</p> <p>How do you handle large-scale machine learning with PySpark?</p> <p>Handling large-scale machine learning involves leveraging the distributed computing capabilities of Spark and MLlib, using algorithms optimized for parallel processing, effectively partitioning data, and tuning Spark resources to balance the workload across the cluster.</p> <p>What are the challenges one faces while implementing machine learning algorithms using PySpark?</p> <p>Challenges include dealing with data skewness, selecting the right algorithms that scale efficiently, managing resource allocation in a distributed environment, ensuring data quality, and integrating with other systems for real-time predictions.</p> <p>\u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive.html","title":"Creating Delta Tables in Hive with Spark","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive.html#overview","title":"Overview","text":"<p>This article will explain two things: - How to create tables in Hive. - How to create Delta tables in Hive.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive.html#the-setup","title":"The Setup:","text":"<p>Spark: You can run this example on any Spark setup\u2014Spark clusters or PySpark. There's no special Spark setup required, meaning the code will work on any basic Spark environment. However, there is one important condition: the Spark version must be 3.2. Don\u2019t use newer versions like 3.4 or 3.5, as they can cause compatibility issues. Also, the Delta package must be <code>delta-core_2.12:1.2.1</code>. After many trials, this combination works smoothly.</p> <p>Hive: The Hive Metastore can be accessed using the address <code>thrift://metastore:9083</code>. You need to make sure Spark can connect to this location. Apart from this, there's nothing else to configure.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive.html#the-code","title":"The Code:","text":"<pre><code>from pyspark.sql import SparkSession\n\n# Create a Spark session and load Delta Lake dependencies dynamically from Maven\nspark = SparkSession.builder \\\n    .appName(\"Spark Hive Delta Integration\") \\\n    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n    .config(\"hive.metastore.uris\", \"thrift://metastore:9083\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.1\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n\n# Sample data for Delta table\ndata = [(\"Rambo\", 30), (\"Titanic\", 25)]\ndf = spark.createDataFrame(data, [\"name\", \"release\"])\n\n# Create a Delta table in Hive\ndf.write.format(\"delta\").saveAsTable(\"default.movies_delta\")\n</code></pre>"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive.html#the-output","title":"The Output:","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive.html#showing-the-table-data","title":"Showing the Table Data:","text":"<pre><code># Read Delta table and show schema\ndf = spark.read.format(\"delta\").table(\"default.movies_delta\")\ndf.printSchema()\n</code></pre>"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive.html#showing-the-history-a-delta-feature","title":"Showing the History (A Delta Feature):","text":"<pre><code>from delta.tables import *\n\n# Load Delta table\ndelta_table = DeltaTable.forName(spark, \"default.movies_delta\")\n\n# Show the history of the Delta table\ndelta_table.history().show()\n</code></pre> <p>Here is how the history will look:</p> <p></p>"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive.html#if-you-dont-want-to-use-hive","title":"If You Don't Want to Use Hive:","text":"<p>If you don\u2019t want to use Hive and just want to save data to any location, you can also do that. Here\u2019s the code:</p> <pre><code>spark = SparkSession.builder \\\n    .appName(\"Spark Delta Integration\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.1\") \\\n    .getOrCreate()\n\n# Sample data for Delta table\ndata = [(\"Rambo\", 30), (\"Titanic\", 25)]\ndf = spark.createDataFrame(data, [\"name\", \"release\"])\n\n# Write the DataFrame to a Delta table\ndf.write.format(\"delta\").save(\"/tmp/delta-table\")\n\n# Read back the Delta table\ndelta_df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\ndelta_df.show()\n</code></pre>"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive.html#errors-encountered","title":"Errors Encountered","text":"<p>Here are some errors you might face due to incompatibility between Delta and Spark versions:</p> <p>Py4JJavaError: An error occurred while calling o79.save.  java.lang.NoClassDefFoundError: org/apache/spark/sql/execution/datasources/FileFormatWriter$Empty2Null</p> <p>java.lang.NoSuchMethodError: 'org.apache.spark.sql.catalyst.TableIdentifier org.apache.spark.sql.catalyst.TableIdentifier.copy(java.lang.String, scala.Option)'</p> <p>Warning</p> <p>You might see a warning like this:</p> <p>WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table default.movies_delta into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.</p> <p>Reason: Hive can handle many table types, but Delta Lake (a storage format by Databricks) is not supported by Hive directly. Spark uses its own format to save Delta tables, which doesn\u2019t match Hive\u2019s SerDe (Serializer/Deserializer). This warning comes up because Hive can\u2019t read Delta tables directly unless special Delta support is added or managed via Spark.</p> <p>Solution: Since Delta tables are built for Spark, the simplest way is to keep using Spark to query your Delta tables. Don\u2019t register them in Hive\u2019s metastore. Instead, manage them directly with Spark to avoid Hive trying to read the Delta format.</p> <p>Note: Hive doesn\u2019t natively support Delta, but you can use Delta Lake open-source libraries to connect Hive and Delta. However, this setup is complicated and not very reliable.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive.html#jars-download","title":"JARs Download","text":"<p>When you use the following configuration:</p> <pre><code>.config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.1\")\n</code></pre> <p>It automatically downloads the necessary JARs from these locations:</p> <ul> <li>Delta Core 2.12 JAR</li> <li>Delta Storage JAR</li> <li>ANTLR Runtime JAR</li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/1.16_ConnectingSparkToHive.html#seeing-the-table-using-beeline","title":"Seeing the table using beeline","text":"<p>From any server, not hiveserver2, connect to the beeling in hiveserver2 using this command:</p> <pre><code>beeline -u jdbc:hive2://hiveserver2:10000/default\n</code></pre> <p>Issue a command </p> <pre><code>DESCRIBE FORMATTED default.movies_delta;\n</code></pre> <p>YOu will see an output like this:</p> <p></p>"},{"location":"Spark-DataBricks/1.0_Spark/1.1_NarrowVsWideTransformation.html","title":"Background","text":"<p>In Spark, transformations are classified into two types: narrow and wide.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.1_NarrowVsWideTransformation.html#narrow-transformations","title":"Narrow Transformations","text":"<ul> <li>Definition: Each input partition contributes to only one output partition.</li> <li>Example Operations: <code>map()</code>, <code>filter()</code>, <code>select()</code></li> <li>Characteristics:</li> <li>No data shuffling across the network.</li> <li>Fast and efficient.</li> <li>Data can be processed in parallel without dependencies on other partitions.</li> </ul> <p> Remember: When we say \"each input partition contributes to only one output partition,\" we mean that in a narrow transformation, the data from an input partition is processed and stored in exactly one corresponding output partition without the need to shuffle or move data between different partitions. </p>"},{"location":"Spark-DataBricks/1.0_Spark/1.1_NarrowVsWideTransformation.html#example","title":"Example","text":"<p>Consider a simple <code>filter()</code> operation that filters out rows based on some condition:</p> <pre><code># Assume we have an RDD with 4 partitions\nrdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8], 4)\n\n# Apply a filter transformation\nfiltered_rdd = rdd.filter(lambda x: x % 2 == 0)\n</code></pre> <ul> <li>Input Partitions: [ [1, 2], [3, 4], [5, 6], [7, 8] ]</li> <li> <p>Filter Condition: Keep only even numbers</p> </li> <li> <p>Output Partitions:</p> </li> <li>Partition 1 (Input: [1, 2]) \u2192 Partition 1 (Output: [2])</li> <li>Partition 2 (Input: [3, 4]) \u2192 Partition 2 (Output: [4])</li> <li>Partition 3 (Input: [5, 6]) \u2192 Partition 3 (Output: [6])</li> <li>Partition 4 (Input: [7, 8]) \u2192 Partition 4 (Output: [8])</li> </ul> <p>Each input partition (e.g., [1, 2]) is processed and produces output that remains within the same partition (e.g., [2]). There is no data movement between partitions.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.1_NarrowVsWideTransformation.html#wide-transformations","title":"Wide Transformations","text":"<ul> <li>Definition: Each input partition contributes to multiple output partitions.</li> <li>Example Operations: <code>reduceByKey()</code>, <code>groupBy()</code>, <code>join()</code></li> <li>Characteristics:</li> <li>Data is shuffled across the network.</li> <li>Slower compared to narrow transformations due to data movement.</li> <li>Requires synchronization between partitions.</li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/1.1_NarrowVsWideTransformation.html#example_1","title":"Example","text":"<p>Now, consider a <code>reduceByKey()</code> operation which involves shuffling:</p> <pre><code># Assume we have an RDD with key-value pairs\nrdd = sc.parallelize([(1, 2), (3, 4), (3, 6), (1, 4)], 2)\n\n# Apply a reduceByKey transformation\nreduced_rdd = rdd.reduceByKey(lambda x, y: x + y)\n</code></pre> <ul> <li> <p>Input Partitions: [ [(1, 2), (3, 4)], [(3, 6), (1, 4)] ]</p> </li> <li> <p>Reduce by Key: Combine values with the same key</p> </li> <li> <p>Output Partitions:</p> </li> <li>Partition 1 (Input: [(1, 2), (1, 4)]) \u2192 Partition 1 (Output: [(1, 6)])</li> <li>Partition 2 (Input: [(3, 4), (3, 6)]) \u2192 Partition 2 (Output: [(3, 10)])</li> </ul> <p>In this case, keys with the same value (e.g., key = 1 or key = 3) need to be brought together into the same partition to perform the reduction. This requires shuffling data across the network, making it a wide transformation.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.1_NarrowVsWideTransformation.html#key-differences","title":"Key Differences","text":"<p>Data Movement: Narrow transformations do not shuffle data, while wide transformations do. Performance: Narrow transformations are faster and more efficient because there's no data movement. Parallelism: Narrow transformations are easier to process in parallel, while wide transformations need to handle data dependencies and synchronization.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture.html","title":"Spark Architecture","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture.html#spark-components","title":"Spark Components","text":"<p>Spark Core: The main part of Spark with a core engine.</p> <p>Spark SQL: A SQL engine, but different from traditional databases. Here, data is processed mainly using DataFrames.</p> <p>Spark Streaming: This part allows Spark to process real-time data.</p> <p>Spark MLlib: A collection of machine learning libraries.</p> <p>GraphX: Used for graphs in reports, such as data collected from networks like Facebook.</p> <p>RDDs: Spark Core has RDDs (Resilient Distributed Datasets), which are the building blocks of Spark.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture.html#cluster-nodes","title":"Cluster &amp; Nodes","text":"<p>Nodes are individual machines (physical or virtual). Cluster is a group of nodes.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture.html#driver-worker","title":"Driver &amp; Worker","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture.html#driver","title":"Driver","text":"<ul> <li>Machine where Main() method runs. It contains the SparkContext object.</li> <li>Converts the application into stages using a DAG (Directed Acyclic Graph).</li> <li>Schedules tasks on worker nodes and collects the results.</li> <li>Should be close to worker nodes for better performance.</li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture.html#workersnow-executors","title":"Workers(now Executors)","text":"<p>Workers are simply machines(Virtual/Real). These workers run JVM processes, called Executors. Multiple JVM Process(Executor) can be configured in a worker.</p> <p>Configuration: In your spark configuration you can set:</p> <ul> <li><code>--num-executors</code>: Specifies the total number of executors to be launched for the application.</li> <li><code>--executor-cores</code>: Specifies the number of cores (slots) to be used by each executor.</li> <li><code>--executor-memory</code>: Specifies the amount of memory to be allocated to each executor.</li> </ul> <p>Example: With a worker machine having 16 CPU cores and 64 GB of memory, you can configure Spark to run either 4 executors (4 cores, 16 GB each) or 2 executors (8 cores, 32 GB each).</p> <p> Note: Executor is a JVM process running on a worker node that executes tasks. The Spark cluster manager (e.g., YARN, Mesos, or the standalone cluster manager) is responsible for allocating resources to executors.   </p>"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture.html#slots-spark-cores-synapse-vcore-threads","title":"Slots = Spark cores =  Synapse vCore \u2248 Threads","text":"<p>Cores in Spark = vCores in Synapse = Slots in Databricks = Total threads = Total parallel tasks</p> <p> Note: Don't confuse cores with Intel/AMD CPU Ads. Cores in Spark means threads. </p> <p> Note: Spark supports one task for each virtual CPU (vCPU) core by default. For example, if an executor has four CPU cores, it can run four concurrent tasks. </p> <p> Note: Multiple threads can run on each core, but Spark typically uses one thread per core for each task to simplify execution and avoid the complexities of managing multiple threads on a single core. </p> <p>This Databricks spark cluster can run 32 tasks parallely:</p> <p></p> <p>In Docker Compose, <code>SPARK_WORKER_CORES</code> sets worker threads (cores/slots). A cluster with 3 workers, each set to 2 cores, has 6 total threads.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture.html#executors-cores-memory-for-a-10-gb-data","title":"Executors, cores &amp; memory for a 10 GB data","text":"<p>Say, you have 10 GB of data to be processed. How can you calcultate the executors, cores and memory for such a secenairo?</p> Step Description Calculation/Value Calculate number of partitions Default partition size: 128 MB 10 GB / 128 MB = 80 partitions Determine CPU cores needed One core per partition for maximum parallelism 80 cores Max cores per executor Cores per executor in YARN 5 cores per executor Calculate number of executors Total cores / Cores per executor 80 / 5 = 16 executors Partition size Default partition size: 128 MB 128 MB Memory per core Minimum memory per core (4x partition size) 128 MB * 4 = 512 MB Executor cores Cores per executor 5 cores Executor memory Memory per core * Number of cores per executor 512 MB * 5 = 2560 MB Each Executor Requires Cores 5 CPU cores Memory 2560 MB"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture.html#application-jobs-stages-tasks","title":"Application, Jobs, Stages, Tasks","text":"<p>Applications -&gt; jobs -&gt; stages -&gt; tasks.</p> Term Definition Example Application An application in Spark is a complete program that runs on the Spark cluster. This program includes the user's code that uses Spark\u2019s API to perform data processing. A Spark application can be a Python script that processes data from a CSV file, performs transformations, and writes the results to a database. Job A job is triggered by an action (e.g., <code>count()</code>, <code>collect()</code>, <code>saveAsTextFile()</code>) in a Spark application. Each action in the code triggers a new job. If your application has two actions, like counting the number of rows and saving the result to a file, it will trigger two jobs. Stages A job is divided into stages, where each stage is a set of tasks that can be executed in parallel. Stages are separated by shuffle operations. If a job involves filtering and then aggregating data, the filtering might be one stage, and the aggregation another, especially if a shuffle operation (like a group by) is required between them. Tasks A stage is further divided into tasks, where each task is a unit of work that operates on a partition of the data. Tasks are the smallest unit of execution in Spark. If a stage needs to process 100 partitions of data, it will have 100 tasks, with each task processing one partition."},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture.html#lets-put-it-all-together","title":"Let's put it all together","text":"<p>Let's see an example to understand these concepts:</p> <ol> <li>Application: A Spark application that reads data from a CSV file, filters out certain rows, and then calculates the average of a column.</li> <li> <p>Code snippet:      <pre><code>df1 = spark.read.csv(\"data.csv\") //Job1\ndf2 = df.filter(df[\"value\"] &gt; 10) //Job2\naverage = df2.agg({\"value\": \"avg\"}).collect()\n</code></pre></p> </li> <li> <p>Jobs:</p> </li> <li>Job 1: Triggered by the action <code>spark.read.csv()</code>. This job reads the data from the CSV file.</li> <li> <p>Job 2: Triggered by the action <code>filtered_df.agg().collect()</code>. This job includes filtering the data and then calculating the average.</p> </li> <li> <p>Stages in Job 2:</p> </li> <li>Stage 1: Filtering the data. All tasks in this stage can run in parallel because filtering is a transformation that operates on individual partitions.</li> <li> <p>Stage 2: Aggregating the data. This stage requires a shuffle because the aggregation (calculating the average) involves data from all partitions.</p> </li> <li> <p>Tasks: For each stage, Spark creates tasks based on the number of partitions. If there are 10 partitions, Stage 1 (filtering) will have 10 tasks, and Stage 2 (aggregation) will also have 10 tasks, each processing one partition of data.</p> </li> </ol>"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture.html#transformation-actions","title":"Transformation &amp; Actions","text":"<p>In PySpark, operations on data can be classified into two types: transformations and actions.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture.html#transformations","title":"Transformations","text":"<p>Transformations are operations on RDDs that return a new RDD, meaning they create a new dataset from an existing one. Transformations are lazy, meaning they are computed only when an action is called.</p> <p>(e.g., <code>map</code>, <code>filter</code>): Create a new RDD from an existing one. They are lazy and not executed until an action is called.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture.html#actions","title":"Actions","text":"<p>Actions trigger the execution of the transformations to return a result to the driver program or write it to storage. When an action is called, Spark's execution engine computes the result of the transformations.</p> <p>(e.g., <code>collect</code>, <code>count</code>): Trigger the execution of the transformations and return a result.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture.html#example","title":"Example","text":"<pre><code># Create an RDD from a list\ndata = [1, 2, 3, 4, 5]\nrdd = spark.sparkContext.parallelize(data)\n\n# Transformation 1: Multiply each number by 2\nrdd_transformed = rdd.map(lambda x: x * 2)\n\n# Transformation 2: Filter out even numbers\nrdd_filtered = rdd_transformed.filter(lambda x: x % 2 == 0)\n\n# Action: Collect the results\nresult = rdd_filtered.collect()\n</code></pre>"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture.html#common-transformations-and-actions-in-pyspark","title":"Common transformations and actions in PySpark","text":"Transformation Example API Description <code>map</code> <code>rdd.map(lambda x: x * 2)</code> Applies a function to each element in the RDD. <code>filter</code> <code>rdd.filter(lambda x: x % 2 == 0)</code> Returns a new RDD containing only elements that satisfy a predicate. <code>flatMap</code> <code>rdd.flatMap(lambda x: (x, x * 2))</code> Similar to <code>map</code>, but each input item can be mapped to 0 or more output items (returns a flattened structure). <code>mapPartitions</code> <code>rdd.mapPartitions(lambda iter: [sum(iter)])</code> Applies a function to each partition of the RDD. <code>distinct</code> <code>rdd.distinct()</code> Returns a new RDD containing the distinct elements. <code>union</code> <code>rdd1.union(rdd2)</code> Returns a new RDD containing the union of elements. <code>intersection</code> <code>rdd1.intersection(rdd2)</code> Returns a new RDD containing the intersection of elements. <code>groupByKey</code> <code>rdd.groupByKey()</code> Groups the values for each key in the RDD. <code>reduceByKey</code> <code>rdd.reduceByKey(lambda a, b: a + b)</code> Merges the values for each key using an associative function. <code>sortBy</code> <code>rdd.sortBy(lambda x: x)</code> Returns a new RDD sorted by the specified function. Action Example API Description <code>collect</code> <code>rdd.collect()</code> Returns all the elements of the RDD as a list. <code>count</code> <code>rdd.count()</code> Returns the number of elements in the RDD. <code>first</code> <code>rdd.first()</code> Returns the first element of the RDD. <code>take</code> <code>rdd.take(5)</code> Returns the first <code>n</code> elements of the RDD. <code>reduce</code> <code>rdd.reduce(lambda a, b: a + b)</code> Aggregates the elements of the RDD using a function. <code>saveAsTextFile</code> <code>rdd.saveAsTextFile(\"path\")</code> Saves the RDD to a text file. <code>countByKey</code> <code>rdd.countByKey()</code> Returns the count of each key in the RDD. <code>foreach</code> <code>rdd.foreach(lambda x: print(x))</code> Applies a function to each element of the RDD."},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture.html#what-is-a-shuffle","title":"What is a Shuffle?","text":"<p>A shuffle in Spark is the process of redistributing data across different nodes in the cluster. It involves copying data between Executors(JVM Proceses). It typically happens when a transformation requires data exchange between partitions, involving disk I/O, data serialization, and network I/O.</p> <p>Shuffle is one of the most substantial factors in degraded performance of your Spark application. While storing the intermediate data, it can exhaust space on the executor's local disk, which causes the Spark job to fail.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture.html#when-does-a-shuffle-occur","title":"When Does a Shuffle Occur?","text":"Operation Example Description <code>groupByKey</code> <code>rdd.groupByKey()</code> Groups elements by key, requiring all data for a key to be in the same partition. <code>reduceByKey</code> <code>rdd.reduceByKey(lambda a, b: a + b)</code> Combines values for each key using a function, requiring data colocation. <code>sortByKey</code> <code>rdd.sortByKey()</code> Sorts data, requiring all data for a key to be in the same partition. <code>join</code> <code>rdd1.join(rdd2)</code> Joins two RDDs or DataFrames, requiring data with the same key to be colocated. <code>distinct</code> <code>rdd.distinct()</code> Removes duplicates, requiring comparison across partitions."},{"location":"Spark-DataBricks/1.0_Spark/1.2_SparkArchitecture.html#how-to-optimize-shuffle-in-spark","title":"How to Optimize Shuffle in Spark","text":"Optimization Description Example/Note Avoid join() unless essential The join() operation is a costly shuffle operation and can be a performance bottleneck. Use join only if necessary for your business requirements. Avoid collect() in production The collect() action returns all results to the Spark driver, which can cause OOM errors. Default setting: <code>spark.driver.maxResultSize = 1GB</code>. Cache/Persist DataFrames Use <code>df.cache()</code> or <code>df.persist()</code> to cache repetitive DataFrames to avoid additional shuffle or computation. <code>python df_high_rate = df.filter(col(\"star_rating\") &gt;= 4.0) df_high_rate.persist()</code> Unpersist when done Use <code>unpersist</code> to discard cached data when it is no longer needed. <code>python df_high_rate.unpersist()</code> Overcome data skew Data skew causes uneven distribution of data across partitions, leading to performance bottlenecks. Ensure data is uniformly distributed across partitions. Use bucketing Bucketing pre-shuffles and pre-sorts input on join keys, writing sorted data to intermediary tables to reduce shuffle and sort costs. Reduces load on executors during sort-merge joins. Shuffle and broadcast hash joins Use broadcast hash join for small-to-large table joins to avoid shuffling. Applicable only when the small table can fit in the memory of a single Spark executor. Optimize join Use high-level Spark APIs (SparkSQL, DataFrame, Datasets) for joins instead of RDD API or DynamicFrame join. Convert DynamicFrame to DataFrame if needed. <code>python df_joined = df1.join(df2, [\"key\"])</code>"},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache.html","title":"Cache & Persist","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache.html#understanding-persist-in-spark","title":"Understanding <code>persist()</code> in Spark","text":"<p>In Spark, the <code>persist()</code> method is used to save a dataset (RDD or DataFrame) in memory or on disk, so you can use it multiple times without recalculating it every time. If you need to use the same data repeatedly, <code>persist()</code> can help speed up your work and add some fault tolerance.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache.html#syntax-of-persist","title":"Syntax of <code>persist()</code>","text":"Usage Syntax (DataFrame) Syntax (RDD) No Argument <code>dfPersist = df.persist()</code> <code>rdd.persist()</code> With Argument <code>dfPersist = df.persist(StorageLevel.XXXXXXX)</code> <code>rdd.persist(StorageLevel.XXXXXXX)</code>"},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache.html#storage-levels","title":"Storage Levels","text":"<p>Note: The default storage level is different for DataFrames and RDDs: - DataFrames: Default is <code>MEMORY_AND_DISK</code>. (DataFrame API Documentation) - RDDs: Default is <code>MEMORY_ONLY</code>. (RDD Persistence Documentation)</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache.html#different-values-for-storagelevel","title":"Different Values for StorageLevel","text":"<p>StorageLevel values are available in the <code>pyspark.StorageLevel</code> class. Here is the complete list:</p> <ul> <li>DISK_ONLY: Store data on disk only.</li> <li>DISK_ONLY_2: Store data on disk with replication to two nodes.</li> <li>DISK_ONLY_3: Store data on disk with replication to three nodes.</li> <li>MEMORY_AND_DISK: Store data in memory and spill to disk if necessary.</li> <li>MEMORY_AND_DISK_2: Store data in memory and spill to disk if necessary, with replication to two nodes.</li> <li>MEMORY_AND_DISK_SER: Store data in JVM memory as serialized objects and spill to disk if necessary.</li> <li>MEMORY_AND_DISK_DESER: Store data in JVM memory as deserialized objects and spill to disk if necessary.</li> <li>MEMORY_ONLY: Store data as deserialized objects in JVM memory only.</li> <li>MEMORY_ONLY_2: Store data in memory only, with replication to two nodes.</li> <li>NONE: No storage level. (Note: This can't be used as an argument.)</li> <li>OFF_HEAP: Store data in off-heap memory (experimental).</li> </ul> <p>Note: The official Spark documentation states that the default storage level for <code>RDD.persist()</code> is <code>MEMORY_ONLY</code>. For <code>df.persist()</code>, it is <code>MEMORY_AND_DISK</code>, and starting from version 3.4.0, it is <code>MEMORY_AND_DISK_DESER</code>. (Spark Persistence Documentation)</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache.html#example","title":"Example","text":"<p>Imagine you have a list of numbers, and you want to do some calculations on it multiple times. Here\u2019s how you can use <code>persist()</code>:</p> <pre><code>from pyspark import SparkContext\n\n# Initialize Spark Context\nsc = SparkContext(\"local\", \"PersistExample\")\n\n# Create an RDD\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nrdd = sc.parallelize(numbers)\n\n# Persist the RDD in memory\nrdd.persist()\n\n# Do some actions\nprint(rdd.count())  # Count the numbers\nprint(rdd.collect())  # Collect all the numbers\n# This will show the storage level being used\nprint(dfPersistMemoryOnly.storageLevel)  \n</code></pre> <p>In this example, <code>persist()</code> saves the <code>rdd</code> in memory, so if you do more actions like <code>count()</code> or <code>collect()</code>, it doesn\u2019t have to recalculate the data each time.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache.html#removing-saved-data","title":"Removing Saved Data","text":"<p>When you don\u2019t need the saved data anymore, you can remove it from memory or disk using <code>unpersist()</code>:</p> <pre><code>rdd.unpersist()\n</code></pre>"},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache.html#when-to-use-persist","title":"When to use persist()","text":"<p>So, we have learned about <code>persist()</code>. Does that mean you should use <code>persist()</code> every time you have a DataFrame (df)? No. Here are the situations where it is recommended to use it. </p> <ol> <li> <p>Running Multiple Machine Learning Models:    When you run multiple machine learning models on the same dataset, using <code>persist()</code> can save time.</p> </li> <li> <p>Interactive Data Analysis:    If you are using a notebook for interactive data analysis, where you need to understand the data by running multiple queries, <code>persist()</code> will make the results come faster.</p> </li> <li> <p>ETL Process:    In your ETL (Extract, Transform, Load) process, if you have a cleaned DataFrame and you are using the same DataFrame in many steps, using <code>persist()</code> can help.</p> </li> <li> <p>Graph Processing:    When processing graphs using libraries like GraphX, <code>persist()</code> can improve performance.</p> </li> </ol> <p>Remember, persist() comes later in the tools for fixing speed. For significant improvement, focus on better partitioning of data (like fixing data skewness) and using broadcast variables for joins first. Don't expect persist() to magically improve speed. Also, if your DataFrame (df) is very large and you persist it, it will consume a lot of memory on the worker nodes. This means they won't have enough memory left for their other tasks. To avoid this, first assess the memory of the workers and the size of the df you are persisting. </p> <p>For smaller DataFrames, use <code>MEMORY_ONLY</code>, and for larger ones, use <code>MEMORY_AND_DISK</code> (this will spill some data to disk if memory is low). Remember, <code>MEMORY_AND_DISK</code> also has a performance impact because it increases I/O operations. </p> <p>Additionally, large DataFrames can increase the frequency of garbage collection in the JVM, which affects overall performance.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache.html#conclusion","title":"Conclusion","text":"<p>Using <code>persist()</code> in Spark is like telling Spark to remember your data so it doesn\u2019t have to start from scratch every time you need it. This can make your work much faster, especially when working with large datasets, and also adds fault tolerance.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.3_persist_and_cache.html#understanding-cache-in-spark","title":"Understanding <code>cache()</code> in Spark","text":"<p>The <code>cache()</code> function is a shorthand method for <code>persist()</code> with the default storage level, which is <code>MEMORY_ONLY</code>. This means,</p> <p><code>cache()</code> is same as <code>persist(StorageLevel.MEMORY_ONLY)</code></p>"},{"location":"Spark-DataBricks/1.0_Spark/1.4_broadcastvariables.html","title":"Broadcast Variables","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.4_broadcastvariables.html#understanding-broadcast-variables-in-spark","title":"Understanding Broadcast Variables in Spark","text":"<p>Imagine a scenario from the movie \"The Matrix\" where Morpheus shares a training program with Neo and the other rebels. Instead of loading the training program into the simulation multiple times for each person, Morpheus sends it once to each person's mind, and they use it as needed.</p> <p>This is similar to broadcast variables in Spark. These variables are used mainly for performance tuning. Using broadcast variables, you can send small read-only tables to all worker nodes in a cluster. This can reduce shuffling and make operations faster.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.4_broadcastvariables.html#when-to-use-broadcast-variables","title":"When to Use Broadcast Variables","text":"<p>Broadcast variables should be used when you need to send a small table to all nodes. This is ideal for joins where one table is small (like a dimension table). It is not suitable for large tables, as it would cause an out-of-memory exception. If the size of the broadcasted table is larger than the memory, it will surely cause OOM.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.4_broadcastvariables.html#example","title":"Example","text":"<p>Here I will give you two scenario and show you how broadcast variables may imporve joins in a Spark cluster.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.4_broadcastvariables.html#scenario-without-broadcast-variables","title":"Scenario Without Broadcast Variables","text":"<p>Suppose you have a small lookup table (e.g., <code>lookup_dict</code>) and a large dataset (<code>rdd</code>). You want to join the large dataset with the lookup table.</p> <pre><code># Small lookup table\nlookup_dict = {\"A\": 1, \"B\": 2, \"C\": 3}\n\n# Large RDD\nrdd = sc.parallelize([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"])\n\n# Join operation without broadcast\nlookup_rdd = sc.parallelize(list(lookup_dict.items()))\njoined_rdd = rdd.map(lambda x: (x,)).join(lookup_rdd)\n</code></pre> <p>In this scenario, Spark needs to shuffle data to perform the join operation, which is expensive and time-consuming.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.4_broadcastvariables.html#scenario-with-broadcast-variables","title":"Scenario With Broadcast Variables","text":"<p>Using a broadcast variable, you can send the lookup table to all nodes just once, avoiding the shuffle operation.</p> <pre><code># Small lookup table\nlookup_dict = {\"A\": 1, \"B\": 2, \"C\": 3}\n\n# Broadcast the lookup table\nbroadcast_var = sc.broadcast(lookup_dict)\n\n# Large RDD\nrdd = sc.parallelize([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"])\n\n# Use the broadcast variable to look up values\nresult_rdd = rdd.map(lambda x: (x, broadcast_var.value.get(x, 0)))\n\n# Collect and print the results\nprint(result_rdd.collect())\n</code></pre>"},{"location":"Spark-DataBricks/1.0_Spark/1.4_broadcastvariables.html#how-broadcast-variables-improve-performance","title":"How Broadcast Variables Improve Performance","text":"<p>When you join or group DataFrames, Spark usually shuffles data across all worker nodes, which is slow and costly.</p> <ul> <li>Avoiding Shuffle:</li> <li>Without Broadcast: Spark shuffles the lookup table data across the network to join with the large dataset.</li> <li> <p>With Broadcast: The lookup table is sent once to all nodes, allowing local lookups without moving data around.</p> </li> <li> <p>Efficiency:</p> </li> <li>Without Broadcast: The join operation involves heavy data movement and sorting, which is slow.</li> <li>With Broadcast: The lookup is done locally on each node using the cached copy, making the operation much faster and reducing network traffic.</li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/1.4_broadcastvariables.html#knowledge-check","title":"Knowledge Check","text":"<p>A. A broadcast variable is a Spark object that needs to be partitioned onto multiple worker nodes because it's too large to fit on a single worker node.    - This statement is incorrect. Broadcast variables are designed to be efficiently distributed to each worker node. They are not partitioned but rather fully replicated on each worker node to avoid being sent multiple times.</p> <p>B. A broadcast variable can only be created by an explicit call to the broadcast() operation.    - This statement is correct but not complete. While it's true that broadcast variables are created by calling the <code>broadcast()</code> method, this option doesn't describe the defining characteristic of broadcast variables, which is how they are used in the Spark architecture.</p> <p>C. A broadcast variable is entirely cached on the driver node so it doesn't need to be present on any worker nodes.    - This statement is incorrect. Broadcast variables are distributed to all worker nodes, not just cached on the driver node. Their purpose is to be available on each worker node to prevent the need to send them multiple times during execution.</p> <p>D. A broadcast variable is entirely cached on each worker node so it doesn't need to be shipped or shuffled between nodes with each stage.    - This statement is correct. The primary characteristic of a broadcast variable is that it is distributed and cached on each worker node. This reduces communication overhead by avoiding repeated transmission of the variable during different stages of the job.</p> <p>E. A broadcast variable is saved to the disk of each worker node to be easily read into memory when needed.    - This statement is incorrect. Broadcast variables are stored in memory on each worker node to ensure quick access without the latency involved in reading from disk. The purpose of broadcast variables is to provide fast, in-memory access to frequently used data.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.4_broadcastvariables.html#summary","title":"Summary","text":"<p>Broadcast variables are useful for distributing small datasets (like lookup tables) to all nodes in a Spark cluster. By broadcasting the lookup table, you avoid the need to shuffle data during join operations, which can significantly improve performance by reducing network overhead and computation time. This makes operations faster and more efficient.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling.html","title":"Data Skew","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling.html#how-to-handle-data-skew-in-spark","title":"How to handle data skew in Spark","text":"<p>In spark, we partition(split) data to process it parallely. But, if the splitting 'unfair', like some parition has humougus amount of data while others have too little. This is called data skew. This is undesirable. If we process such skewed data, some nodes will be OOM while some others will sit idle(as they alreaady processed the little data they were assigned).</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling.html#what-causes-data-skew","title":"What causes data skew?","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling.html#uneven-key-distribution-in-join-operations","title":"Uneven Key Distribution in Join Operations","text":"<ul> <li>Suppose you have two datasets (key-value pairs) you want to join:</li> <li>Dataset A: <code>[(China, \"A\"), (Fiji, \"B\"), (Bhutan, \"C\")]</code></li> <li>Dataset B: <code>[(China, \"X\"), (China, \"Y\"), (China, \"Z\"), (Fiji, \"P\"), (Bhutan, \"Q\")]</code></li> <li>When joining on the first column (key), the resulting dataset would look like:</li> <li>Joined Data: <code>[(China, \"A\", \"X\"), (China, \"A\", \"Y\"), (China, \"A\", \"Z\"), (Fiji, \"B\", \"P\"), (Bhutan, \"C\", \"Q\")]</code></li> </ul> <p>Now, if you split (partition) the joined data based on the key (China/Fiji/Bhutan), there will be three partitions: <pre><code>Partition 1: [(China, \"A\", \"X\"), (China, \"A\", \"Y\"), (China, \"A\", \"Z\")]\nPartition 2: [(Fiji, \"B\", \"P\")]\nPartition 3: [(Bhutan, \"C\", \"Q\")]\n</code></pre></p> <p>This means the first partition is unfair. The node with Partition 1 will be overloaded, while the other nodes will be sitting idle after some time.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling.html#presence-of-null-values-or-a-dominant-key","title":"Presence of Null Values or a Dominant Key","text":"<ul> <li>Suppose you have a dataset with some entries having a <code>null</code> key or the same key:</li> <li>Dataset: <code>[(null, \"A\"), (null, \"B\"), (1, \"C\"), (1, \"D\"), (2, \"E\")]</code></li> <li>When partitioning based on the key:</li> <li>Partition 1: <code>[(null, \"A\"), (null, \"B\")]</code></li> <li>Partition 2: <code>[(1, \"C\"), (1, \"D\")]</code></li> <li>Partition 3: <code>[(2, \"E\")]</code></li> <li>The partitions for <code>null</code> and <code>1</code> have more entries, causing an imbalance.</li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling.html#poorly-chosen-partitioning-key","title":"Poorly Chosen Partitioning Key","text":"<ul> <li>Suppose you have a dataset and you partition it using a key that does not distribute data well:</li> <li>Dataset: <code>[(10, \"A\"), (20, \"B\"), (30, \"C\"), (10, \"D\"), (20, \"E\")]</code></li> <li>If you partition by the key (first column) and the data is unevenly distributed:</li> <li>Partition 1: <code>[(10, \"A\"), (10, \"D\")]</code></li> <li>Partition 2: <code>[(20, \"B\"), (20, \"E\")]</code></li> <li>Partition 3: <code>[(30, \"C\")]</code></li> <li>Here, Partition 1 and 2 have more entries compared to Partition 3, leading to skew.</li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling.html#faulty-groupby","title":"Faulty GroupBy","text":"<p>Just like join operations, GroupBy operations can also cause data imbalance when some keys have a lot more values than others.</p> <p>Example:</p> <p>If you group sales data by product ID and one product has many more sales records than others, the partition for that product will be much bigger and could slow down processing.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling.html#handling-data-skew","title":"Handling Data Skew","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling.html#salting-for-join-operations","title":"Salting (for join operations)","text":"<p>Add a random number to keys to distribute them more evenly.</p> <p>Example: <pre><code>from pyspark.sql.functions import rand, col\n\n# Assume df1 has skewed keys\nsalted_df1 = df1.withColumn(\"salt\", (rand()*5).cast(\"int\"))\n                .withColumn(\"new_key\", col(\"key\") * 10 + col(\"salt\"))\n\nsalted_df2 = df2.withColumn(\"salt\", explode(array([lit(i) for i in range(5)])))\n                .withColumn(\"new_key\", col(\"key\") * 10 + col(\"salt\"))\n\nresult = salted_df1.join(salted_df2, \"new_key\").drop(\"salt\", \"new_key\")\n</code></pre></p>"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling.html#custom-partitioning","title":"Custom Partitioning","text":"<p>Repartition data based on a more evenly distributed column.</p> <p>Example: <pre><code>from pyspark.sql.functions import col\n\n# Assume 'id' is more evenly distributed than 'skewed_column'\nevenly_distributed_df = df.repartition(col(\"id\"))\n</code></pre></p>"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling.html#broadcast-join","title":"Broadcast Join","text":"<p>For scenarios where one DataFrame is small enough to fit in memory.</p> <p>Example: <pre><code>from pyspark.sql.functions import broadcast\n\nresult = large_df.join(broadcast(small_df), \"key\")\n</code></pre></p>"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling.html#separate-out-the-skewed-keys","title":"Separate out the skewed keys","text":"<p>Handle the most skewed keys separately.</p> <p>Example: <pre><code># Identify skewed keys\nkey_counts = df.groupBy(\"key\").count().orderBy(col(\"count\").desc())\nskewed_keys = key_counts.limit(5).select(\"key\").collect()\n\n# Separate skewed and non-skewed data\nskewed_data = df.filter(col(\"key\").isin(skewed_keys))\nnon_skewed_data = df.filter(~col(\"key\").isin(skewed_keys))\n\n# Process separately and union results\nskewed_result = process_skewed_data(skewed_data)\nnon_skewed_result = process_non_skewed_data(non_skewed_data)\nfinal_result = skewed_result.union(non_skewed_result)\n</code></pre></p>"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling.html#using-aqe-adaptive-query-execution","title":"Using AQE (Adaptive Query Execution):","text":"<p>Enable AQE in Spark 3.0+ to dynamically optimize query plans.</p> <p>Example: <pre><code>spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n</code></pre></p>"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling.html#avoid-groupby-for-large-datasets","title":"Avoid GroupBy for Large Datasets","text":"<p>When one key has a much larger presence in the dataset, try to avoid using GroupBy with that key or avoid GroupBy altogether. Instead, use alternatives like reduceByKey, which first combines data locally on each partition before grouping, making it more efficient.</p> <p>Example with DataFrames:</p> <p>Instead of this: <pre><code># GroupBy operation\ngrouped_data = df.groupBy(\"key\").agg(sum(\"value\"))\n</code></pre></p> <p>Use this: <pre><code>from pyspark.sql import functions as F\n\n# ReduceByKey equivalent\nreduced_data = df.groupBy(\"key\").agg(F.sum(\"value\"))\n</code></pre></p> <p>In this example with DataFrames, the data is aggregated locally on each partition before the final grouping, helping to avoid data imbalance and improve efficiency.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.5_DataSkewHandling.html#check-your-knowledge","title":"Check  your knowledge","text":"<p>Question: Which of the following operations is most likely to induce a skew in the size of your data's partitions?</p> <p>A. DataFrame.collect() B. DataFrame.cache() C. DataFrame.repartition(n) D. DataFrame.coalesce(n) E. DataFrame.persist()</p> <p>Answer: D. DataFrame.coalesce(n)</p> <p>Explanation:</p> <ul> <li>DataFrame.collect(): This operation brings all the data to the driver node, which doesn\u2019t affect the partition sizes.</li> <li>DataFrame.cache(): This operation stores the DataFrame in memory but doesn\u2019t change the partition sizes.</li> <li>DataFrame.repartition(n): This operation reshuffles the data into a specified number of partitions with a relatively even distribution.</li> <li>DataFrame.coalesce(n): This operation reduces the number of partitions by merging them without a shuffle, which can lead to uneven partition sizes if the original data was not evenly distributed.</li> <li>DataFrame.persist(): Similar to <code>cache()</code>, this stores the DataFrame in memory/disk but doesn\u2019t change partition sizes.</li> </ul> <p>Therefore, coalesce(n) is the operation that is most likely to cause skew in the size of your data's partitions.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.6_dropna_fillna_df_missing_val_handling.html","title":"Missing Values","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.6_dropna_fillna_df_missing_val_handling.html#dropna-fillna-handling-missing-values-in-dfs","title":"dropna &amp; fillna - Handling missing values in dfs","text":"<p>In PySpark dataframes, missing values are represented as <code>NULL</code> or <code>None</code>. Here, I will show you how to handle these missing values using various functions in PySpark.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.6_dropna_fillna_df_missing_val_handling.html#dropping-rows-with-null-values","title":"Dropping Rows with Null Values","text":"<ul> <li>Drop Rows with Any Null Values:   <pre><code>df.dropna()  # or df.na.drop()\n</code></pre></li> </ul> <p>This will drop rows that have even one null value.</p> <ul> <li> <p>Drop Rows Where All Values Are Null:   <pre><code>df.dropna(how='all')  # or df.na.drop(\"all\")\n</code></pre>   This will drop rows where all values are null.</p> </li> <li> <p>Drop Rows with Null Values in Specific Columns:</p> </li> </ul> <p>Drop rows if <code>country</code> OR <code>region</code> have null values   <pre><code>df = df.dropna(subset=[\"country\", \"region\"])\n# Alternative: df.na.drop(subset=[\"country\", \"region\"])\n</code></pre></p>"},{"location":"Spark-DataBricks/1.0_Spark/1.6_dropna_fillna_df_missing_val_handling.html#filling-missing-values","title":"Filling Missing Values","text":"<ul> <li>Fill Null Values in Specific Columns:   <pre><code>df.fillna({\"price\": 0, \"country\": \"unknown\"})\n</code></pre>   If the <code>price</code> column has null values, replace them with <code>0</code>. If the <code>country</code> column has null values, replace them with <code>\"unknown\"</code>.</li> <li>Using a Dictionary <pre><code>replacements = {\n    \"age\": 0,\n    \"country\": \"Unknown\",\n    \"region\": \"Unknown\",\n    \"income\": 0,  # Adding more columns as needed\n    \"population\": 0\n}\ndf = df.fillna(replacements)\n# Alternative: df.na.fill(replacements)\n</code></pre></li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/1.6_dropna_fillna_df_missing_val_handling.html#replacing-specific-values","title":"Replacing Specific Values","text":"<ul> <li> <p>Using replace:   <pre><code>df.replace({None: \"godknows\"}, subset=[\"country\"])\n</code></pre>   This will replace <code>None</code> (null) values in the <code>country</code> column with <code>\"godknows\"</code>.</p> </li> <li> <p>Using withColumn, when &amp; otherwise:   <pre><code>from pyspark.sql.functions import when\n\ndf = df.withColumn(\"country\", when(df[\"country\"].isNull(), \"godknows\").otherwise(df[\"country\"]))\n</code></pre>   This will replace null values in the <code>country</code> column with <code>\"godknows\"</code>.</p> </li> <li> <p>Using Filter <pre><code>df.filter(df[\"age\"] &gt; 30)\n# Alternative: df.where(df[\"age\"] &gt; 30)\n</code></pre></p> </li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/1.6_dropna_fillna_df_missing_val_handling.html#imputation","title":"Imputation","text":"<ul> <li>Fill Null Values with Mean of the Column:   <pre><code>from pyspark.sql.functions import mean\n\nmean_price = df.select(mean(\"price\")).collect()[0][0]\ndf = df.na.fill({\"price\": mean_price})\n</code></pre>   This will replace null values in the <code>price</code> column with the mean value of that column.</li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/1.7_distinct_dropDuplicate_windowsFunc.html","title":"Windows Functions","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.7_distinct_dropDuplicate_windowsFunc.html#removing-duplicate-rows-in-pyspark-dataframes","title":"Removing Duplicate Rows in PySpark DataFrames","text":"<p>Duplicate rows in dataframes are very common. Here, I will show you some common methods to remove duplicate rows from dataframes.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.7_distinct_dropDuplicate_windowsFunc.html#what-is-a-duplicate-row","title":"What is a Duplicate Row?","text":"<p>A duplicate row is a row that is an exact copy of another row. Let's look at an example:</p> <pre><code>data = [\n    (\"Donald\", \"Trump\", 70),\n    (\"Jo\", \"Biden\", 99),\n    (\"Barrack\", \"Obama\", 60),  \n    (\"Donald\", \"Trump\", 70),  # A real duplicate row. A mirror copy of another row\n    (\"Donald\", \"Duck\", 70)    # Not a duplicate\n]\n</code></pre>"},{"location":"Spark-DataBricks/1.0_Spark/1.7_distinct_dropDuplicate_windowsFunc.html#common-methods-to-remove-duplicates","title":"Common Methods to Remove Duplicates","text":"<ol> <li>Using <code>distinct()</code>:</li> <li> <p>Using <code>distinct()</code>: The first method that comes to mind. It removes duplicate rows that are 100% the same, meaning all columns are checked.    <pre><code>df = df.distinct()\n</code></pre></p> </li> <li> <p>Using <code>dropDuplicates()</code>:</p> </li> <li> <p>Using <code>dropDuplicates()</code>: This method is exactly like <code>distinct()</code>.    <pre><code>df = df.dropDuplicates()\n# Alternative: df = df.drop_duplicates()\n</code></pre></p> </li> <li> <p>Removing Duplicates when only few columns are duplicates:</p> </li> <li>Using <code>subset=[\"col1\", \"col2\"]</code>: If it's not 100% the same, but some columns are the same, you can still remove duplicates based on those columns using <code>df.dropDuplicates(subset=[\"col1\", \"col2\"])</code>.    <pre><code>df = df.dropDuplicates(subset=[\"name\", \"age\"])\n# Alternative: df = df.drop_duplicates(subset=[\"name\", \"age\"])\n</code></pre></li> </ol>"},{"location":"Spark-DataBricks/1.0_Spark/1.7_distinct_dropDuplicate_windowsFunc.html#advanced-duplicate-removal","title":"Advanced Duplicate Removal","text":"<p>Normally, using one function, <code>df.dropDuplicates(subset=[\"name\", \"age\"])</code>, is enough to handle all kinds of duplicate removal. But, in some cases, the requirement may be more customized. In such cases, we have to use window functions.</p> <p>The code below shows how to handle special cases of removing duplicates using window functions in PySpark. Each example shows how window functions can be used for specific needs that <code>dropDuplicates()</code> cannot handle.</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, sum, desc\n\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"AdvancedDuplicateRemoval\").getOrCreate()\n\n# Sample DataFrames for each scenario\ndata1 = [\n    (1, \"Alice\", \"2021-01-01\"),\n    (1, \"Alice\", \"2021-01-02\"),\n    (2, \"Bob\", \"2021-01-01\"),\n    (2, \"Bob\", \"2021-01-03\"),\n    (3, \"Charlie\", \"2021-01-01\")\n]\ncolumns1 = [\"user_id\", \"name\", \"timestamp\"]\ndf1 = spark.createDataFrame(data1, columns1)\n\ndata2 = [\n    (1, \"GroupA\", 5),\n    (2, \"GroupA\", 10),\n    (3, \"GroupB\", 15),\n    (4, \"GroupB\", 8),\n    (5, \"GroupC\", 12)\n]\ncolumns2 = [\"id\", \"group_id\", \"priority\"]\ndf2 = spark.createDataFrame(data2, columns2)\n\ndata3 = [\n    (1, \"TXN1\", 100),\n    (2, \"TXN1\", 200),\n    (3, \"TXN2\", 300),\n    (4, \"TXN2\", 400),\n    (5, \"TXN3\", 500)\n]\ncolumns3 = [\"id\", \"transaction_id\", \"sales_amount\"]\ndf3 = spark.createDataFrame(data3, columns3)\n\n# Scenario 1: Keep the latest duplicate based on a timestamp column\nwindow_spec1 = Window.partitionBy(\"user_id\").orderBy(desc(\"timestamp\"))\ndf1_with_row_num = df1.withColumn(\"row_num\", row_number().over(window_spec1))\nunique_df1 = df1_with_row_num.filter(df1_with_row_num.row_num == 1).drop(\"row_num\")\nprint(\"Scenario 1: Keep the most recent entry\")\nunique_df1.show()\n\n# Scenario 2: Keep the duplicate with the highest priority value within each group\nwindow_spec2 = Window.partitionBy(\"group_id\").orderBy(desc(\"priority\"))\ndf2_with_row_num = df2.withColumn(\"row_num\", row_number().over(window_spec2))\nunique_df2 = df2_with_row_num.filter(df2_with_row_num.row_num == 1).drop(\"row_num\")\nprint(\"Scenario 2: Keep the highest priority entry within each group\")\nunique_df2.show()\n\n# Scenario 3: Aggregate data from duplicate rows before removing them\nwindow_spec3 = Window.partitionBy(\"transaction_id\")\ndf3_with_aggregates = df3.withColumn(\"total_sales\", sum(\"sales_amount\").over(window_spec3))\nunique_df3 = df3_with_aggregates.dropDuplicates([\"transaction_id\"])\nprint(\"Scenario 3: Aggregate sales amounts and keep one entry per transaction ID\")\nunique_df3.show()\n</code></pre>"},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping.html","title":"Partitioning","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping.html#partitioning-and-bucketing-in-spark","title":"Partitioning and Bucketing in Spark","text":"<p>Both are techniques to make things run faster. Both partitioning and bucketing split rows in a table into separate files. But, there are differences. In this article I will try to highlight some main points about these two techniques.</p> <p></p>"},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping.html#partitioning","title":"Partitioning","text":"<p>Partitioning in Spark is simple: it splits data by rows, not by columns. For example, all rows where the country is USA become one partition. Columns are not split, so never confuse this with column-based splitting.</p> <p>Partitioning divides data into separate folders based on the values of one or more columns. Each partition folder contains all rows that share the same value for the partitioning column(s). This helps improve read performance for queries that filter on the partition column.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping.html#example","title":"Example","text":"<p>Consider the following sample data:</p> <pre><code># Sample data\n+---+----+-------+\n| id|name|country|\n+---+----+-------+\n| 1 | A  | USA   |\n| 2 | B  | CAN   |\n| 3 | C  | USA   |\n| 4 | D  | CAN   |\n+---+----+-------+\n</code></pre> <p>Partitioning this data by <code>country</code>:</p> <pre><code># Partitioning by country\npartDF = df.repartition(\"country\")\n</code></pre> <p>Resulting in partitions:</p> <ul> <li> <p>Partition 1 (for USA):   <pre><code>+---+----+-------+\n| id|name|country|\n+---+----+-------+\n| 1 | A  | USA   |\n| 3 | C  | USA   |\n+---+----+-------+\n</code></pre></p> </li> <li> <p>Partition 2 (for Canada):   <pre><code>+---+----+-------+\n| id|name|country|\n+---+----+-------+\n| 2 | B  | CAN   |\n| 4 | D  | CAN   |\n+---+----+-------+\n</code></pre></p> </li> </ul> <p>When you save this partitioned data to disk:</p> <pre><code>df.write.partitionBy(\"country\").parquet(\"/tables_root_folder\")\n</code></pre> <p>This will create a folder structure like this:</p> <pre><code>/table_root_folder/\n  \u251c\u2500\u2500 country=USA/\n  \u2502   \u2514\u2500\u2500 part-00000.parquet\n  \u2514\u2500\u2500 country=CAN/\n      \u2514\u2500\u2500 part-00000.parquet\n</code></pre>"},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping.html#bucketing","title":"Bucketing","text":"<p>Bucketing is another way to organize data. It divides data into a fixed number of files(called buckets) based on the hash value of a specified column. Each bucket is a separate physical file within the table's folder. Rows with the same value for the bucket column are stored in the same bucket file. This makes data operations like joins and aggregations faster.</p> <p>For example, if you bucket a dataset by <code>customer_id</code> into 4 buckets, the table's folder will contain 4 bucket files.</p> <pre><code># Bucketing by customer_id\ndf.write.bucketBy(4, \"customer_id\").saveAsTable(\"tables_root_folder\")\n</code></pre> <p>The folder structure will look like this:</p> <pre><code>/tables_root_folder/\n  \u251c\u2500\u2500 part-00000\n  \u251c\u2500\u2500 part-00001\n  \u251c\u2500\u2500 part-00002\n  \u2514\u2500\u2500 part-00003\n</code></pre>"},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping.html#how-partion-and-bucket-info-is-stored","title":"How partion and bucket info is stored?","text":"<p>When using Hive Metastore, both partitioning and bucketing metadata are stored in the Metastore. This includes information about which columns are used for partitioning and bucketing, and the number of buckets.</p> <ul> <li>Partitioning Metadata: The Metastore keeps track of the partition columns and the structure of the directories.</li> <li>Bucketing Metadata: The Metastore stores information about the bucketing column(s) and the number of buckets.</li> </ul> <p>This metadata allows Spark to efficiently read and write data by knowing the organization of the table.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping.html#adding-new-rows","title":"Adding New Rows","text":"<p>When you add new rows to a partitioned table, Spark places them in the appropriate partition folder without reorganizing the entire dataset.</p> <pre><code># Adding new rows to a partitioned table\nnew_data.write.mode(\"append\").partitionBy(\"country\").parquet(\"/path/to/partitioned_data\")\n</code></pre> <p>For a bucketed table, new rows are appended to the existing bucket files based on the hash of the bucketing column. This keeps the bucketing scheme efficient without needing to rebucket the entire dataset.</p> <pre><code># Adding new rows to a bucketed table\nnew_data.write.bucketBy(4, \"customer_id\").mode(\"append\").saveAsTable(\"bucketed_table\")\n</code></pre>"},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping.html#combining-partitioning-and-bucketing","title":"Combining Partitioning and Bucketing","text":"<p>You can also combine partitioning and bucketing to leverage the benefits of both techniques. This is useful when you need to optimize data access for multiple dimensions.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping.html#example_1","title":"Example","text":"<p>Consider partitioning by <code>country</code> and bucketing by <code>customer_id</code>.</p> <pre><code># Partitioning by country and bucketing by customer_id\ndf.write.partitionBy(\"country\").bucketBy(4, \"customer_id\").saveAsTable(\"part_buck_table\")\n</code></pre> <p>This creates a folder structure where each partition is further divided into buckets:</p> <pre><code>/part_buck_table/\n  \u251c\u2500\u2500 country=USA/\n  \u2502   \u251c\u2500\u2500 part-00000\n  \u2502   \u251c\u2500\u2500 part-00001\n  \u2502   \u251c\u2500\u2500 part-00002\n  \u2502   \u2514\u2500\u2500 part-00003\n  \u2514\u2500\u2500 country=CAN/\n      \u251c\u2500\u2500 part-00000\n      \u251c\u2500\u2500 part-00001\n      \u251c\u2500\u2500 part-00002\n      \u2514\u2500\u2500 part-00003\n</code></pre>"},{"location":"Spark-DataBricks/1.0_Spark/1.8_Partition_Grouping.html#lets-summarize","title":"Lets summarize","text":"Feature Partitioning Bucketing Method Splits data by rows based on column values Splits data into a fixed number of buckets based on hash Storage Separate directories for each partition value Separate files (buckets) within the table folder Example <code>df.write.partitionBy(\"country\").parquet(\"/table_root_folder\")</code> <code>df.write.bucketBy(4, \"customer_id\").saveAsTable(\"tableName\")</code> folder Structure <code>/table_root_folder/country=USA/</code><code>/table_root_folder/country=CAN/</code> <code>/table_root_folder/part-00000</code><code>/table_root_folder/part-00001</code> Efficiency Optimizes read performance for specific queries Optimizes join and aggregation performance Metadata Storage Stored in Hive Metastore for partition columns Stored in Hive Metastore for bucketing columns and number of buckets Adding New Rows New rows go to the appropriate partition folder New rows are appended to the appropriate bucket files Combining Can be combined with bucketing for multi-dimensional optimization Can be combined with partitioning for multi-dimensional optimization"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset.html","title":"RDD vs DataFrame","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset.html#understanding-rdds-dataframes-and-datasets-in-apache-spark","title":"Understanding RDDs, DataFrames, and Datasets in Apache Spark","text":"<p>Apache Spark provides three main ways to handle data: RDDs, DataFrames, and Datasets. Let's see what they are and why three?</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset.html#rdd-resilient-distributed-dataset","title":"RDD (Resilient Distributed Dataset)","text":"<p>Imagine you have a Python list. Now, if we make this list capable of parallel processing, that\u2019s an RDD. It\u2019s just a collection of JVM objects that Spark can process in parallel. Most of the features that Spark boasts about, such as in-memory processing, lazy evaluation, partitioning for parallel processing, and fault tolerance, are all due to RDDs. They are the foundation of Spark's powerful capabilities.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset.html#features-of-rdds","title":"Features of RDDs","text":"<ul> <li>Functional operations like <code>map</code>, <code>filter</code>, and <code>reduce</code>. But, no SQL-capability.</li> <li>This is like C programming. Godo for hard-code programmers who need find-grained control.</li> <li> <p>However, RDDs do not have type safety. This means you might run into runtime errors if your data types are not consistent. Look at this example:</p> <pre><code>from pyspark import SparkContext\n\n# Initialize SparkContext\nsc = SparkContext(\"local\", \"RDD Example\")\n\n# Create an RDD with mixed data types\ndata = [1, 2, \"three\", 4]\nrdd = sc.parallelize(data)\n\n# This will cause a runtime error because 'three' cannot be squared\nrdd.foreach(lambda x: print(x ** 2))\n</code></pre> <p>Here, trying to square the string \"three\" will cause a runtime error. This is a main problem of RDDs \u2013 they are not type safe.</p> </li> <li> <p>Also RDDs are read-only(Immutable). You can't change them:</p> <pre><code># Create an RDD\ndata = [1, 2, 3, 4, 5]\nrdd = sc.parallelize(data)\nrdd[0] = 10  # This will cause an error\n\n# See, how dataframe doesn't have this issue\n# Create a DataFrame\ndata = [(1,), (2,), (3,)]\ndf = spark.createDataFrame(data, [\"value\"])\n\n# Modify the value 1 to 10\ndf = df.withColumn(\"value\", when(col(\"value\") == 1, 10).otherwise(col(\"value\")))\n</code></pre> </li> </ul> <p>Say we have a standard Python list object, containing elements <code>[1, 2, 3, 4, 5]</code>. We call <code>sc.parallelize(data)</code> to create an RDD. What happens?</p> <pre><code>data = [1, 2, 3, 4, 5] # Standard Python List Object\nrdd = sc.parallelize(data) # RDD Object\n</code></pre> <ol> <li> <p>Partitioning: The list <code>[1, 2, 3, 4, 5]</code> is split into partitions, such as <code>[1, 2]</code>, <code>[3, 4]</code>, and <code>[5]</code>.</p> </li> <li> <p>RDD Metadata: An RDD object is created . This RDD object contains info(metadata) about partioning and how they are distributed to which node.</p> </li> <li> <p>The partitions are sent to different worker nodes in the Spark cluster for processing.</p> </li> </ol>"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset.html#what-happens-when-we-create-an-rdd","title":"What happens when we create an RDD","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset.html#functions-in-rdds","title":"Functions in RDDs","text":"<ul> <li>Transformations: map(), filter(), reduceByKey(), groupByKey(), union(), intersection()</li> <li>Actions: count(), collect(), reduce(), take(n), foreach(), first()</li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset.html#dataframe","title":"DataFrame","text":"<p>Now, let\u2019s think about SQL programmers. They are used to working with tables. This is where DataFrames come in. A DataFrame is like a table with rows and columns.</p> <p>Advantages of DataFrames: - Schema Support: Dataframes are just like tables. They have schema, they have columns. Columns are of type, like int, string etc. - SQL Query engine: You can run a SQL query using a DataFrame. When the query is run, it uses the Spark SQL engine. This engine does the query optimization and execution optimization for you. This is not possible with RDDs. When working with big data, this is a big deal. What if your query takes an eternity to complete? The Spark SQL engine ensures that your queries run efficiently and quickly. - Easy to use: Using DataFrames is also very easy, especially if you are familiar with SQL, because you can run SQL queries directly on them. Plus, DataFrames can integrate easily with various data sources like Hive, Avro, Parquet, and JSON. This makes DataFrames not only powerful but also very flexible and user-friendly.</p> <p>However, DataFrames are not type safe, similar to RDDs. This means type errors can still occur at runtime.</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset.html#dataset","title":"Dataset","text":"<p>Datasets combine the best features of RDDs and DataFrames. They offer both type safety and optimized execution.</p> <p>Why Datasets? - Type Safety: Datasets provide compile-time type checking, reducing runtime errors. - Optimized Execution: They use the same optimized execution engine as DataFrames, which means efficient processing and query optimization. - Functional and Relational Operations: Datasets support both functional programming (like RDDs) and SQL operations (like DataFrames). - Ease of Use: They combine the ease of use of DataFrames with the type safety of RDDs.</p> <p>Dataset is only for Java and Scale. No PySpark</p>"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset.html#evolution-summary","title":"Evolution Summary","text":"<ul> <li>RDDs (2011): Just a collection of JVM objects. Good for parallel processing and lets you use many functional APIs like map, filter, etc. However, they lacked type safety and could cause runtime errors.</li> <li>DataFrames (2013): Introduced SQL-like tables and the Spark SQL engine. Now, we have tables to work on with schemas. They are also faster than RDDs due to the optimized engine but still lack type safety.</li> <li>Datasets (2015): Combine type safety, optimized execution, and ease of use, offering the best of both RDDs and DataFrames.</li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset.html#conclusion","title":"Conclusion","text":""},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset.html#rdd-dfs-ds-conversion","title":"RDD, Dfs, Ds conversion","text":"<ul> <li>RDDs: toDF() and rdd() for conversion.</li> <li>DataFrames: rdd() for RDDs, as[] for Datasets.</li> <li>Datasets: toDF() for DataFrames, rdd() for RDDs.</li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/1.9_RDD_Dataframe_Dataset.html#what-to-use","title":"What to use?","text":"<p>If you use Pyspark, use Dataframe. If you use Scale use Dataset. Period. RDD? If you are a coding ninja working for a product company and spend your time living inside a compuer ram. Use RDD.</p>"},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html","title":"The Complete PySpark Gotchas Guide","text":"<p>About This Guide</p> <p>This comprehensive guide covers the most expensive mistakes in PySpark development, with practical solutions and performance optimizations. Each gotcha includes real-world examples and measurable improvements.</p>"},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#overview","title":"Overview","text":"<p>PySpark offers incredible power for big data processing, but with great power comes great responsibility. This guide helps you avoid the most common and costly mistakes that can turn your lightning-fast distributed processing into a crawling disaster.</p> <p>Performance Impact</p> <p>The gotchas in this guide can cause:</p> <ul> <li>10-100x slower job execution</li> <li>OutOfMemoryError crashes</li> <li>Wasted cluster resources costing thousands of dollars</li> <li>Failed production jobs affecting business operations</li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#categories","title":"Categories","text":"Data Loading &amp; I/OMemory &amp; CachingJoins &amp; AggregationsConfigurationAdvanced Topics <ul> <li>Small files problem</li> <li>Schema inference overhead</li> <li>Wrong file formats</li> </ul> <ul> <li>Over/under-caching</li> <li>Wrong storage levels</li> <li>Memory leaks</li> </ul> <ul> <li>Data skew handling</li> <li>Broadcasting decisions</li> <li>Window function optimization</li> </ul> <ul> <li>Default settings trap</li> <li>Resource allocation</li> <li>Dynamic scaling</li> </ul> <ul> <li>UDF optimization</li> <li>Streaming pitfalls</li> <li>Monitoring blind spots</li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#1-data-loading-io-gotchas","title":"1. Data Loading &amp; I/O Gotchas","text":""},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#gotcha-1-the-small-files-performance-killer","title":"Gotcha #1: The Small Files Performance Killer","text":"<p>Performance Impact: 10-50x slower</p> <p>Reading thousands of small files creates excessive task overhead and kills performance.</p> <p>The Problem: <pre><code># BAD: Reading 10,000 small JSON files\ndf = spark.read.json(\"s3://bucket/small-files/*.json\")\n# Creates 10,000 tasks with massive overhead\n</code></pre></p> Why This Happens <ul> <li>Each file becomes a separate task</li> <li>Task scheduling overhead dominates actual processing</li> <li>Executors spend more time on coordination than computation</li> <li>Network latency multiplied by number of files</li> </ul> <p>The Solution:</p> Quick FixBetter ApproachBest Practice <pre><code># GOOD: Coalesce after reading\ndf = spark.read.json(\"s3://bucket/small-files/*.json\").coalesce(100)\n</code></pre> <pre><code># BETTER: Use wholeTextFiles for very small files\nrdd = spark.sparkContext.wholeTextFiles(\"s3://bucket/small-files/*.json\")\ndf = spark.read.json(rdd.values())\n</code></pre> <pre><code># BEST: Combine files during ingestion\ndef optimize_small_files(input_path, output_path, target_size_mb=128):\n    df = spark.read.json(input_path)\n\n    # Calculate optimal partitions\n    total_size_mb = estimate_dataframe_size_mb(df)\n    optimal_partitions = max(1, total_size_mb // target_size_mb)\n\n    df.coalesce(optimal_partitions) \\\n      .write \\\n      .mode(\"overwrite\") \\\n      .parquet(output_path)\n</code></pre> <p>Performance Gain</p> <p>Before: 10,000 tasks, 45 minutes After: 100 tasks, 3 minutes Improvement: 15x faster \u26a1</p>"},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#gotcha-2-schema-inference-double-read-penalty","title":"Gotcha #2: Schema Inference Double-Read Penalty","text":"<p>Performance Impact: 2x slower, 2x I/O cost</p> <p>Schema inference requires reading the entire dataset twice.</p> <p>The Problem: <pre><code># BAD: Schema inference on large datasets\ndf = spark.read.csv(\"huge_dataset.csv\", header=True, inferSchema=True)\n# Spark reads the entire file twice!\n</code></pre></p> What Happens Under the Hood <ol> <li>First read: Spark scans entire dataset to infer schema</li> <li>Second read: Spark reads dataset again with inferred schema</li> <li>Cost: Double I/O, double time, double cloud storage charges</li> </ol> <p>** The Solution:**</p> Predefined SchemaSchema Generation Helper <pre><code>from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n\n# GOOD: Define schema upfront\nschema = StructType([\n    StructField(\"user_id\", StringType(), True),\n    StructField(\"event_time\", TimestampType(), True),\n    StructField(\"event_count\", IntegerType(), True)\n])\n\ndf = spark.read.csv(\"huge_dataset.csv\", header=True, schema=schema)\n</code></pre> <pre><code>def generate_schema_from_sample(file_path, sample_size=1000):\n    \"\"\"Generate schema from a small sample\"\"\"\n    sample_df = spark.read.csv(file_path, header=True, inferSchema=True).limit(sample_size)\n\n    print(\"Generated Schema:\")\n    print(\"schema = StructType([\")\n    for field in sample_df.schema.fields:\n        print(f'    StructField(\"{field.name}\", {field.dataType}, {field.nullable}),')\n    print(\"])\")\n\n    return sample_df.schema\n\n# Use for large datasets\nschema = generate_schema_from_sample(\"huge_dataset.csv\")\ndf = spark.read.csv(\"huge_dataset.csv\", header=True, schema=schema)\n</code></pre> <p>Performance Gain</p> <p>Before: 2 full dataset scans After: 1 dataset scan Improvement: 50% faster, 50% less I/O cost \ud83d\udcb0</p>"},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#gotcha-3-suboptimal-file-format-choices","title":"Gotcha #3: Suboptimal File Format Choices","text":"<p>Performance Impact: 5-20x slower queries</p> <p>Using row-based formats (CSV/JSON) for analytical workloads instead of columnar formats.</p> <p>The Problem: <pre><code># BAD: Large analytical datasets in row-based formats\ndf = spark.read.csv(\"10TB_dataset.csv\")  \n# No compression, no predicate pushdown, reads entire rows\n</code></pre></p> Format Comparison Format Compression Predicate Pushdown Schema Evolution ACID CSV \u274c \u274c \u274c \u274c JSON \u274c \u274c \u2705 \u274c Parquet \u2705 \u2705 \u2705 \u274c Delta \u2705 \u2705 \u2705 \u2705 <p>** The Solution:**</p> Parquet for AnalyticsDelta for ProductionMigration Strategy <pre><code># GOOD: Columnar format with compression\ndf = spark.read.parquet(\"10TB_dataset.parquet\")\n\n# Benefits:\n# - 70-90% compression\n# - Column pruning\n# - Predicate pushdown\n# - Fast aggregations\n</code></pre> <pre><code># BETTER: Delta Lake for ACID transactions\ndf = spark.read.format(\"delta\").load(\"delta-table/\")\n\n# Additional benefits:\n# - Time travel\n# - ACID transactions\n# - Schema enforcement\n# - Automatic optimization\n</code></pre> <pre><code>def migrate_to_optimized_format(source_path, target_path, format_type=\"delta\"):\n    \"\"\"Migrate data to optimized format with partitioning\"\"\"\n\n    # Read source data\n    df = spark.read.csv(source_path, header=True, inferSchema=True)\n\n    # Optimize partitioning\n    if \"date\" in df.columns:\n        df = df.withColumn(\"year\", year(col(\"date\"))) \\\n               .withColumn(\"month\", month(col(\"date\")))\n        partition_cols = [\"year\", \"month\"]\n    else:\n        partition_cols = None\n\n    # Write in optimized format\n    writer = df.write.mode(\"overwrite\")\n\n    if partition_cols:\n        writer = writer.partitionBy(*partition_cols)\n\n    if format_type == \"delta\":\n        writer.format(\"delta\").save(target_path)\n    else:\n        writer.parquet(target_path)\n\n    print(f\"Migration complete. Data saved to {target_path}\")\n</code></pre> <p>Storage &amp; Performance Comparison</p> <p>CSV (1TB) \u2192 Parquet (200GB) \u2192 Delta (180GB) Query Speed: CSV baseline \u2192 Parquet 10x faster \u2192 Delta 12x faster</p>"},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#2-partitioning-nightmares","title":"2. Partitioning Nightmares","text":""},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#gotcha-4-the-goldilocks-partition-problem","title":"Gotcha #4: The Goldilocks Partition Problem","text":"<p>Performance Impact: 5-100x slower</p> <p>Partitions that are too small create overhead; too large cause memory issues.</p> <p>The Problem: <pre><code># BAD: Ignoring partition sizes\ndf = spark.read.parquet(\"data/\")\nprint(f\"Partitions: {df.rdd.getNumPartitions()}\")  # Could be 1 or 10,000!\n</code></pre></p> Partition Size Impact <p>Too Small (&lt; 10MB): - High task scheduling overhead - Underutilized executors - Inefficient network usage</p> <p>Too Large (&gt; 1GB): - Memory pressure - GC overhead - Risk of OOM errors</p> <p>Just Right (100-200MB): - Optimal resource utilization - Balanced parallelism - Efficient processing</p> <p>** The Solution:**</p> Partition AnalysisOptimal Partitioning <pre><code>def analyze_partitions(df, df_name=\"DataFrame\"):\n    \"\"\"Comprehensive partition analysis\"\"\"\n    print(f\"\\n=== {df_name} Partition Analysis ===\")\n\n    num_partitions = df.rdd.getNumPartitions()\n    print(f\"Number of partitions: {num_partitions}\")\n\n    # Sample partition sizes\n    partition_counts = df.rdd.mapPartitions(lambda x: [sum(1 for _ in x)]).collect()\n\n    if partition_counts:\n        min_size = min(partition_counts)\n        max_size = max(partition_counts)\n        avg_size = sum(partition_counts) / len(partition_counts)\n\n        print(f\"Partition sizes:\")\n        print(f\"  Min: {min_size:,} rows\")\n        print(f\"  Max: {max_size:,} rows\") \n        print(f\"  Avg: {avg_size:,.0f} rows\")\n\n        # Skew detection\n        skew_ratio = max_size / avg_size if avg_size &gt; 0 else 0\n        if skew_ratio &gt; 3:\n            print(f\"\u26a0\ufe0f  WARNING: High skew detected! Max is {skew_ratio:.1f}x larger than average\")\n\n        # Size recommendation\n        estimated_mb_per_partition = avg_size * 0.001  # Rough estimate\n        print(f\"Estimated avg partition size: ~{estimated_mb_per_partition:.1f} MB\")\n\n        if estimated_mb_per_partition &lt; 50:\n            print(\"\ud83d\udca1 Consider reducing partitions (coalesce)\")\n        elif estimated_mb_per_partition &gt; 300:\n            print(\"\ud83d\udca1 Consider increasing partitions (repartition)\")\n        else:\n            print(\"\u2705 Partition sizes look good!\")\n\n    return df\n\n# Usage\ndf = analyze_partitions(df, \"Raw Data\")\n</code></pre> <pre><code>def optimize_partitions(df, target_partition_size_mb=128):\n    \"\"\"Calculate and apply optimal partitioning\"\"\"\n\n    # Estimate total size\n    sample_count = df.sample(0.01).count()\n    if sample_count == 0:\n        return df.coalesce(1)\n\n    total_count = df.count()\n    sample_data = df.sample(0.01).take(min(100, sample_count))\n\n    if sample_data:\n        # Estimate row size (rough approximation)\n        avg_row_size_bytes = sum(len(str(row)) for row in sample_data) / len(sample_data) * 2\n        total_size_mb = (total_count * avg_row_size_bytes) / (1024 * 1024)\n\n        optimal_partitions = max(1, int(total_size_mb / target_partition_size_mb))\n        optimal_partitions = min(optimal_partitions, 4000)  # Cap at reasonable max\n\n        print(f\"Estimated dataset size: {total_size_mb:.1f} MB\")\n        print(f\"Target partition size: {target_partition_size_mb} MB\")\n        print(f\"Optimal partitions: {optimal_partitions}\")\n\n        current_partitions = df.rdd.getNumPartitions()\n\n        if optimal_partitions &lt; current_partitions:\n            print(\"Applying coalesce...\")\n            return df.coalesce(optimal_partitions)\n        elif optimal_partitions &gt; current_partitions * 1.5:\n            print(\"Applying repartition...\")\n            return df.repartition(optimal_partitions)\n        else:\n            print(\"Current partitioning is acceptable\")\n            return df\n\n    return df\n\n# Apply optimization\ndf_optimized = optimize_partitions(df)\n</code></pre> <p>Optimization Results</p> <p>Before: 10,000 partitions (1MB each) After: 100 partitions (100MB each) Improvement: 20x fewer tasks, 80% less overhead</p>"},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#gotcha-5-high-cardinality-partitioning-disaster","title":"Gotcha #5: High-Cardinality Partitioning Disaster","text":"<p>Performance Impact: Creates millions of tiny files</p> <p>Partitioning by high-cardinality columns creates too many small partitions.</p> <p>The Problem: <pre><code># BAD: Partitioning by high-cardinality column\ndf.write.partitionBy(\"user_id\").parquet(\"output/\")  \n# Creates millions of tiny partitions (one per user)\n</code></pre></p> Small Files Problem <p>Impact of millions of small files: - Metadata overhead in storage systems - Slow listing operations - Inefficient subsequent reads - Increased storage costs (minimum block sizes)</p> <p>** The Solution:**</p> Smart Partitioning StrategyHash-Based PartitioningCardinality Analysis Helper <pre><code># GOOD: Partition by low-cardinality columns\ndf.write.partitionBy(\"year\", \"month\").parquet(\"output/\")\n\n# For time-based partitioning\nfrom pyspark.sql.functions import date_format\n\ndf_with_partition = df.withColumn(\n    \"year_month\", \n    date_format(col(\"timestamp\"), \"yyyy-MM\")\n)\ndf_with_partition.write.partitionBy(\"year_month\").parquet(\"output/\")\n</code></pre> <pre><code># BETTER: Use hash-based partitioning for high-cardinality\nfrom pyspark.sql.functions import hash, col\n\n# Create buckets for high-cardinality column\nnum_buckets = 100  # Adjust based on data size\n\ndf_bucketed = df.withColumn(\n    \"user_bucket\", \n    hash(col(\"user_id\")) % num_buckets\n)\n\ndf_bucketed.write.partitionBy(\"user_bucket\").parquet(\"output/\")\n</code></pre> <pre><code>def analyze_cardinality(df, columns, sample_fraction=0.1):\n    \"\"\"Analyze cardinality of potential partition columns\"\"\"\n\n    print(\"=== Cardinality Analysis ===\")\n    sample_df = df.sample(sample_fraction)\n    total_rows = df.count()\n    sample_rows = sample_df.count()\n\n    results = {}\n\n    for column in columns:\n        if column in df.columns:\n            distinct_count = sample_df.select(column).distinct().count()\n\n            # Estimate total distinct values\n            estimated_distinct = distinct_count * (total_rows / sample_rows)\n\n            results[column] = {\n                'estimated_distinct': int(estimated_distinct),\n                'cardinality_ratio': estimated_distinct / total_rows\n            }\n\n            # Partitioning recommendation\n            if estimated_distinct &lt; 100:\n                recommendation = \"\u2705 Good for partitioning\"\n            elif estimated_distinct &lt; 1000:\n                recommendation = \"\u26a0\ufe0f  Consider hash bucketing\"\n            else:\n                recommendation = \"\u274c Too high cardinality\"\n\n            print(f\"{column}:\")\n            print(f\"  Estimated distinct values: {int(estimated_distinct):,}\")\n            print(f\"  Cardinality ratio: {estimated_distinct/total_rows:.4f}\")\n            print(f\"  Recommendation: {recommendation}\")\n            print()\n\n    return results\n\n# Usage\npartition_analysis = analyze_cardinality(\n    df, \n    columns=[\"user_id\", \"category\", \"date\", \"region\"]\n)\n</code></pre> <p>Partitioning Guidelines</p> <p>Ideal partition column characteristics:</p> <ul> <li>\u2705 Low cardinality (&lt; 1000 distinct values)</li> <li>\u2705 Evenly distributed data</li> <li>\u2705 Frequently used in WHERE clauses</li> <li>\u2705 Stable over time</li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#3-caching-persistence-pitfalls","title":"3. Caching &amp; Persistence Pitfalls","text":""},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#gotcha-6-the-over-caching-memory-waste","title":"Gotcha #6: The Over-Caching Memory Waste","text":"<p>Performance Impact: Memory exhaustion, slower jobs</p> <p>Caching DataFrames that are used only once wastes precious executor memory.</p> <p>The Problem: <pre><code># BAD: Cache everything approach\ndf1 = spark.read.parquet(\"data1.parquet\").cache()  # Used once\ndf2 = spark.read.parquet(\"data2.parquet\").cache()  # Used once  \ndf3 = spark.read.parquet(\"data3.parquet\").cache()  # Used once\n\nresult = df1.join(df2, \"key\").join(df3, \"key\")  # Memory wasted!\n</code></pre></p> Memory Impact <p>Over-caching consequences: - Executor memory exhaustion - Increased GC pressure - Spilling to disk (defeating cache purpose) - Reduced performance for actually reused data</p> <p>** The Solution:**</p> Strategic CachingSmart Caching Decision <pre><code># GOOD: Cache only reused DataFrames\nexpensive_df = df.groupBy(\"category\").agg(\n    count(\"*\").alias(\"count\"),\n    avg(\"price\").alias(\"avg_price\"),\n    sum(\"revenue\").alias(\"total_revenue\")\n)\n\n# This will be reused multiple times\nexpensive_df.cache()\n\n# Multiple operations using cached data\nhigh_volume = expensive_df.filter(col(\"count\") &gt; 1000)\nlow_volume = expensive_df.filter(col(\"count\") &lt; 100)\nmid_range = expensive_df.filter(\n    (col(\"count\") &gt;= 100) &amp; (col(\"count\") &lt;= 1000)\n)\n\n# Clean up when done\nexpensive_df.unpersist()\n</code></pre> <pre><code>def should_cache(df, usage_count, computation_cost=\"medium\"):\n    \"\"\"Intelligent caching decision based on usage patterns\"\"\"\n\n    cost_weights = {\n        \"low\": 1,      # Simple transformations\n        \"medium\": 3,   # Joins, groupBy\n        \"high\": 10     # Complex aggregations, multiple joins\n    }\n\n    weight = cost_weights.get(computation_cost, 3)\n    cache_benefit_score = usage_count * weight\n\n    # Memory consideration\n    partition_count = df.rdd.getNumPartitions()\n    memory_concern = partition_count &gt; 1000  # Many partitions = more memory\n\n    recommendation = {\n        \"should_cache\": cache_benefit_score &gt;= 6 and not memory_concern,\n        \"score\": cache_benefit_score,\n        \"memory_concern\": memory_concern\n    }\n\n    return recommendation\n\n# Usage example\nexpensive_computation = df.groupBy(\"category\", \"region\").agg(\n    countDistinct(\"user_id\"),\n    percentile_approx(\"amount\", 0.5),\n    collect_list(\"product_id\")\n)\n\ncache_decision = should_cache(\n    expensive_computation, \n    usage_count=3, \n    computation_cost=\"high\"\n)\n\nif cache_decision[\"should_cache\"]:\n    expensive_computation.cache()\n    print(f\"\u2705 Caching recommended (score: {cache_decision['score']})\")\nelse:\n    print(f\"\u274c Caching not recommended (score: {cache_decision['score']})\")\n</code></pre> <p>Caching Best Practices</p> <p>Cache when: - DataFrame is used 2+ times \u2705 - Computation is expensive \u2705 - Memory is available \u2705</p> <p>Don't cache when: - One-time use \u274c - Simple transformations \u274c - Memory is constrained \u274c</p>"},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#gotcha-7-wrong-storage-level-choices","title":"Gotcha #7: Wrong Storage Level Choices","text":"<p>Performance Impact: Cache eviction, memory pressure</p> <p>Using inappropriate storage levels can cause cache thrashing and poor performance.</p> <p>The Problem: <pre><code># BAD: Default MEMORY_ONLY when data doesn't fit\nlarge_df.cache()  # Uses MEMORY_ONLY, causes eviction cascades\n</code></pre></p> Storage Level Comparison Storage Level Memory Disk Serialized Replicated MEMORY_ONLY \u2705 \u274c \u274c \u274c MEMORY_AND_DISK \u2705 \u2705 \u274c \u274c MEMORY_ONLY_SER \u2705 \u274c \u2705 \u274c MEMORY_AND_DISK_SER \u2705 \u2705 \u2705 \u274c DISK_ONLY \u274c \u2705 \u2705 \u274c <p>** The Solution:**</p> Choose Appropriate StorageIntelligent Storage SelectionCache Monitoring <pre><code>from pyspark import StorageLevel\n\n# For large datasets that might not fit in memory\nlarge_df.persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n# For critical data that needs high availability\ncritical_df.persist(StorageLevel.MEMORY_ONLY_2)  # Replicated\n\n# For infrequently accessed but expensive to compute\narchive_df.persist(StorageLevel.DISK_ONLY)\n\n# For iterative algorithms with memory constraints\nml_features.persist(StorageLevel.MEMORY_AND_DISK_SER_2)\n</code></pre> <pre><code>def select_storage_level(df, access_pattern=\"frequent\", memory_available_gb=8):\n    \"\"\"Select optimal storage level based on usage pattern\"\"\"\n\n    # Estimate DataFrame size\n    estimated_size_gb = estimate_dataframe_size_gb(df)\n\n    # Storage level decision matrix\n    if access_pattern == \"frequent\":\n        if estimated_size_gb &lt; memory_available_gb * 0.3:\n            return StorageLevel.MEMORY_ONLY\n        elif estimated_size_gb &lt; memory_available_gb * 0.6:\n            return StorageLevel.MEMORY_ONLY_SER\n        else:\n            return StorageLevel.MEMORY_AND_DISK_SER\n\n    elif access_pattern == \"occasional\":\n        if estimated_size_gb &lt; memory_available_gb * 0.2:\n            return StorageLevel.MEMORY_ONLY\n        else:\n            return StorageLevel.MEMORY_AND_DISK_SER\n\n    elif access_pattern == \"rare\":\n        return StorageLevel.DISK_ONLY\n\n    elif access_pattern == \"critical\":\n        if estimated_size_gb &lt; memory_available_gb * 0.4:\n            return StorageLevel.MEMORY_ONLY_2  # Replicated\n        else:\n            return StorageLevel.MEMORY_AND_DISK_SER_2\n\n    return StorageLevel.MEMORY_AND_DISK_SER  # Safe default\n\n# Usage\nstorage_level = select_storage_level(\n    df=expensive_computation,\n    access_pattern=\"frequent\", \n    memory_available_gb=16\n)\n\nexpensive_computation.persist(storage_level)\nprint(f\"Using storage level: {storage_level}\")\n</code></pre> <pre><code>def monitor_cache_usage():\n    \"\"\"Monitor cache usage across the cluster\"\"\"\n    print(\"=== Cache Usage Report ===\")\n\n    storage_infos = spark.sparkContext._jsc.sc().getRDDStorageInfo()\n\n    total_memory_used = 0\n    total_disk_used = 0\n\n    for storage_info in storage_infos:\n        rdd_id = storage_info.id()\n        memory_size_mb = storage_info.memSize() / (1024 * 1024)\n        disk_size_mb = storage_info.diskSize() / (1024 * 1024)\n        storage_level = storage_info.storageLevel()\n\n        total_memory_used += memory_size_mb\n        total_disk_used += disk_size_mb\n\n        print(f\"RDD {rdd_id}:\")\n        print(f\"  Memory: {memory_size_mb:.1f} MB\")\n        print(f\"  Disk: {disk_size_mb:.1f} MB\")\n        print(f\"  Storage Level: {storage_level}\")\n        print()\n\n    print(f\"Total Cache Usage:\")\n    print(f\"  Memory: {total_memory_used:.1f} MB\")\n    print(f\"  Disk: {total_disk_used:.1f} MB\")\n\n    # Get executor memory info\n    executors = spark.sparkContext.statusTracker().getExecutorInfos()\n    total_executor_memory = sum(exec.maxMemory for exec in executors)\n    cache_memory_ratio = (total_memory_used * 1024 * 1024) / total_executor_memory\n\n    print(f\"  Cache Memory Ratio: {cache_memory_ratio:.1%}\")\n\n    if cache_memory_ratio &gt; 0.8:\n        print(\"\u26a0\ufe0f  WARNING: High cache memory usage!\")\n\n# Monitor periodically\nmonitor_cache_usage()\n</code></pre> <p>Storage Level Guidelines</p> <p>Choose based on: - Data size vs available memory - Access frequency (frequent = memory priority) - Fault tolerance needs (critical = replication) - Cost sensitivity (disk cheaper than memory)</p>"},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#gotcha-8-lazy-cache-evaluation-trap","title":"Gotcha #8: Lazy Cache Evaluation Trap","text":"<p>Performance Impact: Cache never populated</p> <p>Cache is lazy - without triggering an action, the cache remains empty.</p> <p>The Problem: <pre><code># BAD: Cache without triggering action\nexpensive_df = df.groupBy(\"category\").agg(count(\"*\"))\nexpensive_df.cache()  # Nothing cached yet!\n\n# Later operations don't benefit from cache\nresult1 = expensive_df.filter(col(\"count\") &gt; 100).collect()  # Computed\nresult2 = expensive_df.filter(col(\"count\") &lt; 50).collect()   # Recomputed!\n</code></pre></p> <p>** The Solution:**</p> Trigger Cache PopulationCache Context ManagerSmart Cache Verification <pre><code># GOOD: Force cache with action\nexpensive_df = df.groupBy(\"category\").agg(count(\"*\"))\nexpensive_df.cache()\n\n# Trigger cache population\ncache_trigger = expensive_df.count()  # Forces computation and caching\nprint(f\"Cached {cache_trigger} rows\")\n\n# Now subsequent operations use cache\nresult1 = expensive_df.filter(col(\"count\") &gt; 100).collect()  # Uses cache\nresult2 = expensive_df.filter(col(\"count\") &lt; 50).collect()   # Uses cache\n</code></pre> <pre><code>from contextlib import contextmanager\n\n@contextmanager\ndef cached_dataframe(df, storage_level=None, trigger_action=True):\n    \"\"\"Context manager for automatic cache management\"\"\"\n\n    if storage_level:\n        df.persist(storage_level)\n    else:\n        df.cache()\n\n    try:\n        if trigger_action:\n            # Trigger cache population\n            row_count = df.count()\n            print(f\"\u2705 Cached DataFrame with {row_count:,} rows\")\n\n        yield df\n\n    finally:\n        df.unpersist()\n        print(\"\ud83e\uddf9 Cache cleaned up\")\n\n# Usage\nexpensive_computation = df.groupBy(\"category\").agg(\n    count(\"*\").alias(\"count\"),\n    avg(\"price\").alias(\"avg_price\")\n)\n\nwith cached_dataframe(expensive_computation) as cached_df:\n    # All operations within this block use cache\n    high_count = cached_df.filter(col(\"count\") &gt; 1000).collect()\n    low_count = cached_df.filter(col(\"count\") &lt; 100).collect()\n    stats = cached_df.describe().collect()\n\n# Cache automatically cleaned up here\n</code></pre> <pre><code>def verify_cache_usage(df, operation_name=\"operation\"):\n    \"\"\"Verify that cache is actually being used\"\"\"\n\n    # Check if DataFrame is cached\n    if not df.is_cached:\n        print(f\"\u26a0\ufe0f  WARNING: {operation_name} - DataFrame not cached!\")\n        return False\n\n    # Get RDD storage info\n    rdd_id = df.rdd.id()\n    storage_infos = spark.sparkContext._jsc.sc().getRDDStorageInfo()\n\n    for storage_info in storage_infos:\n        if storage_info.id() == rdd_id:\n            memory_size = storage_info.memSize()\n            disk_size = storage_info.diskSize()\n\n            if memory_size &gt; 0 or disk_size &gt; 0:\n                print(f\"\u2705 {operation_name} - Cache verified: \"\n                      f\"{memory_size/(1024**2):.1f}MB memory, \"\n                      f\"{disk_size/(1024**2):.1f}MB disk\")\n                return True\n            else:\n                print(f\"\u26a0\ufe0f  {operation_name} - Cache empty, triggering population...\")\n                df.count()  # Trigger cache\n                return True\n\n    print(f\"\u274c {operation_name} - Cache not found!\")\n    return False\n\n# Usage\nexpensive_df.cache()\nverify_cache_usage(expensive_df, \"Expensive Computation\")\n</code></pre> <p>Cache Checklist</p> <p>Before relying on cache:</p> <ul> <li>[ ] DataFrame is marked as cached (<code>.cache()</code> or <code>.persist()</code>)</li> <li>[ ] Action has been triggered (<code>.count()</code>, <code>.collect()</code>, etc.)</li> <li>[ ] Verify cache population with monitoring</li> <li>[ ] Plan cache cleanup (<code>.unpersist()</code>)</li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#4-join-operation-hell","title":"4. Join Operation Hell","text":""},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#gotcha-9-data-skew-the-silent-performance-killer","title":"Gotcha #9: Data Skew - The Silent Performance Killer","text":"<p>Performance Impact: Some tasks take 100x longer</p> <p>Uneven key distribution causes massive partitions while others remain tiny, creating severe bottlenecks.</p> <p>The Problem: <pre><code># BAD: Join with severely skewed data\nuser_events = spark.read.table(\"user_events\")    # Some users: millions of events\nuser_profiles = spark.read.table(\"user_profiles\") # Even distribution\n\n# Hot keys create massive partitions\nresult = user_events.join(user_profiles, \"user_id\")\n# 99% of tasks finish in 30 seconds, 1% take 2 hours!\n</code></pre></p> Why Skew Kills Performance <p>The anatomy of skew: - 95% of partitions: 1,000 records each (finish quickly) - 5% of partitions: 1,000,000 records each (become stragglers) - Result: Entire job waits for slowest partition</p> <p>Real-world impact: - Job that should take 10 minutes takes 3 hours - Cluster sits 95% idle waiting for stragglers - Potential executor OOM on large partitions</p> <p>** The Solution:**</p> Automatic Skew DetectionStrategy 1: Broadcast JoinStrategy 2: Salting for Severe SkewStrategy 3: Bucketing for Repeated Joins <pre><code>def detect_join_skew(df, join_column, sample_fraction=0.1, skew_threshold=1000):\n    \"\"\"Detect data skew in join keys\"\"\"\n\n    print(f\"=== Skew Analysis for '{join_column}' ===\")\n\n    # Sample for performance on large datasets\n    sample_df = df.sample(sample_fraction)\n\n    # Get key distribution\n    key_counts = sample_df.groupBy(join_column).count() \\\n                          .orderBy(col(\"count\").desc())\n\n    stats = key_counts.agg(\n        min(\"count\").alias(\"min_count\"),\n        max(\"count\").alias(\"max_count\"), \n        avg(\"count\").alias(\"avg_count\"),\n        expr(\"percentile_approx(count, 0.95)\").alias(\"p95_count\")\n    ).collect()[0]\n\n    # Scale up from sample\n    scale_factor = 1 / sample_fraction\n    scaled_max = stats[\"max_count\"] * scale_factor\n    scaled_avg = stats[\"avg_count\"] * scale_factor\n\n    skew_ratio = scaled_max / scaled_avg if scaled_avg &gt; 0 else 0\n\n    print(f\"Key distribution (scaled from {sample_fraction*100}% sample):\")\n    print(f\"  Average count per key: {scaled_avg:,.0f}\")\n    print(f\"  Maximum count per key: {scaled_max:,.0f}\")\n    print(f\"  Skew ratio (max/avg): {skew_ratio:.1f}\")\n\n    # Show top skewed keys\n    print(f\"\\nTop 10 most frequent keys:\")\n    top_keys = key_counts.limit(10).collect()\n    for row in top_keys:\n        scaled_count = row[\"count\"] * scale_factor\n        print(f\"  {row[join_column]}: {scaled_count:,.0f} records\")\n\n    # Skew assessment\n    if skew_ratio &gt; 10:\n        print(f\"\\n\ud83d\udea8 SEVERE SKEW DETECTED! Ratio: {skew_ratio:.1f}\")\n        return \"severe\"\n    elif skew_ratio &gt; 3:\n        print(f\"\\n\u26a0\ufe0f  Moderate skew detected. Ratio: {skew_ratio:.1f}\")\n        return \"moderate\"\n    else:\n        print(f\"\\n\u2705 No significant skew. Ratio: {skew_ratio:.1f}\")\n        return \"none\"\n\n# Usage\nskew_level = detect_join_skew(user_events, \"user_id\")\n</code></pre> <pre><code># GOOD: Broadcast small table to avoid shuffle\ndef smart_broadcast_join(large_df, small_df, join_keys):\n    \"\"\"Intelligently decide on broadcast join\"\"\"\n\n    # Estimate small table size\n    small_sample = small_df.sample(0.1)\n    if small_sample.count() &gt; 0:\n        sample_rows = small_sample.count()\n        total_rows = small_df.count()\n        sample_size_mb = len(str(small_sample.take(100))) * sample_rows / (1024 * 1024)\n        estimated_size_mb = sample_size_mb * (total_rows / sample_rows)\n\n        print(f\"Small table estimated size: {estimated_size_mb:.1f} MB\")\n\n        if estimated_size_mb &lt; 200:  # Safe broadcast threshold\n            print(\"\u2705 Using broadcast join\")\n            return large_df.join(broadcast(small_df), join_keys)\n        else:\n            print(\"\u274c Table too large for broadcast, using regular join\")\n            return large_df.join(small_df, join_keys)\n\n    return large_df.join(small_df, join_keys)\n\n# Apply smart broadcast\nresult = smart_broadcast_join(user_events, user_profiles, \"user_id\")\n</code></pre> <pre><code># BETTER: Salting technique for severe skew\ndef salted_join(large_df, small_df, join_key, salt_buckets=100):\n    \"\"\"Handle severe skew using salting technique\"\"\"\n\n    print(f\"Applying salting with {salt_buckets} buckets...\")\n\n    # Add salt to large table\n    large_salted = large_df.withColumn(\n        \"salt\", \n        (rand() * salt_buckets).cast(\"int\")\n    ).withColumn(\n        \"salted_key\",\n        concat(col(join_key).cast(\"string\"), lit(\"_\"), col(\"salt\").cast(\"string\"))\n    )\n\n    # Explode small table across all salt values\n    salt_range = spark.range(salt_buckets).select(col(\"id\").alias(\"salt\"))\n    small_exploded = small_df.crossJoin(salt_range).withColumn(\n        \"salted_key\",\n        concat(col(join_key).cast(\"string\"), lit(\"_\"), col(\"salt\").cast(\"string\"))\n    )\n\n    # Join on salted keys\n    result = large_salted.join(small_exploded, \"salted_key\") \\\n                        .drop(\"salt\", \"salted_key\")  # Clean up helper columns\n\n    print(\"\u2705 Salted join completed\")\n    return result\n\n# Apply when severe skew detected\nif skew_level == \"severe\":\n    result = salted_join(user_events, user_profiles, \"user_id\", salt_buckets=200)\nelse:\n    result = user_events.join(broadcast(user_profiles), \"user_id\")\n</code></pre> <pre><code># BEST: Pre-bucketing for repeated skewed joins\ndef create_bucketed_tables(df, table_name, bucket_column, num_buckets=200):\n    \"\"\"Create bucketed table for optimal joins\"\"\"\n\n    print(f\"Creating bucketed table: {table_name}\")\n\n    df.write \\\n      .mode(\"overwrite\") \\\n      .option(\"path\", f\"/bucketed_tables/{table_name}\") \\\n      .bucketBy(num_buckets, bucket_column) \\\n      .sortBy(bucket_column) \\\n      .saveAsTable(table_name)\n\n    print(f\"\u2705 Bucketed table created with {num_buckets} buckets\")\n\n# Create bucketed tables (one-time setup)\ncreate_bucketed_tables(user_events, \"user_events_bucketed\", \"user_id\", 200)\ncreate_bucketed_tables(user_profiles, \"user_profiles_bucketed\", \"user_id\", 200)\n\n# Fast joins on bucketed tables (no shuffle needed!)\nbucketed_events = spark.table(\"user_events_bucketed\")\nbucketed_profiles = spark.table(\"user_profiles_bucketed\")\n\n# This join will be much faster - no shuffle required\nresult = bucketed_events.join(bucketed_profiles, \"user_id\")\n</code></pre> <p>Skew Handling Results</p> <p>Before (skewed join): 3 hours, 95% cluster idle After (salted join): 25 minutes, even distribution Improvement: 7x faster, better resource utilization</p>"},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#gotcha-10-broadcasting-memory-bombs","title":"Gotcha #10: Broadcasting Memory Bombs","text":"<p>Performance Impact: OutOfMemoryError, cluster crashes</p> <p>Broadcasting tables larger than executor memory causes catastrophic failures.</p> <p>The Problem: <pre><code># BAD: Broadcasting without size validation\nlarge_lookup = spark.read.table(\"product_catalog\")  # 5GB table!\norders = spark.read.table(\"orders\")\n\n# This will crash executors\nresult = orders.join(broadcast(large_lookup), \"product_id\")  # OOM!\n</code></pre></p> Broadcast Memory Requirements <p>Memory needed for broadcast: - Table size \u00d7 Number of executor cores - Example: 1GB table \u00d7 100 cores = 100GB total memory needed - Each executor must hold entire broadcasted table in memory</p> <p>Failure cascade: - Executors run out of memory - Tasks fail and retry - Driver struggles with retries - Eventually entire application crashes</p> <p>** The Solution:**</p> Safe Broadcasting with Size ChecksDynamic Broadcast ThresholdBroadcast Health Monitoring <pre><code>def safe_broadcast_join(left_df, right_df, join_keys, max_broadcast_mb=200):\n    \"\"\"Safely determine optimal join strategy\"\"\"\n\n    def estimate_dataframe_size_mb(df, sample_fraction=0.01):\n        \"\"\"Estimate DataFrame size in MB\"\"\"\n        try:\n            sample = df.sample(sample_fraction)\n            sample_count = sample.count()\n\n            if sample_count == 0:\n                return float('inf')  # Cannot estimate\n\n            # Get sample data for size estimation\n            sample_data = sample.take(min(100, sample_count))\n            if not sample_data:\n                return float('inf')\n\n            # Estimate average row size\n            avg_row_size = sum(len(str(row)) for row in sample_data) / len(sample_data)\n            total_rows = df.count()\n\n            # Conservative size estimate (includes serialization overhead)\n            estimated_size_mb = (total_rows * avg_row_size * 2) / (1024 * 1024)\n            return estimated_size_mb\n\n        except Exception as e:\n            print(f\"Size estimation failed: {e}\")\n            return float('inf')\n\n    # Estimate sizes\n    left_size = estimate_dataframe_size_mb(left_df)\n    right_size = estimate_dataframe_size_mb(right_df)\n\n    print(f\"Join size analysis:\")\n    print(f\"  Left table:  {left_size:.1f} MB\")\n    print(f\"  Right table: {right_size:.1f} MB\")\n    print(f\"  Broadcast threshold: {max_broadcast_mb} MB\")\n\n    # Determine join strategy\n    if right_size &lt;= max_broadcast_mb:\n        print(\"\u2705 Broadcasting right table\")\n        return left_df.join(broadcast(right_df), join_keys)\n    elif left_size &lt;= max_broadcast_mb:\n        print(\"\u2705 Broadcasting left table\")\n        return broadcast(left_df).join(right_df, join_keys)\n    else:\n        print(\"\u26a0\ufe0f  Both tables too large for broadcast, using shuffle join\")\n\n        # Optimize shuffle join\n        optimized_left = left_df.repartition(col(join_keys[0]) if isinstance(join_keys, list) else col(join_keys))\n        optimized_right = right_df.repartition(col(join_keys[0]) if isinstance(join_keys, list) else col(join_keys))\n\n        return optimized_left.join(optimized_right, join_keys)\n\n# Usage\nresult = safe_broadcast_join(orders, product_catalog, \"product_id\", max_broadcast_mb=150)\n</code></pre> <pre><code>def calculate_safe_broadcast_threshold():\n    \"\"\"Calculate safe broadcast threshold based on cluster resources\"\"\"\n\n    # Get executor information\n    executors = spark.sparkContext.statusTracker().getExecutorInfos()\n\n    if not executors:\n        return 50  # Conservative default\n\n    # Calculate available memory per executor\n    min_executor_memory = min(exec.maxMemory for exec in executors)\n    executor_count = len(executors)\n\n    # Reserve memory for other operations (conservative 30%)\n    available_memory_per_executor = min_executor_memory * 0.3\n\n    # Convert to MB\n    safe_threshold_mb = available_memory_per_executor / (1024 * 1024)\n\n    print(f\"Cluster broadcast analysis:\")\n    print(f\"  Executors: {executor_count}\")\n    print(f\"  Min executor memory: {min_executor_memory/(1024**3):.1f} GB\")\n    print(f\"  Safe broadcast threshold: {safe_threshold_mb:.0f} MB\")\n\n    return min(safe_threshold_mb, 500)  # Cap at 500MB for safety\n\n# Auto-calculate threshold\nsafe_threshold = calculate_safe_broadcast_threshold()\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", f\"{int(safe_threshold)}MB\")\n</code></pre> <pre><code>def monitor_broadcast_usage():\n    \"\"\"Monitor broadcast variable usage across cluster\"\"\"\n\n    print(\"=== Broadcast Usage Report ===\")\n\n    # Get broadcast information\n    broadcast_vars = spark.sparkContext._jsc.sc().getBroadcastInfos()\n\n    total_broadcast_size = 0\n\n    for broadcast_info in broadcast_vars:\n        broadcast_id = broadcast_info.id()\n        broadcast_size_mb = broadcast_info.size() / (1024 * 1024)\n        total_broadcast_size += broadcast_size_mb\n\n        print(f\"Broadcast {broadcast_id}: {broadcast_size_mb:.1f} MB\")\n\n    print(f\"Total broadcast size: {total_broadcast_size:.1f} MB\")\n\n    # Check against executor memory\n    executors = spark.sparkContext.statusTracker().getExecutorInfos()\n    if executors:\n        total_executor_memory_gb = sum(exec.maxMemory for exec in executors) / (1024**3)\n        broadcast_ratio = (total_broadcast_size / 1024) / total_executor_memory_gb\n\n        print(f\"Broadcast memory ratio: {broadcast_ratio:.1%}\")\n\n        if broadcast_ratio &gt; 0.1:  # More than 10% of cluster memory\n            print(\"\u26a0\ufe0f  WARNING: High broadcast memory usage!\")\n\n    return total_broadcast_size\n\n# Monitor after joins\nmonitor_broadcast_usage()\n</code></pre> <p>Broadcasting Guidelines</p> <p>Safe broadcast sizes:</p> <ul> <li>Small cluster: &lt; 50MB</li> <li>Medium cluster: &lt; 200MB  </li> <li>Large cluster: &lt; 500MB</li> <li>Rule of thumb: &lt; 10% of executor memory</li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#gotcha-11-inefficient-join-ordering","title":"Gotcha #11: Inefficient Join Ordering","text":"<p>Performance Impact: 10x larger intermediate results</p> <p>Joining large tables first instead of filtering with smaller ones creates massive intermediate datasets.</p> <p>The Problem: <pre><code># BAD: Join large tables first\nevents = spark.read.table(\"events\")           # 1TB\nsessions = spark.read.table(\"sessions\")       # 500GB  \nactive_users = spark.read.table(\"active_users\")  # 10MB\n\n# Creates massive intermediate result\nresult = events.join(sessions, \"session_id\") \\    # 1.5TB intermediate!\n               .join(active_users, \"user_id\")     # Then filter to 50GB\n</code></pre></p> <p>** The Solution:**</p> Optimal Join OrderingAutomated Join OptimizationJoin Performance Monitoring <pre><code># GOOD: Filter early, join strategically\n\n# Step 1: Start with most selective filters\nactive_events = events.join(broadcast(active_users), \"user_id\")  # Filter first\n\n# Step 2: Apply additional filters before expensive joins\nrecent_events = active_events.filter(col(\"event_date\") &gt;= \"2023-01-01\")\n\n# Step 3: Join with larger tables only on filtered data\nresult = recent_events.join(sessions, \"session_id\")\n\n# Result: 50GB intermediate instead of 1.5TB!\n</code></pre> <pre><code>class JoinOptimizer:\n    def __init__(self, spark_session):\n        self.spark = spark_session\n\n    def estimate_join_selectivity(self, left_df, right_df, join_key):\n        \"\"\"Estimate how much a join will reduce data size\"\"\"\n\n        # Sample to estimate selectivity\n        left_sample = left_df.sample(0.01)\n        right_sample = right_df.sample(0.01)\n\n        left_keys = set(row[join_key] for row in left_sample.select(join_key).collect())\n        right_keys = set(row[join_key] for row in right_sample.select(join_key).collect())\n\n        # Estimate join selectivity\n        intersection_ratio = len(left_keys.intersection(right_keys)) / max(len(left_keys), 1)\n\n        return intersection_ratio\n\n    def optimize_join_order(self, tables_with_info):\n        \"\"\"\n        Optimize join order based on table sizes and selectivity\n        tables_with_info: [(df, name, estimated_size_mb, join_key)]\n        \"\"\"\n\n        print(\"=== Join Order Optimization ===\")\n\n        # Sort by size (smallest first for broadcast candidates)\n        sorted_tables = sorted(tables_with_info, key=lambda x: x[2])\n\n        optimized_plan = []\n\n        for i, (df, name, size_mb, join_key) in enumerate(sorted_tables):\n            if size_mb &lt; 200:  # Broadcast candidate\n                optimized_plan.append({\n                    'table': df,\n                    'name': name,\n                    'size_mb': size_mb,\n                    'strategy': 'broadcast',\n                    'order': i\n                })\n            else:\n                optimized_plan.append({\n                    'table': df,\n                    'name': name, \n                    'size_mb': size_mb,\n                    'strategy': 'shuffle',\n                    'order': i\n                })\n\n        print(\"Optimized join plan:\")\n        for plan in optimized_plan:\n            print(f\"  {plan['order']}: {plan['name']} ({plan['size_mb']:.1f}MB) - {plan['strategy']}\")\n\n        return optimized_plan\n\n    def execute_optimized_joins(self, join_plan, base_df=None):\n        \"\"\"Execute joins in optimized order\"\"\"\n\n        if not join_plan:\n            return base_df\n\n        result_df = base_df or join_plan[0]['table']\n\n        for i, plan in enumerate(join_plan[1:], 1):\n            join_df = plan['table']\n\n            if plan['strategy'] == 'broadcast':\n                print(f\"Step {i}: Broadcasting {plan['name']}\")\n                result_df = result_df.join(broadcast(join_df), plan.get('join_key', 'id'))\n            else:\n                print(f\"Step {i}: Shuffle joining {plan['name']}\")\n                result_df = result_df.join(join_df, plan.get('join_key', 'id'))\n\n        return result_df\n\n# Usage\noptimizer = JoinOptimizer(spark)\n\ntables_info = [\n    (events, \"events\", 1000000, \"user_id\"),      # 1TB\n    (sessions, \"sessions\", 500000, \"session_id\"), # 500GB\n    (active_users, \"active_users\", 10, \"user_id\") # 10MB\n]\n\njoin_plan = optimizer.optimize_join_order(tables_info)\noptimized_result = optimizer.execute_optimized_joins(join_plan)\n</code></pre> <pre><code>def monitor_join_performance(df, operation_name):\n    \"\"\"Monitor join operation performance\"\"\"\n\n    print(f\"\\n=== {operation_name} Performance ===\")\n\n    start_time = time.time()\n\n    # Get initial metrics\n    initial_partitions = df.rdd.getNumPartitions()\n\n    # Trigger computation\n    result_count = df.count()\n\n    end_time = time.time()\n    duration = end_time - start_time\n\n    print(f\"Operation: {operation_name}\")\n    print(f\"Duration: {duration:.2f} seconds\")\n    print(f\"Result rows: {result_count:,}\")\n    print(f\"Partitions: {initial_partitions}\")\n    print(f\"Throughput: {result_count/duration:,.0f} rows/second\")\n\n    # Check for skew in result\n    partition_counts = df.rdd.mapPartitions(lambda x: [sum(1 for _ in x)]).collect()\n    if partition_counts:\n        max_partition = max(partition_counts)\n        avg_partition = sum(partition_counts) / len(partition_counts)\n        skew_ratio = max_partition / avg_partition if avg_partition &gt; 0 else 0\n\n        print(f\"Result skew ratio: {skew_ratio:.1f}\")\n        if skew_ratio &gt; 3:\n            print(\"\u26a0\ufe0f  High skew in join result!\")\n\n    return duration, result_count\n\n# Monitor join performance\nwith monitor_join_performance(result, \"Optimized Multi-Join\"):\n    final_result = result.collect()\n</code></pre> <p>Join Ordering Benefits</p> <p>Unoptimized: 1TB \u2192 1.5TB \u2192 50GB (45 min) Optimized: 1TB \u2192 100GB \u2192 50GB (8 min) Improvement: 5.6x faster, 93% less shuffle data</p>"},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#5-aggregation-groupby-traps","title":"5. Aggregation &amp; GroupBy Traps","text":""},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#gotcha-12-multiple-pass-aggregation-waste","title":"Gotcha #12: Multiple-Pass Aggregation Waste","text":"<p>Performance Impact: 3-5x unnecessary data scans</p> <p>Calling separate aggregation functions triggers multiple passes over the same dataset.</p> <p>The Problem: <pre><code># BAD: Multiple scans of the same data\ntotal_sales = df.agg(sum(\"sales\")).collect()[0][0]      # Scan 1\navg_sales = df.agg(avg(\"sales\")).collect()[0][0]        # Scan 2  \nmax_sales = df.agg(max(\"sales\")).collect()[0][0]        # Scan 3\ncount_sales = df.agg(count(\"sales\")).collect()[0][0]    # Scan 4\n# Four full dataset scans!\n</code></pre></p> <p>** The Solution:**</p> Single-Pass AggregationComprehensive Statistics Helper <pre><code># GOOD: Single pass for multiple aggregations\nfrom pyspark.sql.functions import sum, avg, max, min, count, stddev, expr\n\n# Compute all statistics in one pass\nstats = df.agg(\n    sum(\"sales\").alias(\"total_sales\"),\n    avg(\"sales\").alias(\"avg_sales\"), \n    max(\"sales\").alias(\"max_sales\"),\n    min(\"sales\").alias(\"min_sales\"),\n    count(\"sales\").alias(\"count_sales\"),\n    stddev(\"sales\").alias(\"stddev_sales\"),\n    expr(\"percentile_approx(sales, 0.5)\").alias(\"median_sales\"),\n    expr(\"percentile_approx(sales, array(0.25, 0.75))\").alias(\"quartiles\")\n).collect()[0]\n\n# Extract results\ntotal_sales = stats[\"total_sales\"]\navg_sales = stats[\"avg_sales\"]\nmax_sales = stats[\"max_sales\"]\n\nprint(f\"Sales Statistics (single pass):\")\nprint(f\"  Total: ${total_sales:,.2f}\")\nprint(f\"  Average: ${avg_sales:,.2f}\")\nprint(f\"  Range: ${stats['min_sales']:,.2f} - ${max_sales:,.2f}\")\n</code></pre> <pre><code>def comprehensive_stats(df, numeric_columns, categorical_columns=None):\n    \"\"\"Generate comprehensive statistics in a single pass\"\"\"\n\n    print(\"=== Comprehensive Statistics Analysis ===\")\n\n    # Build aggregation expressions\n    agg_exprs = []\n\n    # Numeric column statistics\n    for col_name in numeric_columns:\n        if col_name in df.columns:\n            agg_exprs.extend([\n                count(col_name).alias(f\"{col_name}_count\"),\n                sum(col_name).alias(f\"{col_name}_sum\"),\n                avg(col_name).alias(f\"{col_name}_avg\"),\n                min(col_name).alias(f\"{col_name}_min\"),\n                max(col_name).alias(f\"{col_name}_max\"),\n                stddev(col_name).alias(f\"{col_name}_stddev\"),\n                expr(f\"percentile_approx({col_name}, 0.5)\").alias(f\"{col_name}_median\")\n            ])\n\n    # Categorical column statistics  \n    if categorical_columns:\n        for col_name in categorical_columns:\n            if col_name in df.columns:\n                agg_exprs.extend([\n                    countDistinct(col_name).alias(f\"{col_name}_distinct_count\"),\n                    count(col_name).alias(f\"{col_name}_non_null_count\")\n                ])\n\n    # Execute single aggregation\n    if agg_exprs:\n        stats_result = df.agg(*agg_exprs).collect()[0]\n\n        # Format and display results\n        for col_name in numeric_columns:\n            if f\"{col_name}_count\" in stats_result.asDict():\n                print(f\"\\n{col_name.upper()} Statistics:\")\n                print(f\"  Count: {stats_result[f'{col_name}_count']:,}\")\n                print(f\"  Sum: {stats_result[f'{col_name}_sum']:,.2f}\")\n                print(f\"  Average: {stats_result[f'{col_name}_avg']:,.2f}\")\n                print(f\"  Range: {stats_result[f'{col_name}_min']:,.2f} - {stats_result[f'{col_name}_max']:,.2f}\")\n                print(f\"  Std Dev: {stats_result[f'{col_name}_stddev']:,.2f}\")\n                print(f\"  Median: {stats_result[f'{col_name}_median']:,.2f}\")\n\n        if categorical_columns:\n            print(f\"\\nCATEGORICAL COLUMNS:\")\n            for col_name in categorical_columns:\n                if f\"{col_name}_distinct_count\" in stats_result.asDict():\n                    distinct_count = stats_result[f\"{col_name}_distinct_count\"]\n                    non_null_count = stats_result[f\"{col_name}_non_null_count\"]\n                    print(f\"  {col_name}: {distinct_count:,} distinct values, {non_null_count:,} non-null\")\n\n        return stats_result.asDict()\n\n    return {}\n\n# Usage\nstats = comprehensive_stats(\n    df,\n    numeric_columns=[\"sales\", \"quantity\", \"profit\"],\n    categorical_columns=[\"category\", \"region\", \"customer_segment\"]\n)\n</code></pre> <p>Performance Improvement</p> <p>Before: 4 separate scans (20 minutes) After: 1 comprehensive scan (5 minutes) Improvement: 4x faster data processing</p>"},{"location":"Spark-DataBricks/1.0_Spark/2.0_PySpark_Gotchas.html#gotcha-13-high-cardinality-groupby-memory-explosion","title":"Gotcha #13: High-Cardinality GroupBy Memory Explosion","text":"<p>Performance Impact: OutOfMemoryError from massive state</p> <p>GroupBy operations on high-cardinality columns create enormous intermediate state.</p> <p>The Problem: <pre><code># BAD: GroupBy on millions of unique values\nuser_stats = df.groupBy(\"user_id\").agg(        # 10M unique users\n    count(\"*\").alias(\"event_count\"),\n    sum(\"amount\").alias(\"total_spent\")\n)\n# Creates 10M groups in memory - potential OOM!\n</code></pre></p> <p>** The Solution:**</p> Cardinality Reduction StrategiesApproximate AggregationsSmart GroupBy with Memory Monitoring <pre><code># Strategy 1: Binning/Bucketing\nfrom pyspark.sql.functions import floor, col, when\n\n# GOOD: Reduce cardinality with intelligent binning\nuser_binned = df.withColumn(\n    \"user_bucket\", \n    floor(col(\"user_id\") / 1000)  # Group users into buckets of 1000\n)\n\nbucket_stats = user_binned.groupBy(\"user_bucket\").agg(\n    count(\"*\").alias(\"total_events\"),\n    countDistinct(\"user_id\").alias(\"unique_users\"),\n    avg(\"amount\").alias(\"avg_amount\")\n)\n\n# Strategy 2: Sampling for exploration\nsample_stats = df.sample(0.1).groupBy(\"user_id\").agg(\n    count(\"*\").alias(\"sample_event_count\"),\n    sum(\"amount\").alias(\"sample_total\")\n)\n\n# Strategy 3: Top-N analysis instead of full groupby\ntop_users = df.groupBy(\"user_id\") \\\n              .agg(sum(\"amount\").alias(\"total_spent\")) \\\n              .orderBy(col(\"total_spent\").desc()) \\\n              .limit(1000)  # Only top 1000 users\n</code></pre> <pre><code># BETTER: Use approximate functions for large-scale analytics\nfrom pyspark.sql.functions import approx_count_distinct, expr\n\n# Approximate statistics (much faster, less memory)\napprox_stats = df.agg(\n    approx_count_distinct(\"user_id\", 0.05).alias(\"approx_unique_users\"),  # 5% error\n    expr(\"percentile_approx(amount, 0.5)\").alias(\"median_amount\"),\n    expr(\"percentile_approx(amount, array(0.25, 0.75))\").alias(\"quartiles\")\n)\n\n# Compare exact vs approximate\nprint(\"Approximate vs Exact Comparison:\")\n\n# Exact (expensive)\nexact_unique = df.select(\"user_id\").distinct().count()\n\n# Approximate (fast)\napprox_unique = approx_stats.collect()[0][\"approx_unique_users\"]\n\nerror_rate = abs(exact_unique - approx_unique) / exact_unique\nprint(f\"Exact unique users: {exact_unique:,}\")\nprint(f\"Approximate unique users: {approx_unique:,}\")  \nprint(f\"Error rate: {error_rate:.2%}\")\n</code></pre> <p>```python def memory_aware_groupby(df, group_columns, agg_exprs, max_groups=1000000):     \"\"\"Perform GroupBy with memory awareness\"\"\"</p>"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows.html","title":"Setup Guide","text":""},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>Getting Started with Data Engineering: Key Installations of Java, Spark, \\&amp; Python PySpark</li> <li>Install Java [Oracle JDK]</li> <li>Install Full Apache SPARK</li> <li>Install Python [python.org]</li> <li>Set Env Variables<ul> <li>Entries</li> <li>Explanation</li> <li>Link python.exe with <code>PYSPARK_PYTHON</code></li> <li><code>%JAVA_HOME%\\bin</code> to PATH</li> </ul> </li> <li>Install Pyspark<ul> <li>Background</li> <li>Install Pyspark System-Wide</li> <li>Check the Installation</li> <li>See Actual Working</li> </ul> </li> <li>Appendix<ul> <li><code>PYSPARK_PYTHON</code> Overview</li> <li>Pyspark Vs Full Spark Overview</li> </ul> </li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows.html#getting-started-with-data-engineering-key-installations-of-java-spark-python-pyspark","title":"Getting Started with Data Engineering: Key Installations of Java, Spark, &amp; Python PySpark","text":"<p> In this guide, I will show how to set up a complete data engineering setup including Java, Full Hadoop, Python, and PySpark. Additionally, I'll describe the significance of setting different environment variables, their roles, and the key differences between Pyspark and a complete Spark setup. </p>"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows.html#install-java-oracle-jdk","title":"Install Java [Oracle JDK]","text":"<p>I've opted for the traditional Java, bringing with it the familiar folder system. Feel free to explore variants like OpenJDK.</p> <ul> <li>Download the JDK 11 (or later) installer from Oracle JDK Downloads page</li> <li> <p>Install JDK in the default directory (typically <code>C:\\Program Files\\Java\\jdk-11</code>).</p> </li> <li> <p>To verify the installation, enter <code>java -version</code> in your command prompt. You should see output similar to this:</p> </li> </ul> <p></p>"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows.html#install-full-apache-spark","title":"Install Full Apache SPARK","text":"<ul> <li>Download <code>spark-3.5.0-bin-hadoop3.tgz</code> from spark.apache.org</li> </ul> <ul> <li>Create a folder <code>C:\\Spark</code>. Place the unzipped contents of <code>spark-3.5.0-bin-hadoop3.tgz</code> inside it. Your <code>C:\\Spark</code> folder should now contain lib, bin etc.</li> </ul> <ul> <li>Establish this folder structure: <code>C:\\hadoop\\bin</code>.</li> <li>Download <code>winutils.exe</code> from github/cdarlint and place it inside <code>C:\\hadoop\\bin</code></li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows.html#install-python-pythonorg","title":"Install Python [python.org]","text":"<ul> <li>Download <code>python-3.12.0-amd64.exe</code> (or similar) from the Python Downloads page</li> <li> <p>Execute the downloaded installer and opt for Customize Installation. Ensure you select Add python.exe to PATH.</p> <p></p> </li> <li> <p>Proceed with all optional features and click Next.</p> <p></p> </li> <li> <p>In Advanced Options, select \"Install Python 3.12 for all users\".</p> <p></p> </li> <li> <p>A successful setup should show up in a Setup Success message.</p> <p></p> </li> <li> <p>Verify the installation by typing <code>python --version</code> in your command prompt. The Python version number indicates a successful installation.</p> <p></p> </li> </ul>"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows.html#set-env-variables","title":"Set Env Variables","text":""},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows.html#entries","title":"Entries","text":"<p>Navigate to Run \u27a4 SYSDM.CPL \u27a4 Advanced \u27a4 Environment Variables, and create or set these environment variables at the system (recommended) or user levels:</p> Variable Value <code>JAVA_HOME</code> <code>C:\\Program Files\\Java\\jdk-11</code> <code>SPARK_HOME</code> <code>C:\\Spark</code> <code>HADOOP_HOME</code> <code>C:\\hadoop</code> <code>PYSPARK_PYTHON</code> <code>C:\\Python39\\python.exe</code> Path <code>%JAVA_HOME%\\bin</code> <code>%SPARK_HOME%\\bin</code> <code>%HADOOP_HOME%\\bin</code> <p> For a PowerShell command to set these variables with Admin privileges, remember to change 'Machine' (for system-wide level) to 'User' (for user level) as required. </p>"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows.html#explanation","title":"Explanation","text":""},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows.html#link-pythonexe-with-pyspark_python","title":"Link python.exe with <code>PYSPARK_PYTHON</code>","text":"<p>We set the <code>PYSPARK_PYTHON</code> environment variable to <code>C:\\Python39\\python.exe</code> to specify which Python executable Spark should use. This is vital, particularly if you have multiple Python installations.</p>"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows.html#java_homebin-to-path","title":"<code>%JAVA_HOME%\\bin</code> to PATH","text":"<p>While <code>C:\\Program Files\\Common Files\\Oracle\\Java\\javapath</code> might already be in your system's <code>Path</code> environment variable, it's generally advisable to add <code>%JAVA_HOME%\\bin</code> to your <code>Path</code>. This ensures your system uses the JDK's executables, rather than those from another Java installation.</p>"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows.html#install-pyspark","title":"Install Pyspark","text":""},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows.html#background","title":"Background","text":"<p>If your code involves creating a Spark session and dataframes, you'll need PySpark Libraries. Install it using <code>pip install pyspark</code>. This does two things: - Installs the libraries - Installs a 'miniature, standalone' Spark environment for testing</p> <p>However, in our case, we don't need the 'miniature Spark' that comes with PySpark libraries. We'll manage potential Spark conflicts with the <code>SPARK_HOME</code> variable set to our full Spark environment.</p>"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows.html#install-pyspark-system-wide","title":"Install Pyspark System-Wide","text":"<p>Open a command prompt with Admin privilege. Use pip (included with our Python) and execute <code>pip install pyspark</code> for a system-wide installation.</p>"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows.html#check-the-installation","title":"Check the Installation","text":"<p>After installation, confirm if PySpark is in the global <code>site-packages</code>: <pre><code>pip show pyspark\n</code></pre> The <code>Location:</code> field in the output reveals the installation location.</p> <p></p>"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows.html#see-actual-working","title":"See Actual Working","text":"<p>Test your PySpark installation by starting a Spark session in a Python environment: <pre><code>from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .appName(\"TestApp\") \\\n    .getOrCreate()\nprint(spark.version)\nspark.stop()\n</code></pre> If Spark starts without errors, your PySpark setup with Python is successful.</p>"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows.html#appendix","title":"Appendix","text":""},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows.html#pyspark_python-overview","title":"<code>PYSPARK_PYTHON</code> Overview","text":"<ol> <li> <p>Selects Python Interpreter: Designates which Python version Spark executors should use for UDFs and transformations. Key in setups with multiple Python versions.</p> </li> <li> <p>Uniformity in Clusters: Guarantees that all cluster nodes use the same Python environment, maintaining consistency in PySpark.</p> </li> </ol>"},{"location":"Spark-DataBricks/1.0_Spark/Install-Pyspark-Windows/Install-Pyspark-Windows.html#pyspark-vs-full-spark-overview","title":"Pyspark Vs Full Spark Overview","text":"<p>I have put this in another section here. Read More..</p> <p>\u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com</p>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/Data%20Skew-TheSilentPerformanceKiller.html","title":"Data Skew - The Silent Performance Killer","text":"<p>Performance Impact</p> <p>Some tasks take 100x longer - Uneven key distribution creates massive partitions while others remain tiny.</p>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/Data%20Skew-TheSilentPerformanceKiller.html#the-problem","title":"The Problem","text":"<p>Data skew occurs when join keys are unevenly distributed. A few keys have millions of records while most have just a few. This creates severe bottlenecks where 99% of tasks finish quickly but 1% take hours.</p> \u274c Problematic Code<pre><code># BAD: Join with severely skewed data\nuser_events = spark.read.table(\"user_events\")    # Some users: millions of events\nuser_profiles = spark.read.table(\"user_profiles\") # Even distribution\n\n# Hot keys create massive partitions\nresult = user_events.join(user_profiles, \"user_id\")\n# 99% of tasks finish in 30 seconds, 1% take 2 hours!\n</code></pre>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/Data%20Skew-TheSilentPerformanceKiller.html#why-skew-kills-performance","title":"Why Skew Kills Performance","text":"Anatomy of SkewReal-World Impact <ul> <li>95% of partitions: 1,000 records each (finish quickly)</li> <li>5% of partitions: 1,000,000 records each (become stragglers)</li> <li>Result: Entire job waits for slowest partition</li> </ul> <ul> <li>Job that should take 10 minutes takes 3 hours</li> <li>Cluster sits 95% idle waiting for stragglers</li> <li>Potential executor OOM on large partitions</li> </ul>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/Data%20Skew-TheSilentPerformanceKiller.html#solutions","title":"Solutions","text":"Skew DetectionBroadcast Join Strategy \u2705 Detect Join Skew<pre><code>def detect_join_skew(df, join_column, sample_fraction=0.1):\n    \"\"\"Detect data skew in join keys\"\"\"\n\n    print(f\"=== Skew Analysis for '{join_column}' ===\")\n\n    # Sample for performance on large datasets\n    sample_df = df.sample(sample_fraction)\n\n    # Get key distribution\n    key_counts = sample_df.groupBy(join_column).count() \\\n                          .orderBy(col(\"count\").desc())\n\n    stats = key_counts.agg(\n        min(\"count\").alias(\"min_count\"),\n        max(\"count\").alias(\"max_count\"), \n        avg(\"count\").alias(\"avg_count\")\n    ).collect()[0]\n\n    # Scale up from sample\n    scale_factor = 1 / sample_fraction\n    scaled_max = stats[\"max_count\"] * scale_factor\n    scaled_avg = stats[\"avg_count\"] * scale_factor\n\n    skew_ratio = scaled_max / scaled_avg if scaled_avg &gt; 0 else 0\n\n    print(f\"Key distribution (scaled from {sample_fraction*100}% sample):\")\n    print(f\"  Average count per key: {scaled_avg:,.0f}\")\n    print(f\"  Maximum count per key: {scaled_max:,.0f}\")\n    print(f\"  Skew ratio (max/avg): {skew_ratio:.1f}\")\n\n    # Show top skewed keys\n    print(f\"\\nTop 10 most frequent keys:\")\n    top_keys = key_counts.limit(10).collect()\n    for row in top_keys:\n        scaled_count = row[\"count\"] * scale_factor\n        print(f\"  {row[join_column]}: {scaled_count:,.0f} records\")\n\n    # Skew assessment\n    if skew_ratio &gt; 10:\n        print(f\"\\n\ud83d\udea8 SEVERE SKEW DETECTED! Ratio: {skew_ratio:.1f}\")\n        return \"severe\"\n    elif skew_ratio &gt; 3:\n        print(f\"\\n\u26a0\ufe0f  Moderate skew detected. Ratio: {skew_ratio:.1f}\")\n        return \"moderate\"\n    else:\n        print(f\"\\n\u2705 No significant skew. Ratio: {skew_ratio:.1f}\")\n        return \"none\"\n\n# Usage\nskew_level = detect_join_skew(user_events, \"user_id\")\n</code></pre> <p>```python title=\"\u2705 Smart Broadcast Join\" def smart_broadcast_join(large_df, small_df, join_keys):     \"\"\"Intelligently decide on broadcast join to avoid skew\"\"\"</p> <pre><code># Estimate small table size\nsmall_sample = small_df.sample(0.1)\nif small_sample.count() &gt; 0:\n    sample_rows = small_sample.count()\n    total_rows = small_df.count()\n</code></pre>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/High-CardinalityPartitioningDisaster.html","title":"High-Cardinality Partitioning Disaster","text":"<p>Performance Impact</p> <p>Creates millions of tiny files - Partitioning by high-cardinality columns destroys performance.</p>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/High-CardinalityPartitioningDisaster.html#the-problem","title":"The Problem","text":"<p>Partitioning by columns with many unique values (like user_id, transaction_id) creates one partition per unique value, resulting in millions of tiny files that are expensive to list and read.</p> \u274c Problematic Code<pre><code># BAD: Partitioning by high-cardinality column\ndf.write.partitionBy(\"user_id\").parquet(\"output/\")\n# Creates millions of tiny partitions (one per user)\n</code></pre>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/High-CardinalityPartitioningDisaster.html#small-files-problem-impact","title":"Small Files Problem Impact","text":"Metadata OverheadStorage CostsQuery Performance <ul> <li>Slow file listing operations</li> <li>Increased storage system load</li> <li>Poor subsequent read performance</li> </ul> <ul> <li>Minimum block sizes waste space</li> <li>More inodes consumed</li> <li>Backup and replication overhead</li> </ul> <ul> <li>Task per file overhead</li> <li>Poor parallelism</li> <li>Inefficient resource usage</li> </ul>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/High-CardinalityPartitioningDisaster.html#solutions","title":"Solutions","text":"Smart Partitioning StrategyHash-Based PartitioningCardinality AnalysisHybrid PartitioningPartition Validation \u2705 Low-Cardinality Partitioning<pre><code># GOOD: Partition by low-cardinality columns\ndf.write.partitionBy(\"year\", \"month\").parquet(\"output/\")\n\n# For time-based partitioning\nfrom pyspark.sql.functions import date_format\n\ndf_with_partition = df.withColumn(\n    \"year_month\", \n    date_format(col(\"timestamp\"), \"yyyy-MM\")\n)\ndf_with_partition.write.partitionBy(\"year_month\").parquet(\"output/\")\n</code></pre> \u2705 Bucketing High-Cardinality<pre><code># BETTER: Use hash-based partitioning for high-cardinality\nfrom pyspark.sql.functions import hash, col\n\n# Create buckets for high-cardinality column\nnum_buckets = 100  # Adjust based on data size\n\ndf_bucketed = df.withColumn(\n    \"user_bucket\", \n    hash(col(\"user_id\")) % num_buckets\n)\n\ndf_bucketed.write.partitionBy(\"user_bucket\").parquet(\"output/\")\n</code></pre> \u2705 Analyze Before Partitioning<pre><code>def analyze_cardinality(df, columns, sample_fraction=0.1):\n    \"\"\"Analyze cardinality of potential partition columns\"\"\"\n\n    print(\"=== Cardinality Analysis ===\")\n    sample_df = df.sample(sample_fraction)\n    total_rows = df.count()\n    sample_rows = sample_df.count()\n\n    results = {}\n\n    for column in columns:\n        if column in df.columns:\n            distinct_count = sample_df.select(column).distinct().count()\n\n            # Estimate total distinct values\n            estimated_distinct = distinct_count * (total_rows / sample_rows)\n\n            results[column] = {\n                'estimated_distinct': int(estimated_distinct),\n                'cardinality_ratio': estimated_distinct / total_rows\n            }\n\n            # Partitioning recommendation\n            if estimated_distinct &lt; 100:\n                recommendation = \"\u2705 Good for partitioning\"\n            elif estimated_distinct &lt; 1000:\n                recommendation = \"\u26a0\ufe0f  Consider hash bucketing\"\n            else:\n                recommendation = \"\u274c Too high cardinality\"\n\n            print(f\"{column}:\")\n            print(f\"  Estimated distinct: {int(estimated_distinct):,}\")\n            print(f\"  Cardinality ratio: {estimated_distinct/total_rows:.4f}\")\n            print(f\"  Recommendation: {recommendation}\")\n            print()\n\n    return results\n\n# Usage\npartition_analysis = analyze_cardinality(\n    df, \n    columns=[\"user_id\", \"category\", \"date\", \"region\"]\n)\n</code></pre> \u2705 Multi-Level Partitioning Strategy<pre><code>def create_hybrid_partitioning(df, high_card_col, low_card_cols, bucket_count=50):\n    \"\"\"Combine low-cardinality and bucketed high-cardinality partitioning\"\"\"\n\n    # Add hash bucket for high-cardinality column\n    df_bucketed = df.withColumn(\n        f\"{high_card_col}_bucket\",\n        hash(col(high_card_col)) % bucket_count\n    )\n\n    # Combine with low-cardinality columns\n    partition_cols = low_card_cols + [f\"{high_card_col}_bucket\"]\n\n    print(f\"Partitioning by: {partition_cols}\")\n    print(f\"Expected partitions: ~{bucket_count * len(set(df.select(*low_card_cols).collect()))}\")\n\n    return df_bucketed, partition_cols\n\n# Usage\ndf_final, partition_columns = create_hybrid_partitioning(\n    df, \n    high_card_col=\"user_id\", \n    low_card_cols=[\"year\", \"month\"],\n    bucket_count=100\n)\n\ndf_final.write.partitionBy(*partition_columns).parquet(\"output/\")\n</code></pre> \u2705 Validate Partition Strategy<pre><code>def validate_partitioning_strategy(df, partition_cols, max_partitions=1000):\n    \"\"\"Validate that partitioning won't create too many partitions\"\"\"\n\n    # Estimate partition count\n    if isinstance(partition_cols, str):\n        partition_cols = [partition_cols]\n\n    # Sample to estimate combinations\n    sample_df = df.sample(0.1)\n    distinct_combinations = sample_df.select(*partition_cols).distinct().count()\n\n    # Scale up estimate\n    estimated_partitions = distinct_combinations * 10  # Conservative scaling\n\n    print(f\"Estimated partitions: {estimated_partitions}\")\n    print(f\"Maximum recommended: {max_partitions}\")\n\n    if estimated_partitions &gt; max_partitions:\n        print(\"\u274c Too many partitions predicted!\")\n        print(\"Recommendations:\")\n        print(\"  - Reduce cardinality with bucketing\")\n        print(\"  - Use fewer partition columns\")\n        print(\"  - Increase bucket sizes\")\n        return False\n    else:\n        print(\"\u2705 Partition count looks reasonable\")\n        return True\n\n# Validate before writing\nis_valid = validate_partitioning_strategy(df, [\"year\", \"month\", \"user_bucket\"])\nif is_valid:\n    df.write.partitionBy(\"year\", \"month\", \"user_bucket\").parquet(\"output/\")\n</code></pre>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/High-CardinalityPartitioningDisaster.html#key-takeaways","title":"Key Takeaways","text":"<p>Partitioning Guidelines</p> <ul> <li>Ideal cardinality: &lt; 1000 distinct values per column</li> <li>Use hash bucketing for high-cardinality columns</li> <li>Combine strategies - low-cardinality + bucketed high-cardinality</li> <li>Always validate estimated partition count before writing</li> </ul>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/High-CardinalityPartitioningDisaster.html#measuring-impact","title":"Measuring Impact","text":"Partitioning Comparison<pre><code># Bad: 1M user_id partitions \u2192 1M tiny files\n# Good: 12 month \u00d7 100 buckets \u2192 1,200 reasonably-sized files\n# Improvement: 99.9% fewer files, 10x faster reads\n</code></pre> <p>Thoughtful partitioning strategy is essential for maintainable data lakes. The upfront analysis prevents costly rework later.</p>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/LazyCacheEvaluationTrap.html","title":"Lazy Cache Evaluation Trap","text":"<p>Performance Impact</p> <p>Cache never populated - Cache is lazy and won't populate without triggering an action.</p>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/LazyCacheEvaluationTrap.html#the-problem","title":"The Problem","text":"<p>Spark's caching is lazy - calling <code>.cache()</code> doesn't actually cache anything until an action is performed. Without triggering cache population, subsequent operations get no benefit.</p> \u274c Problematic Code<pre><code># BAD: Cache without triggering action\nexpensive_df = df.groupBy(\"category\").agg(count(\"*\"))\nexpensive_df.cache()  # Nothing cached yet!\n\n# Later operations don't benefit from cache\nresult1 = expensive_df.filter(col(\"count\") &gt; 100).collect()  # Computed\nresult2 = expensive_df.filter(col(\"count\") &lt; 50).collect()   # Recomputed!\n</code></pre>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/LazyCacheEvaluationTrap.html#why-cache-fails","title":"Why Cache Fails","text":"Lazy EvaluationCommon Mistake <ul> <li><code>.cache()</code> only marks for caching</li> <li>No computation happens until action</li> <li>First operation computes and caches</li> <li>Only subsequent operations benefit</li> </ul> <ul> <li>Assuming cache is immediately populated</li> <li>Not triggering cache population</li> <li>Missing cache benefits entirely</li> </ul>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/LazyCacheEvaluationTrap.html#solutions","title":"Solutions","text":"Trigger Cache PopulationCache Context ManagerCache VerificationSmart Cache Helper \u2705 Force Cache with Action<pre><code># GOOD: Force cache with action\nexpensive_df = df.groupBy(\"category\").agg(count(\"*\"))\nexpensive_df.cache()\n\n# Trigger cache population\ncache_trigger = expensive_df.count()  # Forces computation and caching\nprint(f\"Cached {cache_trigger} rows\")\n\n# Now subsequent operations use cache\nresult1 = expensive_df.filter(col(\"count\") &gt; 100).collect()  # Uses cache\nresult2 = expensive_df.filter(col(\"count\") &lt; 50).collect()   # Uses cache\n</code></pre> \u2705 Automatic Cache Management<pre><code>from contextlib import contextmanager\n\n@contextmanager\ndef cached_dataframe(df, storage_level=None, trigger_action=True):\n    \"\"\"Context manager for automatic cache management\"\"\"\n\n    if storage_level:\n        df.persist(storage_level)\n    else:\n        df.cache()\n\n    try:\n        if trigger_action:\n            # Trigger cache population\n            row_count = df.count()\n            print(f\"\u2705 Cached DataFrame with {row_count:,} rows\")\n\n        yield df\n\n    finally:\n        df.unpersist()\n        print(\"\ud83e\uddf9 Cache cleaned up\")\n\n# Usage\nexpensive_computation = df.groupBy(\"category\").agg(\n    count(\"*\").alias(\"count\"),\n    avg(\"price\").alias(\"avg_price\")\n)\n\nwith cached_dataframe(expensive_computation) as cached_df:\n    # All operations within this block use cache\n    high_count = cached_df.filter(col(\"count\") &gt; 1000).collect()\n    low_count = cached_df.filter(col(\"count\") &lt; 100).collect()\n    stats = cached_df.describe().collect()\n\n# Cache automatically cleaned up here\n</code></pre> \u2705 Verify Cache Usage<pre><code>def verify_cache_population(df, operation_name=\"operation\"):\n    \"\"\"Verify that cache is actually populated and being used\"\"\"\n\n    # Check if DataFrame is cached\n    if not df.is_cached:\n        print(f\"\u26a0\ufe0f  WARNING: {operation_name} - DataFrame not cached!\")\n        return False\n\n    # Get RDD storage info\n    rdd_id = df.rdd.id()\n    storage_infos = spark.sparkContext._jsc.sc().getRDDStorageInfo()\n\n    for storage_info in storage_infos:\n        if storage_info.id() == rdd_id:\n            memory_size = storage_info.memSize()\n            disk_size = storage_info.diskSize()\n\n            if memory_size &gt; 0 or disk_size &gt; 0:\n                print(f\"\u2705 {operation_name} - Cache populated: \"\n                      f\"{memory_size/(1024**2):.1f}MB memory, \"\n                      f\"{disk_size/(1024**2):.1f}MB disk\")\n                return True\n            else:\n                print(f\"\u26a0\ufe0f  {operation_name} - Cache empty, triggering population...\")\n                df.count()  # Trigger cache\n                return True\n\n    print(f\"\u274c {operation_name} - Cache not found!\")\n    return False\n\n# Usage\nexpensive_df.cache()\nverify_cache_population(expensive_df, \"Expensive Computation\")\n</code></pre> \u2705 Intelligent Cache Helper<pre><code>class SmartCache:\n    \"\"\"Helper class for intelligent caching with verification\"\"\"\n\n    def __init__(self):\n        self.cached_dataframes = {}\n\n    def cache_dataframe(self, df, name, storage_level=None, verify=True):\n        \"\"\"Cache DataFrame with automatic verification\"\"\"\n\n        if storage_level:\n            df.persist(storage_level)\n        else:\n            df.cache()\n\n        # Trigger cache population\n        start_time = time.time()\n        row_count = df.count()\n        cache_time = time.time() - start_time\n\n        # Store metadata\n        self.cached_dataframes[name] = {\n            'dataframe': df,\n            'row_count': row_count,\n            'cache_time': cache_time,\n            'access_count': 0\n        }\n\n        print(f\"\u2705 Cached {name}: {row_count:,} rows in {cache_time:.2f}s\")\n\n        if verify:\n            self._verify_cache_efficiency(df, name)\n\n        return df\n\n    def access_cached_dataframe(self, name):\n        \"\"\"Access cached DataFrame and track usage\"\"\"\n        if name in self.cached_dataframes:\n            self.cached_dataframes[name]['access_count'] += 1\n            return self.cached_dataframes[name]['dataframe']\n        else:\n            print(f\"\u274c DataFrame '{name}' not found in cache\")\n            return None\n\n    def _verify_cache_efficiency(self, df, name):\n        \"\"\"Verify that caching is worthwhile\"\"\"\n\n        # Test cache access speed\n        start_time = time.time()\n        df.count()  # Should be fast if cached\n        access_time = time.time() - start_time\n\n        cache_info = self.cached_dataframes[name]\n        cache_time = cache_info['cache_time']\n\n        # Cache is efficient if access is much faster than initial computation\n        efficiency_ratio = cache_time / access_time if access_time &gt; 0 else float('inf')\n\n        if efficiency_ratio &gt; 5:\n            print(f\"\u2705 Cache efficient for {name}: {efficiency_ratio:.1f}x speedup\")\n        else:\n            print(f\"\u26a0\ufe0f  Cache may not be efficient for {name}: {efficiency_ratio:.1f}x speedup\")\n\n    def get_cache_stats(self):\n        \"\"\"Get statistics about cached DataFrames\"\"\"\n        print(\"=== Cache Statistics ===\")\n\n        for name, info in self.cached_dataframes.items():\n            print(f\"{name}:\")\n            print(f\"  Rows: {info['row_count']:,}\")\n            print(f\"  Cache time: {info['cache_time']:.2f}s\")\n            print(f\"  Access count: {info['access_count']}\")\n            print()\n\n    def cleanup_all(self):\n        \"\"\"Clean up all cached DataFrames\"\"\"\n        for name, info in self.cached_dataframes.items():\n            info['dataframe'].unpersist()\n            print(f\"\ud83e\uddf9 Unpersisted {name}\")\n\n        self.cached_dataframes.clear()\n\n# Usage\ncache_manager = SmartCache()\n\n# Cache expensive computation\nexpensive_df = df.groupBy(\"category\", \"region\").agg(count(\"*\"))\ncached_df = cache_manager.cache_dataframe(expensive_df, \"region_stats\")\n\n# Access cached data multiple times\nresult1 = cache_manager.access_cached_dataframe(\"region_stats\").filter(col(\"count\") &gt; 100)\nresult2 = cache_manager.access_cached_dataframe(\"region_stats\").filter(col(\"count\") &lt; 50)\n\n# Check statistics\ncache_manager.get_cache_stats()\n\n# Cleanup\ncache_manager.cleanup_all()\n</code></pre>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/LazyCacheEvaluationTrap.html#key-takeaways","title":"Key Takeaways","text":"<p>Cache Population Checklist</p> <ul> <li>Always trigger cache with an action (<code>.count()</code>, <code>.collect()</code>, etc.)</li> <li>Verify cache is actually populated with monitoring</li> <li>Use context managers for automatic cleanup</li> <li>Track cache access patterns for optimization</li> </ul>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/LazyCacheEvaluationTrap.html#measuring-impact","title":"Measuring Impact","text":"Cache Usage Verification<pre><code># Without trigger: 0% cache hits, full recomputation every time\n# With trigger: 100% cache hits after first operation\n# Improvement: 5-20x faster subsequent operations\n</code></pre> <p>Lazy evaluation means good intentions aren't enough - you must actively trigger cache population to get performance benefits.</p>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/Over-CachingMemoryWaste.html","title":"Over-Caching Memory Waste","text":"<p>Performance Impact</p> <p>Memory exhaustion, slower jobs - Caching DataFrames used only once wastes precious executor memory.</p>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/Over-CachingMemoryWaste.html#the-problem","title":"The Problem","text":"<p>Caching every DataFrame \"just in case\" consumes executor memory that could be used for actual computation. This leads to memory pressure, spilling, and degraded performance.</p> \u274c Problematic Code<pre><code># BAD: Cache everything approach\ndf1 = spark.read.parquet(\"data1.parquet\").cache()  # Used once\ndf2 = spark.read.parquet(\"data2.parquet\").cache()  # Used once  \ndf3 = spark.read.parquet(\"data3.parquet\").cache()  # Used once\n\nresult = df1.join(df2, \"key\").join(df3, \"key\")  # Memory wasted!\n</code></pre>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/Over-CachingMemoryWaste.html#over-caching-consequences","title":"Over-Caching Consequences","text":"Memory IssuesPerformance Impact <ul> <li>Executor memory exhaustion</li> <li>Increased GC pressure</li> <li>Spilling to disk (defeats cache purpose)</li> </ul> <ul> <li>Reduced performance for actually reused data</li> <li>Slower job execution</li> <li>Resource contention</li> </ul>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/Over-CachingMemoryWaste.html#solutions","title":"Solutions","text":"Strategic CachingSmart Caching DecisionCache ManagementCache Monitoring \u2705 Cache Only Reused DataFrames<pre><code># GOOD: Cache only reused DataFrames\nexpensive_df = df.groupBy(\"category\").agg(\n    count(\"*\").alias(\"count\"),\n    avg(\"price\").alias(\"avg_price\"),\n    sum(\"revenue\").alias(\"total_revenue\")\n)\n\n# This will be reused multiple times\nexpensive_df.cache()\n\n# Multiple operations using cached data\nhigh_volume = expensive_df.filter(col(\"count\") &gt; 1000)\nlow_volume = expensive_df.filter(col(\"count\") &lt; 100)\nmid_range = expensive_df.filter(\n    (col(\"count\") &gt;= 100) &amp; (col(\"count\") &lt;= 1000)\n)\n\n# Clean up when done\nexpensive_df.unpersist()\n</code></pre> \u2705 Intelligent Caching Logic<pre><code>def should_cache(df, usage_count, computation_cost=\"medium\"):\n    \"\"\"Decide whether to cache based on usage patterns\"\"\"\n\n    cost_weights = {\n        \"low\": 1,      # Simple transformations\n        \"medium\": 3,   # Joins, groupBy\n        \"high\": 10     # Complex aggregations, multiple joins\n    }\n\n    weight = cost_weights.get(computation_cost, 3)\n    cache_benefit_score = usage_count * weight\n\n    # Memory consideration\n    partition_count = df.rdd.getNumPartitions()\n    memory_concern = partition_count &gt; 1000\n\n    return {\n        \"should_cache\": cache_benefit_score &gt;= 6 and not memory_concern,\n        \"score\": cache_benefit_score,\n        \"memory_concern\": memory_concern\n    }\n\n# Usage example\nexpensive_computation = df.groupBy(\"category\", \"region\").agg(\n    countDistinct(\"user_id\"),\n    avg(\"amount\")\n)\n\ncache_decision = should_cache(\n    expensive_computation, \n    usage_count=3, \n    computation_cost=\"high\"\n)\n\nif cache_decision[\"should_cache\"]:\n    expensive_computation.cache()\n    print(f\"\u2705 Caching (score: {cache_decision['score']})\")\nelse:\n    print(f\"\u274c Not caching (score: {cache_decision['score']})\")\n</code></pre> \u2705 Automatic Cache Cleanup<pre><code>from contextlib import contextmanager\n\n@contextmanager\ndef managed_cache(df, storage_level=None):\n    \"\"\"Context manager for automatic cache cleanup\"\"\"\n    if storage_level:\n        df.persist(storage_level)\n    else:\n        df.cache()\n\n    try:\n        df.count()  # Trigger cache\n        yield df\n    finally:\n        df.unpersist()\n\n# Usage - automatic cleanup\nexpensive_df = df.groupBy(\"category\").agg(count(\"*\"))\n\nwith managed_cache(expensive_df) as cached_df:\n    result1 = cached_df.filter(col(\"count\") &gt; 100).collect()\n    result2 = cached_df.filter(col(\"count\") &lt; 50).collect()\n    # Automatic cleanup when exiting context\n</code></pre> \u2705 Monitor Cache Usage<pre><code>def monitor_cache_usage():\n    \"\"\"Monitor current cache usage across cluster\"\"\"\n    print(\"=== Cache Usage Report ===\")\n\n    storage_infos = spark.sparkContext._jsc.sc().getRDDStorageInfo()\n\n    total_memory_used = 0\n    total_disk_used = 0\n\n    for storage_info in storage_infos:\n        rdd_id = storage_info.id()\n        memory_size_mb = storage_info.memSize() / (1024 * 1024)\n        disk_size_mb = storage_info.diskSize() / (1024 * 1024)\n\n        total_memory_used += memory_size_mb\n        total_disk_used += disk_size_mb\n\n        print(f\"RDD {rdd_id}: {memory_size_mb:.1f}MB memory, {disk_size_mb:.1f}MB disk\")\n\n    print(f\"Total: {total_memory_used:.1f}MB memory, {total_disk_used:.1f}MB disk\")\n\n    # Get executor memory info\n    executors = spark.sparkContext.statusTracker().getExecutorInfos()\n    total_executor_memory = sum(exec.maxMemory for exec in executors)\n    cache_ratio = (total_memory_used * 1024 * 1024) / total_executor_memory\n\n    print(f\"Cache memory ratio: {cache_ratio:.1%}\")\n\n    if cache_ratio &gt; 0.8:\n        print(\"\u26a0\ufe0f  High cache memory usage!\")\n\n# Monitor periodically\nmonitor_cache_usage()\n</code></pre>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/Over-CachingMemoryWaste.html#key-takeaways","title":"Key Takeaways","text":"<p>Caching Best Practices</p> <ul> <li>Cache when: DataFrame used 2+ times, expensive computation, memory available</li> <li>Don't cache: One-time use, simple transformations, memory constrained</li> <li>Monitor usage regularly to prevent memory issues</li> <li>Clean up with <code>.unpersist()</code> when done</li> </ul>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/Over-CachingMemoryWaste.html#measuring-impact","title":"Measuring Impact","text":"Cache Optimization Results<pre><code># Before: 5 cached DataFrames (3 unused) \u2192 60% memory waste\n# After:  2 strategically cached DataFrames \u2192 optimal memory usage\n# Improvement: 3x better memory efficiency, faster execution\n</code></pre> <p>Strategic caching is about quality over quantity. Cache the right DataFrames at the right time, not everything \"just in case\".</p>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/SchemaInferenceDouble-ReadPenalty.html","title":"Schema Inference Double-Read Penalty","text":"<p>Performance Impact</p> <p>2x slower, 2x I/O cost - Schema inference requires reading the entire dataset twice.</p>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/SchemaInferenceDouble-ReadPenalty.html#the-problem","title":"The Problem","text":"<p>When you enable schema inference, Spark must scan the entire dataset to determine column types, then read it again to actually process the data.</p> \u274c Problematic Code<pre><code># BAD: Schema inference on large datasets\ndf = spark.read.csv(\"huge_dataset.csv\", header=True, inferSchema=True)\n# Spark reads the entire file twice!\n</code></pre>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/SchemaInferenceDouble-ReadPenalty.html#what-happens-under-the-hood","title":"What Happens Under the Hood","text":"First ReadSecond ReadCost Impact <p>Spark scans entire dataset to infer schema</p> <p>Spark reads dataset again with inferred schema</p> <ul> <li>Double I/O operations</li> <li>Double processing time  </li> <li>Double cloud storage charges</li> </ul>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/SchemaInferenceDouble-ReadPenalty.html#solutions","title":"Solutions","text":"Predefined SchemaSchema GenerationSmart Schema Caching \u2705 Define Schema Upfront<pre><code>from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n\n# Define schema once\nschema = StructType([\n    StructField(\"user_id\", StringType(), True),\n    StructField(\"event_time\", TimestampType(), True),\n    StructField(\"event_count\", IntegerType(), True)\n])\n\n# Single read with known schema\ndf = spark.read.csv(\"huge_dataset.csv\", header=True, schema=schema)\n</code></pre> \u2705 Generate from Sample<pre><code>def generate_schema_from_sample(file_path, sample_size=1000):\n    \"\"\"Generate schema from small sample\"\"\"\n    sample_df = spark.read.csv(file_path, header=True, inferSchema=True).limit(sample_size)\n\n    print(\"Generated Schema:\")\n    print(\"schema = StructType([\")\n    for field in sample_df.schema.fields:\n        print(f'    StructField(\"{field.name}\", {field.dataType}, {field.nullable}),')\n    print(\"])\")\n\n    return sample_df.schema\n\n# Use for large datasets\nschema = generate_schema_from_sample(\"huge_dataset.csv\")\ndf = spark.read.csv(\"huge_dataset.csv\", header=True, schema=schema)\n</code></pre> \u2705 Schema Persistence<pre><code>import json\n\ndef save_schema(schema, file_path):\n    \"\"\"Save schema for reuse\"\"\"\n    schema_json = schema.json()\n    with open(f\"{file_path}.schema\", \"w\") as f:\n        f.write(schema_json)\n\ndef load_schema(file_path):\n    \"\"\"Load saved schema\"\"\"\n    from pyspark.sql.types import StructType\n    try:\n        with open(f\"{file_path}.schema\", \"r\") as f:\n            return StructType.fromJson(json.loads(f.read()))\n    except FileNotFoundError:\n        return None\n\n# Usage\nschema = load_schema(\"dataset.csv\")\nif schema is None:\n    # Generate and save schema once\n    schema = generate_schema_from_sample(\"dataset.csv\")\n    save_schema(schema, \"dataset.csv\")\n\ndf = spark.read.csv(\"dataset.csv\", header=True, schema=schema)\n</code></pre>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/SchemaInferenceDouble-ReadPenalty.html#key-takeaways","title":"Key Takeaways","text":"<p>Schema Best Practices</p> <ul> <li>Always define schemas for production workloads</li> <li>Generate from samples for unknown data</li> <li>Cache schemas for repeated use</li> <li>Document schema changes in version control</li> </ul>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/SchemaInferenceDouble-ReadPenalty.html#measuring-impact","title":"Measuring Impact","text":"Performance Comparison<pre><code># Before: 2 full dataset scans\n# After:  1 dataset scan\n# Improvement: 50% faster, 50% less I/O cost \ud83d\udcb0\n</code></pre> <p>Schema inference is convenient for exploration but costly for production. Taking a few minutes to define schemas can save hours of processing time.</p>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/SmallFilesPerformanceKiller.html","title":"The Small Files Performance Killer","text":"<p>Performance Impact</p> <p>10-50x slower execution - Reading thousands of small files creates excessive task overhead.</p>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/SmallFilesPerformanceKiller.html#the-problem","title":"The Problem","text":"<p>When Spark reads many small files, it creates one task per file. This leads to massive scheduling overhead where executors spend more time coordinating than processing data.</p> \u274c Problematic Code<pre><code># BAD: Reading 10,000 small JSON files\ndf = spark.read.json(\"s3://bucket/small-files/*.json\")\n# Creates 10,000 tasks with massive overhead\nresult = df.filter(col(\"date\") &gt; \"2023-01-01\").collect()\n</code></pre>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/SmallFilesPerformanceKiller.html#why-this-happens","title":"Why This Happens","text":"Task OverheadResource Waste <ul> <li>Each file becomes a separate task</li> <li>Task scheduling overhead dominates processing</li> <li>Network latency multiplied by file count</li> </ul> <ul> <li>Executors underutilized</li> <li>More coordination than computation</li> <li>Poor cluster efficiency</li> </ul>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/SmallFilesPerformanceKiller.html#solutions","title":"Solutions","text":"Quick FixBetter ApproachBest Practice \u2705 Coalesce After Reading<pre><code># Combine small files into larger partitions\ndf = spark.read.json(\"s3://bucket/small-files/*.json\").coalesce(100)\n</code></pre> \u2705 Whole Text Files<pre><code># For very small files, use wholeTextFiles\nrdd = spark.sparkContext.wholeTextFiles(\"s3://bucket/small-files/*.json\")\ndf = spark.read.json(rdd.values())\n</code></pre> \u2705 File Optimization Utility<pre><code>def optimize_small_files(input_path, output_path, target_size_mb=128):\n    \"\"\"Combine small files into optimally-sized partitions\"\"\"\n    df = spark.read.json(input_path)\n\n    # Calculate optimal partitions\n    total_size_mb = estimate_dataframe_size_mb(df)\n    optimal_partitions = max(1, total_size_mb // target_size_mb)\n\n    df.coalesce(optimal_partitions) \\\n      .write \\\n      .mode(\"overwrite\") \\\n      .parquet(output_path)\n\n    print(f\"Optimized: {optimal_partitions} partitions, ~{target_size_mb}MB each\")\n\n# Usage\noptimize_small_files(\"s3://bucket/small-files/\", \"s3://bucket/optimized/\")\n</code></pre>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/SmallFilesPerformanceKiller.html#key-takeaways","title":"Key Takeaways","text":"<p>Performance Rules</p> <ul> <li>Target: 100-200MB per partition</li> <li>Method: Use <code>.coalesce()</code> after reading</li> <li>Alternative: <code>wholeTextFiles()</code> for very small files</li> <li>Prevention: Optimize during ingestion</li> </ul>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/SmallFilesPerformanceKiller.html#measuring-impact","title":"Measuring Impact","text":"Performance Comparison<pre><code># Before: 10,000 tasks, 45 minutes\n# After:  100 tasks, 3 minutes  \n# Improvement: 15x faster \u26a1\n</code></pre> <p>The small files problem is one of the most common performance killers in Spark. A simple coalesce operation can provide dramatic speedups with minimal code changes.</p>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/SuboptimalFileFormatChoices.html","title":"Suboptimal File Format Choices","text":"<p>Performance Impact</p> <p>5-20x slower queries - Using row-based formats for analytical workloads instead of columnar formats.</p>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/SuboptimalFileFormatChoices.html#the-problem","title":"The Problem","text":"<p>Using CSV or JSON for large analytical datasets forces Spark to read entire rows even when you only need specific columns, and provides no compression or optimization benefits.</p> \u274c Problematic Code<pre><code># BAD: Large analytical datasets in row-based formats\ndf = spark.read.csv(\"10TB_dataset.csv\")\n# No compression, no predicate pushdown, reads entire rows\nresult = df.select(\"revenue\").filter(col(\"date\") &gt; \"2023-01-01\").sum()\n</code></pre>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/SuboptimalFileFormatChoices.html#format-comparison","title":"Format Comparison","text":"Format Compression Predicate Pushdown Schema Evolution ACID Best For CSV \u274c \u274c \u274c \u274c Small datasets, exports JSON \u274c \u274c \u2705 \u274c Semi-structured data Parquet \u2705 \u2705 \u2705 \u274c Analytics, data lakes Delta \u2705 \u2705 \u2705 \u2705 Production data lakes"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/SuboptimalFileFormatChoices.html#solutions","title":"Solutions","text":"Parquet for AnalyticsDelta for ProductionMigration StrategyFormat Selection Guide \u2705 Columnar Format<pre><code># GOOD: Columnar format with compression\ndf = spark.read.parquet(\"10TB_dataset.parquet\")\n\n# Benefits:\n# - 70-90% compression ratio\n# - Column pruning (only read needed columns)\n# - Predicate pushdown (filter at file level)\n# - Fast aggregations\n</code></pre> \u2705 ACID Transactions<pre><code># BETTER: Delta Lake for production\ndf = spark.read.format(\"delta\").load(\"delta-table/\")\n\n# Additional benefits:\n# - Time travel (access historical versions)\n# - ACID transactions (concurrent reads/writes)\n# - Schema enforcement and evolution\n# - Automatic file optimization\n</code></pre> \u2705 Format Migration Utility<pre><code>def migrate_to_optimized_format(source_path, target_path, format_type=\"delta\"):\n    \"\"\"Migrate data to optimized format with partitioning\"\"\"\n\n    # Read source data (with schema to avoid inference)\n    df = spark.read.option(\"inferSchema\", \"false\").csv(source_path, header=True)\n\n    # Add partitioning columns if date column exists\n    if \"date\" in df.columns:\n        from pyspark.sql.functions import year, month\n        df = df.withColumn(\"year\", year(col(\"date\"))) \\\n               .withColumn(\"month\", month(col(\"date\")))\n        partition_cols = [\"year\", \"month\"]\n    else:\n        partition_cols = None\n\n    # Write in optimized format\n    writer = df.write.mode(\"overwrite\")\n\n    if partition_cols:\n        writer = writer.partitionBy(*partition_cols)\n\n    if format_type == \"delta\":\n        writer.format(\"delta\").save(target_path)\n    else:\n        writer.parquet(target_path)\n\n    print(f\"Migration complete: {format_type} format at {target_path}\")\n\n    # Report compression ratio\n    original_size = get_directory_size(source_path)\n    new_size = get_directory_size(target_path)\n    compression_ratio = original_size / new_size\n    print(f\"Compression ratio: {compression_ratio:.1f}x smaller\")\n\n# Usage\nmigrate_to_optimized_format(\n    source_path=\"s3://bucket/csv-data/\",\n    target_path=\"s3://bucket/delta-data/\",\n    format_type=\"delta\"\n)\n</code></pre> \u2705 Choose Right Format<pre><code>def recommend_file_format(use_case, data_size_gb, update_frequency):\n    \"\"\"Recommend optimal file format based on requirements\"\"\"\n\n    if use_case == \"analytics\" and data_size_gb &gt; 1:\n        if update_frequency == \"frequent\":\n            return \"delta\"  # ACID transactions needed\n        else:\n            return \"parquet\"  # Read-optimized\n\n    elif use_case == \"streaming\":\n        return \"delta\"  # Schema evolution, ACID\n\n    elif use_case == \"ml_training\":\n        return \"parquet\"  # Fast column access\n\n    elif data_size_gb &lt; 0.1:\n        return \"csv\"  # Small data, simplicity matters\n\n    else:\n        return \"parquet\"  # Safe default for most cases\n\n# Usage\nformat_choice = recommend_file_format(\n    use_case=\"analytics\",\n    data_size_gb=100,\n    update_frequency=\"daily\"\n)\nprint(f\"Recommended format: {format_choice}\")\n</code></pre>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/SuboptimalFileFormatChoices.html#key-takeaways","title":"Key Takeaways","text":"<p>Format Selection Rules</p> <ul> <li>Analytics workloads: Use Parquet or Delta</li> <li>Production systems: Use Delta for ACID guarantees</li> <li>Small datasets (&lt; 100MB): CSV is acceptable</li> <li>Schema evolution needs: Avoid CSV</li> </ul>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/SuboptimalFileFormatChoices.html#measuring-impact","title":"Measuring Impact","text":"Storage &amp; Performance Comparison<pre><code># CSV (1TB) \u2192 Parquet (200GB) \u2192 Delta (180GB)\n# Query Speed: CSV baseline \u2192 Parquet 10x faster \u2192 Delta 12x faster\n# Storage Cost: CSV baseline \u2192 Parquet 80% savings \u2192 Delta 82% savings\n</code></pre> <p>File format choice is one of the highest-impact optimizations you can make. The migration effort pays for itself within days through improved performance and reduced storage costs.</p>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/TheGoldilocksPartitionProblem.html","title":"The Goldilocks Partition Problem","text":"<p>Performance Impact</p> <p>5-100x slower - Partitions that are too small create overhead; too large cause memory issues.</p>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/TheGoldilocksPartitionProblem.html#the-problem","title":"The Problem","text":"<p>Spark performance is highly sensitive to partition size. Too small creates scheduling overhead, too large causes memory pressure. Finding the \"just right\" size is critical.</p> \u274c Problematic Code<pre><code># BAD: Ignoring partition sizes\ndf = spark.read.parquet(\"data/\")\nprint(f\"Partitions: {df.rdd.getNumPartitions()}\")  # Could be 1 or 10,000!\nresult = df.groupBy(\"category\").count().collect()\n</code></pre>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/TheGoldilocksPartitionProblem.html#partition-size-impact","title":"Partition Size Impact","text":"Too Small (&lt; 10MB)Too Large (&gt; 1GB)Just Right (100-200MB) <ul> <li>High task scheduling overhead</li> <li>Underutilized executors  </li> <li>Inefficient network usage</li> <li>More coordination than computation</li> </ul> <ul> <li>Memory pressure and spilling</li> <li>GC overhead</li> <li>Risk of OOM errors</li> <li>Poor parallelism</li> </ul> <ul> <li>Optimal resource utilization</li> <li>Balanced parallelism</li> <li>Efficient processing</li> <li>Good memory usage</li> </ul>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/TheGoldilocksPartitionProblem.html#solutions","title":"Solutions","text":"Partition AnalysisOptimal PartitioningAdvanced Partitioning \u2705 Analyze Current Partitions<pre><code>def analyze_partitions(df, df_name=\"DataFrame\"):\n    \"\"\"Comprehensive partition analysis\"\"\"\n    print(f\"=== {df_name} Partition Analysis ===\")\n\n    num_partitions = df.rdd.getNumPartitions()\n    print(f\"Number of partitions: {num_partitions}\")\n\n    # Sample partition sizes\n    partition_counts = df.rdd.mapPartitions(lambda x: [sum(1 for _ in x)]).collect()\n\n    if partition_counts:\n        min_size = min(partition_counts)\n        max_size = max(partition_counts)\n        avg_size = sum(partition_counts) / len(partition_counts)\n\n        print(f\"Partition sizes:\")\n        print(f\"  Min: {min_size:,} rows\")\n        print(f\"  Max: {max_size:,} rows\") \n        print(f\"  Avg: {avg_size:,.0f} rows\")\n\n        # Skew detection\n        skew_ratio = max_size / avg_size if avg_size &gt; 0 else 0\n        if skew_ratio &gt; 3:\n            print(f\"\u26a0\ufe0f  High skew: Max is {skew_ratio:.1f}x larger than average\")\n\n        # Size recommendations\n        estimated_mb_per_partition = avg_size * 0.001  # Rough estimate\n        print(f\"Estimated avg size: ~{estimated_mb_per_partition:.1f} MB\")\n\n        if estimated_mb_per_partition &lt; 50:\n            print(\"\ud83d\udca1 Consider reducing partitions (coalesce)\")\n        elif estimated_mb_per_partition &gt; 300:\n            print(\"\ud83d\udca1 Consider increasing partitions (repartition)\")\n        else:\n            print(\"\u2705 Partition sizes look good!\")\n\n    return df\n\n# Usage\ndf = analyze_partitions(df, \"Sales Data\")\n</code></pre> \u2705 Calculate Optimal Partitions<pre><code>def optimize_partitions(df, target_partition_size_mb=128):\n    \"\"\"Calculate and apply optimal partitioning\"\"\"\n\n    # Estimate total size\n    sample_count = df.sample(0.01).count()\n    if sample_count == 0:\n        return df.coalesce(1)\n\n    total_count = df.count()\n    sample_data = df.sample(0.01).take(min(100, sample_count))\n\n    if sample_data:\n        # Estimate row size\n        avg_row_size_bytes = sum(len(str(row)) for row in sample_data) / len(sample_data) * 2\n        total_size_mb = (total_count * avg_row_size_bytes) / (1024 * 1024)\n\n        optimal_partitions = max(1, int(total_size_mb / target_partition_size_mb))\n        optimal_partitions = min(optimal_partitions, 4000)  # Cap at reasonable max\n\n        print(f\"Dataset size: {total_size_mb:.1f} MB\")\n        print(f\"Target partition size: {target_partition_size_mb} MB\")\n        print(f\"Optimal partitions: {optimal_partitions}\")\n\n        current_partitions = df.rdd.getNumPartitions()\n\n        if optimal_partitions &lt; current_partitions:\n            print(\"Applying coalesce...\")\n            return df.coalesce(optimal_partitions)\n        elif optimal_partitions &gt; current_partitions * 1.5:\n            print(\"Applying repartition...\")\n            return df.repartition(optimal_partitions)\n        else:\n            print(\"Current partitioning is acceptable\")\n            return df\n\n    return df\n\n# Apply optimization\ndf_optimized = optimize_partitions(df)\n</code></pre> \u2705 Intelligent Partitioning Strategy<pre><code>def intelligent_partition(df, operation_type=\"general\"):\n    \"\"\"Apply partitioning strategy based on operation type\"\"\"\n\n    current_partitions = df.rdd.getNumPartitions()\n\n    # Estimate data characteristics\n    sample_df = df.sample(0.01)\n    estimated_rows = df.count()\n\n    if operation_type == \"join\":\n        # Joins benefit from more partitions for parallelism\n        target_rows_per_partition = 50000\n        optimal_partitions = max(200, estimated_rows // target_rows_per_partition)\n\n    elif operation_type == \"aggregation\":\n        # Aggregations need fewer, larger partitions to reduce shuffle\n        target_rows_per_partition = 200000\n        optimal_partitions = max(50, estimated_rows // target_rows_per_partition)\n\n    elif operation_type == \"io\":\n        # I/O operations benefit from larger partitions\n        target_rows_per_partition = 500000\n        optimal_partitions = max(20, estimated_rows // target_rows_per_partition)\n\n    else:  # general\n        target_rows_per_partition = 100000\n        optimal_partitions = max(100, estimated_rows // target_rows_per_partition)\n\n    optimal_partitions = min(optimal_partitions, 2000)  # Reasonable upper bound\n\n    print(f\"Operation: {operation_type}\")\n    print(f\"Current partitions: {current_partitions}\")\n    print(f\"Recommended partitions: {optimal_partitions}\")\n\n    if abs(optimal_partitions - current_partitions) / current_partitions &gt; 0.3:\n        if optimal_partitions &lt; current_partitions:\n            return df.coalesce(optimal_partitions)\n        else:\n            return df.repartition(optimal_partitions)\n\n    return df\n\n# Usage for different operations\ndf_for_joins = intelligent_partition(df, \"join\")\ndf_for_agg = intelligent_partition(df, \"aggregation\")\n</code></pre>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/TheGoldilocksPartitionProblem.html#key-takeaways","title":"Key Takeaways","text":"<p>Partitioning Guidelines</p> <ul> <li>Target size: 100-200MB per partition</li> <li>Row count: 50K-500K rows per partition</li> <li>Use coalesce when reducing partitions (no shuffle)</li> <li>Use repartition when increasing partitions (with shuffle)</li> <li>Monitor skew - max should be &lt; 3x average</li> </ul>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/TheGoldilocksPartitionProblem.html#measuring-impact","title":"Measuring Impact","text":"Optimization Results<pre><code># Before: 10,000 partitions (1MB each) \u2192 20x task overhead\n# After:  100 partitions (100MB each) \u2192 optimal performance\n# Improvement: 20x fewer tasks, 80% less overhead\n</code></pre> <p>Proper partitioning is the foundation of good Spark performance. It's worth spending time to get this right as it affects every subsequent operation.</p>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/WrongStorageLevelChoices.html","title":"Wrong Storage Level Choices","text":"<p>Performance Impact</p> <p>Cache eviction, memory pressure - Using inappropriate storage levels causes cache thrashing.</p>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/WrongStorageLevelChoices.html#the-problem","title":"The Problem","text":"<p>Using the default MEMORY_ONLY storage level when data doesn't fit in memory leads to constant cache eviction and poor performance. Different use cases need different storage strategies.</p> \u274c Problematic Code<pre><code># BAD: Default MEMORY_ONLY when data doesn't fit\nlarge_df.cache()  # Uses MEMORY_ONLY, causes eviction cascades\n</code></pre>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/WrongStorageLevelChoices.html#storage-level-comparison","title":"Storage Level Comparison","text":"Storage Level Memory Disk Serialized Replicated Best For MEMORY_ONLY \u2705 \u274c \u274c \u274c Small datasets, fast access MEMORY_AND_DISK \u2705 \u2705 \u274c \u274c Medium datasets MEMORY_ONLY_SER \u2705 \u274c \u2705 \u274c Memory-constrained MEMORY_AND_DISK_SER \u2705 \u2705 \u2705 \u274c Large datasets DISK_ONLY \u274c \u2705 \u2705 \u274c Rarely accessed"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/WrongStorageLevelChoices.html#solutions","title":"Solutions","text":"Choose Appropriate StorageIntelligent Storage SelectionStorage Performance TestingDynamic Storage Adjustment \u2705 Match Storage to Use Case<pre><code>from pyspark import StorageLevel\n\n# For large datasets that might not fit in memory\nlarge_df.persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n# For critical data that needs high availability\ncritical_df.persist(StorageLevel.MEMORY_ONLY_2)  # Replicated\n\n# For infrequently accessed but expensive to compute\narchive_df.persist(StorageLevel.DISK_ONLY)\n\n# For iterative algorithms with memory constraints\nml_features.persist(StorageLevel.MEMORY_AND_DISK_SER_2)\n</code></pre> \u2705 Auto-Select Storage Level<pre><code>def select_optimal_storage_level(df, access_pattern=\"frequent\", memory_available_gb=8):\n    \"\"\"Select optimal storage level based on usage pattern\"\"\"\n\n    # Estimate DataFrame size\n    sample_count = df.sample(0.01).count()\n    if sample_count == 0:\n        return StorageLevel.MEMORY_ONLY\n\n    total_count = df.count()\n    estimated_size_gb = (total_count / sample_count) * 0.1  # Rough estimate\n\n    # Storage level decision matrix\n    if access_pattern == \"frequent\":\n        if estimated_size_gb &lt; memory_available_gb * 0.3:\n            return StorageLevel.MEMORY_ONLY\n        elif estimated_size_gb &lt; memory_available_gb * 0.6:\n            return StorageLevel.MEMORY_ONLY_SER\n        else:\n            return StorageLevel.MEMORY_AND_DISK_SER\n\n    elif access_pattern == \"occasional\":\n        if estimated_size_gb &lt; memory_available_gb * 0.2:\n            return StorageLevel.MEMORY_ONLY\n        else:\n            return StorageLevel.MEMORY_AND_DISK_SER\n\n    elif access_pattern == \"rare\":\n        return StorageLevel.DISK_ONLY\n\n    elif access_pattern == \"critical\":\n        if estimated_size_gb &lt; memory_available_gb * 0.4:\n            return StorageLevel.MEMORY_ONLY_2  # Replicated\n        else:\n            return StorageLevel.MEMORY_AND_DISK_SER_2\n\n    return StorageLevel.MEMORY_AND_DISK_SER  # Safe default\n\n# Usage\nstorage_level = select_optimal_storage_level(\n    df=expensive_computation,\n    access_pattern=\"frequent\", \n    memory_available_gb=16\n)\n\nexpensive_computation.persist(storage_level)\nprint(f\"Using storage level: {storage_level}\")\n</code></pre> \u2705 Test Storage Performance<pre><code>def compare_storage_performance(df, test_operations=3):\n    \"\"\"Compare performance across different storage levels\"\"\"\n\n    import time\n\n    storage_levels = {\n        \"MEMORY_ONLY\": StorageLevel.MEMORY_ONLY,\n        \"MEMORY_ONLY_SER\": StorageLevel.MEMORY_ONLY_SER,\n        \"MEMORY_AND_DISK\": StorageLevel.MEMORY_AND_DISK,\n        \"MEMORY_AND_DISK_SER\": StorageLevel.MEMORY_AND_DISK_SER\n    }\n\n    results = {}\n    test_df = df.limit(10000)  # Use smaller dataset for testing\n\n    for name, level in storage_levels.items():\n        print(f\"Testing {name}...\")\n\n        # Cache with this storage level\n        test_df.persist(level)\n\n        # Trigger cache population\n        start_time = time.time()\n        cache_count = test_df.count()\n        cache_time = time.time() - start_time\n\n        # Test access performance\n        access_times = []\n        for i in range(test_operations):\n            start_time = time.time()\n            test_df.filter(col(\"amount\") &gt; 100).count()\n            access_times.append(time.time() - start_time)\n\n        avg_access_time = sum(access_times) / len(access_times)\n\n        results[name] = {\n            \"cache_time\": cache_time,\n            \"avg_access_time\": avg_access_time,\n            \"total_time\": cache_time + avg_access_time * test_operations\n        }\n\n        # Clean up\n        test_df.unpersist()\n\n        print(f\"  Cache time: {cache_time:.2f}s\")\n        print(f\"  Avg access: {avg_access_time:.2f}s\")\n\n    # Report best option\n    best_option = min(results.keys(), key=lambda x: results[x][\"total_time\"])\n    print(f\"\\n\u2705 Best performing: {best_option}\")\n\n    return results\n\n# Usage\nperformance_results = compare_storage_performance(df)\n</code></pre> \u2705 Adaptive Storage Management<pre><code>class AdaptiveStorageManager:\n    \"\"\"Dynamically adjust storage levels based on memory pressure\"\"\"\n\n    def __init__(self, spark_session):\n        self.spark = spark_session\n        self.cached_dataframes = {}\n\n    def cache_with_monitoring(self, df, name, initial_level=StorageLevel.MEMORY_AND_DISK_SER):\n        \"\"\"Cache DataFrame with monitoring and potential adjustment\"\"\"\n\n        df.persist(initial_level)\n        df.count()  # Trigger cache\n\n        self.cached_dataframes[name] = {\n            'dataframe': df,\n            'storage_level': initial_level,\n            'access_count': 0\n        }\n\n        # Check memory pressure after caching\n        if self._is_memory_pressure():\n            print(f\"Memory pressure detected, adjusting {name}\")\n            self._adjust_storage_levels()\n\n    def access_dataframe(self, name):\n        \"\"\"Access cached DataFrame and track usage\"\"\"\n        if name in self.cached_dataframes:\n            self.cached_dataframes[name]['access_count'] += 1\n            return self.cached_dataframes[name]['dataframe']\n        return None\n\n    def _is_memory_pressure(self):\n        \"\"\"Check if cluster is under memory pressure\"\"\"\n        executors = self.spark.sparkContext.statusTracker().getExecutorInfos()\n\n        for executor in executors:\n            memory_usage = executor.memoryUsed / executor.maxMemory\n            if memory_usage &gt; 0.8:  # 80% threshold\n                return True\n        return False\n\n    def _adjust_storage_levels(self):\n        \"\"\"Adjust storage levels based on access patterns\"\"\"\n\n        for name, info in self.cached_dataframes.items():\n            df = info['dataframe']\n            access_count = info['access_count']\n            current_level = info['storage_level']\n\n            # Frequently accessed: keep in memory\n            if access_count &gt; 5:\n                continue\n\n            # Rarely accessed: move to disk\n            elif access_count &lt; 2:\n                df.unpersist()\n                df.persist(StorageLevel.DISK_ONLY)\n                info['storage_level'] = StorageLevel.DISK_ONLY\n                print(f\"Moved {name} to DISK_ONLY\")\n\n    def cleanup_all(self):\n        \"\"\"Clean up all cached DataFrames\"\"\"\n        for name, info in self.cached_dataframes.items():\n            info['dataframe'].unpersist()\n        self.cached_dataframes.clear()\n\n# Usage\nstorage_manager = AdaptiveStorageManager(spark)\nstorage_manager.cache_with_monitoring(expensive_df, \"aggregated_data\")\n\n# Access DataFrame\nresult = storage_manager.access_dataframe(\"aggregated_data\")\n</code></pre>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/WrongStorageLevelChoices.html#key-takeaways","title":"Key Takeaways","text":"<p>Storage Level Guidelines</p> <ul> <li>Data size vs available memory determines primary choice</li> <li>Access frequency - frequent access favors memory</li> <li>Fault tolerance needs - critical data should be replicated</li> <li>Cost sensitivity - disk is cheaper than memory</li> </ul>"},{"location":"Spark-DataBricks/1.1_PySparkGotchas/WrongStorageLevelChoices.html#measuring-impact","title":"Measuring Impact","text":"Storage Optimization Results<pre><code># MEMORY_ONLY (large dataset): Constant eviction, 5x slower\n# MEMORY_AND_DISK_SER: Stable performance, 2x compression\n# Improvement: Consistent performance, better resource utilization\n</code></pre> <p>The right storage level prevents cache thrashing and ensures predictable performance. Choose based on data characteristics and access patterns, not defaults.</p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html","title":"Project Sparkzure Part1 - Connecting Local Spark to Azure Data Lake","text":""},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#overview","title":"Overview","text":"<p>Azure Databricks to Azure Data Lake is easy and straightforward. All the requied jars pre-installed in Databricks. All you need to do is to create a session and connect. However, connecting a local Spark instance to Azure Data Lake can be complicated, especially when managing JAR dependencies. In this project, I will show you how to connect your local Spark application to ADLS and run a Spark query using Visual Studio Code. The local Spark application will be hosted in a container, but it can also be hosted locally locally ;-)</p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#kickstart-integrating-spark-with-azure-data-lake","title":"Kickstart: Integrating Spark with Azure Data Lake","text":""},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#create-the-containerized-setup","title":"Create the containerized setup","text":"<p>Our environment is set up inside a Docker container running Ubuntu on a Windows OS host. Within this container, Python 3 and Spark are installed. But the steps can be used in local environments as well.</p> <ul> <li>Check the python version in the container and find out site-packages directory</li> <li> <p>Often, systems have both Python 2.x and Python 3.x installed. Use the following commands to determine which versions are available:      <pre><code>python --version\npython3 --version\n</code></pre> </p> </li> <li> <p>Determine where PySpark is installed using <code>pip</code>. Your enviornment may have multiple python installation especially if its linux or in a docker. You need to find the right <code>site-packages</code> directory so that the packages are copied to right location. To find out run this command in docker terminal or normal command prompt: </p> <p><pre><code>pip3 show pyspark | grep Location\n</code></pre>  Alternatively, you can get the location by running the command:  <pre><code>python3 -c \"from distutils.sysconfig import get_python_lib; print(get_python_lib())\"\n</code></pre> </p> </li> <li> <p>Install <code>wget</code></p> </li> <li> <p><code>wget</code> is a tool for downloading files from the internet. If you don\u2019t have it in your environment you can get it using the given command:      <pre><code>apt-get update &amp;&amp; apt-get install -y wget\n</code></pre></p> </li> <li> <p>Download Hadoop ADLS JARs</p> <ul> <li>I've downloaded and placed the jars here. Download and copy it to a desired location.</li> <li>Alternatively, run the command below to download jars to your home directory</li> </ul> <p><code>bash cd ~ wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.3.3/hadoop-azure-3.3.3.jar wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure-datalake/3.3.3/hadoop-azure-datalake-3.3.3.jar wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.3.3/hadoop-common-3.3.3.jar wget https://repo1.maven.org/maven2/com/microsoft/azure/azure-storage/8.6.6/azure-storage-8.6.6.jar wget https://repo1.maven.org/maven2/com/azure/azure-security-keyvault-secrets/4.3.0/azure-security-keyvault-secrets-4.3.0.jar wget https://repo1.maven.org/maven2/com/azure/azure-identity/1.3.0/azure-identity-1.3.0.jar</code> - After downloading, place the jars in any desired folder. These jars will be referenced during spark session creation. - Alternatively you can use download the jars on-the-fly using maven coordinates using <code>.config('spark.jars.packages', '...')</code></p> </li> </ul>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#register-an-app-for-oauth-authentication","title":"Register an App for OAuth Authentication","text":"<p>If you want to access a file(say CSV) in Azure through OAuth authentication, you need to create an App registration and grant this app permission to the CSV. This registered App's identity is used by Spark to authenticate. The same principle applies in Databricks, where an app is already created, named AzureDatabricks. Follow the steps below to register the app and give it permission to the file.</p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#register-a-new-app-using-app-registration","title":"Register a new App using App Registration","text":"<ul> <li>In the Azure Portal to search for App registrations', select it, and opt for '+ New registration'. </li> </ul> <ul> <li>Give a name, say <code>adlssparkapp</code>, choose 'Accounts in this organizational directory only', keep the Redirect URI empty, and click Register. </li> </ul>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#copy-ids-and-secret-from-the-app","title":"Copy Ids and secret From The App","text":"<ul> <li>After registration, jot down the Application ID and Directory ID. </li> </ul> <ul> <li>Go to Manage &gt; Certificates &amp; secrets, select + New client secret, label it <code>SparkAppSecret</code>, set an expiration, and click  'Add'. </li> </ul> <ul> <li>Post-creation, make note of the one-time viewable secret value essential for the Spark-Azure handshake. </li> </ul>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#give-the-app-permission-to-the-container","title":"Give the App Permission to the Container","text":"<ul> <li> <p>Open the container, navigate to Access Control (IAM) &gt; Role assignments, click Add &gt; Add role assignment, select Storage Blob Contributor,    </p> </li> <li> <p>Search for the app <code>adlssparkapp</code>, and click OK.</p> </li> </ul> <p></p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#access-adls-data-from-spark-using-oauth-authentication-and-service-principal","title":"Access ADLS Data From Spark Using OAuth Authentication and Service Principal","text":"<p>With the app now registered and the key, ID, and secret in hand, we can proceed to execute the main code. Follow the steps outlined below to continue:</p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#open-vs-code-and-connect-to-the-container","title":"Open VS Code and Connect To the Container","text":"<ul> <li>Open VS Code: Launch Visual Studio Code and click the remote container icon at the bottom left.</li> </ul> <ul> <li>Attach to Container: From the top menu, choose \"Attach to running container\".</li> </ul> <ul> <li>Select Container: Pick the displayed running container. </li> </ul> <p>This action launches a new VS Code instance connected to that container.</p> <p></p> <ul> <li> <p>Create Notebook: In this instance, create a .ipynb (Jupyter notebook) to execute the subsequent section's code.    </p> </li> <li> <p>Connect to the python version where we copied the hadoop jars    There could be multiple python versions in a linux enviornment. From VS Code choose the python version whcih has our jars</p> </li> </ul> <p></p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#run-the-code","title":"Run the code","text":"<pre><code>Run the code below in the jupyter notebok:\n</code></pre> <pre><code>from pyspark.sql import SparkSession\n# Initialize a Spark session with necessary configurations for connecting to ADLS\n#Offline version\nspark = SparkSession.builder \\\n    .appName(\"ADLS Access\") \\\n    .config(\"spark.jars\", \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar\") \\\n    .getOrCreate()\n\n# Or using maven coordinates\n# Online version\nspark = SparkSession.builder \\\n    .appName(\"ADLS Access\") \\\n    .config(\"spark.jars.packages\", \n            \"org.apache.hadoop:hadoop-azure:3.3.3,\"\n            \"org.apache.hadoop:hadoop-azure-datalake:3.3.3,\"\n            \"org.apache.hadoop:hadoop-common:3.3.3\") \\\n    .getOrCreate()\n\n\n# Define credentials and storage account details for ADLS access\nstorage_account = \"&lt;The_Storage_Act_Name_Containing_Container&gt;\"\napp_client_id = \"&lt;The_Client_ID_From_Registered_App&gt;\"\napp_directory_tenant_id = \"&lt;The_Client_ID_From_Registered_App&gt;\"\napp_client_secret = \"&lt;The_Secret_Value_From_Registered_App&gt;\"\n\n# Configure Spark to use OAuth authentication for ADLS access\nspark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", app_client_id)\nspark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", app_client_secret)\nspark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{app_directory_tenant_id}/oauth2/token\")\n\n# Define the path to the dataset in ADLS and read the CSV file using Spark\npath = \"abfss://&lt;containerName&gt;@&lt;storaegaccountname&gt;.dfs.core.windows.net/&lt;CSV_File_Name.csv&gt;\"\nspark.read.format(\"csv\").load(path).show()\n</code></pre>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#access-data-in-adls-container-using-storage-accounts-access-key-method","title":"Access data in ADLS container using Storage Account's Access Key Method","text":"<p>Another methods to access ADLS is using the Access key method. Here we get the access key from the storage account then use it to access the files inside it. To use this method, follow these steps:</p> <ul> <li>Get the Access Keys from the Storage Account</li> <li>In your storage account, under the <code>Security + networking</code> section in the left sidebar, find and select <code>Access keys</code>.</li> <li> <p>You\u2019ll be presented with two keys: <code>key1</code> and <code>key2</code>. Both keys can be used to authenticate, so choose one and copy it. This will be used in the subsequent steps.</p> <p></p> </li> <li> <p>Execute the code After getting the access key use this code. Replace your access key in the access key location:</p> </li> </ul> <pre><code># Import the required module for creating a Spark session.\nfrom pyspark.sql import SparkSession\n\n# Initialize the Spark session. The builder pattern is utilized to configure the session.\n# We set the application name to \"ADLS Access\" for identification in Spark UI.\n# Necessary JAR files are specified for Spark to connect and interact with Azure Data Lake Storage (ADLS).\nspark = SparkSession.builder \\\n    .appName(\"ADLS Access\") \\\n    .config(\"spark.jars\", \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar\") \\\n    .getOrCreate()\n\n# Specify the Azure storage account name and the associated access key for authentication purposes.\nstorage_account_name = \"&lt;The_Storage_Account_Name&gt;\"\nstorage_account_key = \"&lt;key1_or_key2&gt;\"\n\n# Configure Spark to utilize AzureBlobFileSystem. This is essential for Azure Blob storage connectivity.\nspark.conf.set(f\"fs.azure\", \"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem\")\n\n# Authenticate the Spark session by providing the access key for the specified Azure storage account.\nspark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", storage_account_key)\n\n# Read the desired CSV file located in ADLS into a DataFrame (df) using Spark.\ndf = spark.read.csv(f\"abfss://&lt;container_name&gt;@{storage_account_name}.dfs.core.windows.net/&lt;filename.csv&gt;\")\n</code></pre>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#common-errors","title":"Common Errors","text":""},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#authorizationpermissionmismatch-during-oauth-authenticaiton","title":"AuthorizationPermissionMismatch During OAuth Authenticaiton","text":"<p>While executing the code you may encounter errors like:</p> <p><pre><code>AuthorizationPermissionMismatch, \"This request is not authorized to perform this operation using this permission.\"\n</code></pre> or</p> <pre><code>java.nio.file.AccessDeniedException: Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://strgacweatherapp.dfs.core.windows.net/weather-timer/2023-10-19-09.json?upn=false&amp;action=getStatus&amp;timeout=90\n    at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.checkException(AzureBlobFileSystem.java:1384)\n</code></pre> <p></p> <p>OAuth uses a registered apps identity to connect. This app should have permission to the folder where the file resides.</p> <p></p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#appendix","title":"Appendix","text":""},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#why-does-spark-rely-on-hadoop-libraries-to-access-azure-data-lake-storage-adls","title":"Why Does Spark Rely on Hadoop Libraries to Access Azure Data Lake Storage (ADLS)?","text":"<p>Long story short: In a standalone Spark setup, we use specific Hadoop JARs solely for connecting to ADLS. It's important to note that these are just JARs and don't represent the full Hadoop ecosystem. </p> <p>Apache Spark is used for distributed data processing. But for data storage it relies on other systems like ADLS, S3 etc. But why, when connecting Spark to ADLS, do we bring Hadoop into the picture? Let\u2019s find out.</p> <p>Spark's Core Functionality: Spark is designed to process data, not to understand the intricacies of every storage system. It can pull data from various sources, but it doesn't always have native integrations for each one.</p> <p>Hadoop's Role: Hadoop, primarily known for its distributed file system (HDFS), also offers connectors to diverse storage systems. Over time, it has become the standard bridge between storage solutions and big data tools.</p> <p>ADLS and Hadoop Integration: When Microsoft developed ADLS, they provided a connector to the Hadoop FileSystem API. This approach made sense. Why reinvent the wheel when big data tools already communicate efficiently with HDFS via Hadoop's API?</p> <p>Conclusion</p> <p>HSpark uses Hadoop libraries to access ADLS due to the standardized and robust nature of the Hadoop FileSystem API. Microsoft integrated ADLS with this Hadoop API to ensure that ADLS would be compatible with a broad range of big data tools, such as Spark and Hive. This decision was to use the extensive community support of the Hadoop ecosystem and also allowed Microsoft to reuse what was already working In essence, the Hadoop API serves as a bridge between Spark and ADLS.</p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#understanding-essential-jars-for-azure-data-lake-operations-with-spark","title":"Understanding Essential JARs for Azure Data Lake Operations with Spark","text":"<ol> <li>hadoop-azure-3.3.3.jar:</li> <li>Description: This library provides support for Azure Blob Storage integration with Hadoop. It contains the <code>WASB</code> (Windows Azure Storage Blob) file system connector.</li> <li> <p>Use-Cases: Reading/writing data from/to Azure Blob Storage (often ADLS Gen1) using Hadoop's FileSystem API.</p> </li> <li> <p>hadoop-azure-datalake-3.3.3.jar:</p> </li> <li>Description: This is the Data Lake connector for Hadoop, providing support for ADLS Gen1.</li> <li> <p>Use-Cases: If you're working with ADLS Gen1, this JAR lets Spark access the data lake using the Hadoop FileSystem API.</p> </li> <li> <p>hadoop-common-3.3.3.jar:</p> </li> <li>Description: The core library for Hadoop, it contains common utilities and the Hadoop FileSystem API.</li> <li> <p>Use-Cases: Fundamental for almost all Hadoop-related operations. It's the foundational library upon which other Hadoop components rely.</p> </li> <li> <p>azure-storage-8.6.6.jar:</p> </li> <li>Description: Azure's storage SDK, facilitating interaction with Azure Storage services like Blob, Queue, and Table.</li> <li> <p>Use-Cases: Interacting with Azure Blob Storage (and by extension, ADLS Gen2 which is built on Blob). It's essential for Spark to communicate and access Azure storage services.</p> </li> <li> <p>azure-security-keyvault-secrets-4.3.0.jar:</p> </li> <li>Description: Provides capabilities to interact with Azure Key Vault's secrets. It facilitates fetching, setting, or managing secrets.</li> <li> <p>Use-Cases: Whenever you need to securely access or manage secrets (like storage account keys or database connection strings) stored in Azure Key Vault from your Spark application.</p> </li> <li> <p>azure-identity-1.3.0.jar:</p> </li> <li>Description: Azure SDK's identity library, providing various credentials classes for Azure Active Directory (AAD) token authentication.</li> <li>Use-Cases: Authenticating against Azure services using AAD-based credentials, especially when trying to securely access resources like Key Vault or ADLS Gen2.</li> </ol>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#project-sparkzure-part2-sorting-files-in-adls-container-using-standalone-spark","title":"Project Sparkzure Part2 - Sorting Files in ADLS Container Using Standalone Spark","text":""},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#overview-of-the-article","title":"Overview of the Article","text":"<p>In Part 1, we dived into accessing ADLS files with Pyspark and Hadoop Jars. Now, let's switch gears a bit. In this article, we'll explore how to sort\u2014by creating containers and moving/renaming files\u2014the content in an Azure Data Lake Container using just a Standalone Spark application. While there's always the route of Azure Data Factory, Databricks, or Azure Logic Apps, I want to spotlight this approach. Why? Because it's not only a viable alternative, but it also comes with the perk of being nearly cost-free compared to the other Azure services I mentioned.</p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#my-environment","title":"My Environment","text":"<ul> <li>Deployment Platform: Docker</li> <li>Operating System: Ubuntu</li> <li>Python Version: 3.8.10</li> <li>Development IDE: Visual Studio Code connected to the container</li> <li>Spark Setup: Standalone Spark installed as part of Pyspark(<code>pip install pyspark</code>)</li> <li>Spark Home /usr/local/lib/python3.8/dist-packages/pyspark/</li> <li>Jars Location /usr/local/lib/python3.8/dist-packages/pyspark/jars/</li> </ul>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#the-scenario","title":"The scenario","text":"<p>We have a container name \"weather-timer\" that contains JSON files formatted as <code>YYYY-10-22-12.json</code>. These files hold weather information retrieved from a web API. The files need to be sorted in the format  <code>year=yyyy/month=mm/day=dd/hour=hh.json</code>. This is a real-world requirement, as a structure like this can make partition pruning more efficient during query time if you're using a system like Apache Hive or Delta Lake.</p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#kickstart","title":"Kickstart","text":""},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#environment-setup","title":"Environment Setup","text":"<p>For ADLS connectivity in standalone PySpark, we need to download these 3 super-important Jars: - <code>hadoop-azure-&lt;version&gt;.jar</code>: Supports Azure Blob Storage and Spark integration. - <code>hadoop-azure-datalake-&lt;version&gt;.jar</code>: For for ADLS access, including authentication features. - <code>hadoop-common-&lt;version&gt;.jar</code>: Contains utilities for the other JARs.</p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#download-the-jars-by-running-this-command-from-terminal","title":"Download the jars by running this command from terminal:","text":"<p>Run teh following command in terminal. Note: The version of jars might change over time.</p> <p>```bash cd ~ wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.3.3/hadoop-azure-3.3.3.jar wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure-datalake/3.3.3/hadoop-azure-datalake-3.3.3.jar wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.3.3/hadoop-common-3.3.3.jar</p> <pre><code>##### Copy the jars to the SPARK_HOME/Jars location\n\n```bash\n   cd ~  # assuming you downloaded the JARs in the home directory\n   cp *.jar /usr/local/lib/python3.8/dist-packages/pyspark/jars/\n</code></pre>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#run-the-spark-code","title":"Run the Spark Code","text":"<p>This is the code for performing the sorting. It checks if there are containers. If not, it makes them. Before you run the code, make sure you replace placeholders like <code>&lt;YOUR_STORAGE_ACT_NAME&gt;</code>, <code>&lt;YOUR_REG_APP_CLIENT_ID&gt;</code>, <code>&lt;YOUR_REG_APP_TENANT_ID&gt;</code>, <code>&lt;YOUR_REG_APP_CLIENT_SECRET&gt;</code>, <code>&lt;YOUR_CONTAINER_NAME&gt;</code>, and paths to JAR files with the actual values.. Also, update any other settings to match your system.</p> <pre><code># Importing the necessary module for SparkSession from the PySpark library.\nfrom pyspark.sql import SparkSession\n\n#Note: The location of jars is where we copied them after downloadign with wget. Just 3 jars.\nspark = SparkSession.builder \\\n    .appName(\"ADLS Access\") \\\n    .config(\"spark.jars\", \n            \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,\"\\\n            \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,\"\\\n            \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar\") \\\n    .getOrCreate()\n\n\n# Configuring PySpark for Azure Data Lake Storage (ADLS) Authentication using OAuth and Service Principal\n# Credentials and configurations\nstorage_account_name = \"&lt;YOUR_STORAGE_ACT_NAME&gt;\"\nregapp_client_id = \"&lt;YOUR_REG_APP_CLIENT_ID&gt;\" # Application (client) ID of the registered app\nregapp_directory_id = \"&lt;YOUR_REG_APP_TENANT_ID&gt;\" # Directory (tenant) ID of the registered app\nregapp_client_secret = \"&lt;YOUR_REG_APP_CLIENT_SECRET&gt;\"\n\n# Set the authentication type to OAuth for the specified storage account---------------\nspark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\")\n\n# Define the token provider type for OAuth. The 'ClientCredsTokenProvider' is specified for the client credentials flow.\nspark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n\n# Provide the client ID (application ID) of the registered application in Azure Active Directory (AD).\nspark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", regapp_client_id)\n\n# Set the client secret of the registered application. This acts as a password for the application to verify its identity.\nspark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", regapp_client_secret)\n\n# Specify the OAuth 2.0 token endpoint, allowing the application to retrieve tokens for authentication.\nspark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{regapp_directory_id}/oauth2/token\")\n#----------------------------------------------------------------------------------------\n#---------Code to perform the sorting----------------------------------------------------\n# Define the ADLS Gen2 base path\nbase_path = f\"abfss://&lt;YOUR_CONTAINER_NAME&gt;@&lt;YOUR_STORAGE_ACT_NAME&gt;.dfs.core.windows.net/\"\n\nconf = spark._jsc.hadoopConfiguration()\nconf.set(\"fs.abfss.impl\", \"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem\")\nuri = spark._jvm.java.net.URI\npath_obj = spark._jvm.org.apache.hadoop.fs.Path(base_path)\nfile_system = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri(base_path), conf)\n\nold_files = [status.getPath().toString() for status in file_system.globStatus(spark._jvm.org.apache.hadoop.fs.Path(base_path + \"*-*.json\"))]\n\n# Diagnostic: Check the number of files fetched\nprint(f\"Number of files to be processed: {len(old_files)}\")\n\n# Test with a subset (for diagnostic purposes)\nsubset_of_files = old_files[:5]\n\nfor old_file_path in old_files:\n    # Extract year, month, day, and hour from the old file path\n    filename = old_file_path.split('/')[-1]\n    year, month, day, hour = filename.split('-')[:4]\n\n    # Construct the new directory structure based on the desired format\n    new_directory = base_path + f\"year={year}/month={month}/day={day}/\"\n\n    # Check if the directory exists; if not, create it\n    if not file_system.exists(spark._jvm.org.apache.hadoop.fs.Path(new_directory)):\n        file_system.mkdirs(spark._jvm.org.apache.hadoop.fs.Path(new_directory))\n\n    # Construct the new file path\n    new_file_path = new_directory + f\"hour={hour}\"\n\n    # Diagnostic: Printing the move action\n    print(f\"Moving {old_file_path} to {new_file_path}\")\n\n    # Rename (move) the file to the new path and check if it's successful\n    success = file_system.rename(spark._jvm.org.apache.hadoop.fs.Path(old_file_path), spark._jvm.org.apache.hadoop.fs.Path(new_file_path))\n\n    # Diagnostic: Check if the move was successful\n    print(f\"Move success: {success}\")\n\nprint(\"Files rearranged successfully for the subset!\")\n#---------Code to perform the sorting----------------------------------------------------\n</code></pre>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#conclusion","title":"Conclusion","text":"<p>There are many ways to organize files in a container, like using ADF, Databricks, or Logic Apps. But this way is good too because it's free, unlike some pretty-expensive options like Databricks. I shared this article to let us know there's another option out there. It shows how we perform such operation on Azure DataLake from an outside standalone application.</p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#appendix_1","title":"Appendix","text":""},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#programmatic-options-for-creating-containers-sorting-files-etc","title":"Programmatic options for Creating Containers, Sorting Files etc:","text":"<p>If you want to compare what other programmatic options we have to peroform such operation, here is the comparison:</p> <ol> <li> <p>Reading/Writing Large Datasets: </p> <ul> <li>Best Tool: PySpark.</li> <li>Reason: Spark is designed for distributed data processing. Reading and processing large datasets from ADLS Gen2 into Spark dataframes will be efficient.</li> </ul> </li> <li> <p>Listing Files in a Container/Directory:</p> <ul> <li>Best Tool: PySpark or Hadoop FileSystem API.</li> <li>Reason: PySpark provides simple methods to list files, but if you're already interfacing with Hadoop's FileSystem API for other tasks, it's also a good choice.</li> </ul> </li> <li> <p>Renaming or Moving Files:</p> <ul> <li>Best Tool: Hadoop FileSystem API.</li> <li>Reason: While this can be done with the Azure SDK, the Hadoop FileSystem API provides a more direct interface when working alongside Spark.</li> </ul> </li> <li> <p>Creating Containers or Directories:</p> <ul> <li>Best Tool: Azure SDK (<code>azure-storage-file-datalake</code>).</li> <li>Reason: Creating containers or directories is a simple storage management task. The Azure SDK provides direct methods to do this without unnecessary overhead.</li> </ul> </li> <li> <p>Setting Permissions or Managing Access:</p> <ul> <li>Best Tool: Azure SDK.</li> <li>Reason: Managing permissions or access control is more straightforward with the Azure SDK, which provides methods tailored for these tasks.</li> </ul> </li> </ol>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#understanding-spark-configuration-sparkjarspackages-vs-sparkjars","title":"Understanding Spark Configuration: <code>spark.jars.packages</code> vs <code>spark.jars</code>","text":"<p>Apache Spark offers robust options for integrating external libraries, crucial for expanding its native capabilities. Two such configurations often used are <code>spark.jars.packages</code> and <code>spark.jars</code>. Understanding the distinct roles and applications of these configurations can significantly enhance how you manage dependencies in your Spark applications.</p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#the-role-of-configsparkjarspackages","title":"The Role of <code>.config('spark.jars.packages', '...')</code>","text":"<p>This configuration is quintessential when it comes to managing library dependencies via Maven coordinates. It's designed to streamline the process of including external libraries in your Spark application.</p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#how-it-works","title":"How it Works","text":"<ul> <li>Maven Coordinates: You specify the library using its Maven coordinates in the format <code>'groupId:artifactId:version'</code>.</li> <li>Automatic Download: Spark automates the download process, fetching the specified library from Maven Central or another configured Maven repository.</li> <li>Ease of Use: This method is particularly user-friendly, ensuring you're incorporating the correct library version without manually downloading the JAR files.</li> </ul>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#example-usage","title":"Example Usage","text":"<p><pre><code>.config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0')\n</code></pre> In this instance, Spark is instructed to download and include the Kafka connector compatible with Spark version 2.12 and version 3.3.0 of the library.</p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#the-utility-of-configsparkjars","title":"The Utility of <code>.config('spark.jars', '...')</code>","text":"<p>Contrasting <code>spark.jars.packages</code>, the <code>spark.jars</code> configuration is utilized when directly referencing locally stored JAR files.</p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#how-it-functions","title":"How it Functions","text":"<ul> <li>Local File Path: You provide the absolute path to the JAR file already present on your system.</li> <li>No Automatic Download: Spark bypasses any downloading process, relying on the specified JAR file's presence in the given location.</li> <li>Custom or Offline Use: This approach is ideal for using custom library versions or in environments with restricted internet access.</li> </ul>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#example-implementation","title":"Example Implementation","text":"<p><pre><code>.config('spark.jars', '/opt/shared-data/spark-sql-kafka-0-10_2.13-3.4.0.jar')\n</code></pre> Here, Spark is directed to incorporate a Kafka connector JAR file located at <code>/opt/shared-data/spark-sql-kafka-0-10_2.13-3.4.0.jar</code>.</p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.0_Spark_To_ADLS.html#conclusion_1","title":"Conclusion","text":"<p>In summary, <code>spark.jars.packages</code> is a hassle-free solution for incorporating libraries using Maven coordinates, automating the downloading and version management. In contrast, <code>spark.jars</code> is suited for scenarios where you have a local JAR file, offering more control over the specific version and source of the library being used. The choice between these configurations hinges on your project's requirements and operational environment, providing flexibility in managing your Spark application's dependencies.</p> <p>\u00a9 D Das \ud83d\udce7 das.d@hotmail.com | ddasdocs@gmail.com</p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary.html","title":"Connecting to Azure Storage from Spark: Methods and Code Samples","text":"<p> In this guide, I will walk you through the different methods to connect Apache Spark to Azure Storage services including Azure Blob Storage and Azure Data Lake Storage. You will learn how to set up your Spark session to read and write data from and to Azure Storage using various authentication methods. </p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary.html#overview","title":"Overview","text":"<p>Connecting Apache Spark to Azure Storage can be achieved through several methods, each suited for different scenarios and security requirements. We will cover the use of ABFS driver for Azure Data Lake Storage Gen2, managed identities in Azure-hosted Spark, shared access signatures, and more.</p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary.html#method-1-abfs-driver-for-adls-gen2","title":"Method 1: ABFS Driver for ADLS Gen2","text":"<p> The ABFS (Azure Blob File System) driver is specially designed for Azure Data Lake Storage Gen2 and supports OAuth2 authentication, providing a secure method to access your data. </p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary.html#sample-code-for-abfs-driver","title":"Sample Code for ABFS Driver:","text":"<p>Here is a sample code to connect using OAuth authentication and service principal. The code requires Haddop Azure Storagae Jars which needs to be downloaded spearately.</p> <pre><code>from pyspark.sql import SparkSession\n\n# Replace with your Azure Storage account information\nstorage_account_name = \"your_storage_account_name\"\nclient_id = \"your_client_id_of_the_registered_app\"\nclient_secret = \"your_client_secret_of_the_registered_app\"\ntenant_id = \"your_tenant_id_of_the_registered_app\"\n\nspark = SparkSession.builder \\\n    .appName(\"Any_App_Name\") \\\n    .config(\"spark.jars\", \n             \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-3.3.3.jar,\"\\\n             \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-azure-datalake-3.3.3.jar,\"\\\n             \"/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-common-3.3.3.jar\") \\\n    .config(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\") \\\n    .config(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") \\\n    .config(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", client_id) \\\n    .config(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", client_secret) \\\n    .config(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\") \\\n    .getOrCreate()\n</code></pre>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary.html#method-2-managed-identity-for-azure-hosted-spark","title":"Method 2: Managed Identity for Azure-hosted Spark","text":"<p> For Spark clusters hosted on Azure, Managed Identity offers a way to securely access Azure services without storing credentials in your code. </p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary.html#sample-code-for-managed-identity","title":"Sample Code for Managed Identity:","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"Azure Blob with Managed Identity\") \\\n    .config(\"fs.azure.account.auth.type.&lt;storage_account_name&gt;.blob.core.windows.net\", \"CustomAccessToken\") \\\n    .config(\"fs.azure.account.custom.token.provider.class.&lt;storage_account_name&gt;.blob.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ManagedIdentityCredentialProvider\") \\\n    .getOrCreate()\n</code></pre>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary.html#method-3-azure-blob-storage-with-access-key","title":"Method 3: Azure Blob Storage with Access Key","text":"<p> Using the Azure Blob Storage access key is a straightforward method to establish a connection, but it's less secure than using OAuth2 or Managed Identities. </p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary.html#sample-code-for-access-key","title":"Sample Code for Access Key:","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"Azure Blob Access with Access Key\") \\\n    .config(\"fs.azure.account.key.&lt;storage_account_name&gt;.blob.core.windows.net\", \"&lt;access_key&gt;\") \\\n    .getOrCreate()\n</code></pre>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary.html#method-4-shared-access-signature-sas","title":"Method 4: Shared Access Signature (SAS)","text":"<p> Shared Access Signatures (SAS) provide a secure way to grant limited access to your Azure Storage resources without exposing your account key. </p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary.html#sample-code-for-sas","title":"Sample Code for SAS:","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"Azure Blob Access with SAS\") \\\n    .config(\"fs.azure.sas.&lt;container_name&gt;.&lt;storage_account_name&gt;.blob.core.windows.net\", \"&lt;sas_token&gt;\") \\\n    .getOrCreate()\n</code></pre>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary.html#method-5-environment-variablessecrets","title":"Method 5: Environment Variables/Secrets","text":"<p> For an extra layer of security, use environment variables or a secret scope to manage your credentials, keeping them out of your code base. </p>"},{"location":"Spark-DataBricks/2.0_Spark_To_ADLS/2.1_Spark-To_ADLS_Summary.html#sample-code-for-using-environment-variablessecrets","title":"Sample Code for Using Environment Variables/Secrets:","text":"<pre><code>import os\nfrom pyspark.sql import SparkSession\n\n# Assume the environment variables or secrets are already set\nstorage_account_name = os.getenv('STORAGE_ACCOUNT_NAME')\nsas_token = os.getenv('SAS_TOKEN')\n\nspark = SparkSession.builder \\\n    .appName(\"Azure Blob Access with Environment Variables\") \\\n    .config(f\"fs.azure.sas.&lt;container_name&gt;.{storage_account_name}.blob.core.windows.net\", sas_token) \\\n    .getOrCreate()\n</code></pre>"},{"location":"Spark-DataBricks/3.0_Databricks/3.0_Databricks_Concepts.html","title":"What is databricks?","text":"<p>Databricks is like repackaged Apache Spark with some extra goodies inside the box. You can\u2019t use Databricks on its own\u2014you always have to start it from within Azure, GCP, or AWS. When you use Databricks, it uses your Azure subscription and resources like Azure VMs, Azure Storage, network and secuirty, to set up a Spark environment in the background. So, everything runs on your Azure account. You don\u2019t get a separate bill from Databricks; you just pay your regular Azure bill, and the money is split between Databricks and Microsoft.</p> <p>Fun fact. All Databricks Spark cluster is created inside Azure. Azure manages it using Azure Kubernetes Service.</p> <p>Now, when I say Databricks is Spark with some extra features, here\u2019s what I mean:</p> <ul> <li>Databricks Runtime: This is Spark, but it\u2019s been made faster with some <code>Databricks engine oil</code>.</li> <li>Databricks Workflows: This is a special feature from Databricks that you won\u2019t find in open-source Spark.</li> <li>Databricks Workspace: This is like the main dashboard, and it\u2019s completely from Databricks.</li> </ul> <p>Databricks is like a branded bottled water where the content, water, is open-source, but you still pay extra for the assurance and conveneince. Also, Databricks works like Uber\u2014it uses Azure, GCP, etc.\u2019s infrastructure and charges a fancy service fee.</p> <p>If you don\u2019t want Databricks and prefer to stick to open-source, you can have your own cluster (on-prem or on Azure VMs), use Azure HDInsight (a bit of management and extra features from Microsoft, but just Microsoft, no other party), or Google Dataproc (similar to Microsoft).</p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.0_Databricks_Concepts.html#architecture-of-azure-databricks-spark-cluster","title":"Architecture of Azure Databricks Spark Cluster","text":"<p>There are drivers/masters and workers/slaves. So how does the Driver look like? Eh. The your spark notebook is the driver. It contains the main progreamms which make use of workers.</p> <p>Driver Worker Jobs Task Slot</p> <p>Every cluseter has Only One Driver JVM and multiple Executor JVMs</p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.0_Databricks_Concepts.html#hive_metastore-folder","title":"hive_metastore folder","text":""},{"location":"Spark-DataBricks/3.0_Databricks/3.0_Databricks_Concepts.html#databricks-architecture","title":"Databricks Architecture","text":"<ul> <li> <p>Control Plane: This is where all the management happens. It includes the Databricks web app, Unity Catalog, Cluster Manager, Workflow Manager, Jobs, Notebooks, etc. Think of it as the control center where you manage everything.</p> </li> <li> <p>Data/Compute Plane: This is where the actual work gets done. It has the clusters and the data. The Data Plane is where your data is processed, and where the machines that do the work are located.</p> </li> <li>Serverless Compute Plane: Here, everything is managed inside the Databricks account, even though the actual servers are in Azure, AWS, or GCP.</li> <li> <p>Classic Compute Plane: In this setup, the servers are mainly managed from your cloud account (like Azure, AWS, or GCP).</p> </li> <li> <p>Personas: This is the look and feel of your Databricks web UI. There are different personas like Data Science and Engineering, Analyst, and Practitioner, each giving you a different experience based on what you do.</p> </li> </ul>"},{"location":"Spark-DataBricks/3.0_Databricks/3.0_Databricks_Concepts.html#databricks-clusters","title":"Databricks Clusters","text":"<ul> <li>Databricks has two types of clusters: </li> <li>All-purpose: Mainly for interactive work. Like, you want to run cells and see what's happening. Many users can use an all-purpose cluster.</li> <li> <p>Job: When things don't need any interaction. To run jobs.</p> </li> <li> <p>And two modes of clusters</p> </li> <li>Standard/Multi-Node: THis is the default mode. 1 Driver + Multiple Worker Nodes.</li> <li> <p>Single node: 1-Drive. No Worker. Driver works as both driver and worker. For light work load.</p> </li> <li> <p>Runtime has three versions</p> </li> <li>Standard: Normal spark and normal stuff.</li> <li>Machine Learning: Has some useful machine learning libraries installed</li> <li> <p>Photon: Has some speed for SQL</p> </li> <li> <p>And Access mode</p> </li> <li>Single User:  is always there</li> <li>Shared: Many users can access the same nodes. Seprate environment for each user. One fails, doesn't affecct other.</li> <li>No Isolattioin shared: Multiple users can access. But, enviornment is the same. One process fails, all users affected.</li> </ul>"},{"location":"Spark-DataBricks/3.0_Databricks/3.1_Catalogs_And_Metastore.html","title":"Hive Metastore and the <code>hive_metastore</code> folder","text":"<p>Hive is a data warehouse which stores data in HDFS(usually). Hive metastore is the database which gives 'front facing' tables whose data is in HDFS. Any database with JDBC can become a Hive metastore. </p> <p>By default Hive uses in-build Apache  derby database. In prod it uses usually MYSQL or Postgress.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.1_Catalogs_And_Metastore.html#catalogs","title":"Catalogs","text":""},{"location":"Spark-DataBricks/3.0_Databricks/3.1_Catalogs_And_Metastore.html#catalogs-and-metastore","title":"Catalogs and Metastore","text":"<p>Catalogs, Metastore related concepts come into picture the moment you start something SQL in Spark. The moment you start to use Spark as a Database rather than just on-the-fly processor of data.</p> <p>What is a metastore? Simply a store for metadata. When you store your Data in Spark itself, you store it as tables inside databases. This metastore is a way to keep that information. Whcih databas stores which table etc. And interstingly this metastore itself ia antoher database to store information about database. Database to store database info. haha!</p> <p>By default, Spark uses 'Hive metatore' as a metastore technology. metastore_db is the Hive's database name which stores the metadata. When you hear Hive catalog, this is the catalog to further classify the data. Like different levels. Folder, subfolder, files.</p> <p>By detault, when you use Spark to store data the data is stored in spark-warehouse folder. B</p> <p>Databricks is actually spark. So initially they used to use Hive metastore to manage the info about tables and db.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.1_Catalogs_And_Metastore.html#hive-metastore-in-databricks","title":"Hive Metastore in Databricks","text":"<p>Before Unity catalog Databricks used to use the default Hive metastore. This is what Spark uses normally.</p> <p>Here the naming schema used to be db_name.table_name. Its two-level.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.1_Catalogs_And_Metastore.html#unity-catalog-metastore-in-databricks","title":"Unity Catalog Metastore in Databricks","text":"<p>After Unity catalog. Table names becamse: catalog_name.schema_name.table_name(three  tier)</p> <p></p> <p>Unity Catalog in Azure Databricks helps you manage your data by keeping all access control, tracking, and data discovery in one place. It makes it easier to control who can see and use your data, keeps track of data usage, and helps you find the data you need quickly.</p> <p>Key Features:</p> <ol> <li>One-Time Setup for All: Set up your data access rules once and they work everywhere.</li> <li>Easy Security: Use familiar SQL commands to set who can see what.</li> <li>Automatic Tracking: Keeps a log of who accessed the data and shows how the data is used.</li> <li>Find Data Easily: Tag and search for your data easily.</li> <li>System Data Access: Check audit logs and usage details.</li> </ol> <p>How It\u2019s Organized:</p> <ul> <li>Metastore: The main place where all data info and access rules are kept.</li> <li>One Per Region: Usually set up automatically with a new workspace.</li> <li>Hierarchy:</li> <li>Catalogs: Big containers for your data, like folders.</li> <li>Schemas: Inside catalogs, like subfolders, holding tables and more.</li> <li>Detailed Data:<ul> <li>Volumes: Storage for unstructured data.</li> <li>Tables: Data organized in rows and columns.</li> <li>Views: Saved searches or queries on your tables.</li> <li>Functions: Small bits of code that do calculations.</li> <li>Models: AI models stored and managed.</li> </ul> </li> </ul> <p>This setup makes it simple to manage, secure, and find your data across all your Azure Databricks workspaces.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.1_Catalogs_And_Metastore.html#query-hive-metastore-from-unity-metastore","title":"Query Hive Metastore from Unity Metastore","text":"<p>Query the Hive metastore in Unity Catalog The Unity Catalog metastore is additive, meaning it can be used with the per-workspace Hive metastore in Azure Databricks. The Hive metastore appears as a top-level catalog called hive_metastore in the three-level namespace.</p> <p>For example, you can refer to a table called sales_raw in the sales schema in the legacy Hive metastore by using the following notation:</p> <p>SQL SQL</p> <p>Copy SELECT * from hive_metastore.sales.sales_raw; Python Python</p> <p>Copy display(spark.table(\"hive_metastore.sales.sales_raw\"))</p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods.html","title":"Authentication","text":""},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods.html#authentication-methods-to-connect-with-adls","title":"Authentication methods to connect with ADLS","text":"<p>Azure Data Lake Storage (ADLS) offers several methods to authenticate access from Databricks. This guide will cover the three main methods:</p> <ol> <li>ADLS Account Keys (Shared Access Key method)</li> <li>Shared Access Signature (SAS) tokens</li> <li>Service Principal with OAuth2</li> </ol> <p>Each method has its use cases and security implications. </p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods.html#1-accessing-adls-using-account-keys","title":"1. Accessing ADLS Using Account Keys","text":"<p>The Shared Access Key method is the simplest but least secure. </p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods.html#steps-to-use-adls-account-keys","title":"Steps to Use ADLS Account Keys:","text":"<ol> <li>Retrieve the Account Key:</li> <li>Go to the Azure Portal, navigate to your Storage account.</li> <li> <p>Click on Access keys and copy either key1 or key2.</p> </li> <li> <p>Use the Account Key in Databricks:</p> </li> <li>Set up the configuration in your Databricks notebook.</li> </ol> <pre><code># Configuration for accessing ADLS using the Shared Access Key method:\nstorageAccountName = \"your_storage_account_name\"\naccountKey = \"your_account_key\"  # For production, use Databricks secret scope\ncontainerName = \"your_container_name\"\nfilename = \"your_file.csv\"\n\nspark.conf.set(f\"fs.azure.account.key.{storageAccountName}.dfs.core.windows.net\", accountKey)\n\nadls_path = f\"abfss://{containerName}@{storageAccountName}.dfs.core.windows.net/\"\nspark.read.csv(adls_path + filename).show()\n</code></pre>"},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods.html#recommended-production-setup","title":"Recommended Production Setup:","text":"<p>Instead of embedding the account key in your code, use Databricks secrets for better security:</p> <pre><code># Using Databricks secret management utility\naccountKey = dbutils.secrets.get(scope=\"your_scope_name\", key=\"your_key_name\")\n\nspark.conf.set(f\"fs.azure.account.key.{storageAccountName}.dfs.core.windows.net\", accountKey)\n\nadls_path = f\"abfss://{containerName}@{storageAccountName}.dfs.core.windows.net/\"\nspark.read.csv(adls_path + filename).show()\n</code></pre>"},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods.html#2-accessing-adls-using-sas-tokens","title":"2. Accessing ADLS Using SAS Tokens","text":"<p>SAS tokens provide more granular control over permissions and are generally more secure than account keys.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods.html#steps-to-use-sas-tokens","title":"Steps to Use SAS Tokens:","text":"<ol> <li>Generate a SAS Token:</li> <li>In the Azure Portal, select your Storage account.</li> <li>Navigate to Shared access signature under Security + networking.</li> <li> <p>Configure the permissions and generate the token.</p> </li> <li> <p>Use the SAS Token in Databricks:</p> </li> <li>Set up the configuration in your Databricks notebook.</li> </ol> <pre><code>storageAccountName = \"your_storage_account_name\"\nsasToken = \"your_sas_token\"\ncontainerName = \"your_container_name\"\nfilename = \"your_file.csv\"\n\nspark.conf.set(f\"fs.azure.sas.{containerName}.{storageAccountName}.dfs.core.windows.net\", sasToken)\n\nadls_path = f\"abfss://{containerName}@{storageAccountName}.dfs.core.windows.net/\"\nspark.read.csv(adls_path + filename).show()\n</code></pre>"},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods.html#3-accessing-adls-using-service-principal-with-oauth2","title":"3. Accessing ADLS Using Service Principal with OAuth2","text":"<p>This method is the most secure and suitable for production environments as it uses Azure AD for authentication.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods.html#steps-to-use-service-principal-with-oauth2","title":"Steps to Use Service Principal with OAuth2:","text":"<ol> <li>Set Up Azure AD:</li> <li>Register an application in Azure AD.</li> <li>Assign roles to the application for accessing ADLS.</li> <li> <p>Note down the Client ID, Client Secret, and Directory (Tenant) ID.</p> </li> <li> <p>Configure Databricks to Use Service Principal:</p> </li> <li>Store your credentials in Databricks secrets.</li> </ol> <pre><code>storageAccountName = \"your_storage_account_name\"\ncontainerName = \"your_container_name\"\nfilename = \"your_file.csv\"\n\nclientID = dbutils.secrets.get(scope=\"your_scope_name\", key=\"your_client_id_key\")\nclientSecret = dbutils.secrets.get(scope=\"your_scope_name\", key=\"your_client_secret_key\")\ndirectoryID = dbutils.secrets.get(scope=\"your_scope_name\", key=\"your_directory_id_key\")\n\nspark.conf.set(f\"fs.azure.account.auth.type.{storageAccountName}.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(f\"fs.azure.account.oauth.provider.type.{storageAccountName}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(f\"fs.azure.account.oauth2.client.id.{storageAccountName}.dfs.core.windows.net\", clientID)\nspark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storageAccountName}.dfs.core.windows.net\", clientSecret)\nspark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storageAccountName}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{directoryID}/oauth2/token\")\n\nadls_path = f\"abfss://{containerName}@{storageAccountName}.dfs.core.windows.net/\"\nspark.read.csv(adls_path + filename).show()\n</code></pre>"},{"location":"Spark-DataBricks/3.0_Databricks/3.2_AuthenticationMethods.html#conclusion","title":"Conclusion","text":"<p>The Shared Access Key method is the simplest but least secure, while Service Principal with OAuth2 provides the highest level of security suitable for production environments. SAS tokens offer a balance between ease of use and security. </p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.3_Mount_ADLS_on_Databricks.html","title":"ADLS Mount","text":""},{"location":"Spark-DataBricks/3.0_Databricks/3.3_Mount_ADLS_on_Databricks.html#mount-adls-gen2-on-databricks-using-aad-oauth-service-principal","title":"Mount ADLS Gen2 on Databricks Using AAD OAuth &amp; Service Principal","text":"<p>Integrate Databricks with Azure Data Lake Storage Gen2 (ADLS Gen2) securely using Azure Active Directory (AAD) OAuth and a service principal. </p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.3_Mount_ADLS_on_Databricks.html#for-the-busy-people","title":"For the busy people","text":"<p>Execute this in a databricks notebook:</p> <pre><code>dbutils.fs.mount(\n  source = \"adl://.azuredatalakestore.net/\",\n  mount_point = \"/mnt/\",\n  extra_configs = {\n    \"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\",\n    \"dfs.adls.oauth2.client.id\": dbutils.secrets.get(scope = \"\", key = \"client-id\"),\n    \"dfs.adls.oauth2.credential\": dbutils.secrets.get(scope = \"\", key = \"client-secret\"),\n    \"dfs.adls.oauth2.refresh.url\": \"https://login.microsoftonline.com//oauth2/token\"}\n)\n</code></pre>"},{"location":"Spark-DataBricks/3.0_Databricks/3.3_Mount_ADLS_on_Databricks.html#detailed-steps","title":"Detailed steps","text":"<ol> <li>Azure Setup:</li> <li>Create Service Principal: In the Azure Portal, navigate to Azure Active Directory &gt; App registrations &gt; New registration. Provide a name and register the application. Save the Application (client) ID and create a new client secret under Certificates &amp; secrets. Save the client secret value.</li> <li> <p>Assign Role: In your storage account, assign the <code>Storage Blob Data Contributor</code> role to the service principal.</p> </li> <li> <p>Store Credentials in Key Vault: In Azure Key Vault, add the client ID, client secret, and tenant ID as secrets.</p> </li> <li> <p>Databricks Configuration: In your Databricks notebook, follow these steps to configure and mount ADLS Gen2:</p> </li> <li> <p>Fetch Credentials:</p> <pre><code>clientID = dbutils.secrets.get(scope=\"azbackedscope\", key=\"regappClientID\")\nclientSecret = dbutils.secrets.get(scope=\"azbackedscope\", key=\"regappClientSecret\")\ndirectoryID = dbutils.secrets.get(scope=\"azbackedscope\", key=\"regappDirectoryID\")\n</code></pre> </li> <li> <p>Set OAuth Configs:</p> <pre><code>configs = {\n    \"fs.azure.account.auth.type\": \"OAuth\",\n    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n    \"fs.azure.account.oauth2.client.id\": clientID,\n    \"fs.azure.account.oauth2.client.secret\": clientSecret,\n    \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{directoryID}/oauth2/token\"\n}\n</code></pre> </li> <li> <p>Mount Storage:</p> <pre><code>storageAccountName = \"your_storage_account_name\"\ncontainerName = \"your_container_name\"\nmountPoint = \"/mnt/your_mount_name\"\n\nadlsPath = f\"abfss://{containerName}@{storageAccountName}.dfs.core.windows.net/\"\n\ndbutils.fs.mount(\n    source=adlsPath,\n    mount_point=mountPoint,\n    extra_configs=configs\n)\n</code></pre> </li> <li> <p>Verify Mount:</p> <pre><code>display(dbutils.fs.ls(mountPoint))\n</code></pre> </li> </ol>"},{"location":"Spark-DataBricks/3.0_Databricks/3.3_Mount_ADLS_on_Databricks.html#conclusion","title":"Conclusion","text":"<p>With ADLS Gen2 storage mounted on DBFS, you can read and write data more conveniently. The code becomes simpler as you don't have to authenticate every time, making it feel like accessing local storage.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html","title":"Working with Secret Scopes in Azure Databricks","text":""},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#introduction","title":"Introduction","text":"<p>Azure Databricks offers two secret scopes: Azure Key Vault-backed and Databricks-backed. This guide walks you through their creation, access in PySpark, and prerequisites such as Databricks CLI installation and understanding key vault-related roles. By the end, you'll possess all the fundamental knowledge required to use key vaults with Azure Databricks.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#different-types-of-secret-scopes","title":"Different Types of Secret Scopes","text":"<ol> <li> <p>Databricks-backed Secret Scope:     This is a proprietary scope managed by Databricks. Secrets stored in this scope are encrypted and can be accessed only by users with the necessary permissions.</p> </li> <li> <p>Azure key-vault backed Secret Scope:     If you create a secret scope in Databricks that references an Azure Key Vault, then the scope is Azure Key Vault-backed. The actual secrets are stored in Azure Key Vault, and Databricks retrieves them from there when needed.</p> </li> </ol>"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#create-databricks-backed-secret-scope","title":"Create Databricks-backed Secret Scope","text":""},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#pre-requiesites","title":"Pre-requiesites","text":"<p>You may need to install databricks CLI if you don't have it installed already.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#install-databricks-cli","title":"Install Databricks CLI","text":"<p>Databricks-backed secret scopes are created using the Databricks CLI and Secrets API. The #secrets/createScope UI in Databricks is reserved for Azure Key Vault-backed secret scopes. Thus, we'll employ the Databricks CLI for this task:</p> <ol> <li>Ensure you have Python installed on your system.</li> <li>Use pip, Python\u2019s package manager, to install the CLI:    <pre><code>pip install databricks-cli\n</code></pre></li> </ol>"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#log-into-databricks-using-the-cli","title":"Log into Databricks using the CLI","text":"<p>Once the CLI is installed, the next step is to log in to your Databricks workspace:</p> <ol> <li> <p>Open a terminal or command prompt and enter    <pre><code>databricks configure --token\n</code></pre></p> </li> <li> <p>You will be asked to enter Host. Copy the databricks URI from the browser which will be of format, <code>https://&lt;databricks-instance&gt;#workspace</code>.</p> </li> <li> <p>You'll be prompted for a Token. In the Databricks UI's top-right, go to User Settings &gt; Developer Tab &gt; 'Generate new token'. Set a name, validity days, and click 'Generate'. Copy the token and paste in the CLI.</p> <p>Note: The databricks Host and token is saved inside \"C:\\Users\\user-name\\databrickscfg\"</p> </li> </ol> <p></p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#create-secret-scope","title":"Create secret scope","text":"<p>Suppose you want to create a secret scope with the following details:</p> <ul> <li>Scope Name: <code>my-scope</code></li> <li>Backend Type: Databricks-managed (i.e., <code>DATABRICKS</code>)</li> <li>Initial Manage Principal: <code>users</code> (Means, all users. Must for non-premium acts)</li> </ul> <p>With these details, the command will look like:</p> <p><pre><code>databricks secrets create-scope --scope myscope --scope-backend-type DATABRICKS --initial-manage-principal users\n</code></pre> </p> <p>Note: In Databricks, when creating a secret scope, the default assigns <code>MANAGE</code> permission only to the creator. For non-Premium accounts, override this by granting <code>MANAGE</code> permission to all users using <code>--initial-manage-principal users</code> during scope creation.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#create-azure-key-vault-managed-secret-scope","title":"Create Azure-Key-Vault-Managed Secret Scope","text":"<ol> <li> <p>Create a Key Vault: Set up a Key Vault in Azure using the standard way.</p> </li> <li> <p>Get Key Vault Details: Go to the 'Properties' of your Key Vault and make a note of the 'Vault URI' and 'Resource ID'.</p> </li> <li> <p>Open Databricks Secret Scope UI: </p> </li> <li>Go to your Databricks URL.</li> <li> <p>Add <code>#secrets/createScope</code> right after <code>.net</code>. 'S' is caps. Here's how it should look:</p> <p>https://databricks-instance.azuredatabricks.net#secrets/createScope</p> </li> </ol> <p>This is a workaround, as there's no direct link in the UI to this page.</p> <ol> <li>Set Up the Secret Scope:</li> <li>Give it a name under 'Scope'.</li> <li>Pick a 'Manage Principal'.</li> <li>In the 'DNS Name' field, put the 'Vault URI' you copied earlier from Key Vault properties.</li> <li> <p>For 'Resource ID', use the 'Resource ID' you noted down, something like:</p> <p><code>/subscriptions/someid/resourceGroups/resgropname/providers/Microsoft.KeyVault/vaults/keyvaultname</code>  5. Create the Secret Scope: Click 'Create'. After you do, you'll get a confirmation. But keep in mind, once created, you can't see this scope in the Databricks UI. To check it, you'll have to use the <code>databricks secrets list-scopes</code> command in the Databricks CLI.</p> </li> </ol> <p></p> <ol> <li>Check the Secret Scope: Keep in mind, once created, you can't see this scope in the Databricks UI. To check it, you'll have to use the <code>databricks secrets list-scopes</code> command in the Databricks CLI. </li> </ol>"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#working-with-secret-scopes","title":"Working with secret scopes","text":""},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#adding-secrets-to-scope","title":"Adding secrets to scope","text":"<p>Add secrets to Databricks scopes using dbrk CLI. For azure-key-vault-backed secrets, use the Azure portal. Ensure admin role on the azure keyvault, even if you created it. More details in the following sections.</p> <p>Add Secret to Databricks-backed scope:</p> <p>To add run the following command in databricks CLI</p> <pre><code>databricks secrets put --scope &lt;scope-name&gt; --key &lt;key-name&gt; --string-value &lt;your-actual-secret-value&gt;\n</code></pre> <p>If your secret is stored in a file, you can use the following instead:</p> <pre><code>databricks secrets put --scope &lt;scope-name&gt; --key &lt;key-name&gt; --binary-file &lt;path-to-file&gt;\n</code></pre> <p>Add Secret to Azure-Key-Vault-backed scope:</p> <p>Note: To add secrets in Azure Key Vault, you must use the Azure SetSecret REST API or Azure portal UI. <code>Put</code> operation using databricks CLI will NOT work.  <p></p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#listing-secrets-from-scope","title":"Listing Secrets from Scope","text":""},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#list-azure-key-vault-backed-scopes","title":"List Azure-Key-Vault-Backed scopes","text":"<p>To list Azure Key Vault-backed scopes, run <code>dbutils.secrets.list(scope='az-kv-backed-secret-scope-name')</code>, in Azure Databricks. </p> <p>Remember:</p> <ol> <li>Databricks interacts with Azure Key Vault using the AzureDatabricks application identity, not the logged-in user's identity.</li> <li>Grant required role to AzureDatabricks app in Azure Key Vault:</li> <li>Open your Azure Key Vault.</li> <li>Go to Access control(IAM), Role assignments. Click Add Icon. Select Add role assignment.   </li> <li>In the Members tab, select the desired role, e.g. Key Vaults Secrets.User, click next   </li> <li>Now click on Select members and choose AzureDatabricks. Then click Select.   </li> <li>Finally click Review + assign </li> <li>Finally you can run the list command in databricks without issues    </li> </ol>"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#list-databricks-backed-secret-scopes","title":"List Databricks-backed Secret scopes","text":"<p>To list Databricks-backed scopes, run <code>dbutils.secrets.list(scope='databricks-backed-scope-name')</code>, in Databricks. This is relatively simple operation and doesn't require additinal roles in Azure.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#delete-a-secret-scope","title":"Delete a Secret Scope","text":"<p>To delete a specific secret scope:</p> <pre><code>databricks secrets delete-scope --scope &lt;scope-name&gt;\n</code></pre> <p>Replace <code>&lt;scope-name&gt;</code> with the name of the secret scope you wish to delete.</p> <p>Caution: Be certain about deleting a secret scope, as this action cannot be undone and any secrets within the scope will be permanently removed.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#connecting-adls-to-databricks-using-oauth2-with-service-principal-using-azure-backed-secret-scopes","title":"Connecting ADLS to Databricks using OAuth2 with Service Principal Using Azure-backed Secret Scopes","text":""},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#quick-check","title":"Quick check:","text":"<ul> <li>Ensure you have an Azure Key Vault set up.</li> <li>The Azure Key Vault should be configured as the backend of the Databricks Azure-backed Secret Scope you've created.</li> <li>To execute dbutils.secrets.get an App callled AzureDatabricks should have role added to the Azure Keyvault</li> <li>Refer to Common Error section to see common errors for such operations</li> </ul>"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#setting-up-secrets-in-azure-key-vault","title":"Setting Up Secrets in Azure Key Vault:","text":"<ol> <li>Using the steps below add 3 entries in Azure KeyVault:<ul> <li>Navigate to the Azure Key Vault that's linked to the Databricks Secret Scope you created.</li> <li>Under Secrets, click on the '+ Generate/Import' option.</li> <li>Add the following three secret entries:<ol> <li>regappDirectoryID: This is the Directory (Tenant) ID found under the 'Overview' section of your registered application in Azure.</li> <li>regappClientID: This is the Application (Client) ID, also found under the 'Overview' section of your registered app.</li> <li>regappClientSecret: Navigate to Certificates and Secrets in your registered app. Create a New Client Secret and use its value here. </li> </ol> </li> </ul> </li> </ol>"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#code-for-oauth-authentication","title":"Code for OAuth Authentication:","text":"<ol> <li> <p>Code for OAuth Authentication:     To access a file in ADLS, use the following code:</p> </li> </ol>"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#storage_account-saforone-regappclientid-dbutilssecretsgetscopeazbackedscope-keyregappclientid-regappdirectoryid-dbutilssecretsgetscopeazbackedscope-keyregappdirectoryid-regappclientsecret-dbutilssecretsgetscopeazbackedscope-keyregappclientsecret-set-oauth-as-the-authentication-type-for-the-specified-storage-account-sparkconfsetffsazureaccountauthtypestorage_accountdfscorewindowsnet-oauth-define-the-token-provider-type-for-oauth-client-credentials-flow-sparkconfsetffsazureaccountoauthprovidertypestorage_accountdfscorewindowsnet-orgapachehadoopfsazurebfsoauth2clientcredstokenprovider-provide-the-application-client-id-of-the-azure-ad-registered-application-sparkconfsetffsazureaccountoauth2clientidstorage_accountdfscorewindowsnet-regappclientid-set-the-client-secret-of-the-registered-application-sparkconfsetffsazureaccountoauth2clientsecretstorage_accountdfscorewindowsnet-regappclientsecret-specify-the-oauth-20-token-endpoint-sparkconfsetffsazureaccountoauth2clientendpointstorage_accountdfscorewindowsnet-fhttpsloginmicrosoftonlinecomregappdirectoryidoauth2token-for-debugging-retrieve-and-print-the-client-id-configuration-printsparkconfgetffsazureaccountoauth2clientidstorage_accountdfscorewindowsnet","title":"<pre><code>storage_account = \"saforone\"\n\nregappClientID = dbutils.secrets.get(scope=\"azbackedscope\", key=\"regappClientID\")\nregappDirectoryID = dbutils.secrets.get(scope=\"azbackedscope\", key=\"regappDirectoryID\")\nregappClientSecret = dbutils.secrets.get(scope=\"azbackedscope\", key=\"regappClientSecret\")\n\n# Set OAuth as the authentication type for the specified storage account.\nspark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n\n# Define the token provider type for OAuth (client credentials flow).\nspark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n\n# Provide the **Application (Client) ID** of the Azure AD registered application.\nspark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", regappClientID)\n\n# Set the client secret of the registered application.\nspark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", regappClientSecret)\n\n# Specify the **OAuth 2.0 token endpoint**.\nspark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{regappDirectoryID}/oauth2/token\")\n\n# For debugging: retrieve and print the client ID configuration.\nprint(spark.conf.get(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\"))\n</code></pre>","text":""},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#common-errors-and-resolutions","title":"Common Errors And Resolutions","text":""},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#key-vault-access-denied","title":"Key Vault Access Denied","text":"<p>When attempting to add keys to the key vault, one might encounter errors such as:</p> <p>The operation is not allowed by RBAC. </p> <p>Solution: Assign a role like Key Vault Administrator to the logged-in user. This should allow the user to add or remove keys. </p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#deniedwithnovalidrbac","title":"DeniedWithNoValidRBAC","text":"<p>After addressing the first issue, another challenge might arise. When trying to access keys from Databricks using commands like <code>dbutils.secrets.list(scope='foronescope')</code>, errors such as DeniedWithNoValidRBAC and ForbiddenByRbac might appear. </p> <p>Root Cause: Insufficient permission for AzureDatabricks Identity. A hint lies in the error log:</p> <p>Caller: name=AzureDatabricks;appid=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d</p> <p>Solution: Recognize that it's the AzureDatabricks service's identity Azure evaluates, not our group identity. Ensure AzureDatabricks has the right permissions, especially the Admin role.</p> <p>Reference: A related issue can be found in this GitHub discussion.</p> <p>Summary:  1. For AzureDatabricks: Assign the Admin role. 2. Assign the Key Vault Administrator role to the logged-in user</p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.4_Databricks_Secret_Scope.html#-","title":"---","text":""},{"location":"Spark-DataBricks/3.0_Databricks/3.5_Databricks_SQL.html","title":"CREATE TABLE USING","text":"<p>Using this SQL technique, you define the table schema, and create the table directly from the CSV file without switching to PySpark for initial data loading and df creation etc.</p> <pre><code>-- Drop the table if it exists\nDROP TABLE IF EXISTS hollywood;\n\n-- Create an external table from the CSV file\nCREATE TABLE hollywood (\n  movieName STRING,\n  actor STRING\n)\nUSING CSV\nOPTIONS (\n  path '/mnt/movies.csv',\n  header 'true',\n  inferSchema 'true'\n);\n</code></pre>"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands.html","title":"Useful Databricks Magic Commands","text":"Some frequently used magic commands in Databricks Magic Command Description Example %run Runs a Python file or a notebook. <code>%run ./path/to/notebook</code> %sh Executes shell commands on the cluster nodes. <code>%sh ls /dbfs</code> %fs Interacts with the Databricks file system. <code>%fs ls /mnt/data</code> %sql Runs SQL queries. <code>%sql SELECT * FROM table_name</code> %scala Switches the notebook context to Scala. <code>%scala val x = 10</code> %python Switches the notebook context to Python. <code>%python print(\"Hello, Databricks!\")</code> %md Writes markdown text. <code>%md # This is a Markdown Header</code> %r Switches the notebook context to R. <code>%r summary(data_frame)</code> %lsmagic Lists all the available magic commands. <code>%lsmagic</code> %jobs Lists all the running jobs. <code>%jobs</code> %config Sets configuration options for the notebook. <code>%config notebook.display.max_rows=1000</code> %reload Reloads the contents of a module. <code>%reload module_name</code> %pip Installs Python packages. <code>%pip install pandas</code> %load Loads the contents of a file into a cell. <code>%load ./path/to/file.py</code> %matplotlib Sets up the matplotlib backend. <code>%matplotlib inline</code> %who Lists all the variables in the current scope. <code>%who</code> %env Sets environment variables. <code>%env MY_VARIABLE=my_value</code>"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands.html#mounting-and-unmounting-storage","title":"Mounting and Unmounting Storage","text":"Command Example Mount ADLS dbutils.fs.mount(\u00a0\u00a0source = \"adl://.azuredatalakestore.net/\",\u00a0\u00a0mount_point = \"/mnt/\",\u00a0\u00a0extra_configs = {\u00a0\u00a0\u00a0\u00a0\"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\",\u00a0\u00a0\u00a0\u00a0\"dfs.adls.oauth2.client.id\": dbutils.secrets.get(scope = \"\", key = \"client-id\"),\u00a0\u00a0\u00a0\u00a0\"dfs.adls.oauth2.credential\": dbutils.secrets.get(scope = \"\", key = \"client-secret\"),\u00a0\u00a0\u00a0\u00a0\"dfs.adls.oauth2.refresh.url\": \"https://login.microsoftonline.com//oauth2/token\"}) Unmount Storage dbutils.fs.unmount(\"/mnt/\") List Mount Points display(dbutils.fs.mounts())"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands.html#file-operations","title":"File Operations","text":"Command Example Read CSV File df = spark.read.csv(\"/mnt//data/file.csv\", header=True, inferSchema=True)display(df) Write CSV File df.write.mode(\"overwrite\").csv(\"/mnt//output/\") List Files display(dbutils.fs.ls(\"/mnt/\"))"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands.html#secret-management","title":"Secret Management","text":"Command Example Set a Secret databricks secrets create-scope --scope databricks secrets put --scope  --key  --string-value  Get a Secret secret = dbutils.secrets.get(scope = \"\", key = \"\")"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands.html#shell-commands","title":"Shell Commands","text":"Command Example Run Shell Command %sh ls -lh /dbfs/mnt//"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands.html#sql-and-context-switching","title":"SQL and Context Switching","text":"Command Example Run SQL Query %sql SELECT * FROM table_name WHERE column = 'value' Switch to Scala %scala val x = 10 Switch to Python %python print(\"Hello, Databricks!\") Switch to R %r summary(data_frame)"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands.html#package-management","title":"Package Management","text":"Command Example Install Packages %pip install pandas matplotlib"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands.html#environment-and-module-management","title":"Environment and Module Management","text":"Command Example Set Environment Variable %env MY_VARIABLE=my_value Load Python File %load ./scripts/helper.py Reload Module %reload my_module"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands.html#variable-and-configuration-management","title":"Variable and Configuration Management","text":"Command Example List Variables %who Notebook Configuration %config notebook.display.max_rows=1000"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands.html#markdown-and-plotting","title":"Markdown and Plotting","text":"Command Example Write Markdown %md # This is a Markdown HeaderHere is some detailed description. Setup Matplotlib %matplotlib inlineimport matplotlib.pyplot as pltplt.plot([1, 2, 3], [4, 5, 6])plt.show()"},{"location":"Spark-DataBricks/3.0_Databricks/3.6_DatabricksMagicCommands.html#job-management","title":"Job Management","text":"Command Example List Running Jobs %jobs"},{"location":"Spark-DataBricks/3.0_Databricks/3.7_DeltaLake_And_Lakehouse.html","title":"Demystifying Lakehouse and Delta Lake","text":"<p>Many people get confused by terms like Lakehouse architecture, Data Warehouse, and Data Lake. But the concepts are easy to understand. We just have to oversimplify them a little bit to get the hang ot it:</p> <ul> <li>Data Warehouse: A very large SQL database.</li> <li>Data Lake: A cloud-based file system, like Amazon S3 or Google Drive.</li> <li>Data Lakehouse/Lakehouse Architecture: If you store data in a Data Lake in Delta format, it becomes a Data Lakehouse. \"Lakehouse Architecture\" is just a term for this approach.</li> </ul> <p></p>"},{"location":"Spark-DataBricks/3.0_Databricks/3.7_DeltaLake_And_Lakehouse.html#the-special-_delta_log-folder","title":"The special _delta_log folder","text":"<p>Spark knows a table is a Delta table if the <code>_delta_log</code> folder is present. This folder signals to Spark that the directory is a Delta table. If you try to write data in a different format to this directory, Spark will throw an error because it already recognizes it as a Delta table. Without the <code>_delta_log</code> folder, Spark will not treat the directory as a Delta table; it could be just a regular directory or another type of table.</p> <p></p> <p>Delta Lake cheatsheet pdf</p>"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1.html","title":"Real-World Data Management Using Databricks, DBT, and ADF","text":"<p>Let\u2019s explore a setup to manage data using Databricks, DBT (Data Build Tool), Azure Data Lake Storage (ADLS), and Azure Data Factory (ADF) in a real-world scenario.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1.html#scenario-e-commerce-analytics-platform","title":"Scenario: E-Commerce Analytics Platform","text":"<p>Imagine we're working for an e-commerce company, and we need to manage and analyze data related to customer interactions, transactions, and website clicks. Here's how we'd handle the entire process using these modern tools.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1.html#1-data-ingestion-with-azure-data-factory-adf","title":"1. Data Ingestion with Azure Data Factory (ADF)","text":"<p>First, the raw data coming from various sources like transaction logs, customer profiles, and clickstreams needs to be stored in a data lake. Azure Data Factory (ADF) is a great tool for this.</p> <ul> <li>ADF helps me pull data from multiple sources (APIs, databases, files) into Azure Data Lake Storage (ADLS), which acts as my data lake.</li> <li>we can set up pipelines in ADF that will regularly fetch new data and store it in ADLS, keeping the data updated.</li> </ul> <p>For example, a pipeline can be set up to move daily transaction data into ADLS in formats like CSV or JSON.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1.html#2-storing-data-in-azure-data-lake-storage-adls","title":"2. Storing Data in Azure Data Lake Storage (ADLS)","text":"<p>Now, we have raw data being continuously stored in ADLS. This data lake stores both structured (e.g., transaction records) and unstructured data (e.g., user interaction logs).</p> <p>ADLS helps me scale my storage as the amount of data grows, and it is cost-effective for large datasets. In the data lake, files can be stored in directories like:</p> <ul> <li><code>/raw/transactions/</code></li> <li><code>/raw/customer_profiles/</code></li> <li><code>/raw/click_stream/</code></li> </ul> <p>At this point, the data is still raw and needs to be cleaned and transformed for analysis.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1.html#3-transforming-data-using-databricks-and-dbt","title":"3. Transforming Data Using Databricks and DBT","text":"<p>Here comes the power of Databricks and DBT.</p> <ul> <li> <p>Databricks: Databricks provides a unified platform for big data processing. Using Apache Spark on Databricks, we can easily process the raw data stored in ADLS. It integrates well with ADLS, so we can read and write data efficiently.</p> </li> <li> <p>DBT: While Databricks is great for processing, DBT helps me automate and manage the transformation logic. DBT allows me to write SQL queries to transform data into the structure we need for analytics, and it keeps everything version-controlled.</p> </li> </ul> <p>Let\u2019s walk through an example of how we would clean and transform transaction data.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1.html#example-workflow","title":"Example Workflow:","text":"<ol> <li> <p>Reading raw data:    we use Databricks to read raw data from ADLS:    <pre><code>raw_transactions_df = spark.read.csv(\"/mnt/adls/raw/transactions/\")\n</code></pre></p> </li> <li> <p>Writing a DBT model:    In DBT, we create a model that cleans the data. A DBT model is essentially a SQL query that transforms raw data into a usable format:    <pre><code>-- models/cleaned_transactions.sql\nSELECT\n  transaction_id,\n  customer_id,\n  CAST(amount AS DOUBLE) AS amount,\n  status\nFROM raw.transactions\nWHERE status = 'completed'\n</code></pre></p> </li> <li> <p>Running DBT:    we run the DBT model, and DBT handles the transformation in Databricks:    <pre><code>dbt run\n</code></pre>    This creates a new cleaned table, <code>cleaned_transactions</code>, in Delta Lake, which is Databricks\u2019 transactional data storage layer.</p> </li> </ol>"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1.html#4-storing-transformed-data-in-delta-lake","title":"4. Storing Transformed Data in Delta Lake","text":"<p>Delta Lake adds important features like ACID transactions and version control to the data stored in ADLS. So, after transforming the data with DBT, it gets stored as a Delta Table.</p> <p>In this case, my cleaned transaction data might be saved in Delta Lake at:</p> <pre><code>/mnt/adls/delta/cleaned_transactions/\n</code></pre> <p>Delta Lake makes it easy for analysts to query this transformed data and trust its accuracy because of its strong transactional guarantees.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1.html#5-metadata-management-with-hive-metastore","title":"5. Metadata Management with Hive Metastore","text":"<p>The transformed data is registered in the Hive Metastore, which is part of Databricks. This metastore acts like a catalog that tracks where the data is stored and what its structure looks like.</p> <p>For example, the cleaned transaction data is now a table called <code>cleaned_transactions</code>, and Hive Metastore tracks this table so anyone using Databricks can easily query it.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1.html#6-data-governance-with-unity-catalog","title":"6. Data Governance with Unity Catalog","text":"<p>As the amount of data grows and more people start using it, we need to make sure that the data is secure and governed properly. Unity Catalog in Databricks helps with this.</p> <ul> <li>It ensures that the right people have the right access (for example, analysts can only view data, while engineers can edit it).</li> <li>It also tracks the lineage of data, so we know where the data comes from and how it\u2019s been transformed.</li> </ul>"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1.html#conclusion","title":"Conclusion","text":"<p>By combining Databricks, DBT, ADF, and ADLS, we can build a powerful, scalable data management system for my e-commerce platform. Here\u2019s a quick summary of what each tool does:</p> <ul> <li>Azure Data Factory (ADF): Ingests raw data into ADLS.</li> <li>Azure Data Lake Storage (ADLS): Stores raw and transformed data.</li> <li>Databricks: Processes and transforms large datasets using Spark.</li> <li>DBT: Manages SQL-based transformations in Databricks.</li> <li>Delta Lake: Stores transformed data with transactional guarantees.</li> <li>Hive Metastore: Tracks the metadata for all tables and datasets.</li> <li>Unity Catalog: Ensures governance and security.</li> </ul>"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1.html#end-to-end-project-using-databricks-and-unity-catalog","title":"End to end Project using Databricks and Unity Catalog","text":""},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1.html#concepts-covered","title":"Concepts Covered:","text":"<p>We will use Delta Lake format as this is the format recommended for real-time projects. For access control we will use Unity Catalog We will use Spark structured streaming to see how  We will use batch processing mode We will use CI/CD using Azure Devops</p>"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1.html#drawbacks-of-adls","title":"Drawbacks of ADLS","text":"<p>ADLS != Database</p> <p>Relational database is acidic. They store quality data. This quality is can't be guarantted in ADLS</p> <p>Delta lake makes a lakehouse.</p>"},{"location":"Spark-DataBricks/3.0_Databricks/4.8_Databricks_ProjectA1.html#importing-projectdbc-file","title":"Importing project(.dbc) file","text":"<p>It will import all the notebooks and your import will look like this:</p> <p></p>"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts.html","title":"Concepts","text":""},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts.html#apache-hive-architecture","title":"Apache Hive Architecture","text":""},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts.html#what-are-hive-clients","title":"What are Hive Clients?","text":"<p>Hive clients are different ways you can connect to and interact with Hive. They help you run queries and manage data in Hive. Here are the main Hive clients:</p> <p></p>"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts.html#1-command-line-interface-cli","title":"1. Command Line Interface (CLI):","text":"<p>The original Hive CLI allowed users to interact with Hive by typing SQL queries directly into the terminal (CMD or Linux shell). However, the older CLI has been deprecated in favor of Beeline, which connects to HiveServer2. Beeline is now the preferred way to run Hive queries from the command line as it supports multiple users, security (Kerberos authentication), and handles queries in a distributed environment, making it ideal for production.</p> <p>Example:    - If you're using Beeline, you would start a session like this:      <pre><code>beeline -u jdbc:hive2://localhost:10000 -n myuser -p mypassword\n</code></pre>      This command connects to HiveServer2 running on <code>localhost</code> at port <code>10000</code>. You can then run queries like:      <pre><code>SELECT * FROM sales_data;\n</code></pre></p>"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts.html#2-jdbcodbc-clients","title":"2. JDBC/ODBC Clients:","text":"<p>JDBC and ODBC are protocols used to connect applications with Hive. These clients allow various tools and programs to interact with Hive and run SQL queries on the data. JDBC is most commonly used in Java applications, while ODBC is used by tools like Power BI, Excel, and Tableau.</p> <ul> <li>In a Java program, you can connect to Hive using JDBC like this:      <pre><code>import java.sql.Connection;\nimport java.sql.DriverManager;\nimport java.sql.ResultSet;\nimport java.sql.Statement;\n\npublic class HiveJDBCExample {\n    public static void main(String[] args) throws Exception {\n        String url = \"jdbc:hive2://localhost:10000/default\";\n        Connection conn = DriverManager.getConnection(url, \"myuser\", \"mypassword\");\n        Statement stmt = conn.createStatement();\n        ResultSet rs = stmt.executeQuery(\"SELECT * FROM my_table\");\n\n        while (rs.next()) {\n            System.out.println(rs.getString(1));\n        }\n\n        stmt.close();\n        conn.close();\n    }\n}\n</code></pre></li> </ul>"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts.html#3-apache-hue","title":"3. Apache Hue:","text":"<p>Apache Hue is a popular web-based UI for running queries and managing data. It offers a simple interface to write queries, view results, and visualize data without needing the command line. Hue supports multiple Hadoop components like Hive, HBase, and also works with systems like Apache Impala, Presto, SparkSQL, Flink SQL, ElasticSearch, PostgreSQL, Redshift, BigQuery, and more. It\u2019s widely used for its ease of use and visual environment for working with big data.</p>"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts.html#4-thrift-clients","title":"4. Thrift Clients:","text":"<p>Thrift is useful when you want to work with Hive in languages other than Java or use Hive in a broader range of applications. It allows for multi-language support, making Hive more flexible in different ecosystems.</p> <p>Example 1: Python Thrift Client:    - Using the <code>PyHive</code> library, you can connect Python programs to Hive:      <pre><code>from pyhive import hive\n\n# Connect to HiveServer2\nconn = hive.Connection(host='localhost', port=10000, username='myuser')\n\n# Create a cursor object to run SQL queries\ncursor = conn.cursor()\ncursor.execute('SELECT * FROM employees')\n\n# Fetch and print results\nfor row in cursor.fetchall():\n    print(row)\n</code></pre></p>"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts.html#what-is-hive-metastore","title":"What is Hive Metastore?","text":"<p>Hive is a Data Warehouse software. The Hive Metastore is like a database where all the table information of the Hive Warehouse is stored. The actual data of the tables might be stored somewhere else(HDFS, ADLS, S3).</p>"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts.html#hive-metastore-modes","title":"Hive Metastore Modes","text":"<p>Hive Metastore has three installation Mode:</p>"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts.html#1-embedded-metastore-mode","title":"1. Embedded Metastore Mode","text":"<p>This is the default mode that comes with Hive. In this mode, the metastore service and the Hive service run in the same JVM (Java Virtual Machine) and use an embedded Apache Derby database, which is stored on the local file system.</p> <p>Limitation: Only one Hive session can run at a time because only one connection can be made to the embedded Derby database. If you try to open another Hive session, you\u2019ll get an error. This mode is good for testing but not for real-world use. If you need more than one session, you can configure Derby as a network server, but it's still mainly used for testing.</p>"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts.html#2-local-metastore-mode","title":"2. Local Metastore Mode","text":"<p>To solve the problem of only one session, local metastore mode was introduced. In this mode, many Hive sessions can run at the same time, meaning multiple users can use the metastore.</p> <p>This is done by using any JDBC-compliant database, like MySQL, which runs separately from the Hive service. Both Hive and the metastore run in the same JVM, but the database is external. Before starting Hive, you need to add the MySQL JDBC driver to Hive\u2019s lib folder.</p> <p>For MySQL, the <code>javax.jdo.option.ConnectionURL</code> property should be set to <code>jdbc:mysql://host/dbname?createDatabaseIfNotExist=true</code>, and <code>javax.jdo.option.ConnectionDriverName</code> should be set to <code>com.mysql.jdbc.Driver</code>. The JDBC driver JAR file (Connector/J) must be in Hive's classpath by placing it in Hive's lib directory.</p>"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts.html#3-remote-metastore-mode","title":"3. Remote Metastore Mode","text":"<p>In this mode, the metastore runs in its own separate JVM, not in the same JVM as the Hive service. Other processes, like Hive clients or other services, communicate with the metastore using Thrift Network APIs.</p> <p>You can also have multiple metastore servers for higher availability. This setup improves manageability and security because the database can be completely firewalled, and clients don\u2019t need direct access to the database.</p> <p>To use this mode, set the <code>hive.metastore.uris</code> property in Hive's configuration to point to the metastore server\u2019s URI, which looks like <code>thrift://host:port</code>. The port is set by the <code>METASTORE_PORT</code> when starting the metastore server.</p>"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts.html#databases-supported-by-hive","title":"Databases Supported by Hive","text":"<p>Hive supports the following backend databases: - Derby - MySQL - MS SQL Server - Oracle - Postgres</p> <p>Hive clients are categorized into three types:</p> <ol> <li> <p>Thrift Clients The Hive server is based on Apache Thrift so that it can serve the request from a thrift client.</p> </li> <li> <p>JDBC client Hive allows for the Java applications to connect to it using the JDBC driver. JDBC driver uses Thrift to communicate with the Hive Server.</p> </li> <li> <p>ODBC client Hive ODBC driver allows applications based on the ODBC protocol to connect to Hive. Similar to the JDBC driver, the ODBC driver uses Thrift to communicate with the Hive Server.</p> </li> </ol>"},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts.html#apache-hive-services","title":"Apache Hive Services","text":""},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts.html#some-confusion-regarding-hive","title":"Some confusion regarding Hive","text":""},{"location":"Spark-DataBricks/4.0_Hive/Hive_Concepts.html#is-hadoop-and-mapreduce-mandatory-for-hive","title":"Is Hadoop and MapReduce mandatory for Hive?","text":"<p>No, absolutely not. You might be wondering, \"What is Hadoop? I use Databricks and Azure Cloud and have never used any Hadoop components.\"</p> <p>When Hive was first introduced, there weren\u2019t many reliable systems that could store huge amounts of data safely (like with disaster recovery and clustering). At that time, Hadoop was the only system that provided those features, so Hive was closely tied to it. But nowadays, Hive is used with storage systems like S3 or Azure Data Lake Storage (ADLS), so Hive and Hadoop are no longer dependent on each other.</p> <p>As for MapReduce, Spark has fully replaced it because Spark is much better and faster. Hive still supports MapReduce and Tez, but these are optional. You can always use Spark instead.</p> <p>To sum it up, you can have a Hive system without using any Hadoop or MapReduce.</p>"},{"location":"StreamProcessing/1.0_What_Is_Stream_Processing.html","title":"Introduction","text":""},{"location":"StreamProcessing/1.0_What_Is_Stream_Processing.html#what-is-stream-processing","title":"What is Stream Processing?","text":"<p>In today's world, real-time data is everywhere\u2014viral posts, online sales, stock market, card transactions etc. Processing such data as it arrives is called stream processing.</p>"},{"location":"StreamProcessing/1.0_What_Is_Stream_Processing.html#what-are-the-main-items-in-stream-processing","title":"What are the Main Items in Stream Processing?","text":"<ol> <li> <p>Source:    This is where the data starts its journey. For example, data could come from Twitter hashtags, IoT devices, or transaction logs.</p> </li> <li> <p>Broker(Event-Catch-and-Hold Product):    Why is this needed? Because data comes in so fast, there has to be something like a dam to buffer or hold it for a while. This is where tools like Kafka, Azure Event Hubs, Amazon Kinesis come in\u2014they temporarily hold the data to prevent the processing system from getting overwhelmed.</p> </li> <li> <p>Processing Engine:    The next step is processing the data, and for this, you use a processing engine. Think of Spark or Kinesis, the superstars of data engineering. These engines take the buffered data and process it in near real-time.</p> </li> <li> <p>Storage:    Finally, the processed data needs to be stored. This is a no-brainer\u2014it's just plain old storage. Without stream processing, it would store something else. It's not any special or fancy storage, just where the results are kept.</p> </li> </ol>"},{"location":"StreamProcessing/1.0_What_Is_Stream_Processing.html#spark-in-stream-processing","title":"Spark in Stream Processing:","text":"<p>When Spark is used for stream processing, it\u2019s not in its usual mode where it processes static data. Instead, it operates in Spark Streaming mode, where it continuously processes incoming data as it arrives.</p>"},{"location":"StreamProcessing/1.0_What_Is_Stream_Processing.html#real-time-is-it-really-real-time","title":"Real-Time? Is It Really Real-Time?","text":"<p>No, it's technically impossible to have true real-time processing with zero delay. Even if there were no Kafka or Azure Event Hubs to buffer the data between the source and the processing engine, it still couldn't be 0-second real-time. Even light takes some time to travel! So, real-time in stream processing means something so fast that we humans can't catch the delay. But in today's world, factors like latency, network speed, and processing time mean there's always a tiny bit of delay.</p>"},{"location":"StreamProcessing/2.0.1_EventHubs_Vs_Kafka.html","title":"vs Kafka","text":""},{"location":"StreamProcessing/2.0.1_EventHubs_Vs_Kafka.html#azure-event-hubs-vs-kafka-a-quick-comparison","title":"Azure Event Hubs Vs Kafka: A Quick Comparison","text":""},{"location":"StreamProcessing/2.0.1_EventHubs_Vs_Kafka.html#what-is-azure-event-hubs","title":"What is Azure Event Hubs?","text":"<p>Azure Event Hubs is an event ingestion and stream processing service. Sounds complex? Well, it can receive data from millions of events (like posts from a viral hashtag) and process them, then maybe send them to be stored in a database. It\u2019s almost the same as Kafka, the open-source and free product, and also similar to Amazon Kinesis and Google Pub/Sub.</p> <p> I mentioned Event Hubs is similar to Kafka. That means Azure Event Hubs is not Kafka. Kafka is an open-source platform that anyone can set up and run. On the other hand, Azure Event Hubs is a proprietary product by Microsoft. However, Event Hubs offers a Kafka-compatible endpoint, meaning you can run your existing Kafka work without any code changes. </p> <p> Again, Event Hubs is not Kafka. But it supports Kafka natively. \u201cNatively\u201d is the key part. </p>"},{"location":"StreamProcessing/2.0.1_EventHubs_Vs_Kafka.html#azure-event-hubs-vs-kafka","title":"Azure Event Hubs Vs Kafka","text":"<p>Who manages the platform? With Kafka, you have full control since you\u2019re running it yourself. With Azure Event Hubs, Microsoft manages the service (backend servers, etc.), so you don\u2019t have to worry about maintenance or scaling.</p> <p>How easy is it?  Event Hubs is easier if you want a managed service without the hassle of setup. Kafka requires you to handle everything from setup to scaling.</p> <p> Event Hubs:There are no servers, disks, or networks to manage and monitor and no brokers to consider or configure, ever. You create a namespace, which is an endpoint with a fully qualified domain name, and then you create Event Hubs (topics) within that namespace. </p> <p>Integration:  Event Hubs integrates smoothly with other Azure services, making it a good choice if you\u2019re already in the Azure ecosystem.</p>"},{"location":"StreamProcessing/2.0.1_EventHubs_Vs_Kafka.html#why-use-azure-event-hubs","title":"Why Use Azure Event Hubs?","text":"<p>Open-source Kafka on-premises is appealing because you don\u2019t pay for licensing. But it\u2019s not a \u201cset it and forget it\u201d solution. Servers need updates, maintenance, and support. All this is covered if you use Event Hubs. You won\u2019t have to worry about backend servers or patching. And, the product will be supported by Microsoft, meaning you can chase them if something goes wrong. Can you do this with open-source products like Kafka? No, you can\u2019t.</p>"},{"location":"StreamProcessing/2.0.1_EventHubs_Vs_Kafka.html#how-is-auzre-eventhub-setupinstalled","title":"How is Auzre EventHub Setup/Installed?","text":"<p>All this while, I've been saying that Kafka installation is complex. It's not as complex as I've made it seem, though it is still more involved than setting up Event Hubs. For example, in HDInsight, Microsoft provides a ready-made template for Kafka setup, making it as simple as setting up Event Hubs. With HDInsight, everything is done through the browser, with all setup on the cloud. Additionally, there's Confluent Cloud on Azure (and other clouds) that further simplifies the process.</p> <p> Did you know you can have a kind of Azure-Kafka? These are actual Kafka servers, which can be easily set up using Azure HDInsight. This means you don\u2019t have to install the software on-prem or on Docker. It\u2019s like a hybrid solution. You use a bit of Azure (their servers, etc.) but still have actual Kafka. </p> <p>Also, Confluent Company provides Kafka on Azure MarketPlace.</p> <p></p>"},{"location":"StreamProcessing/2.0.1_EventHubs_Vs_Kafka.html#all-installation-methods","title":"All installation methods","text":"Installation Method Details On Your Own Servers Manual Installation Set up Kafka on your own servers when you want full control. Docker Run Kafka inside a Docker container for easy management and portability. Kubernetes Deploy Kafka on a Kubernetes cluster when you need easy scaling. On the Cloud Virtual Machines Install Kafka on cloud VMs when using cloud infrastructure. Managed Services Confluent Cloud Use Confluent Cloud for fully managed Kafka when you want hassle-free management. Amazon MSK Opt for Amazon MSK if you're on AWS and need managed Kafka. Azure Event Hubs Choose Azure Event Hubs for a Kafka-compatible service on Azure. Hybrid Deploy Kafka on both servers and cloud when you need both on-premises and cloud. For Developers (Local Use) Docker Run Kafka locally inside Docker for development and testing. Confluent Platform Use Confluent Platform for an easy local setup with extra tools."},{"location":"StreamProcessing/2.0.1_EventHubs_Vs_Kafka.html#can-you-replace-kafka-fully-with-azure-event-hubs","title":"Can you replace Kafka fully with Azure Event Hubs?","text":"<p>If you have a streaming data source, you can replace Kafka with Azure Event Hubs instead. Event Hubs can handle the ingestion, streaming, and processing of real-time data. You won\u2019t need Kafka at all. Here\u2019s how open-source tools map to Azure services for streaming projects:</p> <ol> <li>Kafka - Azure Event Hubs</li> <li>Kafka Streams - Azure Stream Analytics</li> <li>Kafka Connect - Azure Data Factory</li> </ol> <p>With these, there will not be a need for Kafka in the entire project.Look at the table below, it shows open-source products and their Azure counterpart.</p> Open-Source Tool Azure Equivalent Functionality Kafka (Streaming Data Platform) Azure Event Hubs Can replace Kafka for ingesting and streaming large volumes of real-time data. Kafka Streams (Stream Processing) Azure Stream Analytics Can replace Kafka Streams for real-time data processing and transformation as data flows through Event Hubs. Kafka Connect (Data Integration) Azure Data Factory Can replace Kafka Connect by connecting and transforming data across different sources. Schema Registry (Data Schema Management) Azure Event Grid / Azure Schema Registry Can replace Schema Registry for managing and ensuring compatibility of event schemas. Kafka Topics (Data Segmentation) Event Hubs Partitions Can replace Kafka Topics by segmenting data streams for organization and scaling. Monitoring and Management Azure Monitor / Azure Metrics Can replace custom Kafka monitoring tools for managing and monitoring streaming data."},{"location":"StreamProcessing/2.0.2_Project_Hello_EventHubs.html","title":"Overview","text":""},{"location":"StreamProcessing/2.0.2_Project_Hello_EventHubs.html#components-in-the-project","title":"Components in the project","text":"<p>Data Sources: Streaming data from IoT devices or social media feeds. (Simulated in Event Hubs) Ingestion: Azure Event Hubs for capturing real-time data. Processing: Azure Databricks for stream processing using Structured Streaming. Storage: Processed data stored Azure Data Lake (Delta Format). Visualisation: Data visualized using Power BI.</p> <p>To get started, the first thing would be to create a Event Hubs Service. This service can contain multiple Event Hub.</p> <p>Event Hubs - Can contain multiple Event Hub.</p> <p></p> <p></p>"},{"location":"StreamProcessing/2.0.2_Project_Hello_EventHubs.html#reference-projects","title":"Reference Projects","text":"<p>This project is based on this tutorial. Stream processing with Azure Databricks</p>"},{"location":"StreamProcessing/2.0.3_EventHubsLocalEmulator.html","title":"Azure Event Hubs Local Emulator - End to End","text":""},{"location":"StreamProcessing/2.0.3_EventHubsLocalEmulator.html#introduction","title":"Introduction","text":"<p>Want to learn Azure Event Hubs but don\u2019t have an Azure account or free credits? No credit card? Worried about the charges? No problem!</p> <p>In this article, I\u2019ll show you how to use the Azure Event Hubs Emulator, a setup recently released by Microsoft. It runs two Docker containers to create a fake Azure Event Hubs environment right on your local machine. Once it\u2019s set up, you can develop and learn Event Hubs without needing any Azure connection or login. It\u2019s completely separate from any account, and the best part? It\u2019s all local, so you can run everything even without an internet connection.</p> <p>The setup is pretty easy, they have provided a powershell script which creates the docker setup. After that its just connect-and-code. </p>"},{"location":"StreamProcessing/2.0.3_EventHubsLocalEmulator.html#what-you-need","title":"What You Need","text":"<p>Here\u2019s what you need on your machine:</p> <ul> <li>Docker Desktop: If you don\u2019t have it, grab it from their website.</li> <li>VS Code and Python: You\u2019ll need these, along with Jupyter Notebook and a few plugins. If something doesn\u2019t work, just install the necessary plugin.</li> </ul>"},{"location":"StreamProcessing/2.0.3_EventHubsLocalEmulator.html#setting-it-up","title":"Setting It Up","text":"<ol> <li>Get the Emulator: Head over to the GitHub page and download the zip file.</li> </ol> <ol> <li> <p>Unzip and Get Ready: Unzip the file to a folder on your computer.</p> </li> <li> <p>Start Docker: Launch Docker Desktop.</p> </li> <li> <p>Run the Script:</p> </li> <li>Open PowerShell as an admin (right-click and choose \u201cRun as administrator\u201d).</li> <li>Run this command to allow scripts to run:      <pre><code>Start-Process powershell -Verb RunAs -ArgumentList 'Set-ExecutionPolicy Bypass \u2013Scope CurrentUser'\n</code></pre></li> <li>Navigate to <code>\\EventHub-Emulator\\Scripts\\Windows</code> in the unzipped folder and run:      <pre><code>.\\LaunchEmulator.ps1\n</code></pre></li> <li>You should be able to see two containers running.</li> </ol> <p></p>"},{"location":"StreamProcessing/2.0.3_EventHubsLocalEmulator.html#lets-test-the-fake-local-event-hubs","title":"Lets test the Fake Local Event Hubs","text":"<p>Now that your fake, free, local Azure Event Hubs environment is up and running, let\u2019s see if it actually works:</p> <ul> <li>Check the Logs: In Docker, click on the <code>eventhubs-emulator</code> container to view the logs. This will give you the connection info:</li> </ul> <p></p> <ul> <li>Namespace: <code>emulatorns1</code></li> <li>Event Hub: <code>eh1</code></li> <li>Consumer Groups: <code>cg1</code> and <code>$default</code></li> </ul> <p></p> <ul> <li>Run the Notebook: Instead of cluttering this guide with code, I\u2019ve put everything you need into a Jupyter notebook. Just download it here, and run it cell by cell. The setup is straightforward, and the code should run smoothly.</li> </ul>"},{"location":"StreamProcessing/2.0.3_EventHubsLocalEmulator.html#resources","title":"Resources","text":"<p>Here are a couple of links you might find useful:</p> <ul> <li>GitHub: Azure Event Hubs Emulator Installer</li> <li>Blog: Introducing Local Emulator for Azure Event Hubs</li> </ul> <p> That's it! Now you have your own local setup to practice with Azure Event Hubs without any of the usual hassles. Enjoy! </p> <p>If you have questions, reach out to me at das.d@hotmail.com</p>"},{"location":"StreamProcessing/2.0_Azure_EventHubs.html","title":"What is Azure Event Hubs?","text":"<p>It's like a bridge between Twitter and your Spark cluster. It buffers and forwards the large volume of data that flows from the source to a processing application.</p>"},{"location":"StreamProcessing/2.0_Azure_EventHubs.html#the-confusing-event-hubs-can-stream-statement","title":"The confusing \"Event Hubs can  stream\" statement","text":"<p>As per Microsoft, \"Azure Event Hubs is a cloud-native data streaming service that can stream millions of events.\" Anyone reading this would think Event Hubs streams, i.e., generates some 'live content.' No, it doesn't. Twitter streams. Netflix streams. Azure Event Hubs does not.</p> <p>The term \"stream\" means Event Hubs handles the streams, temporarily stores them for some time, and then sends them to Spark (or something similar). Why? If there is no bridge, city would flood. Hence, without the Azure Event Hubs(bridge)</p> <p>So, \"stream\" here means \"handle and forward\" large volumes of data from the source to the destination, not that Event Hubs is the source of the data itself.</p> <p></p>"},{"location":"StreamProcessing/4_EventProcessingChoices.html","title":"Processing Options","text":""},{"location":"StreamProcessing/4_EventProcessingChoices.html#how-many-different-workflows-can-you-have-for-real-time-data-processing","title":"How many different Workflows can you have for Real-Time Data Processing?","text":"<p>Real-time data processing can be quite confusing because there are so many tools and services. But let\u2019s make it simple. We\u2019ll look at different ways to handle real-time events, like tweets, and see how they work with open-source tools, Azure, Aws, Google Cloud and Databricks. </p>"},{"location":"StreamProcessing/4_EventProcessingChoices.html#open-source-workflow","title":"Open-Source Workflow","text":"<p>In this setup, everything is open-source, giving you full control over the technology stack.</p> <pre><code>[Twitter Users] -&gt; [Kafka (Event Broker)] -&gt; [Apache Flink/Spark Streaming (Event Processing)] -&gt; [Apache Hudi/Iceberg (Storage)]\n</code></pre> <p>Here\u2019s how it works: - Kafka: Acts as the middleman, receiving and managing events (tweets). - Apache Flink/Spark Streaming: These tools process the data in real-time, filtering or enriching tweets as needed. - Apache Hudi/Iceberg: These storage solutions handle storing the processed data, offering features like time travel and ACID transactions, similar to Delta Lake but in the open-source world.</p>"},{"location":"StreamProcessing/4_EventProcessingChoices.html#azure-workflow","title":"Azure Workflow","text":"<p>This workflow leverages Azure\u2019s managed services for a streamlined, cloud-based solution.</p> <pre><code>[Twitter Users] -&gt; [Azure Event Hubs (Event Broker)] -&gt; [Azure Stream Analytics (Event Processing)] -&gt; [Azure Data Lake Storage + Delta Lake (Storage)]\n</code></pre> <p>Here\u2019s the breakdown: - Azure Event Hubs: This is Azure\u2019s version of Kafka, handling real-time data ingestion. - Azure Stream Analytics: Processes the tweets as they come in, performing tasks like filtering and aggregation. - Azure Data Lake Storage + Delta Lake: Stores the processed data in a scalable and efficient way, allowing for further analysis and querying.</p>"},{"location":"StreamProcessing/4_EventProcessingChoices.html#databricks-workflow","title":"Databricks Workflow","text":"<p>This workflow combines the power of Databricks with Delta Lake for advanced analytics and machine learning.</p> <pre><code>[Twitter Users] -&gt; [Kafka (Event Broker)] -&gt; [Databricks (Event Processing &amp; Advanced Analytics)] -&gt; [Delta Lake (Storage with Databricks)]\n</code></pre> <p>Here\u2019s how it functions: - Kafka: As usual, Kafka receives and queues the events. - Databricks: Handles the heavy lifting of real-time processing and advanced analytics, including machine learning if needed. - Delta Lake: Integrated with Databricks, Delta Lake stores the data efficiently, allowing for complex queries and historical data analysis.</p>"},{"location":"StreamProcessing/4_EventProcessingChoices.html#google-cloud-workflow","title":"Google Cloud Workflow","text":"<p>If you\u2019re into Google Cloud, this setup might be right for you.</p> <p><pre><code>[Twitter Users] -&gt; [Google Cloud Pub/Sub (Event Broker)] -&gt; [Google Dataflow (Event Processing)] -&gt; [Google BigQuery (Storage)]\n</code></pre> - Google Cloud Pub/Sub: Like Kafka, but for Google Cloud, handling event distribution. - Google Dataflow: Processes the data using Apache Beam under the hood, perfect for both stream and batch processing. - Google BigQuery: A serverless data warehouse where you can store and analyze all your processed data.</p>"},{"location":"StreamProcessing/4_EventProcessingChoices.html#aws-workflow","title":"AWS Workflow","text":"<p>Here\u2019s a combination using Amazon Web Services for a fully managed experience.</p> <p><pre><code>[Twitter Users] -&gt; [Amazon Kinesis (Event Broker)] -&gt; [AWS Lambda or Kinesis Data Analytics (Event Processing)] -&gt; [Amazon S3 + AWS Glue (Storage &amp; ETL)]\n</code></pre> - Amazon Kinesis: Manages real-time data streams like Kafka. - AWS Lambda or Kinesis Data Analytics: Lambda handles event-driven processing, while Kinesis Data Analytics can process streams using SQL. - Amazon S3 + AWS Glue: S3 stores the data, and Glue can be used for ETL (Extract, Transform, Load) operations and cataloging.</p>"},{"location":"StreamProcessing/4_EventProcessingChoices.html#hybrid-workflow-with-confluent-and-snowflake","title":"Hybrid Workflow with Confluent and Snowflake","text":"<p>This setup combines a managed Kafka service with Snowflake\u2019s cloud data platform.</p> <p><pre><code>[Twitter Users] -&gt; [Confluent Cloud (Kafka as a Service)] -&gt; [Kafka Streams or KSQL (Event Processing)] -&gt; [Snowflake (Storage &amp; Analytics)]\n</code></pre> - Confluent Cloud: A managed Kafka service, making Kafka easier to deploy and scale. - Kafka Streams or KSQL: These tools allow for processing streams directly within Kafka. - Snowflake: A powerful cloud data platform for storing and analyzing your processed data.</p>"},{"location":"StreamProcessing/4_EventProcessingChoices.html#azure-with-databricks-and-synapse-analytics","title":"Azure with Databricks and Synapse Analytics","text":"<p>This combination leverages Azure\u2019s data services for powerful analytics.</p> <p><pre><code>[Twitter Users] -&gt; [Azure Event Hubs (Event Broker)] -&gt; [Databricks (Event Processing &amp; Machine Learning)] -&gt; [Azure Synapse Analytics + Delta Lake (Storage &amp; Data Warehousing)]\n</code></pre> - Azure Event Hubs: Captures the events in real-time. - Databricks: Processes the data and can apply machine learning models. - Azure Synapse Analytics + Delta Lake: Synapse provides advanced analytics and data warehousing, with Delta Lake ensuring reliable storage.</p>"},{"location":"StreamProcessing/4_EventProcessingChoices.html#on-premise-with-cloud-integration","title":"On-Premise with Cloud Integration","text":"<p>If you\u2019re starting on-premise but want to integrate with the cloud, here\u2019s an option:</p> <p><pre><code>[Twitter Users] -&gt; [Kafka (Event Broker)] -&gt; [Apache Flink/Spark Streaming (Event Processing)] -&gt; [On-Premises HDFS + Cloud Storage (e.g., AWS S3 or Azure Data Lake Storage) with Delta Lake (Storage)]\n</code></pre> - Kafka: Manages your events locally. - Apache Flink/Spark Streaming: Processes the events on-premise. - On-Premises HDFS + Cloud Storage: You can store the data locally on HDFS or integrate it with cloud storage services like S3 or Azure Data Lake Storage, using Delta Lake for additional features like ACID transactions.</p>"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration.html","title":"Spark Amazon Kinesis Integration in 7 Steps Using VS Code","text":"<ol> <li>Create a Project in VS Code:</li> <li>Open Visual Studio Code and create a new folder for your project.</li> <li>Initialize your project by creating a <code>build.sbt</code> file (for SBT) or <code>pom.xml</code> (for Maven) to manage dependencies.</li> <li> <p>If you\u2019re using SBT, your <code>build.sbt</code> should include the necessary configurations and dependencies for Spark and Kinesis.</p> </li> <li> <p>Add Spark Kinesis Jars:</p> </li> <li>In your <code>build.sbt</code> or <code>pom.xml</code>, add dependencies for <code>spark-streaming-kinesis-asl</code> and any other necessary libraries.</li> <li>For example, in <code>build.sbt</code>:      <pre><code>libraryDependencies += \"org.apache.spark\" %% \"spark-streaming-kinesis-asl\" % \"2.4.8\"\n</code></pre></li> <li> <p>Run <code>sbt update</code> or <code>mvn install</code> to download the dependencies.</p> </li> <li> <p>Initialize Streaming Context:</p> </li> <li>In your main Scala or Java file, initialize the <code>StreamingContext</code>.</li> <li> <p>Example:      <pre><code>val conf = new SparkConf().setAppName(\"KinesisSparkIntegration\").setMaster(\"local[*]\")\nval ssc = new StreamingContext(conf, Seconds(10))\n</code></pre></p> </li> <li> <p>Initialize Kinesis Utils:</p> </li> <li>Use <code>KinesisUtils.createStream</code> to create a DStream from the Kinesis stream.</li> <li> <p>Example:      <pre><code>val kinesisStream = KinesisUtils.createStream(\n  ssc,\n  \"KinesisAppName\",\n  \"KinesisStreamName\",\n  \"kinesis.us-east-1.amazonaws.com\",\n  \"us-east-1\",\n  InitialPositionInStream.LATEST,\n  Seconds(10),\n  StorageLevel.MEMORY_AND_DISK_2\n)\n</code></pre></p> </li> <li> <p>Byte Array Deserialization:</p> </li> <li>Convert the byte array data from Kinesis into a more usable format, such as a string or JSON.</li> <li> <p>Example:      <pre><code>val stringStream = kinesisStream.map(record =&gt; new String(record))\n</code></pre></p> </li> <li> <p>Print the Stream:</p> </li> <li>Use the <code>print()</code> action to output the contents of the stream for testing and debugging.</li> <li> <p>Example:      <pre><code>stringStream.print()\n</code></pre></p> </li> <li> <p>Start the Stream:</p> </li> <li>Start the streaming context and keep the application running.</li> <li>Example:      <pre><code>ssc.start()\nssc.awaitTermination()\n</code></pre></li> </ol>"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration.html#extra-note","title":"Extra Note","text":"<ul> <li>Extensions: Install the Scala and SBT extensions for VS Code to improve code highlighting, auto-completion, and build management.</li> <li>Terminal: Use the integrated terminal in VS Code to run <code>sbt</code> or <code>mvn</code> commands.</li> <li> <p>Debugging: Configure VS Code to debug Spark jobs by setting up a launch configuration in the <code>launch.json</code> file.</p> </li> <li> <p>Some users use Eclipse or IntelliJ and not VS Code.</p> </li> </ul>"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration.html#some-kinesis-and-kafka-concepts","title":"Some Kinesis and Kafka Concepts","text":""},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration.html#1-streaming-context","title":"1. Streaming Context","text":"<ul> <li>Definition: In Apache Spark, <code>StreamingContext</code> is the main entry point for all Spark Streaming functionality. It is used to define a streaming computation by specifying the sources of streaming data (like Kafka or Kinesis), the transformations to apply to this data, and the output operations.</li> <li>Purpose: It manages the execution of the streaming job, setting the batch interval (how often the data is processed) and initiating the actual processing of data streams.</li> </ul>"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration.html#2-topic-name-stream-name-kafka-topic","title":"2. Topic Name / Stream Name / Kafka Topic","text":"<ul> <li>Topic Name (Kafka): A Kafka Topic is a category or feed name to which records are published. It is the basic building block of Kafka's messaging system. Topics are partitioned and replicated across Kafka brokers for scalability and fault tolerance.</li> <li>Stream Name (Kinesis): Similar to Kafka topics, Stream Name in Amazon Kinesis refers to the name of a data stream where data records are continuously ingested. Data streams in Kinesis are sharded, which is akin to partitions in Kafka.</li> </ul>"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration.html#3-consumption-model","title":"3. Consumption Model","text":"<ul> <li>Definition: The consumption model defines how messages are read from the streaming data source (like Kafka or Kinesis).</li> <li>Earliest: This model instructs the consumer to start reading from the earliest available data in the stream or topic. In Kafka, this means starting from the earliest offset, and in Kinesis, from the beginning of the shard.</li> <li>Latest: This model instructs the consumer to start reading only new messages arriving after the consumer starts, ignoring older messages. In Kafka, this means starting from the latest offset, and in Kinesis, from the end of the shard.</li> </ul>"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration.html#4-consumption-duration","title":"4. Consumption Duration","text":"<ul> <li>Definition: This refers to the period during which data is consumed from the stream. It could be defined as the length of time the consumer stays active, or it could be tied to how long the consumer is configured to keep processing data (e.g., indefinitely, until the end of a batch, or a specified time frame).</li> </ul>"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration.html#5-storage-levels","title":"5. Storage Levels","text":"<ul> <li>Definition: In Spark, storage levels determine how RDDs (Resilient Distributed Datasets) are stored in memory or on disk. These levels help optimize performance by controlling the persistence and redundancy of data.</li> <li> <p>Types of Storage Levels: There are 8 primary storage levels in Spark:</p> <ol> <li>MEMORY_ONLY: Store RDDs in memory only. If the data doesn't fit in memory, it won't be stored.</li> <li>MEMORY_ONLY_SER: Store RDDs in memory in a serialized format. Useful to reduce memory usage.</li> <li>MEMORY_AND_DISK: Store RDDs in memory and spill to disk if memory is insufficient.</li> <li>MEMORY_AND_DISK_SER: Store RDDs in memory in a serialized format, and spill to disk if needed.</li> <li>DISK_ONLY: Store RDDs only on disk.</li> <li>MEMORY_ONLY_2: Same as MEMORY_ONLY but with replication for fault tolerance.</li> <li>MEMORY_AND_DISK_2: Same as MEMORY_AND_DISK but with replication.</li> <li>OFF_HEAP: Store RDDs in off-heap memory (outside the Java heap). Useful for certain memory configurations.</li> </ol> </li> <li> <p>Kinesis Storage Level: When integrating Spark with Kinesis, specifying a storage level is mandatory. Typically, you'd choose a storage level like <code>MEMORY_AND_DISK_2</code> to ensure data resiliency across Spark nodes, given that streaming data is often critical and needs to be preserved even if some nodes fail.</p> </li> </ul>"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration.html#6-kinesis-storage-level-is-mandatory","title":"6. Kinesis Storage Level is Mandatory","text":"<ul> <li>Explanation: When using <code>KinesisUtils.createStream</code> in Spark Streaming, you must specify a storage level. This is to ensure that the data fetched from Kinesis is appropriately cached or stored, allowing for fault tolerance and reprocessing if necessary. The choice of storage level impacts how the data is handled, either being kept in memory, disk, or both, depending on your setup.</li> </ul>"},{"location":"StreamProcessing/5_AmazonKinesisSparkIntegration.html#summary","title":"Summary","text":"<ul> <li>Streaming Context: Main entry point for Spark Streaming.</li> <li>Topic Name/Stream Name: Identifiers for data streams in Kafka (Topic) or Kinesis (Stream Name).</li> <li>Consumption Model: Determines how messages are consumed (Earliest or Latest).</li> <li>Consumption Duration: Time frame or condition for consuming data.</li> <li>Storage Levels: Defines how RDDs are persisted in Spark, with 8 levels available.</li> <li>Kinesis Storage Level: Must be specified when integrating Kinesis with Spark to ensure data is stored properly.</li> </ul>"},{"location":"Synapse-ADF/1.0_SynapseConcepts.html","title":"Synapse Analytics Core Concepts","text":"<p>Here are the building blocks of Azure Synapse Analytics</p>"},{"location":"Synapse-ADF/1.0_SynapseConcepts.html#synapse-workspace","title":"Synapse Workspace","text":"<p>It's the main portal of Synapse. It is present in a particular region. It has a ADLS G2 account linked and a folder there. It is always under a resource group.</p> <p> A Synapse workspace must be connected to a storage account (ADLS Gen2) and a file system (container inside that ADLS Gen2). Synapse will ask you to choose/create a new ADLS account and a container inside it </p> <p></p> <p>To see the storage account linked to your Synapase workspace, go to Data Tab then Linked Tab. </p>"},{"location":"Synapse-ADF/1.0_SynapseConcepts.html#linked-services","title":"Linked Services","text":"<p> Linked services in Synapase and ADF are just cconnection strings. </p> <p></p> <p>Go to Manage Tab, External connections contains Linked Services section in Synapase and In Azure Data Factory to open Linked Services screen.</p> <p></p> <p>The link services window makes it super easy to create connection strings. Now, you just have to select the irmspa</p> <p></p>"},{"location":"Synapse-ADF/1.0_SynapseConcepts.html#synapse-sql-pools","title":"Synapse SQL Pools","text":"<p>Don't get confused with the term <code>pool</code>. Synapse gives you two SQL products:</p> <ol> <li>A serverless MSSQL database</li> <li>A dedicated MSSQL database.</li> </ol> <p>For serverless, all characters are fictitious. It's not like the old-school MSSQL where data stays inside in SQL's own format. For serverless, it's mainly data stored in ADLS folders.</p> <p>Question: So, serverless is just a query engine with no actual tables, master db, etc., like MSSQL?</p> <p>Answer: Yes, it has master db, tables, views, schemas, etc., but all the tables and databases there are fictitious. They are made-up showpieces derived from ADLS files.</p> <p>For instance, if you create a database using the serverless SQL pool:</p> <p></p> <p>This is what you will see. Notice that everything is just a shell. The data is external:</p> <p></p> <p>However, with the dedicated MSSQL, everything is real and traditional. It is a SQL warehouse. Hence, it old name was SQL Data Warehouse. The Dedicated pool is just a fancy name.</p>"},{"location":"Synapse-ADF/1.0_SynapseConcepts.html#serverless-sql-pool","title":"Serverless SQL Pool","text":"<p>The only thing real here is the SQL query engine. All data are fictiocious.</p> <ul> <li>Just a query running engine. All data is external.</li> <li>On-demand: Only pay for the queries you run. It stays online, but don\u2019t worry. You don\u2019t pay anything until you run something.</li> <li>Doesn\u2019t have its own storage: Doesn\u2019t store anything. It only runs queries in ADLS, etc.</li> <li>Cheap: Very cheap. $0 if you don\u2019t run a single SQL query.</li> </ul> <p></p>"},{"location":"Synapse-ADF/1.0_SynapseConcepts.html#dedicated-pool-aka-sql-dw","title":"Dedicated Pool (AKA SQL DW)","text":"<p>A full SQL Warehouse (a large SQL server) that you own. This means there is a traditional, old-school SQL database with real, dedicated storage, not just some abstract storage solution using ADLS (no insults to Serverless Pool ;-). It\u2019s the poor man\u2019s engine).</p> <ul> <li>Full-blown SQL Warehouse: Just a few years ago, it was called SQL Data Warehouse.</li> <li>Own local storage, not international calls to ADLS: It has its own storage, just like SQL Server. No, it\u2019s not ADLS; it\u2019s real SQL storage.</li> <li>Once on, you pay by the hour: Since it\u2019s dedicated, Microsoft covers the hardware costs for your dedicated SQL server. Whether you use it or not, you pay by the hour, and it\u2019s quite expensive. Run it for a day, and your full trial subscription might be gone.</li> </ul> <p></p>"},{"location":"Synapse-ADF/1.0_SynapseConcepts.html#synapse-spark-pool","title":"Synapse Spark Pool","text":"<p>Synapse gives you a ready-to-use Apache Spark environment called a Serverless Spark Pool. It's like the Serverless SQL Pool, meaning you only pay when you use it. In the background, it's a Spark cluster, but it's called a Pool. In Databricks, you create a Spark cluster for a Spark environment. In Synapse, you create a Spark Pool. The end result is the same.</p> <p>Points to Remember: - It's just an Apache Spark cluster behind the scenes. - Use it to run Spark jobs or Spark queries. - You can write Spark logic in PySpark, SparkSQL, Scala, or even C#. - There are two ways to run Spark code in Synapse:   - Spark Notebooks: Like Jupyter notebooks.   - Spark Job Definitions: For running batch Spark jobs using jar files.</p> <p></p>"},{"location":"Synapse-ADF/1.0_SynapseConcepts.html#pipelines","title":"Pipelines","text":"<p>Pipelines in Synapse are the same as Azure Data Factory Pipelines. They are 100% identical.</p> <p>An activity is a task you want to perform. A pipeline is a group of activities, like copying data or running a notebook.</p> <p>Points to Remember: - Syapse Pipelines is actually ADF. - Pipelines are collections of activities (tasks like copying data, running a notebook, etc.). - Data Flows: A type of activity that lets you create transformations without code using graphics. It runs on Spark behind the scenes. - Triggers: To execute pipelines on a schedule.</p> <p></p>"},{"location":"Synapse-ADF/1.0_SynapseConcepts.html#integration-datasets","title":"Integration Datasets","text":"<p>These are pointers to your data. They are required when you create an activity like a Copy activity.</p> <p></p> <ul> <li>Tell me where the data is? In a blob storage.</li> <li>What is the format? CSV.</li> <li>Tell me the connection string (Linked service) to that file/folder.</li> </ul> <p>You create it from the Data section in Synapse.</p> <p></p>"},{"location":"Synapse-ADF/1.0_SynapseConcepts.html#integration-runtime","title":"Integration runtime","text":"<p>Copying data is one of the main activities in Synapse and ADF. The main tool used for this is Integration Runtime. It's like a bridge that connects Synapse and ADF with data from on-premises and external sources.</p> <p>There are 3 types of Integration Runtime:</p> <ol> <li>Azure Integration Runtime: This is used to copy data within Azure or between cloud services.</li> <li>Self-hosted Integration Runtime: This is the bridge to copy data from your local machine to the Azure cloud. It is a software you install on your local computer.</li> <li>Azure-SSIS Integration Runtime: This allows you to lift and shift your SSIS packages to Azure.</li> </ol>"},{"location":"Synapse-ADF/1.0_SynapseConcepts.html#the-databases-types-in-synapse","title":"The Databases Types in Synapse","text":"<p>Before we dive into this, let's ask ourselves: How many types of pools are there in Synapse? Even though I don't like the term \"pool\" (and I can't use \"server\"), we have to use it. There are three types: Serverless SQL, Dedicated SQL, and Serverless Spark pool.</p> <p>So, it's simple\u2014there are three types of databases, one for each:</p> <ol> <li>Serverless SQL Database: Created using the Serverless SQL pool.</li> <li>Dedicated SQL Database: Created using the Dedicated SQL pool. Note: This is essentially a data warehouse.</li> <li>Spark Database:</li> <li>v1 [Lake Database]: Created using a serverless Spark pool and PySpark, called a Spark [Lake] database.</li> <li>v2 [Delta Lake Database/Lakehouse]: Similar to v1, but the format of the .parquet files is Delta.</li> </ol> <p>So, does Apache Spark have database connections? Yes, of course. How else would it run SparkSQL and what about that magic command %%sql? It has robust database capabilities.</p>"},{"location":"Synapse-ADF/1.1_Pools.html","title":"Pools","text":""},{"location":"Synapse-ADF/1.1_Pools.html#use-azure-synapse-serverless-sql-pool-to-query-files-in-a-data-lake","title":"Use Azure Synapse Serverless SQL Pool to Query Files in a Data Lake","text":""},{"location":"Synapse-ADF/1.1_Pools.html#what-are-serverless-sql-pools-and-what-they-are-capable-of","title":"What are Serverless SQL pools and what they are capable of?","text":"<p>Serverless SQL pools are less expensive SQL databases. Compared to Dedicated SQL pool which is more expensive and you have to pay it if you use it or not. Serverless sql pools are good for on-demand data query.</p>"},{"location":"Synapse-ADF/1.1_Pools.html#query-csv-json-and-parquet-files-using-a-serverless-sql-pool","title":"Query CSV, JSON, and Parquet Files Using a Serverless SQL Pool","text":"<p>Querying CSV Files: <pre><code>SELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.csv',\n    FORMAT = 'csv',\n    PARSER_VERSION = '2.0'\n) WITH (\n    product_id INT,\n    product_name VARCHAR(20) COLLATE Latin1_General_100_BIN2_UTF8,\n    list_price DECIMAL(5,2)\n) AS rows\n</code></pre></p> <p>Querying JSON Files: <pre><code>SELECT doc\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.json',\n    FORMAT = 'csv',\n    FIELDTERMINATOR ='0x0b',\n    FIELDQUOTE = '0x0b',\n    ROWTERMINATOR = '0x0b'\n) WITH (doc NVARCHAR(MAX)) as rows\n</code></pre></p> <p>Querying Parquet Files: <pre><code>SELECT *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/orders/year=*/month=*/*.*',\n    FORMAT = 'parquet'\n) AS orders\nWHERE orders.filepath(1) = '2020'\n  AND orders.filepath(2) IN ('1','2');\n</code></pre></p>"},{"location":"Synapse-ADF/1.1_Pools.html#create-external-database-objects-in-a-serverless-sql-pool","title":"Create External Database Objects in a Serverless SQL Pool","text":"<p>Creating a Database: <pre><code>CREATE DATABASE SalesDB\n    COLLATE Latin1_General_100_BIN2_UTF8\n</code></pre></p> <p>Creating an External Data Source: <pre><code>CREATE EXTERNAL DATA SOURCE files\nWITH (\n    LOCATION = 'https://mydatalake.blob.core.windows.net/data/files/'\n)\n</code></pre></p> <p>Using External Data Source in Queries: <pre><code>SELECT *\nFROM OPENROWSET(\n    BULK 'orders/*.csv',\n    DATA_SOURCE = 'files',\n    FORMAT = 'csv',\n    PARSER_VERSION = '2.0'\n) AS orders\n</code></pre></p> <p>Creating a Database Scoped Credential: <pre><code>CREATE DATABASE SCOPED CREDENTIAL sqlcred\nWITH\n    IDENTITY = 'SHARED ACCESS SIGNATURE',\n    SECRET = 'sv=xxx...';\nGO\n\nCREATE EXTERNAL DATA SOURCE secureFiles\nWITH (\n    LOCATION = 'https://mydatalake.blob.core.windows.net/data/secureFiles/',\n    CREDENTIAL = sqlcred\n);\nGO\n</code></pre></p> <p>Creating an External File Format: <pre><code>CREATE EXTERNAL FILE FORMAT CsvFormat\nWITH (\n    FORMAT_TYPE = DELIMITEDTEXT,\n    FORMAT_OPTIONS (\n        FIELD_TERMINATOR = ',',\n        STRING_DELIMITER = '\"'\n    )\n);\nGO\n</code></pre></p> <p>Creating an External Table: <pre><code>CREATE EXTERNAL TABLE dbo.products\n(\n    product_id INT,\n    product_name VARCHAR(20),\n    list_price DECIMAL(5,2)\n)\nWITH\n(\n    DATA_SOURCE = files,\n    LOCATION = 'products/*.csv',\n    FILE_FORMAT = CsvFormat\n);\nGO\n\n-- Query the table\nSELECT * FROM dbo.products;\n</code></pre></p>"},{"location":"Synapse-ADF/1.1_Pools.html#how-to-transform-data-using-a-serverless-sql-pool","title":"How to transform data using a serverless SQL pool","text":""},{"location":"Synapse-ADF/1.1_Pools.html#use-a-create-external-table-as-select-cetas-statement-to-transform-data","title":"Use a CREATE EXTERNAL TABLE AS SELECT (CETAS) Statement to Transform Data","text":"<p>Steps: 1. Create External Data Source: <pre><code>CREATE EXTERNAL DATA SOURCE files\nWITH (\n    LOCATION = 'https://mydatalake.blob.core.windows.net/data/files/',\n    TYPE = BLOB_STORAGE,  -- For serverless SQL pool\n    CREDENTIAL = storageCred\n);\n</code></pre></p> <ol> <li> <p>Create Database Scoped Credential: <pre><code>CREATE DATABASE SCOPED CREDENTIAL storagekeycred\nWITH\n    IDENTITY='SHARED ACCESS SIGNATURE',  \n    SECRET = 'sv=xxx...';\n\nCREATE EXTERNAL DATA SOURCE secureFiles\nWITH (\n    LOCATION = 'https://mydatalake.blob.core.windows.net/data/secureFiles/',\n    CREDENTIAL = storagekeycred\n);\n</code></pre></p> </li> <li> <p>Create External File Format: <pre><code>CREATE EXTERNAL FILE FORMAT ParquetFormat\nWITH (\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n);\n</code></pre></p> </li> <li> <p>Use CETAS to Transform Data: <pre><code>CREATE EXTERNAL TABLE SpecialOrders\n    WITH (\n        LOCATION = 'special_orders/',\n        DATA_SOURCE = files,\n        FILE_FORMAT = ParquetFormat\n    )\nAS\nSELECT OrderID, CustomerName, OrderTotal\nFROM\n    OPENROWSET(\n        BULK 'sales_orders/*.csv',\n        DATA_SOURCE = 'files',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0',\n        HEADER_ROW = TRUE\n    ) AS source_data\nWHERE OrderType = 'Special Order';\n</code></pre></p> </li> </ol>"},{"location":"Synapse-ADF/1.1_Pools.html#encapsulate-a-cetas-statement-in-a-stored-procedure","title":"Encapsulate a CETAS Statement in a Stored Procedure","text":"<p>Benefits: - Reduces network traffic. - Provides a security boundary. - Eases maintenance. - Improves performance.</p> <p>Example Stored Procedure: <pre><code>CREATE PROCEDURE usp_special_orders_by_year @order_year INT\nAS\nBEGIN\n    -- Drop the table if it already exists\n    IF EXISTS (\n            SELECT * FROM sys.external_tables\n            WHERE name = 'SpecialOrders'\n        )\n        DROP EXTERNAL TABLE SpecialOrders\n\n    -- Create external table with special orders\n    -- from the specified year\n    CREATE EXTERNAL TABLE SpecialOrders\n        WITH (\n            LOCATION = 'special_orders/',\n            DATA_SOURCE = files,\n            FILE_FORMAT = ParquetFormat\n        )\n    AS\n    SELECT OrderID, CustomerName, OrderTotal\n    FROM\n        OPENROWSET(\n            BULK 'sales_orders/*.csv',\n            DATA_SOURCE = 'files',\n            FORMAT = 'CSV',\n            PARSER_VERSION = '2.0',\n            HEADER_ROW = TRUE\n        ) AS source_data\n    WHERE OrderType = 'Special Order'\n    AND YEAR(OrderDate) = @order_year;\nEND\n</code></pre></p>"},{"location":"Synapse-ADF/1.1_Pools.html#include-a-data-transformation-stored-procedure-in-a-pipeline","title":"Include a Data Transformation Stored Procedure in a Pipeline","text":"<p>Pipeline Activities: 1. Delete Activity: Deletes the target folder for the transformed data in the data lake if it already exists. 2. Stored Procedure Activity: Connects to your serverless SQL pool and runs the stored procedure that encapsulates your CETAS operation.</p> <p>Example Pipeline Steps: - Delete Target Folder:     - Use a Delete activity to remove the existing folder. - Run Stored Procedure:     - Use a Stored procedure activity to execute the <code>usp_special_orders_by_year</code> procedure.</p> <p>Benefits: - Schedules operations to run at specific times or based on events (e.g., new files added to the source storage location).</p>"},{"location":"Synapse-ADF/1.1_Pools.html#create-a-lake-database-in-azure-synapse-analytics","title":"Create a Lake Database in Azure Synapse Analytics","text":""},{"location":"Synapse-ADF/1.1_Pools.html#understand-lake-database-concepts-and-components","title":"Understand Lake Database Concepts and Components","text":"<p>Traditional Relational Database: - Schema composed of tables, views, and other objects. - Tables define entities with attributes as columns, enforcing data types, nullability, key uniqueness, and referential integrity. - Data is tightly coupled with table definitions, requiring all manipulations to be through the database system.</p> <p>Data Lake: - No fixed schema; data stored in structured, semi-structured, or unstructured files. - Analysts can work directly with files using various tools, without relational database constraints.</p> <p>Lake Database: - Provides a relational metadata layer over files in a data lake. - Includes definitions for tables, column names, data types, and relationships. - Tables reference files in the data lake, allowing SQL queries and relational semantics. - Data storage is decoupled from the database schema, offering more flexibility.</p> <p>Components: - Schema: Define tables and relationships using data modeling principles. - Storage: Data stored in Parquet or CSV files in the data lake, managed independently of database tables. - Compute: Use serverless SQL pools or Apache Spark pools to query and manipulate data.</p>"},{"location":"Synapse-ADF/1.1_Pools.html#describe-database-templates-in-azure-synapse-analytics","title":"Describe Database Templates in Azure Synapse Analytics","text":"<p>Lake Database Designer: - Start with a new lake database on the Data page. - Choose a template from the gallery or start with a blank database. - Add and customize tables using the visual database designer interface.</p> <p>Creating Tables: - Specify the type and location of files for storing underlying data. - Create tables from existing files in the data lake. - Store database files in a consistent format within the same root folder.</p> <p>Database Designer Interface: - Drag-and-drop surface for editing tables and relationships. - Specify names, storage settings, key usage, nullability, and data types for columns. - Define relationships between key columns in tables. - Publish the database when the schema is ready for use.</p>"},{"location":"Synapse-ADF/1.1_Pools.html#create-a-lake-database","title":"Create a Lake Database","text":"<p>Steps: 1. Create a Lake Database:     - Use the lake database designer in Azure Synapse Studio.     - Add a new lake database on the Data page.     - Select a template or start with a blank database.     - Add and customize tables using the visual interface.</p> <ol> <li> <p>Specify Table Settings:</p> <ul> <li>Define the type and location of files for data storage.</li> <li>Create tables from existing files in the data lake.</li> <li>Ensure all database files are in a consistent format within the same root folder.</li> </ul> </li> <li> <p>Database Designer Features:</p> <ul> <li>Drag-and-drop interface to edit tables and relationships.</li> <li>Define schema by specifying table names, storage settings, columns, and relationships.</li> <li>Publish the database to start using it.</li> </ul> </li> </ol>"},{"location":"Synapse-ADF/1.1_Pools.html#use-a-lake-database","title":"Use a Lake Database","text":"<p>Using a Serverless SQL Pool: - Query lake database tables using a serverless SQL pool in a SQL script. - Example:     <pre><code>USE RetailDB;\nGO\n\nSELECT CustomerID, FirstName, LastName\nFROM Customer\nORDER BY LastName;\n</code></pre> - No need for OPENROWSET function; the serverless SQL pool handles file mapping.</p> <p>Using an Apache Spark Pool: - Work with lake database tables using Spark SQL in an Apache Spark pool. - Example to insert a new record:     <pre><code>%%sql\nINSERT INTO `RetailDB`.`Customer` VALUES (123, 'John', 'Yang')\n</code></pre> - Example to query the table:     <pre><code>%%sql\nSELECT * FROM `RetailDB`.`Customer` WHERE CustomerID = 123\n</code></pre></p>"},{"location":"Synapse-ADF/1.1_Pools.html#transform-data-with-spark-in-azure-synapse-analytics","title":"Transform Data with Spark in Azure Synapse Analytics","text":""},{"location":"Synapse-ADF/1.1_Pools.html#use-apache-spark-to-modify-and-save-dataframes","title":"Use Apache Spark to Modify and Save Dataframes","text":"<p>Modify and Save Dataframes: - Load Data: Use <code>spark.read</code> to load data into a dataframe.     <pre><code>order_details = spark.read.csv('/orders/*.csv', header=True, inferSchema=True)\ndisplay(order_details.limit(5))\n</code></pre></p> <ul> <li> <p>Transform Data: Use dataframe methods and Spark functions for transformations.     <pre><code>from pyspark.sql.functions import split, col\n\n# Create new columns and remove the original column\ntransformed_df = order_details.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\ntransformed_df = transformed_df.drop(\"CustomerName\")\n\ndisplay(transformed_df.limit(5))\n</code></pre></p> </li> <li> <p>Save Data: Save the transformed dataframe to the data lake.     <pre><code>transformed_df.write.mode(\"overwrite\").parquet('/transformed_data/orders.parquet')\nprint(\"Transformed data saved!\")\n</code></pre></p> </li> </ul>"},{"location":"Synapse-ADF/1.1_Pools.html#partition-data-files-for-improved-performance-and-scalability","title":"Partition Data Files for Improved Performance and Scalability","text":"<p>Partitioning Data: - Create Derived Field and Partition Data: <pre><code>from pyspark.sql.functions import year, col\n\n# Load source data\ndf = spark.read.csv('/orders/*.csv', header=True, inferSchema=True)\n\n# Add Year column\ndated_df = df.withColumn(\"Year\", year(col(\"OrderDate\")))\n\n# Partition by year\ndated_df.write.partitionBy(\"Year\").mode(\"overwrite\").parquet(\"/data\")\n</code></pre></p> <p>Filtering Partitioned Data: - Read Partitioned Data: <pre><code>orders_2020 = spark.read.parquet('/partitioned_data/Year=2020')\ndisplay(orders_2020.limit(5))\n</code></pre></p>"},{"location":"Synapse-ADF/1.1_Pools.html#transform-data-with-sql","title":"Transform Data with SQL","text":"<p>Define Tables and Views: - Save Dataframe as an External Table: <pre><code>order_details.write.saveAsTable('sales_orders', format='parquet', mode='overwrite', path='/sales_orders_table')\n</code></pre></p> <p>Query and Transform Data Using SQL: - Create Derived Columns and Save Results: <pre><code># Create derived columns\nsql_transform = spark.sql(\"SELECT *, YEAR(OrderDate) AS Year, MONTH(OrderDate) AS Month FROM sales_orders\")\n\n# Save the results\nsql_transform.write.partitionBy(\"Year\",\"Month\").saveAsTable('transformed_orders', format='parquet', mode='overwrite', path='/transformed_orders_table')\n</code></pre></p> <p>Query the Metastore: - Query the Transformed Data: <pre><code>%%sql\n\nSELECT * FROM transformed_orders\nWHERE Year = 2021\n    AND Month = 1\n</code></pre></p> <p>Drop Tables: - Clean Up Metastore: <pre><code>%%sql\n\nDROP TABLE transformed_orders;\nDROP TABLE sales_orders;\n</code></pre></p>"},{"location":"Synapse-ADF/1.1_Pools.html#background","title":"Background","text":"<p>Here we will see how to handle data usig Spark in Synapse.</p>"},{"location":"Synapse-ADF/1.1_Pools.html#lets-get-started","title":"Let's get started","text":"<p>First thing we would need is a Spark Pool. The serverless Spark is similar to Serverless SQL Pool. That means, only when it is working you need to pay money. No need to worry when you are not proccessing anything. You wont be charged a penny. The spark pool decides the number of nodes in the actual spark cluster in the background. If you want to work with Spark, you must create a Serverless Spark pool.</p> <p>In Azure Databricks and in general Spark enviorments you create a spark cluster. But, in synapse you create a Serverless Spark Pool. This pool manages the Spark cluster for you.</p> <p></p> <p>It will ahve auto pause etc. </p>"},{"location":"Synapse-ADF/1.1_Pools.html#background_1","title":"Background","text":"<p>Here I will show you how to run query using Serverless SQL pool. Every SA workspae comes with a built-in serverless SQL pool. Its just an engine to run your SQL queries with no own storage. Just an engine.</p> <p>Its built-in/serverless/online/Auto</p> <p></p>"},{"location":"Synapse-ADF/1.1_Pools.html#using-serverless-sql-pool","title":"Using Serverless SQL Pool","text":""},{"location":"Synapse-ADF/1.1_Pools.html#lets-get-started_1","title":"Let's get started","text":"<p>Firs let's upload some data. We know every SA workspace is connected to a default ADLS folder. Let's upload a csv file to it.</p> <p></p>"},{"location":"Synapse-ADF/1.1_Pools.html#lets-run-the-script","title":"Let's run the script","text":"<pre><code>SELECT *\n\nFROM\nOPENROWSET( \n    BULK 'abfss://contusendel@adlsusendel.dfs.core.windows.net/customers-100.csv',\n    FORMAT = 'csv',\n    HEADER_ROW = TRUE,\n    PARSER_VERSION = '2.0'\n)AS [result]\n</code></pre> <p><pre><code>SELECT Country, Count(*)\nFROM\nOPENROWSET( \n    BULK 'abfss://contusendel@adlsusendel.dfs.core.windows.net/customers-100.csv',\n    FORMAT = 'csv',\n    HEADER_ROW = TRUE,\n    PARSER_VERSION = '2.0'\n)AS [result]\n\nGROUP by Country\n</code></pre> </p>"},{"location":"Synapse-ADF/1.1_Pools.html#using-dedicated-sql-pool","title":"Using Dedicated SQL Pool","text":""},{"location":"Synapse-ADF/1.3_ETL%20Pipelines.html","title":"Pipelines","text":"Pipeline = Workflow = Collection of Activities"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines.html#activities","title":"Activities","text":"Activities = Steps/Tasks in a Pipeline"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines.html#copy-data-activity","title":"Copy data Activity","text":"<p>Copy data step/task copies data from here to there in the cloud. It's a <code>data import tool</code>. This is one of the most important activities in pipelines. Some projects have only this step doing all the work.</p> <p></p>"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines.html#copy-data-tool","title":"Copy data tool","text":"<p>The Copy data tool is a GUI to make the Copy data activity easier. Using this GUI you can create a pipeline using just Next, Next. It has two type of tasks.</p> <p></p>"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines.html#built-in-copy-task","title":"Built-in copy task","text":"<p>This method is enough for almost all cases. It's the original method. Just click 'next' using the tool, and ADF will take care of everything else.</p> <p>But, this is suitable for situations where we have fixed sources and fixed destinations.</p> <p>Example: Copying Data between two Azure SQL Databases. Azure Blob storage to Azure SQL Databases etc.</p>"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines.html#metadata-driven-copy-task","title":"Metadata-driven copy task","text":"<p>This method is for very large and complex situations. For example, copying hundreds of tables from an on-prem SQL Server to Azure Data Lake Storage (ADLS), with the list of tables and their destinations managed in a control table.</p> <p>Example: Imagine you have an on-premises SQL Server with hundreds of tables. You want to copy these tables to ADLS. Instead of creating individual copy tasks for each table, you create a control table that lists all the source tables and their corresponding destinations. ADF then uses this control table to dynamically copy each table to the specified destination.</p>"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines.html#data-flow-activity","title":"Data flow Activity","text":"<p>This is also the most important activity in Pipelines. Using this you create the transformation work.</p> <p>There are two types of Data flow activities:</p> <ul> <li>Mapping Data Flows: This is the common data flow activity which we usually use. Here we use a  GUI to create data transformation steps. Azure runs these on a behind-the-scenes Spark cluster managed by Azure. Learn more.</li> <li>Wrangling Data Flows: Use Excel-like Power Query for data preparation, integrating with Power Query Online and using Spark for execution.</li> </ul> <p>Now, if you want to bypass ADF and do your own coding, manage your own env etc you you can use these activities.</p> <ul> <li> <p>HDInsight Activities: Use Hive, Pig, MapReduce, or Spark on your HDInsight cluster in Azure.</p> </li> <li> <p>Databricks Activities:</p> </li> <li>Notebook Activity</li> <li>Jar Activity</li> <li> <p>Python Activity</p> </li> <li> <p>Custom Activity: Execute custom code or scripts.</p> </li> </ul> <p>Note: Data Flow activities run on Apache Spark clusters. Microsoft makes things easy by adding a GUI, but Dataflows are essentially Spark activities.</p>"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines.html#other-activities","title":"Other Activities","text":"<ul> <li>Azure ML Studio (Classic) Activity: Execute machine learning pipelines.</li> <li>Stored Procedure Activity: Execute a stored procedure in a database.</li> <li>Custom Activity: Execute custom code or scripts.</li> </ul>"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines.html#control-flow-activities","title":"Control Flow Activities","text":"<p>These activities control the execution flow of a pipeline.</p> <ul> <li>If Condition Activity: Execute different paths based on conditions.</li> <li>For Each Activity: Iterate over a collection of items.</li> <li>Until Activity: Repeat an activity until a condition is met.</li> <li>Wait Activity: Pause the pipeline execution for a specified duration.</li> </ul>"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines.html#additional-activities","title":"Additional Activities","text":"<ul> <li>Web Activity: Make HTTP requests.</li> <li>Azure Function Activity: Invoke an Azure Function.</li> <li>Execute Pipeline Activity: Call another pipeline.</li> </ul>"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines.html#integration-runtime","title":"Integration Runtime","text":"<p>It's the infrastructure part. It provides the hardware and running environment, and it connects to on-premises systems or local laptops.</p>"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines.html#linked-service","title":"Linked Service","text":"<p>These are connection strings or configurations that define how to connect to data sources or compute environments.</p>"},{"location":"Synapse-ADF/1.3_ETL%20Pipelines.html#datasets","title":"Datasets","text":"<p>These represent the data that gets processed in the pipeline.</p>"},{"location":"Synapse-ADF/1.4_Copy-data-tool.html","title":"ADF Built-in copy task scenarios","text":"<p>The ADF has a buil-in Copy task. This is a very powerful tool. There are some projects where this tool alone did the major task. </p> <p>In industry, here are some common scenarios where the Copy data tool is the best choice.</p>"},{"location":"Synapse-ADF/1.4_Copy-data-tool.html#1-copying-data-between-azure-sql-databases","title":"1. Copying Data Between Azure SQL Databases","text":"<p>You have two Azure SQL Databases, and you need to copy data from a table in the first database to a table in the second database. The schema is the same, and you want to transfer data regularly, such as every day.</p> <p>Example Use Case: - Source: Azure SQL Database (Table: SalesData) - Destination: Azure SQL Database (Table: SalesDataBackup) - Steps: Use the built-in copy task to set up a daily data copy job.</p>"},{"location":"Synapse-ADF/1.4_Copy-data-tool.html#2-copying-data-from-azure-blob-storage-to-azure-sql-database","title":"2. Copying Data from Azure Blob Storage to Azure SQL Database","text":"<p>You have CSV files stored in Azure Blob Storage and you need to load this data into a table in an Azure SQL Database.</p> <p>Example Use Case: - Source: Azure Blob Storage (CSV file: customer_data.csv) - Destination: Azure SQL Database (Table: Customers) - Steps: Use the built-in copy task to map the CSV columns to the SQL table columns and set up a regular copy job.</p>"},{"location":"Synapse-ADF/1.4_Copy-data-tool.html#3-copying-data-from-on-premises-sql-server-to-azure-data-lake-storage-adls","title":"3. Copying Data from On-Premises SQL Server to Azure Data Lake Storage (ADLS)","text":"<p>You have an on-premises SQL Server database and you need to copy data to Azure Data Lake Storage for further analysis.</p> <p>Example Use Case: - Source: On-Premises SQL Server (Table: EmployeeRecords) - Destination: Azure Data Lake Storage (Folder: EmployeeData) - Steps: Set up a self-hosted integration runtime, then use the built-in copy task to move data from the on-premises server to ADLS.</p>"},{"location":"Synapse-ADF/1.4_Copy-data-tool.html#4-copying-data-from-one-azure-blob-storage-container-to-another","title":"4. Copying Data from One Azure Blob Storage Container to Another","text":"<p>You want to copy files from one container in Azure Blob Storage to another container, perhaps for archival purposes.</p> <p>Example Use Case: - Source: Azure Blob Storage (Container: raw-data) - Destination: Azure Blob Storage (Container: archived-data) - Steps: Use the built-in copy task to set up the copy job and configure it to run on a schedule.</p>"},{"location":"Synapse-ADF/1.4_Copy-data-tool.html#5-copying-data-from-azure-table-storage-to-azure-sql-database","title":"5. Copying Data from Azure Table Storage to Azure SQL Database","text":"<p>You have data in Azure Table Storage and you want to migrate this data to an Azure SQL Database.</p> <p>Example Use Case: - Source: Azure Table Storage (Table: Orders) - Destination: Azure SQL Database (Table: Orders) - Steps: Use the built-in copy task to configure the source and destination, mapping the columns appropriately.</p>"},{"location":"Synapse-ADF/1.4_Copy-data-tool.html#6-copying-data-from-rest-api-to-azure-sql-database","title":"6. Copying Data from REST API to Azure SQL Database","text":"<p>You need to fetch data from a REST API and load it into an Azure SQL Database table.</p> <p>Example Use Case: - Source: REST API (Endpoint: https://api.example.com/data) - Destination: Azure SQL Database (Table: ApiData) - Steps: Use the built-in copy task to connect to the REST API, transform the JSON data as needed, and load it into the SQL table.</p>"},{"location":"Synapse-ADF/1.4_Copy-data-tool.html#7-copying-data-from-azure-cosmos-db-to-azure-sql-database","title":"7. Copying Data from Azure Cosmos DB to Azure SQL Database","text":"<p>You have data in Azure Cosmos DB and need to transfer it to an Azure SQL Database for reporting purposes.</p> <p>Example Use Case: - Source: Azure Cosmos DB (Collection: Users) - Destination: Azure SQL Database (Table: Users) - Steps: Use the built-in copy task to map the Cosmos DB documents to the SQL table columns.</p>"},{"location":"Synapse-ADF/1.5_IntegrationRuntime.html","title":"Integration Runtime in Azure Synapse Analytics and Azure Data Factory","text":""},{"location":"Synapse-ADF/1.5_IntegrationRuntime.html#what-is-integration-runtime","title":"What is Integration Runtime?","text":"<p>Copying data is one of the main activity in Synapse and ADF. The main tool to do this Integration Runtime is used. It's like a bridge that connects Synapse and ADF with data from on-premises and external sources.</p> <p>However, it's more than just a bridge. It provides the CPU and memory needed for copying and transforming data. It also offers an environment to run your SSIS packages. It can scale up or down if more power is needed. Plus, it ensures that your data transfer is secure.</p> <p>Where is IR used? It is used in ADF/Synapse pipelines.</p>"},{"location":"Synapse-ADF/1.5_IntegrationRuntime.html#types-of-integration-runtime","title":"Types of Integration Runtime","text":"<p>There are three types of Integration runtime.</p>"},{"location":"Synapse-ADF/1.5_IntegrationRuntime.html#1-azure-integration-runtime","title":"1. Azure Integration Runtime:","text":"<p> A cloud-based compute resource for running data integration and transformation tasks in Azure Data Factory and Azure Synapse Analytics. </p> <ul> <li>Managed Service: It is fully managed by Microsoft, running in the cloud.</li> <li>Data Movement: Primarily handles data movement within the cloud or between cloud services.</li> <li>Activity Dispatch: Manages and executes data transformation activities in the cloud.</li> <li>Network Access: Has limitations accessing on-premises resources directly due to network boundaries and security considerations.</li> <li>How to create?</li> <li>Navigate to Azure Synapse Analytics or ADF.</li> <li>Select the Manage tab.</li> <li>Under Integration Runtimes, click on + New.</li> <li>Choose Azure, and follow the setup wizard.</li> </ul>"},{"location":"Synapse-ADF/1.5_IntegrationRuntime.html#2-self-hosted-integration-runtime","title":"2. Self-hosted Integration Runtime:","text":"<p> A tool for connecting and moving data between on-premises sources and Azure cloud services. </p> <ul> <li>Installed On-Premises: Deployed within your on-premises network or in a virtual machine that has network access to your on-premises data sources.</li> <li>Bridge for Connectivity: Acts as a secure bridge between on-premises data sources and the cloud.</li> <li>Data Movement: Facilitates secure and efficient data transfer from on-premises sources to the cloud and vice versa.</li> <li>Security: Ensures secure data transfer using encryption and secure authentication methods.</li> <li>How to create?<ul> <li>Download the Self-hosted IR installer from the Azure portal.</li> <li>Install the runtime on a machine within your network.</li> <li>Register the self-hosted IR with the Azure Data Factory or Synapse workspace.</li> <li>Configure network settings to allow secure data movement.</li> <li>Set up high availability by adding multiple nodes.</li> </ul> </li> </ul>"},{"location":"Synapse-ADF/1.5_IntegrationRuntime.html#3-azure-ssis-integration-runtime","title":"3. Azure-SSIS Integration Runtime:","text":"<p> A service to to run SQL Server Integration Services (SSIS) packages in the Azure cloud. </p> <ul> <li>Allows you to lift and shift SSIS packages to the cloud.</li> <li>Provides a fully managed environment for running SSIS packages in Azure.</li> <li>How to create?<ul> <li>Navigate to the ADF or Synapse workspace.</li> <li>Go to the Manage tab, and click on Integration Runtimes.</li> <li>Choose Azure-SSIS and follow the creation wizard.</li> <li>Select the pricing tier and node size.</li> <li>Configure the custom setup by installing necessary components.</li> <li>Connect to the SSISDB or create a new one in Azure SQL Database.</li> </ul> </li> </ul>"},{"location":"Synapse-ADF/1.5_IntegrationRuntime.html#all-connectivity-tools-in-azure-ecosystem","title":"All connectivity tools in Azure ecosystem","text":"Product Purpose Usage From To On-premises Data Gateway Connects cloud services with on-premises data sources. Power BI, Power Apps, Power Automate, Azure Analysis Services, Logic Apps On-premises data sources Cloud services Self-hosted IR For data movement and transformation between on-premises and cloud services. Azure Data Factory (ADF) and Azure Synapse Analytics On-premises data sources ADF and Synapse Azure IR Executes data flows, data movement, and transformation activities within Azure cloud. Data integration and ETL tasks within Azure cloud Cloud data sources Synapse and other cloud services Azure Synapse Link Provides live data copying from operational stores to Synapse for real-time analytics. Near real-time data replication and analytics Operational data stores (e.g., Azure Cosmos DB) Synapse PolyBase Allows querying of external data as if it were in Synapse. Data virtualization and querying External data sources (e.g., Azure Blob Storage, ADLS, SQL Server, Oracle, Hadoop) Synapse Linked Services It is like a connection string. Managing connections to storage accounts, databases, and other services External resources Synapse and other Azure services"},{"location":"Synapse-ADF/1.5_IntegrationRuntime.html#self-hosted-ir-vs-on-premises-data-gateway","title":"Self-hosted IR vs On-premises Data Gateway","text":"<p>The on-premise gateway is quite similar to the self-hosted IR. Are they the same? Can they be used interchangeably? The table below provides the answers:</p> Feature Self-hosted Integration Runtime On-premises Data Gateway Purpose Facilitates data integration for Azure Data Factory and Synapse Pipelines. Connects on-premises data sources to Power BI, Power Apps, Power Automate, and Logic Apps. Supported Services Azure Data Factory, Azure Synapse Analytics. Power BI, Power Apps, Power Automate, Logic Apps.  Data Transfer Capabilities Handles ETL processes, data movement, and SSIS execution. Enables real-time connectivity for reporting and app development. Installation Download and install the .msi file  on your machine  Same, the .exe needs to be downloaded to your local computer and installed. Just like good-old .exe installation.  Security Secure data transfer with encrypted communication. Secure data transfer with encryption and local network connectivity."},{"location":"Synapse-ADF/1.5_IntegrationRuntime.html#summary","title":"Summary","text":"<p>Azure Integration Runtime (IR) is a key tool in ADF and Azure Synapse Analytics. It helps move and transform data between different places, like from your on-premises servers to the cloud or within the cloud itself. Azure IR provides the computing power needed to handle these tasks efficiently. Without it, moving and preparing data for analysis would be difficult, slow, and require a lot more manual work.</p>"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse.html","title":"The Databases Types in Synapse","text":"<p>Before we dive into this, let's ask ourselves: How many types of pools are there in Synapse?. There are three types: Serverless SQL, Dedicated SQL, and Serverless Spark pool.</p> <p>So, it's simple\u2014there are three types of databases, one for each:</p> <ol> <li>Serverless SQL Database: Created using the Serverless SQL pool.</li> <li>Dedicated SQL Database: Created using the Dedicated SQL pool. Note: This is essentially a data warehouse.</li> <li>Spark Database[Lake Database]:</li> <li>Spark DB v1: Created using a serverless Spark pool and Spark notebook, called a Spark [Lake] database.</li> <li>Spark DB v2(Lakehouse, Delta): Similar to v1, but the format of the .parquet files is Delta.</li> </ol> <p>So, does Apache Spark have database connections? Yes, of course. How else would it run SparkSQL and what about that magic command %%sql? It has robust database capabilities.</p>"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse.html#spark-databases","title":"Spark Databases","text":""},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse.html#spark-db-v1-lake-database","title":"Spark DB v1 - Lake Database","text":"<p>A Lake database is a type of DB created by a Spark Pool using a Spark notebook. It\u2019s all virtual; it's not a good-old SQL database but an illusion. The actual data is stored in the ADLS container as folders\u2014one database, one folder. Let's see this in action.</p> <p>First, the show always starts with a Spark pool. The Serverless Spark pool doesn't come pre-created like the built-in SQL pool. So, let's create one:</p> <p></p> <p>Next, open a PySpark notebook and run this command:</p> <p><pre><code>%%sql\nCREATE DATABASE BhutuSpark -- Or, spark.sql(\"CREATE DATABASE BhutuSpark\")\n</code></pre> </p>"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse.html#what-happens-after-the-create-db-command","title":"What Happens after the CREATE DB Command?","text":"<p>Initially, in a fresh workspace, the Workspace in the Data Tab is completely empty, even if you have an inbuilt SQL pool.</p> <p></p> <p>A few minutes after the Spark SQL command to create the database completes, you will see a Lake database structure appearing under the Data workspace. Initially, this will be completely empty. Also, you will notice two databases: the one you created and a default database.</p> <p></p> <p>Synapse creates a folder in the connected ADLS container. In the UI, you will see a database structure (tables, views, users, etc.), but in the background, every Spark database is actually a folder in the ADLS container. When you create tables inside the database, there will again be subfolders inside the parent folder.</p> <p></p> <p>Now, let's run a command to create an empty table for simplicity:</p> <pre><code>%%sql\nCREATE TABLE bhutuspark.BhutuSparkTable (\n    id INT,\n    name STRING\n) USING parquet\n-- For Pyspark, put inside spark.sql(\"the command above\")\n</code></pre> <p>This is what you will see in the Data Workspace. Notice it created a table.</p> <p></p>"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse.html#what-happens-after-the-create-table-command","title":"What happens after the CREATE TABLE command?","text":"<p>Serverless SQL and Serverless Spark Pools create their own virtual databases. All data is virtual. What does this mean? The data is not stored inside them like old-school databases; the data is actually stored in your ADLS container. Let's see this:</p> <p></p> <p>It creates a folder structure like this:</p> <p>ContainerName/synapse/workspaces/WorkSpaceName/warehouse/DBName.db/TableName</p>"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse.html#what-happens-after-the-insert-data-command","title":"What happens after the INSERT data command?","text":"<p>Now, let's put some data inside the table:</p> <pre><code>INSERT INTO bhutuspark.BhutuSparkTable VALUES (1, 'Alice'), (2, 'Bob')\n-- You can use pyspark also, spark.sql(\"the above command\")\n</code></pre> <p>What will you see? You will see .parquet files inside the table folder. Also, note there are two parquet files. We will just note this for now.</p> <p></p>"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse.html#summary","title":"Summary","text":"<ul> <li>A Lake database, or Spark database, is an illusion-DB created using a Spark pool and a notebook.</li> <li>The actual data is stored in Azure Data Lake Storage (ADLS) as folders.</li> <li>Each Spark database is one folder, and each table is a subfolder inside that folder.</li> <li>The version 1 of Spark tables only supports Insert not update or delete.</li> </ul>"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse.html#spark-db-v2-delta-lakelakehouse","title":"Spark DB v2 - Delta Lake/Lakehouse","text":""},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse.html#managed-and-external-spark-tables","title":"Managed and External Spark Tables","text":"<p>In Spark, there are two main types of tables you can create: Managed Tables and External Tables. Let\u2019s explore what these terms mean and see some examples using both SQL and PySpark.</p> <p> Note: Spark tables are inherently external since their data is stored in an ADLS container, unlike traditional SQL servers. So, when you hear \"External Table,\" it means truly external \u2013 not inside the default ADLS container, but in a location you specify, like another container or S3. Don't get confused; external means external-external! </p>"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse.html#managed-tables","title":"Managed Tables","text":"<p>A managed table is a Spark SQL table where Spark manages both the metadata and the data. When you create a managed table, Spark will create a folder for the table here: warehouse/DBName.db/TableName</p> <p>Full path of the folder will be:</p> <p>DefaultSynapseADLSContainer/synapse/workspaces/WorkSpaceName/warehouse/DBName.db/TableName</p> <p> Note: Don't get excited by hearing \"Internal.\" Here, nothing is internal unlike old-school MSSQL servers. All data is stored outside in some container. \"Internal\" only means that the container is the default, well-known location. </p> <p> Summary: 1. Data is stored in a Spark-connected ADLS container. 2. Spark handles the storage. 3. Dropping a managed table deletes both the table metadata and the data. 4. DO NOT provide the LOCATION field &gt; Table becomes EXTERNAL. </p>"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse.html#external-tables","title":"External Tables","text":"<p>All Spark tables store data externally. But, an external table is where the data is really external. Meaning, the .parquet/.csv files aren't saved in the default location like:</p> <p>DefaultSynapseADLSContainer/synapse/workspaces/WorkSpaceName/warehouse/DBName.db/TableName</p> <p>Instead, it stays where it is. Say in some other container or S3, etc. Here Spark creates the shell, but the data is really far away.</p> <p> Summary: 1. You manage the storage location. 2. Data is stored in a user-specified location. 3. Dropping the table only removes the metadata, not the data. 4. The LOCATION field in SQL is a must </p>"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse.html#how-to-create-managed-and-external-spark-tables","title":"How to create Managed and External Spark Tables","text":""},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse.html#empty-managed-table","title":"Empty Managed Table","text":"<p>SparkSQL: <pre><code>CREATE TABLE dbName.mngd_movies (\n    movieName STRING,\n)\nUSING parquet;  -- Format: parquet, delta csv, etc.\n-- WARNING: DO NOT USE LOCATION. Becomes, external\n</code></pre> PySpark-saveAsTable: <pre><code># Step 1: Define the schema for the table\nschema = StructType([\n    StructField(\"movieName\", StringType(), True)\n])\n\n# Step 2: Create an empty DataFrame using the schema\nempty_df = spark.createDataFrame([], schema)\n\n# Step 3: Write the empty DataFrame to a managed table\nempty_df.write.mode('overwrite').saveAsTable('dbName.mngd_movies')\n</code></pre></p>"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse.html#empty-external-table","title":"Empty External Table","text":"<p>SparkSQL: <pre><code>CREATE TABLE dbName.mngd_movies (\n    movieName STRING,\n)\nUSING parquet;  -- Format: parquet, delta csv, etc.\nLOCATION 'abfss://your-container@your-storage-account.dfs.core.windows.net/your-path/';\n-- WARNING: USE LOCATION. Else, managed\n</code></pre></p>"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse.html#managed-tables-from-existing-data","title":"Managed Tables From Existing Data","text":"<p>PySpark-saveAsTable: <pre><code># Step 1: Read the CSV file into a Spark DataFrame\ncsv_file_path = 'abfss://your-container@your-storage-account.dfs.core.windows.net/movies.csv'\nmovies_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n# Step 2: Create a managed table\nmovies_df.write.saveAsTable('mngd_movies')\n</code></pre> SparkSQL: <pre><code>-- Read the CSV file and create a temporary view\nCREATE OR REPLACE TEMPORARY VIEW temp_movies\nUSING csv\nOPTIONS (\n    path 'abfss://your-container@your-storage-account.dfs.core.windows.net/movies.csv',\n    header 'true'\n);\n\n-- Create a managed table from the temporary view\nCREATE TABLE dbName.mngd_movies AS SELECT * FROM temp_movies;\n</code></pre></p>"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse.html#external-tables-from-existing-data","title":"External Tables From Existing Data","text":"<p>SparkSQL: <pre><code>CREATE TABLE dbName.ext_movies (\n    movieName STRING\n)\nUSING csv\nOPTIONS (header 'true')\nLOCATION 'abfss://your-container@your-storage-account.dfs.core.windows.net/movies.csv';\n</code></pre></p>"},{"location":"Synapse-ADF/1.6_DB_Types_In_Synapse.html#describe-extended-tablename","title":"DESCRIBE EXTENDED TABLENAME","text":"<p>In order to see the details of the table a very useful command is. THis tells if the table is managed and where is the location of the folder for this table in the attached ADLS container.</p> <p><pre><code>DESCRIBE EXTENDED TABLEAME\n</code></pre> </p>"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse.html","title":"Lake DB","text":"Table of contents      {: .text-delta } 1. TOC {:toc}"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse.html#synapse-lake-database-lakehouse-and-delta-lake","title":"Synapse Lake Database, Lakehouse, and Delta Lake","text":""},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse.html#synapse-lake-database-synapse-lakehouse","title":"Synapse Lake Database &amp; Synapse Lakehouse","text":""},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse.html#what-is-lake-database","title":"What is Lake Database?","text":"<p>A Synapse Lake Database is a container for registered tables, whose  data stays in ADLS as .parquet files. Here both the database and the tables are kind of psudo. Only the .parquet files are real and they stay  away from the both.</p> <p>How to Create a Lake Database: 1. Create a Lake Database:    - In Synapse Studio, go to the Data Tab -&gt; click + -&gt; Lake Database.</p> <ol> <li>Register the Parquets as Tables in the Lake Database:    <pre><code>CREATE EXTERNAL TABLE [UberLakeDatabase].[BMWUS0BB]\n(\n    PassengerNo INT,\n    PassengerName DATE\n)\nWITH\n(\n    LOCATION = 'lakedatabase/cars/BMWUS0BB.parquet',\n    DATA_SOURCE = [YourDataSource],\n    FILE_FORMAT = [ParquetFormat]\n);\n</code></pre></li> </ol> <p>Lake Database is like Uber(ordinary):</p> <ul> <li>Lake Database is like Uber. Doesn't own any car. Just like Lake Database stores no data inside it.</li> <li>Registered Tables are like the Taxis. They've no relation with passengers. They just carry them.</li> <li>.parquet Files are the passengers. Each passenger is an entry in the Lake Database table.</li> <li>Spark/Synapse Pools are the drivers.</li> <li>CETAS Activity is like a new Driver registering his taxi with UBER.</li> </ul> <p></p>"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse.html#lakehouse-what-is-it","title":"Lakehouse what is it?","text":"<p>A Synapse Lakehouse is simply a Lake Database with tables that are of type Delta. The underlying files are still .parquet, but they are stored in Delta format, providing additional features like ACID transactions.</p> <p>Analogy: - Lakehouse is like Uber Premium. - Delta Tables offer premium services like ACID transactions, similar to the premium experience in Uber Premium. - Creation Process: The same as creating a Lake Database, but the .parquet files are saved in Delta format.</p> <p>How to Create a Lakehouse:</p> <ol> <li>Creating Delta Lake Files:</li> <li> <p>Use Spark to write data in Delta format:      <pre><code>df.write.format(\"delta\").mode(\"overwrite\").save(\"ADLS Container\")\n</code></pre></p> </li> <li> <p>Registering Delta Lake Files:</p> </li> <li>Use CETAS to register Delta Lake files in the Lake Database:      <pre><code>CREATE EXTERNAL TABLE [UberLakeDatabase].[AudiGK0VC]\n(\n    Name INT,\n    MarriageDate DATE\n)\nWITH\n(\n    LOCATION = 'lakedatabase/cars/AudiGK0VC.parquet',\n    DATA_SOURCE = [YourDataSource],\n    FILE_FORMAT = [DeltaFormat]\n);\n</code></pre></li> </ol>"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse.html#summary","title":"Summary:","text":"<p>A Lake Database is essentially: - Uber: Synapse Studio -&gt; Data Tab\" -&gt; + -&gt; Lake Database - The cars: Plus registered CETAS tables pointing to Parquets in ADLS. - Passengers: The .parquet files. - The Driver: Spark servers(SQL pool) / Loveless SQL server(Synapse pool). Remember, these are like taxis, they have no connection with the passenger</p>"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse.html#synapse-lakehouse","title":"Synapse Lakehouse","text":""},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse.html#what-is-it","title":"What is it?","text":"<p>A Synapse Lakehouse is similar to a Lake Database, but it uses Delta Lake tables instead of regular Parquet files.</p>"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse.html#key-components","title":"Key Components:","text":""},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse.html#summary_1","title":"Summary:","text":"<p>A Lakehouse is: - A Lake Database with files stored in Delta format. - Delta tables registered with CETAS in the Lake Database.</p>"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse.html#synapse-dedicated-sql-pool-formerly-sql-warehouse","title":"Synapse Dedicated SQL Pool (Formerly SQL Warehouse)","text":""},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse.html#what-is-it_1","title":"What is it?","text":"<p>A Synapse Dedicated SQL Pool is a traditional data warehouse with real SQL tables that can handle large-scale, high-performance queries.</p>"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse.html#key-components_1","title":"Key Components:","text":"<ol> <li>Creating a Dedicated SQL Pool:</li> <li> <p>In Synapse Studio, create a dedicated SQL pool for high-capacity data storage.</p> </li> <li> <p>Creating Real Tables:</p> </li> <li>Load data from Lake Database tables into dedicated SQL pool tables.</li> <li>Example:      <pre><code>CREATE TABLE [DedicatedSales]\nWITH\n(\n    DISTRIBUTION = HASH(TransactionID),\n    CLUSTERED COLUMNSTORE INDEX\n)\nAS SELECT * FROM [SalesLakeDatabase].[SalesDelta];\n</code></pre></li> </ol>"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse.html#summary_2","title":"Summary:","text":"<p>A Dedicated SQL Pool is used for: - Creating real SQL tables from pseudo Lake Database tables for high-performance querying.</p>"},{"location":"Synapse-ADF/1.7_SynapseLakeDBAndLakehouse.html#overall-summary","title":"Overall Summary","text":"<ul> <li>Lake Database: Synapse -&gt; \"Data Tab\" -&gt; \"+\" -&gt; \"Lake Database\" + CETAS tables pointing to Parquet files in ADLS.</li> <li>Delta Lake Tables: Parquet files saved as Delta format.</li> <li>Lakehouse: Delta Lake tables registered in a Lake Database.</li> <li>Warehouse: Real SQL tables in a Synapse Dedicated SQL Pool created from Lake Database tables for high-performance queries.</li> </ul>"},{"location":"Synapse-ADF/1.8_ADF_SA_Evolution.html","title":"Storage Evolution","text":""},{"location":"Synapse-ADF/1.8_ADF_SA_Evolution.html#how-azure-data-factory-and-synapse-analytics-evolved-over-time","title":"How Azure Data Factory and Synapse analytics evolved over time","text":""},{"location":"Synapse-ADF/1.8_ADF_SA_Evolution.html#background","title":"Background","text":"<p>Back in the older days, people didn't pay much attention to data engineering. But then, tools like SQL Server Integration Service made things a bit easier. Even though this tool let us bring in data from different sources, like files, its main job was to move data into SQL Server. Then Azure Data Factory showed up. </p>"},{"location":"Synapse-ADF/1.8_ADF_SA_Evolution.html#azure-data-factory","title":"Azure Data Factory","text":"<p>Azure Data Factory(ADF), really has come a long way. It started off pretty simple, just helping move data from here to there in the cloud. But now? It's a full-on powerhouse, making complex data movement and transformations. Let's take a quick walk through how it got from point A to point B:</p> Period Milestone Key Features Pre-2015 Before ADF Data integration handled by on-premises solutions like SSIS. Cloud-based data integration tools not yet mainstream. 2015 Initial Launch of ADF Launch of Azure Data Factory for orchestrating data workflows across various data stores. 2017-2018 Introduction of ADF V2 Visual Data Flows, Trigger-based Scheduling, Integration Runtime, Enhanced Monitoring and Management. 2018-Present Continuous Updates and Integration with Azure Services Regular feature updates, new connectors, integration with Azure Databricks, Azure Synapse Analytics, and Azure Machine Learning. 2019-Present Integration with Azure Synapse Analytics Unified analytics platform, combining data integration, big data, and analytics within Azure Synapse Analytics. 2020-Present Simplification and Advanced Capabilities User Experience Improvements, dynamic content, parameterization, enhanced security and management capabilities."},{"location":"Synapse-ADF/1.8_ADF_SA_Evolution.html#azure-synapse-analytics","title":"Azure Synapse analytics","text":"<p>Similar to ADF, Azure Synapse Analytics has come a long way too from its days as SQL Data Warehouse. It's now a one-stop shop for all things analytics. The table below shows some important milestones.</p> Period Milestone Key Developments Pre-2019 SQL Data Warehouse Development and enhancements of SQL Server Data Warehouse capabilities, setting the stage for Azure Synapse Analytics. 2019 Azure Synapse Analytics Launch Rebranding of SQL Data Warehouse to Azure Synapse Analytics, introducing a unified analytics platform. 2020 Integrated Workspace Introduction of Synapse Studio, providing tools for data integration, exploration, and management within a unified workspace. 2021 Connectivity and Usability Enhancements Integration with Azure Purview for data governance, and expansion of on-demand query capabilities. 2022 Performance and Flexibility Improvements New features for performance optimization and enhanced data lake exploration tools. 2023 Expansion of Ecosystem Support Support for an open data ecosystem and introduction of advanced analytics functions. 2024 (Current) Ongoing Development Continuous enhancements focusing on integration, analytics capabilities, and performance optimizations."},{"location":"Synapse-ADF/1.9_CETAS.html","title":"Background","text":"<p>CETAS is very similar to SQL CREATE TABLE TableName command. But, the table is created in Azure Data Lake and is called an External Table. External here means it's not inside the Synapse SQL data warehouse (Fancy name: Dedicated SQL Pool).</p>"},{"location":"Synapse-ADF/1.9_CETAS.html#if-you-have-synapse-why-not-save-the-data-in-synapse-sql-warehouse","title":"If you have Synapse, why not save the data in Synapse SQL warehouse?","text":"<p>Because Synapse SQL warehouse storage is expensive and not meant for everyone to access.</p> <p></p>"},{"location":"Synapse-ADF/1.9_CETAS.html#give-me-some-real-examples","title":"Give me some real examples","text":""},{"location":"Synapse-ADF/1.9_CETAS.html#removing-patients-identity-and-sharing-their-data-as-adls-tables","title":"Removing Patient's Identity and Sharing Their Data as ADLS Tables","text":"<p>A hospital wants to share patient data stored in its Synapse SQL warehouse with researchers. Will they let the entire world access their Synapse warehouse? No way, that would land them in jail. Patient data can't be shared directly. So, they create CETAS in Azure Data Lake with data from their SQL warehouse.</p> <p>Benefits: Avoids jail time and helps the research world make new medicines.</p> <p></p>"},{"location":"Synapse-ADF/1.9_CETAS.html#inlet-wants-to-store-chip-sensor-data-and-share-as-tables-for-university-students","title":"Inlet wants to store chip Sensor data and Share as Tables for university students","text":"<p>Inlet company collects a huge amount of sensor data from chips in ADLS. They have an external university research team that wants to analyze it. The in-house team connects to Synapse using PySpark to clean the data. Then, using Synapse SQL (serverless), they create neat, report-ready CETAS tables in ADLS. The external university is given access to these tables only, keeping Inlet's internal Synapse warehouse out of the entire process.</p> <p>Hence, CETAS is an important command for creating external tables and storing the data from your SQL query permanently as tables in ADLS.</p>"},{"location":"Synapse-ADF/1.9_CETAS.html#cetas-scenarios","title":"CETAS Scenarios","text":""},{"location":"Synapse-ADF/1.9_CETAS.html#pull-data-from-synapse-warehouse-and-put-it-in-adls-cetas-tables","title":"Pull Data from Synapse Warehouse and put It in ADLS CETAS Tables","text":"<p>Suppose you are a data engineer with access to both the Azure Data Lake Gen2 account and the Synapse workspace. The patient records are present in the SQL warehouse, and you want to export this data into ADLS as a table.</p>"},{"location":"Synapse-ADF/1.9_CETAS.html#get-the-access-sorted","title":"Get the access sorted","text":"<ol> <li> <p>Enable Managed Identity for Synapse Workspace:</p> <p>Here, we will use Managed Identity. Why? Because it's the simplest, and Azure handles everything for us. But there are other options for access too, like the Service Principal method and the SAS method.</p> </li> <li> <p>Go to your Synapse workspace in the Azure portal.</p> </li> <li> <p>Under the \"Identity\" section, ensure the \"System-assigned managed identity\" is enabled.</p> </li> <li> <p>Grant Access to Managed Identity on ADLS Gen2:</p> </li> <li> <p>Go to your ADLS Gen2 account in the Azure portal.</p> </li> <li>Navigate to the \"Access Control (IAM)\" section.</li> <li>Click on \"Add role assignment.\"</li> <li>Assign the role \"Storage Blob Data Contributor\" to the managed identity of your Synapse workspace.</li> </ol>"},{"location":"Synapse-ADF/1.9_CETAS.html#custom-db-to-store-connection-info-serverless-sql-only","title":"Custom DB to store connection info - Serverless SQL only","text":"<p>When using CETAS with a serverless SQL pool, don't use the built-in database for connection info, credentials, or file formats. Instead, create a new database to keep things organized. For a dedicated SQL pool, you can use the built-in database.</p> <ol> <li>Create Custom Database:</li> </ol> <pre><code>CREATE DATABASE MyCustDbForCETASInfo;\n</code></pre> <ol> <li>Use Custom Database:</li> </ol> <pre><code>USE MyCustDbForCETASInfo;\n</code></pre>"},{"location":"Synapse-ADF/1.9_CETAS.html#pull-from-warehouse-and-put-in-adls-cetas","title":"Pull-from-Warehouse and Put-in ADLS CETAS","text":"<p>Next, create an external table in ADLS Gen2 using managed identity for authentication.</p> <ol> <li>Create Database Scoped Credential:</li> </ol> <pre><code>USE MyCustDbForCETASInfo;\n\nCREATE DATABASE SCOPED CREDENTIAL MyADLSCredential\nWITH\nIDENTITY = 'Managed Identity';\n</code></pre> <ol> <li>Create External Data Source:</li> </ol> <pre><code>CREATE EXTERNAL DATA SOURCE MyADLS\nWITH (\n    TYPE = HADOOP,\n    LOCATION = 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net',\n    CREDENTIAL = MyADLSCredential\n);\n</code></pre> <ol> <li>Create External File Format:</li> </ol> <pre><code>CREATE EXTERNAL FILE FORMAT ParquetFormat\nWITH (\n    FORMAT_TYPE = PARQUET\n);\n</code></pre> <ol> <li>Create and fill the CETAS Table:</li> </ol> <pre><code>CREATE EXTERNAL TABLE SalesDataExternal\nWITH (\n    LOCATION = '/salesdata/',\n    DATA_SOURCE = MyADLS,\n    FILE_FORMAT = ParquetFormat\n)\nAS\nSELECT * FROM SalesData;\n</code></pre>"},{"location":"Synapse-ADF/1.9_CETAS.html#pull-data-from-adls-and-pur-as-adls-cetas-tables","title":"Pull Data from ADLS and Pur as ADLS CETAS Tables","text":"<p>Like the Inlet company case study, let's say you have lots of .parquet files as data. You want to clean this data and create tables in ADLS itself using Synapse. To do this follow these steps,</p>"},{"location":"Synapse-ADF/1.9_CETAS.html#get-the-access-sorted_1","title":"Get the access sorted","text":"<p>The creation of managed identity etc have been left out. As I explained them before.</p>"},{"location":"Synapse-ADF/1.9_CETAS.html#pull-from-adls-put-in-adls-as-cetas","title":"Pull from ADLS &amp; Put in ADLS as CETAS","text":"<ol> <li>Create Database Scoped Credential:</li> </ol> <pre><code>CREATE DATABASE SCOPED CREDENTIAL MyADLSCredential\nWITH\nIDENTITY = 'Managed Identity';\n</code></pre> <ol> <li>Create External Data Source:</li> </ol> <pre><code>-- Create an external data source for the Azure storage account\nCREATE EXTERNAL DATA SOURCE MyADLS\nWITH (\n    TYPE = HADOOP, -- For dedicated SQL pool\n    -- TYPE = BLOB_STORAGE, -- For serverless SQL pool\n    LOCATION = 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net',\n    CREDENTIAL = MyADLSCredential\n);\n</code></pre> <ol> <li>Create External File Format:</li> </ol> <pre><code>CREATE EXTERNAL FILE FORMAT ParquetFormat\nWITH (\n    FORMAT_TYPE = PARQUET\n);\n</code></pre> <ol> <li>Query Data from ADLS:</li> </ol> <pre><code>SELECT *\nFROM OPENROWSET(\n    BULK 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net/path/to/data/',\n    FORMAT = 'PARQUET'\n) AS [result]\n</code></pre> <ol> <li>Create the CETAS Table:</li> </ol> <pre><code>CREATE EXTERNAL TABLE ProcessedDataExternal\nWITH (\n    LOCATION = '/processeddata/',\n    DATA_SOURCE = MyADLS,\n    FILE_FORMAT = ParquetFormat\n)\nAS\nSELECT * FROM OPENROWSET(\n    BULK 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net/path/to/data/',\n    FORMAT = 'PARQUET'\n) AS [result];\n</code></pre>"},{"location":"Synapse-ADF/1.9_CETAS.html#service-principal-method-and-sas-methods","title":"Service Principal Method and SAS Methods","text":"<p>We used managed  identity in our examples. Apart from Managed Identity, you have a couple of other options for connecting Synapse to Azure Data Lake Storage (ADLS):</p> <ol> <li> <p>Service Principal Authentication: A service principal is like a special user for applications to access Azure resources. You create a service principal and give it the needed permissions on ADLS. Its like functional id.</p> </li> <li> <p>Shared Access Signature (SAS): A Shared Access Signature (SAS) allows limited access to your storage account for a specific time and with specific permissions.</p> </li> </ol> <p>Here\u2019s how you can set up each method:</p>"},{"location":"Synapse-ADF/1.9_CETAS.html#service-principal-authentication-setup","title":"Service Principal Authentication Setup","text":"<ol> <li> <p>Create a Service Principal: You can create a service principal using the Azure portal, Azure CLI, or PowerShell. Here is an example using Azure CLI:      <pre><code>az ad sp create-for-rbac --name &lt;service-principal-name&gt; --role \"Storage Blob Data Contributor\" --scopes /subscriptions/&lt;subscription-id&gt;/resourceGroups/&lt;resource-group&gt;/providers/Microsoft.Storage/storageAccounts/&lt;storage-account&gt;\n</code></pre></p> </li> <li> <p>Grant Access to the Service Principal on ADLS Gen2: Assign the necessary role to the service principal:      <pre><code>az role assignment create --assignee &lt;appId&gt; --role \"Storage Blob Data Contributor\" --scope /subscriptions/&lt;subscription-id&gt;/resourceGroups/&lt;resource-group&gt;/providers/Microsoft.Storage/storageAccounts/&lt;storage-account&gt;\n</code></pre></p> </li> <li> <p>Configure the External Data Source in Synapse: Use the service principal credentials in your SQL script:      <pre><code>CREATE DATABASE SCOPED CREDENTIAL MyADLSCredential\nWITH\nIDENTITY = 'service-principal-id',\nSECRET = 'service-principal-password';\n\nCREATE EXTERNAL DATA SOURCE MyADLS\nWITH (\n    TYPE = HADOOP,\n    LOCATION = 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net',\n    CREDENTIAL = MyADLSCredential\n);\n</code></pre></p> </li> </ol>"},{"location":"Synapse-ADF/1.9_CETAS.html#shared-access-signature-sas-setup","title":"Shared Access Signature (SAS) Setup","text":"<ol> <li> <p>Generate a SAS Token:   You can generate a SAS token through the Azure portal, Azure Storage Explorer, Azure CLI, or programmatically using Azure Storage SDKs. Here is an example using Azure CLI:      <pre><code>az storage account generate-sas --permissions rwdlacup --account-name &lt;storage-account&gt; --services b --resource-types co --expiry &lt;expiry-date&gt;\n</code></pre></p> </li> <li> <p>Configure the External Data Source in Synapse: Use the SAS token in your SQL script:      <pre><code>CREATE DATABASE SCOPED CREDENTIAL MyADLSSASCredential\nWITH\nIDENTITY = 'SHARED ACCESS SIGNATURE',\nSECRET = 'sas-token';\n\nCREATE EXTERNAL DATA SOURCE MyADLS\nWITH (\n    TYPE = HADOOP,\n    LOCATION = 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net',\n    CREDENTIAL = MyADLSSASCredential\n);\n</code></pre></p> </li> </ol> <p>Managed Identity is usually recommended because it's easy to use and secure, but Service Principal and SAS can be useful in certain situations where Managed Identity might not work.</p>"},{"location":"Synapse-ADF/1.9_CETAS.html#alternative-to-cetas","title":"Alternative to CETAS?","text":""},{"location":"Synapse-ADF/1.9_CETAS.html#1-azure-synapse-spark-pools","title":"1. Azure Synapse Spark Pools","text":"<p>Spark capability, pyspark-notebooks comes with Synapse. You can use spark pools in Synapse. This can be more efficeint than JDBC for huge volume of data handling.</p> <pre><code>from pyspark.sql import SparkSession\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"SynapseSparkPoolExample\") \\\n    .getOrCreate()\n\n# Read data from ADLS\ndf = spark.read \\\n    .format(\"parquet\") \\\n    .load(\"abfss://myfilesystem@myadlsaccount.dfs.core.windows.net/path/to/data/\")\n\n# Process the data\nprocessed_df = df.filter(df['SalesDate'] &gt;= '2023-01-01')\n\n# Write the processed data back to ADLS\nprocessed_df.write \\\n    .mode(\"overwrite\") \\\n    .format(\"parquet\") \\\n    .save(\"abfss://myfilesystem@myadlsaccount.dfs.core.windows.net/processeddata/\")\n\n# Register the Parquet files as a table in the Spark session\nspark.sql(\"CREATE EXTERNAL TABLE IF NOT EXISTS SalesDataExternal USING parquet LOCATION 'abfss://myfilesystem@myadlsaccount.dfs.core.windows.net/processeddata/'\")\n</code></pre>"},{"location":"Synapse-ADF/1.9_CETAS.html#2-azure-data-factory-adf","title":"2. Azure Data Factory (ADF)","text":"<p>ADF is one of the core component of Synapse. It's main job is to tranfer data from here to there. This can be a preferred option to avoid lengthy coding etc. Typical steps to perform the activity would be:</p> <ul> <li>Create a pipeline: This is the workflow for the entire activity.</li> <li>Use Copy Activity: This is a very important activity to copy data and used frequently in pipelines.</li> </ul>"},{"location":"Synapse-ADF/1.9_CETAS.html#3-polybase","title":"3. PolyBase","text":"<p>PolyBase is a technology for MSSQL Server. It allows you to query external data as if it were part of the database.</p> <p>In Synapse we created CETAS etc. We were already using Polybase technology.</p>"},{"location":"Synapse-ADF/1.9_CETAS.html#lets-test-our-knowledge","title":"Let's test our knowledge","text":"<ol> <li> <p>What function is used to read the data in files stored in a data lake? </p> <ul> <li>FORMAT</li> <li>ROWSET</li> <li>OPENROWSET</li> </ul> <p>Answer: OPENROWSET</p> </li> <li> <p>What character in file path can be used to select all the file/folders that match rest of the path? </p> <ul> <li>&amp;</li> <li>*</li> <li>/</li> </ul> <p>Answer: *</p> </li> <li> <p>Which external database object encapsulates the connection information to a file location in a data lake store? </p> </li> <li> <p>FILE FORMAT</p> </li> <li>DATA SOURCE</li> <li> <p>EXTERNAL TABLE</p> <p>Answer: DATA SOURCE</p> <p>Based on the given ferocity rankings for the animals, here's how you can create an output table:</p> </li> </ol>"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS.html","title":"Local to ADLS","text":""},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS.html#a-simple-synapse-pipeline-copy-files-from-laptop-to-adls","title":"A Simple Synapse Pipeline. Copy files from Laptop To ADLS","text":"<p>We have a Synapse workspace and some CSV files on our laptop that we want to upload to ADLS. Here is the Microsoft recommended way to do it:</p> <ol> <li>Install SHIR on the laptop.</li> <li>Create a pipeline with a copy data activity.</li> <li>Run the pipeline.</li> </ol> <p>    In Power Platform, the SHIR is replaced by the on-premise gateway. Both are software installed on your local machine, but one is for Synapse and the other is for Power Platform, Fabric, etc.    </p> <p>Let's get our hands dirty and see how to do it.</p>"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS.html#for-the-busy-people","title":"For the busy people","text":"<p>An outline of all the activities typically involved in this process is provided in the table below. For more details, please continue reading the following sections.</p> Step Action Definition 1: Open Synapse Workspace Synapse Workspace Every synapse instance has a workspace. This is the central place for all synapse activities. 2: Install SHIR on Laptop Self-hosted Integration Runtime (SHIR) A software installed on your laptop to enable data movement from on-premises to the cloud. More details on Integration Pipelines 3: Create Linked Services Linked Services These are connection strings to your data sources (local file system) and destinations (ADLS). 4: Define Datasets Datasets These  are like file type. In this case the source and destination datasets are .csvs 5: Build the Pipeline Pipeline A pipline is a workflow, it contains many tasks like Copy data etc. 6: Add a Copy Data Activity Copy Data Activity This is a very important activity that takes care of the entire data copying. 7: Set Up Triggers Triggers This will tell what will trigger the pipeline"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS.html#create-and-configure-an-integration-runtime","title":"Create and Configure an Integration Runtime","text":"<p>The very first step is to install the integration runtime on the local machine. The IR is the backbone of the connection between the local folder and Synapse.</p> <ul> <li>In your synapse workspace go to Manage -&gt; Integration -&gt; Integration runtime. </li> <li>Click on New, then in the settings, you will have two install options. Choose an express setup. </li> </ul> <p>    Express setup is a quicker option as it both installs and links the local IR environment with the synapse setup. If you prefer to do a manual setup, refer to to appenxis.    </p>"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS.html#create-two-linked-services-connection-strings","title":"Create Two Linked Services (Connection Strings)","text":"<p>Next, we need to create two connection strings (also known as Linked Services): one to the local laptop's folder (source) and another to the ADLS (destination).</p>"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS.html#linked-service-to-laptops-folder","title":"Linked Service to Laptop's Folder","text":"<ol> <li>In Synapse workspace, go to Manage -&gt; Linked Services -&gt; New.</li> <li>Select File System and provide a name for the linked service. </li> <li>Select the Integration Runtime we created earlier.</li> <li>Specify the path to the CSV files on your laptop and provide a user name and password which has read/write access to the folder. </li> </ol> <p>    Here, sa is a local user which  has read/write access to the folder.     </p> <p></p> <p>Go to the properties of the source folder and navigate to the security tab to check if the user has the appropriate permissions for the folder.</p> <p></p>"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS.html#a-common-error","title":"A Common Error","text":"<p>After setting up the linked service when you Test connection it may  fail.     </p> <p>This has nothing to do with the setup but a windows security feature which causes the issue. To resolve this, open Command Prompt as Administrator and run the following commands:</p> <pre><code>cd C:\\Program Files\\Microsoft Integration Runtime\\5.0\\Shared\n.\\dmgcmd.exe -DisableLocalFolderPathValidation\n</code></pre> <p>This will disable local folder path validation, and Test connection will pass this time.    </p>"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS.html#linked-service-to-adls","title":"Linked Service to ADLS","text":"<ol> <li>Navigate to Manage -&gt; Linked Services -&gt; New.</li> <li> <p>Select Azure Data Lake Storage Gen2. </p> </li> <li> <p>In Our case we will use AutoResolveIntegrationRuntime. Sometimes its a good choice.  </p> </li> </ol>"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS.html#create-a-pipeline-with-copy-data-activity","title":"Create a Pipeline with Copy Data Activity","text":"<p>Now that the linked services are configured, create a pipeline to copy data:</p>"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS.html#crate-a-new-pipeline-in-syanpse-workspace","title":"Crate a New Pipeline in Syanpse Workspace","text":"<p>In Synapse workspace, go to Integrate -&gt; Pipelines -&gt; New Pipeline. </p>"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS.html#add-copy-data-activity","title":"Add Copy Data Activity","text":"<p>Drag and drop the Copy Data activity onto the pipeline canvas. </p>"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS.html#configure-the-source-dataset-etc","title":"Configure the Source Dataset etc","text":"<ol> <li> <p>Choose dataset: Go to the Source tab, then Files, select File System, and click Continue. </p> </li> <li> <p>Choose File Format: Now, you have to select the format of the source files. We have CSV, so we will select Delimited Text. </p> </li> <li> <p>Select Linked Service: Next, select the Linked Service which we created earlier. This is the connection string that connects to the Laptops folder. You will see the File path and other details appear. Choose First row as header, which is usually the case for all CSVs. </p> </li> <li> <p>Preview data: If successful, you can preview the data. It will load one of the files to show you how the data looks, displaying a well-formatted table. Note, how we have seleccted *.csv to load all the csv files in the folder. </p> </li> </ol>"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS.html#configure-the-sink-dataset","title":"Configure the Sink Dataset","text":"<ol> <li> <p>Select Integration Dataset: Go to the Sink tab, then select Azure Data Lake Storage Gen2. </p> </li> <li> <p>Selct File Format: Now, we need to provide the format in which the data will be copied to the destination. For this, select DelimitedText. </p> </li> <li> <p>Select Linked Service &amp; IR: Next, select the linked service which has the connection information to the container in ADLS where your data will be stored. You can choose any integration runtime. Here, I have chosen the default AutoResolveIntegrationRuntime as it is the simplest and comes factory-shipped with the Synapse workspace. </p> </li> <li> <p>Choose other properties: Once the sink dataset is configured, you can choose other properties like Copy behavior, etc. </p> </li> </ol>"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS.html#execute-the-pipeline","title":"Execute the Pipeline","text":"<ol> <li>Validate the Pipeline: Ensure all configurations are correct and validate the pipeline.</li> <li>Run the Pipeline: Execute the pipeline to start the data transfer from your laptop to ADLS. If it runs successfully you will see the data copied to your desired ADLS container. </li> </ol>"},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS.html#appendix","title":"Appendix","text":""},{"location":"Synapse-ADF/2.1_Pipeline-Local-ADLS.html#manually-installating-integration-runtime","title":"Manually Installating Integration Runtime <p>The integration runtime can also be downloaded and installed separately from the Microsoft software store. </p> <p>Install it on your local machine. The steps are straightforward. Just click through the installation process.</p> <p></p> <p>In the final step, you will need to register the Runtime by copying and pasting the authentication key from the Synapse portal. </p>","text":""},{"location":"Synapse-ADF/2.2_PySparkWarehouse.html","title":"Create a Warehouse with Just a simple Pyspark setup","text":"<p>I am sad; I have just Python and I managed to pip install PySpark. I want to create a decent warehouse on my laptop. Am I being impractical? I read somewhere a DW is for data at rest, spark is for data in motion. Well, no. Let's see what comes free with PySpark:</p> <p>Built-in Hive: PySpark comes with a small Hive setup plus a Derby database by default as the Hive metastore. This means you can create managed tables just like you did in Synapse Analytics.</p> <p>Automatic spark-warehouse folder: PySpark automatically creates a <code>spark-warehouse</code> directory to store table data. This directory is created in your working directory, and all managed tables are stored as Parquet files within this directory.</p> <p>Automatic .Parquet: When you create tables using Hive support in PySpark, the data is stored in Parquet format by default.</p> <p>Bonus! Delta Table Support: You just have to pip install delta, and then you can save the data in the enhanced Delta format, making it an even better warehouse.</p> <p>First, let me create a Spark session with Hive support.</p> <pre><code>from pyspark.sql import SparkSession\n\n# Start Spark session with Hive support\nspark = SparkSession.builder \\\n    .appName(\"PySpark Data Warehouse with Hive Support\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n</code></pre> <p>Then let me create a database and an empty table inside it:</p> <p><pre><code># Create a database\nspark.sql(\"CREATE DATABASE IF NOT EXISTS dbHollywood\")\n\n# Switch to the new database\nspark.sql(\"USE dbHollywood\")\n\n# Create a managed table\nspark.sql(\"\"\"\nCREATE TABLE IF NOT EXISTS mngd_tbl_HollywoodFilms (\n    id INT,\n    name STRING\n)\n\"\"\")\n</code></pre> THis is how the folders are getting creatd in the spark-warehouse folder:</p> <p></p> <p>Now, let me insert some data:</p> <p><pre><code>spark.sql(\"USE dbHollywood\")\n\n# Insert data into the managed table\nspark.sql(\"\"\"\nINSERT INTO mngd_tbl_HollywoodFilms VALUES\n(1, 'Matrix'),\n(2, 'Inception')\n\"\"\")\n</code></pre> Wow, these parquet fiels were auto-craeted. I didnt mention any format. In hive support data is auto-stored as .parquet.</p> <p></p> <p>Now, if I query the table in teh same session I see this:</p> <p></p> <p>Now, let me stop the session and create a new session to see if things are just a one-night stand or a lifetime friendship. If I have no metatore I can't query using the table name, though the backend data  might be present.</p> <p></p> <p>Now, let me try to delete the table and then the database and see if the data goes away or not. Else, how can I call them managed tables?</p> <p></p> <p>Here is the snapshot of the spark-warehouse folder. Only DB folder, table folder completely deleted.</p> <p></p> <p>Now, let me delete the database(ignore the warning)</p> <p></p> <p>The database parent folder completely gone:</p> <p></p>"},{"location":"Synapse-ADF/2.2_PySparkWarehouse.html#using-delta-tables","title":"Using Delta Tables","text":"<p>Now, I am very satisfied with my warehouse and I want to use Spark Tables v2. Which is Delta table. The difference is that in delta table you can update the fields and also spark tables are basic and delta acidic(haha).</p> <p>To have delta table feature in our plain old python envirnmetn. Let's instll delta-spark library:</p> <pre><code>pip install delta-spark\n</code></pre> <p>Then, we will create the session with both delta and hive support. Create a database, then a managed table with delta. Then we will insert some data and then append the data. </p> <p><pre><code>from pyspark.sql import SparkSession\nfrom delta import configure_spark_with_delta_pip\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\nfrom pyspark.sql import Row\n\n# Configure Spark session with Delta Lake and Hive support\nspark = configure_spark_with_delta_pip(\n    SparkSession.builder\n    .appName(\"DeltaLakeExample\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n    .enableHiveSupport()  # Enable Hive support\n).getOrCreate()\n\n# Create a database\nspark.sql(\"CREATE DATABASE IF NOT EXISTS Hollywood\")\n\n# Switch to the new database\nspark.sql(\"USE Hollywood\")\n\n# Create a managed Delta table\nspark.sql(\"\"\"\nCREATE TABLE IF NOT EXISTS mgd_delta_movies (\n    moviename STRING\n) USING delta\n\"\"\")\n\n# Insert data using SQL\nspark.sql(\"\"\"\nINSERT INTO mgd_delta_movies (moviename)\nVALUES \n    ('Titanic'),\n    ('Inception')\n\"\"\")\n\n# Define schema\nschema = StructType([StructField(\"moviename\", StringType(), True)])\n\n# Create DataFrame with defined schema\ndata = [Row(name='Scream')]\n\ndf = spark.createDataFrame(data, schema=schema)\n\n# Insert the DataFrame data into the managed Delta table\ndf.write.format(\"delta\").mode(\"append\").saveAsTable(\"mgd_delta_movies\")\n</code></pre> How, do we know that the tables are delta and they are manged? Its simple</p> <ul> <li>We didn't give any path for during create table command. </li> <li>And the folder for the table is inside spark-warehouse/db_folder/</li> </ul> <p>Thats not engouh? Lets run some commands dbas use to find out:</p> <p><pre><code>spark.sql(\"DESCRIBE FORMATTED Hollywood.Mgd_Delta_Movies\").show(truncate=False)\n</code></pre> </p> <p>Also</p>"},{"location":"Synapse-ADF/2.2_PySparkWarehouse.html#sparksqldescribe-detail-hollywoodmgd_delta_moviesshowtruncatefalse","title":"<pre><code>spark.sql(\"DESCRIBE DETAIL Hollywood.Mgd_Delta_Movies\").show(truncate=False)\n</code></pre>","text":""},{"location":"Synapse-ADF/2.3_ADF_RestAPI_Databricks.html","title":"REST API","text":"<p>This project will clear three areas:</p> <p>How to ingest data from a REST API to ADLS Mounting ADLS on Databricks Transform data in ADLS using Databricks How to load the processed data into Synapse using ADF Polybase How to use Azure data factory triggers</p> <p>Resoruces used:</p> <p>ADF ADLS Azure Databricks(Transformation) Azure Synapse</p> <p></p>"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines.html","title":"Monitoring","text":""},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines.html#how-to-monitor-adf-pipelines","title":"How to Monitor ADF Pipelines","text":"<p>No matter how good your workflow is, errors will happen. Here are some ways to monitor your ADF pipelines. My favorite way is to set up email alerts with a specific subject line and severity. Alright, here are all the ways you can monitor your pipelines.</p>"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines.html#ways-to-monitor-adf-pipelines","title":"Ways to Monitor ADF Pipelines","text":"<ol> <li>Using Azure Portal</li> <li>Setting Alerts and Metrics</li> <li>Log Analytics and Azure Monitor</li> <li>Custom Monitoring Solutions</li> </ol>"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines.html#1-using-azure-portal","title":"1. Using Azure Portal","text":""},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines.html#monitoring-tab","title":"Monitoring Tab","text":"<ul> <li>Go to ADF: Open the Azure portal and find your Data Factory instance.</li> <li>Monitor Tab: Click on the \"Monitor\" tab on the left side. This section shows details about pipeline runs, activity runs, and triggers.</li> </ul>"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines.html#pipeline-runs","title":"Pipeline Runs","text":"<ul> <li>See Pipeline Runs: Here, you can see all pipeline runs. Filter by status (Succeeded, Failed, In Progress) and time range.</li> <li>Run Details: Click on a pipeline run to see detailed info, including the status of each activity in the pipeline.</li> </ul>"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines.html#activity-runs","title":"Activity Runs","text":"<ul> <li>Activity Details: This section shows individual activities within each pipeline run. You can see input, output, and error messages for each activity.</li> </ul>"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines.html#2-setting-alerts-and-metrics","title":"2. Setting Alerts and Metrics","text":""},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines.html#creating-alerts","title":"Creating Alerts","text":"<ul> <li>Go to Alerts: In your ADF instance, go to the \"Alerts\" section.</li> <li>Create Alert: Click on \"New alert rule\" to make a new alert.</li> <li>Set Alert: Choose the condition (e.g., pipeline failure), threshold, and notification method (e.g., email, SMS).</li> </ul>"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines.html#viewing-metrics","title":"Viewing Metrics","text":"<ul> <li>Metrics: You can check metrics like pipeline run duration, activity duration, and trigger runs. Metrics help you understand performance and spot issues.</li> </ul>"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines.html#3-log-analytics-and-azure-monitor","title":"3. Log Analytics and Azure Monitor","text":""},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines.html#enable-diagnostic-logs","title":"Enable Diagnostic Logs","text":"<ul> <li>Diagnostic Settings: In your ADF instance, go to \"Diagnostic settings\" and turn on diagnostic logs. Send these logs to a Log Analytics workspace, Event Hub, or Storage Account.</li> <li>Log Analytics Workspace: If you use Log Analytics, you can run queries to analyze the logs.</li> </ul>"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines.html#using-log-analytics","title":"Using Log Analytics","text":"<ul> <li>Query Logs: In the Log Analytics workspace, run queries to find details about pipeline runs, failures, and performance.</li> <li>Sample Query: <pre><code>ADFPipelineRun\n| where Status == 'Failed'\n| project RunId, PipelineName, Start, End, Status, ErrorMessage\n</code></pre></li> </ul>"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines.html#azure-monitor","title":"Azure Monitor","text":"<ul> <li>Integration: You can use Azure Monitor with ADF for centralized monitoring.</li> <li>Alerts: Create alerts based on log queries and metrics using Azure Monitor.</li> </ul>"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines.html#4-custom-monitoring-solutions","title":"4. Custom Monitoring Solutions","text":""},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines.html#using-powershell","title":"Using PowerShell","text":"<ul> <li>Automation: Use PowerShell scripts to automate monitoring and reporting of pipeline runs.</li> <li>Sample Script: <pre><code>$dataFactoryName = \"your_data_factory_name\"\n$resourceGroupName = \"your_resource_group_name\"\n\n$pipelineRuns = Get-AzDataFactoryV2PipelineRun -ResourceGroupName $resourceGroupName -DataFactoryName $dataFactoryName\n$failedRuns = $pipelineRuns | Where-Object { $_.Status -eq 'Failed' }\n\nforeach ($run in $failedRuns) {\n    Write-Output \"Pipeline: $($run.PipelineName) | Run ID: $($run.RunId) | Status: $($run.Status)\"\n}\n</code></pre></li> </ul>"},{"location":"Synapse-ADF/2.4_Monitor_ADF_Pipelines.html#using-logic-apps","title":"Using Logic Apps","text":"<ul> <li>Automated Workflows: Create Logic Apps to handle pipeline monitoring tasks automatically, like sending notifications or triggering other workflows based on pipeline statuses.</li> </ul>"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy.html","title":"How to Understand and Export Your Azure Data Factory Pipeline","text":""},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy.html#viewing-the-pipelines-json-code","title":"Viewing the Pipeline's JSON Code","text":"<p>In Azure Data Factory (ADF), you can easily see the JSON code that represents your entire pipeline. To do this, simply click on the curly brackets <code>{}</code> icon. This will display the JSON structure of your pipeline, including all the activities, parameters, variables, and other settings.</p> <p></p>"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy.html#limitations-of-the-json-representation","title":"Limitations of the JSON Representation","text":"<p>However, please note that the JSON file you get by clicking the <code>{}</code> icon does not include everything you might need to fully understand your pipeline. Here are some important components that are usually missing:</p> <ol> <li> <p>Linked Services: If your pipeline connects to external resources like Azure Data Lake, Azure SQL, or Databricks, the configuration details for these linked services are not included in the pipeline's JSON. You will need to check each linked service's JSON definition separately.</p> </li> <li> <p>Referenced Pipelines: If your pipeline calls other pipelines using activities like <code>ExecutePipeline</code>, the details of those pipelines are not included. You will have to access their JSON files individually.</p> </li> <li> <p>Global Parameters: Any global parameters your pipeline uses are not fully defined within its JSON file. You'll need to look at the global parameter configurations separately.</p> </li> <li> <p>Triggers: Triggers that start your pipeline (like those based on schedules or events) are not included in the pipeline's JSON. They are managed separately in the ADF interface.</p> </li> <li> <p>Data Flows: If your pipeline contains data flows, the JSON only references them. You'll need to view the details of the data flows separately.</p> </li> <li> <p>Azure Integration Runtime Configurations: These settings are also referenced but not fully included in the pipeline's JSON.</p> </li> </ol> <p>In short, while the JSON representation gives you a detailed view of your pipeline's structure, you need to explore these additional configurations and resources to fully understand how your pipeline works.</p>"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy.html#how-to-copy-the-entire-adf-pipeline","title":"How to Copy the Entire ADF Pipeline","text":"<p>If you want to copy and export the entire pipeline along with all its related resources (like linked services, datasets, and any referenced pipelines), here's how you can do it:</p>"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy.html#1-use-arm-template-export","title":"1. Use ARM Template Export","text":"<p>Azure Data Factory allows you to export your pipeline and all related resources as an Azure Resource Manager (ARM) Template. This method includes all dependencies and is the most comprehensive way to copy everything.</p> <ul> <li> <p>Go to Your Data Factory: Open your Data Factory instance in the Azure Portal.</p> </li> <li> <p>Access the Manage Tab: Click on the \"Manage\" tab on the left side.</p> </li> <li> <p>Export ARM Template: Under the \"ARM template\" section, click on \"Export ARM template\".</p> </li> <li> <p>Select All Components: Make sure to select all the components you want to export, such as pipelines, linked services, datasets, triggers, and global parameters. This will package everything into a single JSON-based ARM template.</p> </li> <li> <p>Download the Template: Save the template file to your computer.</p> </li> <li> <p>Deploy to Another Environment: You can import this template into another Data Factory by using the \"Deploy ARM template\" option in the \"Manage\" section of the target Data Factory, or by using PowerShell or the Azure CLI.</p> </li> </ul>"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy.html#2-use-git-integration-for-source-control","title":"2. Use Git Integration for Source Control","text":"<p>If your Azure Data Factory is connected to a Git repository like Azure DevOps or GitHub, all your pipeline components are already stored there.</p> <ul> <li> <p>Clone the Repository: Download the repository to your computer. It contains all pipelines, datasets, linked services, triggers, and global parameters.</p> </li> <li> <p>Deploy to Another Environment: Use the files from the repository to deploy to another Azure Data Factory instance or to examine the structure locally.</p> </li> </ul>"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy.html#3-manually-export-pipeline-and-related-resources","title":"3. Manually Export Pipeline and Related Resources","text":"<p>Refer to the section Migrating Azure Data Factory Pipelines Without Rebuilding Them</p>"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy.html#4-use-azure-cli-or-powershell","title":"4. Use Azure CLI or PowerShell","text":"<p>You can also use Azure CLI or Azure PowerShell to export your Data Factory components.</p> <p>Using Azure CLI:</p> <pre><code>az datafactory export-pipeline --factory-name &lt;your_data_factory_name&gt; \\\n                               --resource-group &lt;your_resource_group_name&gt; \\\n                               --name &lt;your_pipeline_name&gt; \\\n                               --output-path &lt;output_file_path&gt;\n</code></pre> <p>You can run similar commands to export linked services and datasets.</p>"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy.html#5-deploy-everything-to-another-environment","title":"5. Deploy Everything to Another Environment","text":"<p>Once you have your ARM template or exported JSON files:</p> <ul> <li> <p>For ARM Templates: Use the \"Deploy ARM template\" option in the new Data Factory or deploy via Azure CLI or PowerShell.</p> </li> <li> <p>For Git Integration: Push the files to the repository associated with the target Data Factory.</p> </li> <li> <p>For Manual Exports: Recreate each component in the new Data Factory by importing their JSON files one by one.</p> </li> </ul>"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy.html#6-validate-after-export","title":"6. Validate After Export","text":"<p>After importing or deploying, make sure to:</p> <ul> <li> <p>Validate the Pipeline: Check that all components are working correctly.</p> </li> <li> <p>Update Environment-Specific Settings: Adjust any settings that are specific to the new environment, such as service credentials, authentication details, or resource URLs.</p> </li> </ul>"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy.html#summary","title":"Summary","text":"<p>To export your entire pipeline with all dependencies:</p> <ul> <li> <p>Best Method: Use the ARM template export feature in Azure Data Factory. This method includes everything\u2014pipelines, datasets, linked services, triggers, and more.</p> </li> <li> <p>Alternative Method: Use Git integration for version control and easy transfer between environments.</p> </li> <li> <p>Manual Method: If necessary, export the pipeline and its components individually.</p> </li> </ul>"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy.html#migrating-azure-data-factory-pipelines-without-rebuilding-them","title":"Migrating Azure Data Factory Pipelines Without Rebuilding Them","text":"<p>Transferring data pipelines and their related resources from one Azure Data Factory (ADF) to another can be done smoothly without the need to recreate anything. Here's how you can achieve this:</p>"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy.html#method-1-moving-a-single-pipeline-to-another-data-factory","title":"Method 1: Moving a Single Pipeline to Another Data Factory","text":"<ol> <li> <p>Access Your Source Data Factory: Log in to the ADF instance where your existing pipelines are located.</p> </li> <li> <p>Select the Pipeline: Navigate to the specific pipeline you want to migrate.</p> </li> <li> <p>Download Support Files:</p> </li> <li>Hover over the pipeline.</li> <li>Click on the action menu (three dots).</li> <li> <p>Choose \"Download Support Files\".</p> </li> <li> <p>Extract the Files: Unzip the downloaded folder. You'll find all the support files, including the pipeline, datasets, linked services, triggers, and integration runtimes.</p> </li> <li> <p>For Multiple Pipelines: If you need to transfer more pipelines, repeat steps 2 to 4 for each one and organize the files accordingly.</p> </li> <li> <p>Clone Your Git Repository:</p> </li> <li>Use <code>git clone</code> to create a local copy of your Azure DevOps Git repository.</li> <li> <p>Create a new branch from your main branch, for example, <code>import_pipelines</code>.</p> </li> <li> <p>Check the Branch Contents: Look into your new branch <code>import_pipelines</code> to see the existing folders and files.</p> </li> <li> <p>Place the Downloaded Resources:</p> </li> <li> <p>Copy the extracted files into the corresponding folders in your repository:</p> <ul> <li>datasets \u2192 place dataset files here.</li> <li>DataFactoryPipelines \u2192 place pipeline files here.</li> <li>trigger \u2192 place trigger files here.</li> <li>linkedService \u2192 place linked service files here.</li> </ul> </li> <li> <p>Commit and Push Changes:</p> </li> <li>Use Git commands or a GUI tool to commit your changes.</li> <li> <p>Push the <code>import_pipelines</code> branch to the <code>develop</code> branch of the target Data Factory.</p> </li> <li> <p>Publish in the New Data Factory: After pushing, go to the target ADF and publish the changes. Your migrated pipeline should now appear there.</p> </li> </ol>"},{"location":"Synapse-ADF/2.5_ADF_Pipeline_Copy.html#method-2-transferring-all-pipelines-to-another-data-factory","title":"Method 2: Transferring All Pipelines to Another Data Factory","text":"<ol> <li>Open Azure DevOps:</li> <li>Log in to your Azure DevOps account.</li> <li> <p>Navigate to the Pipelines section.</p> </li> <li> <p>Create a CI/CD Pipeline:</p> </li> <li>Set up a Continuous Integration/Continuous Deployment pipeline.</li> <li> <p>Configure it to deploy resources from your <code>develop</code> branch to the desired environment (e.g., <code>test</code>).</p> </li> <li> <p>Configure Deployment Settings:</p> </li> <li>In the pipeline, specify the destination resource group and subscription where the new Data Factory resides.</li> <li> <p>Provide the ARM (Azure Resource Manager) template from your source Data Factory.</p> </li> <li> <p>Run the Pipeline and Verify:</p> </li> <li>Execute the pipeline to deploy the resources.</li> <li>Check the target Data Factory to ensure all pipelines and resources have been transferred.</li> </ol>"},{"location":"Synapse-ADF/Q%26A.html","title":"100 Azure Synapse Analytics Questions","text":"<p>Here, I've have put together a list of common questions about Azure Synapse Analytics - beginner to advanced levels. These questions are based on real-life situations and cover all the key features of Synapse. Each question comes with multiple-choice answers, and the correct answer is hidden. You can reveal the answer by highlighting the text.</p>"},{"location":"Synapse-ADF/Q%26A.html#1-what-are-the-steps-to-create-a-new-dedicated-sql-pool-in-azure-synapse-analytics","title":"1. What are the steps to create a new dedicated SQL pool in Azure Synapse Analytics?","text":"<ul> <li>A) Navigate to the Synapse workspace -&gt; Click on 'SQL pools' -&gt; Click 'New' -&gt; Provide a name and select performance level -&gt; Click 'Review + create'.</li> <li>B) Navigate to Azure Portal -&gt; Click on 'Resource Groups' -&gt; Create new Resource Group -&gt; Add SQL Pool.</li> <li>C) Open Synapse Studio -&gt; Go to 'Workspace' -&gt; Select 'Create SQL Pool' -&gt; Configure settings.</li> <li>D) Use Azure CLI to run 'az sql pool create'.</li> </ul> <p>Answer: A) Navigate to the Synapse workspace -&gt; Click on 'SQL pools' -&gt; Click 'New' -&gt; Provide a name and select performance level -&gt; Click 'Review + create'.</p>"},{"location":"Synapse-ADF/Q%26A.html#2-which-built-in-function-can-you-use-to-load-data-from-azure-blob-storage-into-azure-synapse","title":"2. Which built-in function can you use to load data from Azure Blob Storage into Azure Synapse?","text":"<ul> <li>A) COPY</li> <li>B) BULK INSERT</li> <li>C) PolyBase</li> <li>D) Data Factory</li> </ul> <p>Answer: C) PolyBase</p>"},{"location":"Synapse-ADF/Q%26A.html#3-what-is-the-basic-sql-command-to-create-a-table-in-synapse-analytics","title":"3. What is the basic SQL command to create a table in Synapse Analytics?","text":"<ul> <li>A) CREATE NEW TABLE table_name (column1 datatype, column2 datatype, ...);</li> <li>B) CREATE TABLE table_name (column1 datatype, column2 datatype, ...);</li> <li>C) NEW TABLE table_name (column1 datatype, column2 datatype, ...);</li> <li>D) TABLE CREATE table_name (column1 datatype, column2 datatype, ...);</li> </ul> <p>Answer: B) CREATE TABLE table_name (column1 datatype, column2 datatype, ...);</p>"},{"location":"Synapse-ADF/Q%26A.html#4-how-can-you-pause-a-dedicated-sql-pool-to-save-costs","title":"4. How can you pause a dedicated SQL pool to save costs?","text":"<ul> <li>A) In the Synapse workspace, go to the SQL pool you want to pause -&gt; Click 'Pause'.</li> <li>B) Navigate to Azure Portal -&gt; Click on 'Resource Groups' -&gt; Select SQL Pool -&gt; Click 'Pause'.</li> <li>C) Open Synapse Studio -&gt; Go to 'Manage' -&gt; Select 'Pause SQL Pool'.</li> <li>D) Use Azure CLI to run 'az sql pool pause'.</li> </ul> <p>Answer: A) In the Synapse workspace, go to the SQL pool you want to pause -&gt; Click 'Pause'.</p>"},{"location":"Synapse-ADF/Q%26A.html#5-which-tool-can-you-use-for-monitoring-and-troubleshooting-performance-in-synapse-analytics","title":"5. Which tool can you use for monitoring and troubleshooting performance in Synapse Analytics?","text":"<ul> <li>A) Azure Monitor</li> <li>B) Synapse Studio</li> <li>C) Azure Advisor</li> <li>D) Log Analytics</li> </ul> <p>Answer: B) Synapse Studio</p>"},{"location":"Synapse-ADF/Q%26A.html#6-what-feature-should-you-enable-to-secure-data-in-transit-within-synapse-analytics","title":"6. What feature should you enable to secure data in transit within Synapse Analytics?","text":"<ul> <li>A) Advanced Threat Protection</li> <li>B) Firewall Rules</li> <li>C) Transparent Data Encryption (TDE)</li> <li>D) Always Encrypted</li> </ul> <p>Answer: C) Transparent Data Encryption (TDE)</p>"},{"location":"Synapse-ADF/Q%26A.html#7-what-is-the-sql-command-to-create-a-view-in-synapse-analytics","title":"7. What is the SQL command to create a view in Synapse Analytics?","text":"<ul> <li>A) CREATE VIEW view_name AS SELECT column1, column2, ... FROM table_name;</li> <li>B) CREATE NEW VIEW view_name AS SELECT column1, column2, ... FROM table_name;</li> <li>C) VIEW CREATE view_name AS SELECT column1, column2, ... FROM table_name;</li> <li>D) CREATE TABLE view_name AS SELECT column1, column2, ... FROM table_name;</li> </ul> <p>Answer: A) CREATE VIEW view_name AS SELECT column1, column2, ... FROM table_name;</p>"},{"location":"Synapse-ADF/Q%26A.html#8-what-is-the-sql-command-to-delete-a-table-from-your-synapse-analytics-database","title":"8. What is the SQL command to delete a table from your Synapse Analytics database?","text":"<ul> <li>A) DELETE TABLE table_name;</li> <li>B) DROP TABLE table_name;</li> <li>C) REMOVE TABLE table_name;</li> <li>D) DESTROY TABLE table_name;</li> </ul> <p>Answer: B) DROP TABLE table_name;</p>"},{"location":"Synapse-ADF/Q%26A.html#9-what-is-a-best-practice-for-efficiently-loading-large-datasets-into-synapse-analytics","title":"9. What is a best practice for efficiently loading large datasets into Synapse Analytics?","text":"<ul> <li>A) Use BULK INSERT</li> <li>B) Use Data Factory</li> <li>C) Use PolyBase or COPY statement</li> <li>D) Use SQL INSERT INTO</li> </ul> <p>Answer: C) Use PolyBase or COPY statement</p>"},{"location":"Synapse-ADF/Q%26A.html#10-what-is-the-basic-sql-command-for-joining-two-tables-in-synapse-analytics","title":"10. What is the basic SQL command for joining two tables in Synapse Analytics?","text":"<ul> <li>A) SELECT * FROM table1 INNER JOIN table2 ON table1.column = table2.column;</li> <li>B) SELECT * FROM table1 JOIN table2 ON table1.column = table2.column;</li> <li>C) SELECT * FROM table1, table2 WHERE table1.column = table2.column;</li> <li>D) SELECT * FROM table1 LEFT JOIN table2 ON table1.column = table2.column;</li> </ul> <p>Answer: B) SELECT * FROM table1 JOIN table2 ON table1.column = table2.column;</p>"},{"location":"Synapse-ADF/Q%26A.html#11-how-can-azure-synapse-help-a-retail-company-segment-their-customers-based-on-purchase-history","title":"11. How can Azure Synapse help a retail company segment their customers based on purchase history?","text":"<ul> <li>A) Using Synapse SQL pools to run clustering algorithms on customer data</li> <li>B) By creating a new SQL database</li> <li>C) By using Data Factory to transform the data</li> <li>D) By using Azure Monitor to track customer activities</li> </ul> <p>Answer: A) Using Synapse SQL pools to run clustering algorithms on customer data</p>"},{"location":"Synapse-ADF/Q%26A.html#12-which-features-of-synapse-analytics-can-be-utilized-by-a-financial-institution-to-detect-fraudulent-transactions-in-real-time","title":"12. Which features of Synapse Analytics can be utilized by a financial institution to detect fraudulent transactions in real-time?","text":"<ul> <li>A) Synapse Pipelines to integrate with Azure Stream Analytics and Machine Learning models</li> <li>B) Using Azure Monitor</li> <li>C) Implementing SQL triggers</li> <li>D) Using Azure Functions</li> </ul> <p>Answer: A) Synapse Pipelines to integrate with Azure Stream Analytics and Machine Learning models</p>"},{"location":"Synapse-ADF/Q%26A.html#13-what-is-the-sql-syntax-to-partition-a-table-to-improve-query-performance-in-synapse-analytics","title":"13. What is the SQL syntax to partition a table to improve query performance in Synapse Analytics?","text":"<ul> <li>A) CREATE TABLE table_name (...) WITH (DISTRIBUTION = HASH(column_name));</li> <li>B) CREATE PARTITIONED TABLE table_name (...) BY (column_name);</li> <li>C) PARTITION TABLE table_name (...) USING (HASH(column_name));</li> <li>D) CREATE TABLE table_name (...) PARTITIONED BY (column_name);</li> </ul> <p>Answer: A) CREATE TABLE table_name (...) WITH (DISTRIBUTION = HASH(column_name));</p>"},{"location":"Synapse-ADF/Q%26A.html#14-how-do-you-implement-row-level-security-in-synapse-analytics","title":"14. How do you implement row-level security in Synapse Analytics?","text":"<ul> <li>A) By using security policies and predicates to filter data at the row level</li> <li>B) By creating separate tables for each user</li> <li>C) By using Azure RBAC</li> <li>D) By enabling Transparent Data Encryption (TDE)</li> </ul> <p>Answer: A) By using security policies and predicates to filter data at the row level</p>"},{"location":"Synapse-ADF/Q%26A.html#15-which-service-can-you-use-to-automatically-scale-resources-to-handle-a-spike-in-data-ingestion-in-synapse-analytics","title":"15. Which service can you use to automatically scale resources to handle a spike in data ingestion in Synapse Analytics?","text":"<ul> <li>A) Synapse SQL pool's autoscale feature</li> <li>B) Azure Load Balancer</li> <li>C) Azure Auto Scale</li> <li>D) SQL Server Management Studio</li> </ul> <p>Answer: A) Synapse SQL pool's autoscale feature</p>"},{"location":"Synapse-ADF/Q%26A.html#16-what-techniques-can-you-use-to-optimize-a-frequently-run-query-in-synapse-analytics","title":"16. What techniques can you use to optimize a frequently run query in Synapse Analytics?","text":"<ul> <li>A) Indexing, distribution strategies, and partitioning</li> <li>B) Using temporary tables</li> <li>C) Creating a new database</li> <li>D) Increasing the database size</li> </ul> <p>Answer: A) Indexing, distribution strategies, and partitioning</p>"},{"location":"Synapse-ADF/Q%26A.html#17-what-is-the-sql-command-to-grant-access-to-a-specific-user-to-only-one-database-in-synapse-analytics","title":"17. What is the SQL command to grant access to a specific user to only one database in Synapse Analytics?","text":"<ul> <li>A) GRANT CONNECT TO user_name;</li> <li>B) GRANT ALL TO user_name;</li> <li>C) GIVE ACCESS TO user_name;</li> <li>D) PROVIDE CONNECT TO user_name;</li> </ul> <p>Answer: A) GRANT CONNECT TO user_name;</p>"},{"location":"Synapse-ADF/Q%26A.html#18-how-can-you-achieve-data-masking-to-protect-sensitive-data-in-synapse-analytics","title":"18. How can you achieve data masking to protect sensitive data in Synapse Analytics?","text":"<ul> <li>A) Using Dynamic Data Masking (DDM)</li> <li>B) Using Transparent Data Encryption (TDE)</li> <li>C) By encrypting the data at rest</li> <li>D) By using SQL triggers</li> </ul> <p>Answer: A) Using Dynamic Data Masking (DDM)</p>"},{"location":"Synapse-ADF/Q%26A.html#19-what-is-the-sql-syntax-for-creating-an-external-table-to-query-data-in-azure-data-lake","title":"19. What is the SQL syntax for creating an external table to query data in Azure Data Lake?","text":"<ul> <li>A) CREATE EXTERNAL TABLE table_name (...) WITH (LOCATION = '...', DATA_SOURCE = data_source_name, FILE_FORMAT = file_format_name);</li> <li>B) CREATE TABLE table_name EXTERNAL (...) WITH (LOCATION = '...', DATA_SOURCE = data_source_name);</li> <li>C) EXTERNAL CREATE TABLE table_name (...) USING (LOCATION = '...', DATA_SOURCE = data_source_name);</li> <li>D) CREATE EXTERNAL TABLE table_name (...) USING (LOCATION = '...', FILE_FORMAT = file_format_name);</li> </ul> <p>Answer: A) CREATE EXTERNAL TABLE table_name (...) WITH (LOCATION = '...', DATA_SOURCE = data_source_name, FILE_FORMAT = file_format_name);</p>"},{"location":"Synapse-ADF/Q%26A.html#20-what-tool-can-you-use-for-scheduling-and-managing-regular-data-loads-from-an-external-source-into-synapse-analytics","title":"20. What tool can you use for scheduling and managing regular data loads from an external source into Synapse Analytics?","text":"<ul> <li>A) Azure Data Factory</li> <li>B) Azure DevOps</li> <li>C) Azure Monitor</li> <li>D) Azure Functions</li> </ul> <p>Answer: A) Azure Data Factory</p>"},{"location":"Synapse-ADF/Q%26A.html#21-which-service-integrates-with","title":"21. Which service integrates with","text":"<p>Synapse Analytics to implement a machine learning model? - A) Azure Machine Learning service - B) Azure Cognitive Services - C) Azure Functions - D) Azure Bot Service</p> <p>Answer: A) Azure Machine Learning service</p>"},{"location":"Synapse-ADF/Q%26A.html#22-which-tools-can-be-used-for-setting-up-cicd-pipelines-for-synapse-analytics","title":"22. Which tools can be used for setting up CI/CD pipelines for Synapse Analytics?","text":"<ul> <li>A) Azure DevOps or GitHub Actions</li> <li>B) Azure Pipelines</li> <li>C) Azure Logic Apps</li> <li>D) Power Automate</li> </ul> <p>Answer: A) Azure DevOps or GitHub Actions</p>"},{"location":"Synapse-ADF/Q%26A.html#23-what-feature-can-you-use-to-encrypt-data-at-rest-in-synapse-analytics","title":"23. What feature can you use to encrypt data at rest in Synapse Analytics?","text":"<ul> <li>A) Transparent Data Encryption (TDE)</li> <li>B) Always Encrypted</li> <li>C) SSL/TLS</li> <li>D) Row-Level Security</li> </ul> <p>Answer: A) Transparent Data Encryption (TDE)</p>"},{"location":"Synapse-ADF/Q%26A.html#24-which-language-and-environment-would-you-use-to-execute-a-complex-data-transformation-within-synapse","title":"24. Which language and environment would you use to execute a complex data transformation within Synapse?","text":"<ul> <li>A) SQL or Spark within Synapse Studio</li> <li>B) Python within Azure Notebooks</li> <li>C) R within Azure ML Studio</li> <li>D) Java within Eclipse</li> </ul> <p>Answer: A) SQL or Spark within Synapse Studio</p>"},{"location":"Synapse-ADF/Q%26A.html#25-how-can-you-integrate-synapse-analytics-with-power-bi","title":"25. How can you integrate Synapse Analytics with Power BI?","text":"<ul> <li>A) Connect Power BI to Synapse Analytics via the dedicated SQL pool connector</li> <li>B) Export data from Synapse Analytics to CSV and import into Power BI</li> <li>C) Use Azure Functions to transfer data to Power BI</li> <li>D) Use Data Factory to connect Power BI to Synapse Analytics</li> </ul> <p>Answer: A) Connect Power BI to Synapse Analytics via the dedicated SQL pool connector</p>"},{"location":"Synapse-ADF/Q%26A.html#26-what-feature-can-you-use-to-track-data-changes-in-your-data-warehouse-within-synapse-analytics","title":"26. What feature can you use to track data changes in your data warehouse within Synapse Analytics?","text":"<ul> <li>A) Change Data Capture (CDC)</li> <li>B) SQL Triggers</li> <li>C) Data Audit</li> <li>D) Data Logs</li> </ul> <p>Answer: A) Change Data Capture (CDC)</p>"},{"location":"Synapse-ADF/Q%26A.html#27-which-component-is-best-suited-for-real-time-data-processing-in-synapse-analytics","title":"27. Which component is best suited for real-time data processing in Synapse Analytics?","text":"<ul> <li>A) Azure Synapse Data Explorer</li> <li>B) Azure Functions</li> <li>C) Azure Logic Apps</li> <li>D) Azure Databricks</li> </ul> <p>Answer: A) Azure Synapse Data Explorer</p>"},{"location":"Synapse-ADF/Q%26A.html#28-what-tool-can-assist-with-data-lineage-tracking-in-synapse-analytics","title":"28. What tool can assist with data lineage tracking in Synapse Analytics?","text":"<ul> <li>A) Azure Purview</li> <li>B) Azure Monitor</li> <li>C) Azure Log Analytics</li> <li>D) Azure Sentinel</li> </ul> <p>Answer: A) Azure Purview</p>"},{"location":"Synapse-ADF/Q%26A.html#29-which-synapse-feature-allows-for-large-scale-data-analytics-across-various-data-sources","title":"29. Which Synapse feature allows for large-scale data analytics across various data sources?","text":"<ul> <li>A) Synapse Pipelines</li> <li>B) Synapse SQL</li> <li>C) Azure Data Factory</li> <li>D) Azure Databricks</li> </ul> <p>Answer: A) Synapse Pipelines</p>"},{"location":"Synapse-ADF/Q%26A.html#30-how-can-you-execute-r-and-python-scripts-in-synapse-analytics","title":"30. How can you execute R and Python scripts in Synapse Analytics?","text":"<ul> <li>A) Use Synapse Notebooks</li> <li>B) Use Azure Functions</li> <li>C) Use Azure Data Factory</li> <li>D) Use Azure ML Studio</li> </ul> <p>Answer: A) Use Synapse Notebooks</p>"},{"location":"Synapse-ADF/Q%26A.html#31-how-can-a-healthcare-company-ensure-data-compliance-and-privacy-for-sensitive-patient-data-stored-in-synapse-analytics","title":"31. How can a healthcare company ensure data compliance and privacy for sensitive patient data stored in Synapse Analytics?","text":"<ul> <li>A) Implementing Dynamic Data Masking and Row-Level Security</li> <li>B) Using Azure Monitor</li> <li>C) Implementing SQL triggers</li> <li>D) Using Azure Load Balancer</li> </ul> <p>Answer: A) Implementing Dynamic Data Masking and Row-Level Security</p>"},{"location":"Synapse-ADF/Q%26A.html#32-what-feature-in-synapse-analytics-can-help-optimize-query-performance-by-distributing-data-across-different-nodes","title":"32. What feature in Synapse Analytics can help optimize query performance by distributing data across different nodes?","text":"<ul> <li>A) Data Distribution</li> <li>B) Sharding</li> <li>C) Partitioning</li> <li>D) Replication</li> </ul> <p>Answer: A) Data Distribution</p>"},{"location":"Synapse-ADF/Q%26A.html#33-how-can-a-financial-services-company-perform-complex-time-series-analysis-on-transaction-data-in-synapse-analytics","title":"33. How can a financial services company perform complex time-series analysis on transaction data in Synapse Analytics?","text":"<ul> <li>A) Using Spark Pools and time-series libraries</li> <li>B) Using Azure Logic Apps</li> <li>C) By running SQL scripts</li> <li>D) By exporting data to a third-party tool</li> </ul> <p>Answer: A) Using Spark Pools and time-series libraries</p>"},{"location":"Synapse-ADF/Q%26A.html#34-which-method-allows-you-to-automate-data-workflows-and-orchestrate-data-movement-in-and-out-of-synapse-analytics","title":"34. Which method allows you to automate data workflows and orchestrate data movement in and out of Synapse Analytics?","text":"<ul> <li>A) Synapse Pipelines</li> <li>B) Azure Logic Apps</li> <li>C) Power Automate</li> <li>D) Azure Functions</li> </ul> <p>Answer: A) Synapse Pipelines</p>"},{"location":"Synapse-ADF/Q%26A.html#35-how-can-a-manufacturing-company-analyze-iot-sensor-data-in-real-time-using-synapse-analytics","title":"35. How can a manufacturing company analyze IoT sensor data in real-time using Synapse Analytics?","text":"<ul> <li>A) Integrating Synapse with Azure Stream Analytics</li> <li>B) Using SQL triggers</li> <li>C) Storing data in Blob Storage</li> <li>D) Using Azure Functions</li> </ul> <p>Answer: A) Integrating Synapse with Azure Stream Analytics</p>"},{"location":"Synapse-ADF/Q%26A.html#36-how-do-you-manage-user-permissions-and-access-control-in-synapse-analytics","title":"36. How do you manage user permissions and access control in Synapse Analytics?","text":"<ul> <li>A) Using Role-Based Access Control (RBAC)</li> <li>B) Creating separate databases</li> <li>C) Using SQL triggers</li> <li>D) By encrypting the data</li> </ul> <p>Answer: A) Using Role-Based Access Control (RBAC)</p>"},{"location":"Synapse-ADF/Q%26A.html#37-how-can-a-retail-company-use-synapse-analytics-to-forecast-sales-trends","title":"37. How can a retail company use Synapse Analytics to forecast sales trends?","text":"<ul> <li>A) By integrating with Azure Machine Learning for predictive analytics</li> <li>B) By exporting data to Excel</li> <li>C) Using SQL scripts</li> <li>D) By creating new databases</li> </ul> <p>Answer: A) By integrating with Azure Machine Learning for predictive analytics</p>"},{"location":"Synapse-ADF/Q%26A.html#38-what-is-the-best-way-to-ensure-data-quality-before-loading-it-into-synapse-analytics","title":"38. What is the best way to ensure data quality before loading it into Synapse Analytics?","text":"<ul> <li>A) Using Data Flows in Synapse Pipelines to perform data cleansing and transformation</li> <li>B) Using SQL scripts</li> <li>C) By encrypting the data</li> <li>D) By storing data in Blob Storage</li> </ul> <p>Answer: A) Using Data Flows in Synapse Pipelines to perform data cleansing and transformation</p>"},{"location":"Synapse-ADF/Q%26A.html#39-how-can-you-integrate-data-from-various-sources-such-as-sql-databases-blob-storage-and-on-premises-data-into-synapse-analytics","title":"39. How can you integrate data from various sources such as SQL databases, Blob Storage, and on-premises data into Synapse Analytics?","text":"<ul> <li>A) Using Azure Data Factory with Synapse Pipelines</li> <li>B) Using SQL scripts</li> <li>C) Using Azure Functions</li> <li>D) Using Power Automate</li> </ul> <p>Answer: A) Using Azure Data Factory with Synapse Pipelines</p>"},{"location":"Synapse-ADF/Q%26A.html#40-which-feature-allows-a-retail-company-to-visualize-and-explore-large-datasets-interactively-within-synapse-analytics","title":"40. Which feature allows a retail company to visualize and explore large datasets interactively within Synapse Analytics?","text":"<ul> <li>A) Synapse Studio</li> <li>B) Azure Logic Apps</li> <li>C) Azure Monitor</li> <li>D) Azure DevOps</li> </ul> <p>Answer: A) Synapse Studio</p>"},{"location":"Synapse-ADF/Q%26A.html#41-how-can-you-ensure-high-availability-and-disaster-recovery-for-your-synapse-analytics-environment","title":"41. How can you ensure high availability and disaster recovery for your Synapse Analytics environment?","text":"<ul> <li>A) Implementing geo-redundant storage and failover groups</li> <li>B) Using SQL triggers</li> <li>C) Using Azure Monitor</li> <li>D) By encrypting the data</li> </ul> <p>Answer: A) Implementing geo-redundant storage and failover groups</p>"},{"location":"Synapse-ADF/Q%26A.html#42-what-is-the-purpose-of-the-synapse-sql-serverless-pool","title":"42. What is the purpose of the Synapse SQL Serverless pool?","text":"<ul> <li>A) To query data in data lakes without needing to provision dedicated resources</li> <li>B) To run continuous SQL scripts</li> <li>C) To monitor database performance</li> <li>D) To provide disaster recovery solutions</li> </ul> <p>Answer: A) To query data in data lakes without needing to provision dedicated resources</p>"},{"location":"Synapse-ADF/Q%26A.html#43-which-feature-allows-you-to-create-and-manage-data-integration-pipelines-within-synapse-analytics","title":"43. Which feature allows you to create and manage data integration pipelines within Synapse Analytics?","text":"<ul> <li>A) Synapse Pipelines</li> <li>B) Azure Logic Apps</li> <li>C) Power Automate</li> <li>D) Azure Functions</li> </ul> <p>Answer: A) Synapse Pipelines</p>"},{"location":"Synapse-ADF/Q%26A.html#44-how-can-you-optimize-the-performance-of-a-data-warehouse-query-in-synapse-analytics","title":"44. How can you optimize the performance of a data warehouse query in Synapse Analytics?","text":"<ul> <li>A) By creating clustered columnstore indexes</li> <li>B) By increasing database size</li> <li>C) By running queries during off-peak hours</li> <li>D) By exporting data to a third-party tool</li> </ul> <p>Answer: A) By creating clustered columnstore indexes</p>"},{"location":"Synapse-ADF/Q%26A.html#45-what-is-the-best-practice-for-handling-slowly-changing-dimensions-in-synapse-analytics","title":"45. What is the best practice for handling slowly changing dimensions in Synapse Analytics?","text":"<ul> <li>A) Using a combination of SQL and Synapse Pipelines to track changes</li> <li>B) By creating new databases</li> <li>C) By using Azure Logic Apps</li> <li>D) By encrypting the data</li> </ul> <p>Answer: A) Using a combination of SQL and Synapse Pipelines to track changes</p>"},{"location":"Synapse-ADF/Q%26A.html#46-how-can-you-implement-real-time-analytics-on-streaming-data-in-synapse-analytics","title":"46. How can you implement real-time analytics on streaming data in Synapse Analytics?","text":"<ul> <li>A) By integrating Synapse with Azure Stream Analytics</li> <li>B) Using SQL triggers</li> <li>C) By storing data in Blob Storage</li> <li>D) Using Azure Functions</li> </ul> <p>Answer: A) By integrating Synapse with Azure Stream Analytics</p>"},{"location":"Synapse-ADF/Q%26A.html#47-which-feature-of-synapse-analytics-can-help-you-manage-and-control-costs-for-your-data-warehouse","title":"47. Which feature of Synapse Analytics can help you manage and control costs for your data warehouse?","text":"<ul> <li>A) Autoscaling SQL pools</li> <li>B) Using SQL triggers</li> <li>C) By creating separate databases</li> <li>D) By encrypting the data</li> </ul> <p>Answer: A) Autoscaling SQL pools</p>"},{"location":"Synapse-ADF/Q%26A.html#48-what-is-the-benefit-of-using-dedicated-sql-pools-in-synapse-analytics","title":"48. What is the benefit of using dedicated SQL pools in Synapse Analytics?","text":"<ul> <li>A) They provide optimized performance for large-scale analytics workloads</li> <li>B) They are always on and consume fewer resources</li> <li>C) They offer more security features</li> <li>D) They are easier to configure</li> </ul> <p>Answer: A) They provide optimized performance for large-scale analytics workloads</p>"},{"location":"Synapse-ADF/Q%26A.html#49-how-can-you-automate-the-deployment-of-synapse-analytics-resources-using-infrastructure-as-code","title":"49. How can you automate the deployment of Synapse Analytics resources using infrastructure as code?","text":"<ul> <li>A) Using Azure Resource Manager (ARM) templates</li> <li>B) By using Azure Monitor</li> <li>C) Using SQL triggers</li> <li>D) By encrypting the data</li> </ul> <p>Answer: A) Using Azure Resource Manager (ARM) templates</p>"},{"location":"Synapse-ADF/Q%26A.html#50-how-can-a-global-manufacturing-company-use-synapse-analytics-to-unify-data-from-multiple-regions-for-centralized-analysis","title":"50. How can a global manufacturing company use Synapse Analytics to unify data from multiple regions for centralized analysis?","text":"<ul> <li>A) By setting up a Synapse workspace with integrated data pipelines from each region</li> <li>B) By creating separate databases for each region</li> <li>C) Using Azure Logic Apps</li> <li>D) By storing data in Blob Storage</li> </ul> <p>Answer: A) By setting up a Synapse workspace with integrated data pipelines from each region</p>"},{"location":"Synapse-ADF/Q%26A.html#51-which-azure-service-can-be-used-alongside-synapse-analytics-to-provide-data-cataloging-and-governance-capabilities","title":"51. Which Azure service can be used alongside Synapse Analytics to provide data cataloging and governance capabilities?","text":"<ul> <li>A) Azure Purview</li> <li>B) Azure Monitor</li> <li>C) Azure DevOps</li> <li>D) Azure Functions</li> </ul> <p>Answer: A) Azure Purview</p>"},{"location":"Synapse-ADF/Q%26A.html#52-how-can-you-leverage-synapse-analytics-to-perform-batch-processing-of-large-datasets","title":"52. How can you leverage Synapse Analytics to perform batch processing of large datasets?","text":"<ul> <li>A) Using Synapse Pipelines with integrated Spark pools</li> <li>B) Using SQL triggers</li> <li>C) By creating new databases</li> <li>D) Using Power Automate</li> </ul> <p>Answer: A) Using Synapse Pipelines with integrated Spark pools</p>"},{"location":"Synapse-ADF/Q%26A.html#53-what-is-a-common-use-case-for-using-synapse-studio-in-a-data-analytics-workflow","title":"53. What is a common use case for using Synapse Studio in a data analytics workflow?","text":"<ul> <li>A) Interactive data exploration and visualization</li> <li>B) SQL trigger management</li> <li>C) Database encryption configuration</li> <li>D) Network configuration</li> </ul> <p>Answer: A) Interactive data exploration and visualization</p>"},{"location":"Synapse-ADF/Q%26A.html#54-how-can-a-company-ensure-their-synapse-analytics-data-warehouse-is-secure-and-compliant-with-industry-standards","title":"54. How can a company ensure their Synapse Analytics data warehouse is secure and compliant with industry standards?","text":"<ul> <li>A) Implementing security best practices such as data encryption, access control, and monitoring</li> <li>B) Using SQL triggers</li> <li>C) By creating separate databases</li> <li>D) By using Azure Logic Apps</li> </ul> <p>Answer: A) Implementing security best practices such as data encryption, access control, and monitoring</p>"},{"location":"Synapse-ADF/Q%26A.html#55-what-feature-allows-synapse-analytics-to-handle-complex-etl-processes-and-data-transformations","title":"55. What feature allows Synapse Analytics to handle complex ETL processes and data transformations?","text":"<ul> <li>A) Synapse Pipelines with Data Flows</li> <li>B) SQL scripts</li> <li>C) Azure Functions</li> <li>D) Azure DevOps</li> </ul> <p>Answer: A) Synapse Pipelines with Data Flows</p>"},{"location":"Synapse-ADF/Q%26A.html#56-how-can-you-monitor-and-troubleshoot-synapse-analytics-performance-issues","title":"56. How can you monitor and troubleshoot Synapse Analytics performance issues?","text":"<ul> <li>A) Using built-in monitoring tools in Synapse Studio and Azure Monitor</li> <li>B) Using SQL triggers</li> <li>C) By creating new databases</li> <li>D) By encrypting the data</li> </ul> <p>Answer: A) Using built-in monitoring tools in Synapse Studio and Azure Monitor</p>"},{"location":"Synapse-ADF/Q%26A.html#57-which-feature-in-synapse-analytics-helps-you-to-seamlessly-integrate-data-from-on-premises-and-cloud-sources","title":"57. Which feature in Synapse Analytics helps you to seamlessly integrate data from on-premises and cloud sources?","text":"<ul> <li>A) Data integration using Azure Data Factory with Synapse Pipelines</li> <li>B) SQL scripts</li> <li>C) Azure Logic Apps</li> <li>D) Power Automate</li> </ul> <p>Answer: A) Data integration using Azure Data Factory with Synapse Pipelines</p>"},{"location":"Synapse-ADF/Q%26A.html#58-what-type-of-workloads-are-best-suited-for-using-dedicated-sql-pools-in-synapse-analytics","title":"58. What type of workloads are best suited for using dedicated SQL pools in Synapse Analytics?","text":"<ul> <li>A) Large-scale analytical workloads</li> <li>B) Small transactional workloads</li> <li>C) Real-time streaming workloads</li> <li>D) Configuration management workloads</li> </ul> <p>Answer: A) Large-scale analytical workloads</p>"},{"location":"Synapse-ADF/Q%26A.html#59-how-can-a-company-use-synapse-analytics-to-support-data-science-and-machine-learning-initiatives","title":"59. How can a company use Synapse Analytics to support data science and machine learning initiatives?","text":"<ul> <li>A) Integrating with Azure Machine Learning and leveraging Spark pools for data processing</li> <li>B) Using SQL triggers</li> <li>C) By creating new databases</li> <li>D) Using Azure Functions</li> </ul> <p>Answer: A) Integrating with Azure Machine Learning and leveraging Spark pools for data processing</p>"},{"location":"Synapse-ADF/Q%26A.html#60-which-tool-can-be-used-to-create-and-manage-complex-data-transformation-workflows-in-synapse-analytics","title":"60. Which tool can be used to create and manage complex data transformation workflows in Synapse Analytics?","text":"<ul> <li>A) Synapse Pipelines</li> <li>B) Azure Monitor</li> <li>C) SQL Server Management Studio</li> <li>D) Azure DevOps</li> </ul> <p>Answer: A) Synapse Pipelines</p>"},{"location":"Synapse-ADF/Q%26A.html#61-what-method-can-be-used-to-optimize-storage-costs-in-synapse-analytics","title":"61. What method can be used to optimize storage costs in Synapse Analytics?","text":"<ul> <li>A) Using compression techniques and tiered storage options</li> <li>B) By increasing database size</li> <li>C) Using SQL triggers</li> <li>D) By creating separate databases</li> </ul> <p>Answer: A) Using compression techniques and tiered storage options</p>"},{"location":"Synapse-ADF/Q%26A.html#62-how-can-synapse-analytics-support-a-hybrid-data-architecture","title":"62. How can Synapse Analytics support a hybrid data architecture?","text":"<ul> <li>A) By integrating with on-premises and cloud data sources using Synapse Pipelines</li> <li>B) By running SQL scripts only in the cloud</li> <li>C) By using Azure Logic Apps</li> <li>D) By creating separate databases</li> </ul> <p>Answer: A) By integrating with on-premises and cloud data sources using Synapse Pipelines</p>"},{"location":"Synapse-ADF/Q%26A.html#63-what-feature-in-synapse-analytics-can-be-used-to-run-interactive-queries-on-large-datasets-without-pre-provisioned-resources","title":"63. What feature in Synapse Analytics can be used to run interactive queries on large datasets without pre-provisioned resources?","text":"<ul> <li>A) Synapse SQL Serverless pool</li> <li>B) Dedicated SQL pool</li> <li>C) Azure Functions</li> <li>D) SQL triggers</li> </ul> <p>Answer: A) Synapse SQL Serverless pool</p>"},{"location":"Synapse-ADF/Q%26A.html#64-which-tool-can-be-used-to-automate-the-deployment-and-management-of-synapse-analytics-resources","title":"64. Which tool can be used to automate the deployment and management of Synapse Analytics resources?","text":"<ul> <li>A) Azure DevOps with CI/CD pipelines</li> <li>B) Azure Monitor</li> <li>C) SQL Server Management Studio</li> <li>D) Power Automate</li> </ul> <p>Answer: A) Azure DevOps with CI/CD pipelines</p>"},{"location":"Synapse-ADF/Q%26A.html#65-how-can-a-financial-company-use-synapse-analytics-to-perform-regulatory-reporting","title":"65. How can a financial company use Synapse Analytics to perform regulatory reporting?","text":"<ul> <li>A) By using Synapse Pipelines to aggregate data and generate reports</li> <li>B) By exporting data to a third-party tool</li> <li>C) Using SQL triggers</li> <li>D) By creating separate databases</li> </ul> <p>Answer: A) By using Synapse Pipelines to aggregate data and generate reports</p>"},{"location":"Synapse-ADF/Q%26A.html#66-what-is-the-benefit-of-using-spark-pools-in-synapse-analytics-for-big-data-processing","title":"66. What is the benefit of using Spark pools in Synapse Analytics for big data processing?","text":"<ul> <li>A) They provide a scalable and distributed environment for processing large datasets</li> <li>B) They offer more security features</li> <li>C) They are easier to configure</li> <li>D) They consume fewer resources</li> </ul> <p>Answer: A) They provide a scalable and distributed environment for processing large datasets</p>"},{"location":"Synapse-ADF/Q%26A.html#67-how-can-you-integrate-synapse-analytics-with-other-azure-services-like-power-bi-and-azure-ml","title":"67. How can you integrate Synapse Analytics with other Azure services like Power BI and Azure ML?","text":"<ul> <li>A) By using Synapse Studio connectors and integration features</li> <li>B) By using SQL triggers</li> <li>C) By creating separate databases</li> <li>D) By encrypting the data</li> </ul> <p>Answer: A) By using Synapse Studio connectors and integration features</p>"},{"location":"Synapse-ADF/Q%26A.html#68-what-method-can-be-used-to-ensure-data-quality-in-synapse-analytics-before-analysis","title":"68. What method can be used to ensure data quality in Synapse Analytics before analysis?","text":"<ul> <li>A) Using Data Flows for data cleansing and transformation</li> <li>B) Using SQL scripts</li> <li>C) By increasing database size</li> <li>D) By storing data in Blob Storage</li> </ul> <p>Answer: A) Using Data Flows for data cleansing and transformation</p>"},{"location":"Synapse-ADF/Q%26A.html#69-how-can-you-implement-a-data-lakehouse-architecture-using-synapse-analytics","title":"69. How can you implement a data lakehouse architecture using Synapse Analytics?","text":"<ul> <li>A) By combining Synapse SQL pools with Azure Data Lake Storage and Synapse Pipelines</li> <li>B) By using SQL triggers</li> <li>C) By creating new databases</li> <li>D) Using Azure Functions</li> </ul> <p>Answer: A) By combining Synapse SQL pools with Azure Data Lake Storage and Synapse Pipelines</p>"},{"location":"Synapse-ADF/Q%26A.html#70-what-is-the-role-of-synapse-studio-in-a-data-analytics-workflow","title":"70. What is the role of Synapse Studio in a data analytics workflow?","text":"<ul> <li>A) It provides an integrated workspace for data exploration, preparation, management, and visualization</li> <li>B) It monitors network configuration</li> <li>C) It configures database encryption</li> <li>D) It manages SQL triggers</li> </ul> <p>Answer: A) It provides an integrated workspace for data exploration, preparation, management, and visualization</p>"},{"location":"Synapse-ADF/Q%26A.html#71-how-can-a-healthcare-company-use-synapse-analytics-to-support-population-health-management","title":"71. How can a healthcare company use Synapse Analytics to support population health management?","text":"<ul> <li>A) By integrating with electronic health records (EHR) and using machine learning models for predictive analytics</li> <li>B) By exporting data to Excel</li> <li>C) Using SQL triggers</li> <li>D) By creating new databases</li> </ul> <p>Answer: A) By integrating with electronic health records (EHR) and using machine learning models for predictive analytics</p>"},{"location":"Synapse-ADF/Q%26A.html#72-how-can-synapse-analytics-help-in-real-time-customer-sentiment-analysis-for-a-retail-company","title":"72. How can Synapse Analytics help in real-time customer sentiment analysis for a retail company?","text":"<ul> <li>A) By using Spark Streaming with Synapse Pipelines to analyze social media and customer feedback data</li> <li>B) By using SQL scripts</li> <li>C) By creating separate databases</li> <li>D) By storing data in Blob Storage</li> </ul> <p>Answer: A) By using Spark Streaming with Synapse Pipelines to analyze social media and customer feedback data</p>"},{"location":"Synapse-ADF/Q%26A.html#73-what-is-the-best-approach-to-handle-data-archiving-and-retention-in-synapse-analytics","title":"73. What is the best approach to handle data archiving and retention in Synapse Analytics?","text":"<ul> <li>A) Implementing tiered storage options and lifecycle policies</li> <li>B) By using SQL triggers</li> <li>C) By increasing database size</li> <li>D) By creating new databases</li> </ul> <p>Answer: A) Implementing tiered storage options and lifecycle policies</p>"},{"location":"Synapse-ADF/Q%26A.html#74-how-can-you-secure-synapse-analytics-against-unauthorized-access-and-data-breaches","title":"74. How can you secure Synapse Analytics against unauthorized access and data breaches?","text":"<ul> <li>A) By implementing Azure Active Directory integration and role-based access control (RBAC)</li> <li>B) By using SQL scripts</li> <li>C) By creating separate databases</li> <li>D) By using Azure Logic Apps</li> </ul> <p>Answer: A) By implementing Azure Active Directory integration and role-based access control (RBAC)</p>"},{"location":"Synapse-ADF/Q%26A.html#75-how-can-a-logistics-company-optimize-route-planning-and-delivery-schedules-using-synapse-analytics","title":"75. How can a logistics company optimize route planning and delivery schedules using Synapse Analytics?","text":"<ul> <li>A) By integrating with Azure Machine Learning to develop predictive models</li> <li>B) By exporting data to a third-party tool</li> <li>C) Using SQL triggers</li> <li>D) By creating separate databases</li> </ul> <p>Answer: A) By integrating with Azure Machine Learning to develop predictive models</p>"},{"location":"Synapse-ADF/Q%26A.html#76-what-feature-in-synapse-analytics-helps-in-managing-data-schema-changes-and-version-control","title":"76. What feature in Synapse Analytics helps in managing data schema changes and version control?","text":"<ul> <li>A) Schema management tools in Synapse Studio</li> <li>B) Using SQL triggers</li> <li>C) By creating new databases</li> <li>D) Using Power Automate</li> </ul> <p>Answer: A) Schema management tools in Synapse Studio</p>"},{"location":"Synapse-ADF/Q%26A.html#77-how-can-you-perform-sentiment-analysis-on-customer-reviews-stored-in-synapse-analytics","title":"77. How can you perform sentiment analysis on customer reviews stored in Synapse Analytics?","text":"<ul> <li>A) By using Azure Cognitive Services text analytics integrated with Synapse Pipelines</li> <li>B) By exporting data to Excel</li> <li>C) Using SQL scripts</li> <li>D) By creating separate databases</li> </ul> <p>Answer: A) By using Azure Cognitive Services text analytics integrated with Synapse Pipelines</p>"},{"location":"Synapse-ADF/Q%26A.html#78-how-can-synapse-analytics-support-large-scale-data-migration-from-on-premises-systems","title":"78. How can Synapse Analytics support large-scale data migration from on-premises systems?","text":"<ul> <li>A) By using Azure Data Migration Service and Synapse Pipelines</li> <li>B) By using SQL triggers</li> <li>C) By creating new databases</li> <li>D) By using Azure Functions</li> </ul> <p>Answer: A) By using Azure Data Migration Service and Synapse Pipelines</p>"},{"location":"Synapse-ADF/Q%26A.html#79-how-can-an-e-commerce-company-personalize-customer-experiences-using-synapse-analytics","title":"79. How can an e-commerce company personalize customer experiences using Synapse Analytics?","text":"<ul> <li>A) By leveraging Synapse SQL and machine learning models to analyze customer behavior and preferences</li> <li>B) By exporting data to a third-party tool</li> <li>C) Using SQL scripts</li> <li>D) By creating separate databases</li> </ul> <p>Answer: A) By leveraging Synapse SQL and machine learning models to analyze customer behavior and preferences</p>"},{"location":"Synapse-ADF/Q%26A.html#80-what-is-the-role-of-data-flows-in-synapse-analytics","title":"80. What is the role of Data Flows in Synapse Analytics?","text":"<ul> <li>A) To provide a visual interface for designing data transformation logic</li> <li>B) To manage SQL triggers</li> <li>C) To configure database encryption</li> <li>D) To monitor network configuration</li> </ul> <p>Answer: A) To provide a visual interface for designing data transformation logic</p>"},{"location":"Synapse-ADF/Q%26A.html#81-how-can-a-company-use-synapse-analytics-to-implement-a-single-source-of-truth-for-their-data","title":"81. How can a company use Synapse Analytics to implement a single source of truth for their data?","text":"<ul> <li>A) By centralizing data from various sources into a Synapse data warehouse and applying data governance practices</li> <li>B) By using SQL triggers</li> <li>C) By creating separate databases</li> <li>D) By exporting data to Excel</li> </ul> <p>Answer: A) By centralizing data from various sources into a Synapse data warehouse and applying data governance practices</p>"},{"location":"Synapse-ADF/Q%26A.html#82-what-is-the-purpose-of-integrating-synapse-analytics-with-azure-purview","title":"82. What is the purpose of integrating Synapse Analytics with Azure Purview?","text":"<ul> <li>A) To enhance data cataloging, governance, and lineage tracking</li> <li>B) To manage SQL triggers</li> <li>C) To configure database encryption</li> <li>D) To monitor network configuration</li> </ul> <p>Answer: A) To enhance data cataloging, governance, and lineage tracking</p>"},{"location":"Synapse-ADF/Q%26A.html#83-how-can-synapse-analytics-help-a-media-company-analyze-viewer-engagement-data","title":"83. How can Synapse Analytics help a media company analyze viewer engagement data?","text":"<ul> <li>A) By using Synapse SQL and Spark pools to process and analyze large volumes of viewer data</li> <li>B) By exporting data to a third-party tool</li> <li>C) Using SQL triggers</li> <li>D) By creating separate databases</li> </ul> <p>Answer: A) By using Synapse SQL and Spark pools to process and analyze large volumes of viewer data</p>"},{"location":"Synapse-ADF/Q%26A.html#84-what-is-the-advantage-of-using-dedicated-sql-pools-over-serverless-sql-pools-in-synapse-analytics","title":"84. What is the advantage of using dedicated SQL pools over serverless SQL pools in Synapse Analytics?","text":"<ul> <li>A) Dedicated SQL pools provide better performance for high concurrency and complex queries</li> <li>B) Serverless SQL pools offer better security features</li> <li>C) Dedicated SQL pools consume fewer resources</li> <li>D) Serverless SQL pools are easier to configure</li> </ul> <p>Answer: A) Dedicated SQL pools provide better performance for high concurrency and complex queries</p>"},{"location":"Synapse-ADF/Q%26A.html#85-how-can-a-financial-institution-detect-anomalies-in-transaction-data-using-synapse-analytics","title":"85. How can a financial institution detect anomalies in transaction data using Synapse Analytics?","text":"<ul> <li>A) By integrating with Azure Machine Learning for anomaly detection models</li> <li>B) By exporting data to a third-party tool</li> <li>C) Using SQL triggers</li> <li>D) By creating separate databases</li> </ul> <p>Answer: A) By integrating with Azure Machine Learning for anomaly detection models</p>"},{"location":"Synapse-ADF/Q%26A.html#86-what-is-the-best-practice-for-loading-historical-data-into-synapse-analytics-for-analysis","title":"86. What is the best practice for loading historical data into Synapse Analytics for analysis?","text":"<ul> <li>A) Using PolyBase or COPY statement to load large volumes of data efficiently</li> <li>B) By using SQL scripts</li> <li>C) By creating new databases</li> <li>D) By encrypting the data</li> </ul> <p>Answer: A) Using PolyBase or COPY statement to load large volumes of data efficiently</p>"},{"location":"Synapse-ADF/Q%26A.html#87-how-can-synapse-analytics-help-in-optimizing-supply-chain-operations-for-a-manufacturing-company","title":"87. How can Synapse Analytics help in optimizing supply chain operations for a manufacturing company?","text":"<ul> <li>A) By analyzing production and logistics data using Synapse SQL and machine learning models</li> <li>B) By exporting data to Excel</li> <li>C) Using SQL scripts</li> <li>D) By creating separate databases</li> </ul> <p>Answer: A) By analyzing production and logistics data using Synapse SQL and machine learning models</p>"},{"location":"Synapse-ADF/Q%26A.html#88-how-can-you-automate-the-backup-and-recovery-of-synapse-analytics-data","title":"88. How can you automate the backup and recovery of Synapse Analytics data?","text":"<ul> <li>A) By using Azure Backup and Recovery solutions</li> <li>B) Using SQL scripts</li> <li>C) By creating new databases</li> <li>D) Using Power Automate</li> </ul> <p>Answer: A) By using Azure Backup and Recovery solutions</p>"},{"location":"Synapse-ADF/Q%26A.html#89-how-can-a-company-ensure-their-synapse-analytics-data-warehouse-meets-compliance-requirements","title":"89. How can a company ensure their Synapse Analytics data warehouse meets compliance requirements?","text":"<ul> <li>A) By implementing data encryption, access control, and audit logging</li> <li>B) By using SQL triggers</li> <li>C) By creating separate databases</li> <li>D) By exporting data to a third-party tool</li> </ul> <p>Answer: A) By implementing data encryption, access control, and audit logging</p>"},{"location":"Synapse-ADF/Q%26A.html#90-how-can-synapse-analytics-be-used-to-support-customer-segmentation-and-targeting-for-a-marketing-campaign","title":"90. How can Synapse Analytics be used to support customer segmentation and targeting for a marketing campaign?","text":"<ul> <li>A) By using Synapse SQL to analyze customer data and identify segments based on behavior and demographics</li> <li>B) By exporting data to Excel</li> <li>C) Using SQL scripts</li> <li>D) By creating separate databases</li> </ul> <p>Answer: A) By using Synapse SQL to analyze customer data and identify segments based on behavior and demographics</p>"},{"location":"Synapse-ADF/Q%26A.html#91-what-is-the-benefit-of-using-synapse-studio-for-collaborative-data-analytics-projects","title":"91. What is the benefit of using Synapse Studio for collaborative data analytics projects?","text":"<ul> <li>A) It provides a unified workspace for multiple users to collaborate on data preparation, management, and analysis</li> <li>B) It monitors network configuration</li> <li>C) It manages SQL triggers</li> <li>D) It configures database encryption</li> </ul> <p>Answer: A) It provides a unified workspace for multiple users to collaborate on data preparation, management, and analysis</p>"},{"location":"Synapse-ADF/Q%26A.html#92-how-can-synapse-analytics-be-integrated-with-third-party-bi-tools-for-advanced-reporting","title":"92. How can Synapse Analytics be integrated with third-party BI tools for advanced reporting?","text":"<ul> <li>A) By using data connectors and APIs to link Synapse data with BI tools like Tableau or Qlik</li> <li>B) By exporting data to Excel</li> <li>C) Using SQL scripts</li> <li>D) By creating separate databases</li> </ul> <p>Answer: A) By using data connectors and APIs to link Synapse data with BI tools like Tableau or Qlik</p>"},{"location":"Synapse-ADF/Q%26A.html#93-how-can-a-retail-company-use-synapse-analytics-to-optimize-inventory-management","title":"93. How can a retail company use Synapse Analytics to optimize inventory management?","text":"<ul> <li>A) By analyzing sales and inventory data to predict demand and optimize stock levels</li> <li>B) By exporting data to a third-party tool</li> <li>C) Using SQL triggers</li> <li>D) By creating separate databases</li> </ul> <p>Answer: A) By analyzing sales and inventory data to predict demand and optimize stock levels</p>"},{"location":"Synapse-ADF/Q%26A.html#94-what-feature-in-synapse-analytics-allows-you-to-schedule-and-automate-data-processing-tasks","title":"94. What feature in Synapse Analytics allows you to schedule and automate data processing tasks?","text":"<ul> <li>A) Synapse Pipelines</li> <li>B) SQL Server Management Studio</li> <li>C) Azure Monitor</li> <li>D) Power Automate</li> </ul> <p>Answer: A) Synapse Pipelines</p>"},{"location":"Synapse-ADF/Q%26A.html#95-how-can-a-company-use-synapse-analytics-to-perform-cross-regional-data-analysis","title":"95. How can a company use Synapse Analytics to perform cross-regional data analysis?","text":"<ul> <li>A) By setting up data replication and using Synapse SQL to query data from different regions</li> <li>B) By using SQL triggers</li> <li>C) By creating separate databases</li> <li>D) By exporting data to Excel</li> </ul> <p>Answer: A) By setting up data replication and using Synapse SQL to query data from different regions</p>"},{"location":"Synapse-ADF/Q%26A.html#96-how-can-you-improve-query-performance-in-synapse-analytics-when-dealing-with-large-datasets","title":"96. How can you improve query performance in Synapse Analytics when dealing with large datasets?","text":"<ul> <li>A) By optimizing distribution keys and creating columnstore indexes</li> <li>B) By increasing database size</li> <li>C) Using SQL triggers</li> <li>D) By creating separate databases</li> </ul> <p>Answer: A) By optimizing distribution keys and creating columnstore indexes</p>"},{"location":"Synapse-ADF/Q%26A.html#97-how-can-synapse-analytics-support-predictive-maintenance-for-industrial-equipment","title":"97. How can Synapse Analytics support predictive maintenance for industrial equipment?","text":"<ul> <li>A) By integrating with IoT data sources and using machine learning models for predictive analytics</li> <li>B) By exporting data to Excel</li> <li>C) Using SQL scripts</li> <li>D) By creating separate databases</li> </ul> <p>Answer: A) By integrating with IoT data sources and using machine learning models for predictive analytics</p>"},{"location":"Synapse-ADF/Q%26A.html#98-what-is-the-best-practice-for-managing-large-scale-data-transformations-in-synapse-analytics","title":"98. What is the best practice for managing large-scale data transformations in Synapse Analytics?","text":"<ul> <li>A) Using Data Flows and Spark pools for efficient data processing</li> <li>B) By using SQL triggers</li> <li>C) By increasing database size</li> <li>D) By creating new databases</li> </ul> <p>Answer: A) Using Data Flows and Spark pools for efficient data processing</p>"},{"location":"Synapse-ADF/Q%26A.html#99-how-can-synapse-analytics-help-in-developing-a-360-degree-view-of-the-customer","title":"99. How can Synapse Analytics help in developing a 360-degree view of the customer?","text":"<ul> <li>A) By consolidating data from various sources and using analytics to provide insights into customer behavior</li> <li>B) By exporting data to a third-party tool</li> <li>C) Using SQL scripts</li> <li>D) By creating separate databases</li> </ul> <p>Answer: A) By consolidating data from various sources and using analytics to provide insights into customer behavior</p>"},{"location":"Synapse-ADF/Q%26A.html#100-how-can-synapse-analytics-be-used-to-support-real-time-business-intelligence-for-an-e-commerce-platform","title":"100. How can Synapse Analytics be used to support real-time business intelligence for an e-commerce platform?","text":"<ul> <li>A) By integrating with streaming data sources and using serverless SQL pools for real-time querying</li> <li>B) By using SQL triggers</li> <li>C) By creating separate databases</li> <li>D) By exporting data to Excel</li> </ul> <p>Answer: A) By integrating with streaming data sources and using serverless SQL pools for real-time querying</p>"},{"location":"blog/index.html","title":"Blog","text":""}]}